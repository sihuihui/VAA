[
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html",
    "title": "Take-home Exercise 2",
    "section": "",
    "text": "In this take-home exercise, I will be using the data visualisation design principles and best practices learnt in ISSS608 Lesson 1 and 2 to improve on my peer’s visualisations."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#loading-r-packages",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#loading-r-packages",
    "title": "Take-home Exercise 2",
    "section": "2.1 Loading R packages",
    "text": "2.1 Loading R packages\nFor this exercise, we will be using the following packages:\n\ntidyverse : to load the core tidyverse packages, which includes ggplot2 and dplyr.\npatchwork: to create composition of ggplot2 plots using arithmetic operators.\nggrepel: to repel overlapping text labels away from each other.\nggdist: provides stats and geoms for visualising distributions and uncertainty.\nggridges: provides geoms to plot ridgeline plots, which are partially overlapping line plots that create the impression of a mountain range.\nknitr: provides a general-purpose tool for dynamic report generation in R. We will use this to mainly help us generate simple tables.\n\nThe code chunk below uses p_load() of pacman package to check if the abovementioned packages are installed in the computer. If they are, they will be launched in R. Otherwise, pacman will install the relevant packages before launching them.\n\n\nShow the code\npacman::p_load(tidyverse, haven, patchwork, ggrepel, ggdist, ggridges, knitr, hrbrthemes)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#importing-data-into-r",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#importing-data-into-r",
    "title": "Take-home Exercise 2",
    "section": "2.2 Importing Data into R",
    "text": "2.2 Importing Data into R\nFor this exercise, we are using PISA 2022 database’s student questionnaire data file, which is the same dataset as Take-home exercise 1. As we have already filtered out the PISA data from Singapore and saved it in rds file previously in Take-home exercise 1, let us import the rds file using the following code chunk.\n\n\nShow the code\nstu_qqq_SG &lt;- read_rds(\"data/stu_qqq_SG.rds\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#chart-1-comparison-of-scores-amongst-students",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#chart-1-comparison-of-scores-amongst-students",
    "title": "Take-home Exercise 2",
    "section": "4.1 Chart 1: Comparison of Scores Amongst Students",
    "text": "4.1 Chart 1: Comparison of Scores Amongst Students\nThe following charts were from the original submission.\n\n\n\nChart 1 from Original Author\n\n\n\n4.1.1 Critique\nWhat is the message\n\nBefore I critique, I shall attempt to deduce the author’s message for this chart so that we can make recommendations to improve the chart based on the intended message.\nI think the author was trying to find out if there was any difference in student scores between subjects since a notched box plot was plotted for each subject’s scores.\n\nAesthetics\n\nThe title could be more informative. We can also add a subtitle to provide more information on the sample size.\nWe can put the 3 boxplots side by side for easy comparison.\n\nClarity\n\nThe author used notched box plots to give a summary of the minimum, maximum, median and interquartile range of Math, Reading and Science scores for Singapore students. The box plots also indicated “outliers” with those black dots at the end. However, since the message was on the difference in scores and not on the outliers, we can choose to remove these outliers to reduce ink.\nIn the original chart the notches of the box plots were not very obvious. This could be due to the size of the boxplots being very wide, making it hard to see the notches. Also, these charts were placed in different tabs, making it hard for readers to see if there was a difference in scores between subjects. It might be easier for readers to compare if these plots were placed side by side.\nIn the original chart, there was a red dot for the mean scores of each subject. However, there was no text labels so the readers have to try to interpret and gauge the mean value based on the y-axis. It would be more useful if it has text annotation so that readers can easily read off the mean value.\nOne of the pitfalls of box plot is that it does not show the distribution of the data well. It hides multimodality and other features of distributions. Hence, we can also consider alternatives such as violin plot.\n\n\n\n4.1.2 Proposed Sketch\n\nAs having a clear message for the plot was the first step of of data visualisation design process, I shall assume that the author wanted to show if there was any difference in scores between the different subjects (Maths, Reading and Science) among Singapore students.\nIf we want to show difference in scores between subjects, we can consider the following plots:\n\n\n\n\nSuggestions for Chart 1\n\n\nFirstly, for all the options, I proposed to put the scores for all 3 subjects side by side. Also, rather that stitching 3 different plots together (i.e. 1 plot for 1 subject) using patchwork, I suggested to put all three plots together so that they can be on the same axes and easier for comparison.\nThere are pros and cons for the above 3 options:\n\nViolin plot: shows the distribution and summary stats of the data. However, certain parts of it are a bit redundant because half of the violin would already reveal the distribution. So a violin plot might not be that efficient use of space.\nRidge plot: This brings us to ridge plot, which uses “half” of the violin plot and we rotated the “half-violin” to be horizontal, giving us the image of “ridges”. Ridge plot also can show distribution of the data and makes good use of the space. We can add annotations (e.g. lines and texts) to show the median and mean. However, ridge plot might be better for situations where we have medium to high number of groups to represent. In this case, we have less than 5 groups, so we might want to consider other distribution plots.\nRaincloud plot: We can further enhance the ridge plot and make it into a raincloud plot by adding boxplot and dotplot. This will also show us the distribution and summary statistics of the data.\n\nAfter weighing the above pros and cons, I decided to change the box plots to raincloud plot. Below is how I imagined the plot to look like with a title and subtitle:\n\n\n\nFinalised Suggestion for Chart 1\n\n\n\n\n4.1.3 Makeover Design\nWe will first prepare the data by selecting the columns we need (i.e., student ID, Math, Reading and Science scores) then pivot the table longer using pivot_longer() so that we can have all three subjects in 1 chart.\n\n\nShow the code\nstu1 &lt;- stu %&gt;%\n  select(CNTSTUID, MATH, READ, SCI) %&gt;%\n  pivot_longer(!CNTSTUID, names_to = \"Subject\")\n\n\nUsing the data prepared stu1, we will plot the raincloud plot using the following code chunk.\n\n\nShow the code\nggplot(stu1, \n       aes(x = Subject, \n           y = value,\n           color = Subject)) + \n  stat_halfeye(adjust = 0.5,\n               width = 0.5,\n               justification = -0.1, \n               .width = 0, \n               point_color = NA) +\n  geom_boxplot(width = 0.1, outlier.shape = NA) +\n  stat_dots(side = \"left\",\n            justification = 1.1,\n            binwidth = 0.5,\n            dotsize = 2) + \n  coord_flip() + theme_minimal()+\n  stat_summary(fun = median, geom = \"text\", aes(label = paste(\"median:\", round(after_stat(y), 0))),\n               position = position_nudge(x=0.05), vjust=-0.5, size = 2.5, color = \"black\")+\n  stat_summary(fun=mean, geom =\"text\", aes(label = paste(\"mean: \", round(after_stat(y), 0))), position = position_nudge(x = 0.25), vjust = 4, color= \"black\", size = 2.5)+ \n  stat_summary(fun = mean, geom = \"point\", shape = 16, size = 2, color = \"black\",\n               position = position_nudge(x = 0.0)) +\n    stat_summary(fun = min, geom = \"text\", aes(label = paste(round(after_stat(y), 0))),\n               position = position_nudge(y=-0.25), vjust=-0.5, size = 2, color = \"grey40\")+\n  stat_summary(fun = max, geom = \"text\", aes(label = paste(round(after_stat(y), 0))),\n               position = position_nudge(y=-0.25), vjust=-0.5, size = 2, color = \"grey40\")+\n  labs(title = \"Although the scores varied widely for each subject, \\nthe difference in median and mean scores were small.\", subtitle = \"Number of observations: 6606\", caption = \"Data from PISA 2022\") + \n  xlab(\"SUBJECT\") + \n  ylab(\"SCORES\") + \n  theme_ipsum_rc(plot_title_size = 15, plot_title_margin=4, subtitle_size=12, subtitle_margin=4, axis_text_size=8, axis_title_face= \"bold\", plot_margin = margin(10,10,10,10)) + \n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#chart-2-comparison-of-scores-between-gender",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#chart-2-comparison-of-scores-between-gender",
    "title": "Take-home Exercise 2",
    "section": "4.2 Chart 2: Comparison of Scores Between Gender",
    "text": "4.2 Chart 2: Comparison of Scores Between Gender\nThe following charts were from the original submission.\n\n\n\nChart 2 from Original Author\n\n\n\n4.2.1 Critique\nWhat is the message\n\nThe original author seemed to want to see if there are any differences in subject scores between gender.\n\nAesthetics\n\nThe title could be more informative. We can also add a subtitle to provide information on the sample size.\nWe can put the 3 boxplots side by side for easy comparison.\n\nClarity\n\nNotched boxplots were used to display the difference in subject scores between gender.\nSimilar to the first chart, the notches of the box plots were not obvious, possibly due to the size of the boxplots being very wide. Also, these charts were in different tabs, making it hard for readers to see if there was a difference in subject scores between genders. It would be easier for readers to compare if these plots were placed side by side.\nThere was a red dot on each boxplot to indicate the mean scores of each subject and gender but it would be more useful if it has text annotation so that readers can easily read off the mean value.\n\n\n\n4.2.2 Proposed Sketch\nThere are several options to display if there was any difference in scores. I thought we can use either:\n\nOption 1: use ridge plot to plot all the subject scores for both gender plotted on 1 chart,\nOption 2: use ridge plot and box plot (i.e. raincloud plot without the “rain”) to display the scores between genders for each subject in separate charts, then make use of patchwork to combine the 3 charts into 1 for ease of comparison.\n\n\n\n\nSuggestions for Chart 2\n\n\nI suggested ridge plot and put all 6 subject scores (i.e. 3 subjects for each gender) into 1 plot so that we can easily compare differences between gender and also within gender. This chart would easily allow us to know which gender performed well for each subject. However, ridge plot does not have information on the summary statistics so I came up with option 2 (i.e. ridge plot and box plot) since ggridges allow us to have such plots too.\nFor option 2, I separated the charts by subjects to test out if it can display the same message effectively. I realised that having a separate chart for each subject helps us to know which gender does well for a particular subject, but it does not allow us to have an “overview” which gender performed better overall.\nI did not suggest raincloud plot because if there are 6 subject scores in 1 plot, it might be too messy to have so many dots on the plot.\nThere are pros and cons for each option.\nFor option 1, having the scores for all subjects and gender in 1 plot means that we will have 6 distributions in 1 plot. It might be easy to pick out any obvious differences. However it might be too overwhelming since there are 6 “ridges” to look and compare at 1 shot, or some readers might not know where to start. So if we use option 1, we would have to have clear annotations and titles to guide the readers.\nFor option 2, it splits the plots by subject so that we can see if there are any differences in scores between gender for each subject. However, we might not be able easily compare differences in scores between subject for each gender (e.g. to find out if female students performed better in Reading as opposed to Maths).\nAfter weighing the above pros and cons, I decided to change the box plots to combine option 1 and 2 ideas. So we will plot ridge plots and box plots for gender and subjects into 1 plot. This is how it looks like:\n\n\n\nFinalised Suggestion for Chart 2\n\n\n\n\n4.2.3 Makeover Design\nLet us prepare the data for this chart. We will first select the columns that we want, then pivot_longer() so that we can plot all the subjects scores for both gender into 1 plot. We then concatenated the Gender and Subject columns into 1 column so that we can use this new column Gen_Sub to lcreate a chart for each Gender’s Subject.\n\n\nShow the code\nstu$CNTSTUID &lt;- as.factor(stu$CNTSTUID)\n\nstu2 &lt;- stu %&gt;%\n  select(CNTSTUID, Gender, MATH, READ, SCI) %&gt;%\n  pivot_longer(cols = MATH:SCI) %&gt;%\n  rename(\"Subject\" = \"name\",\n         \"Scores\" = \"value\")\n\nstu3 &lt;- stu2 %&gt;%\n  mutate(GENDER = Gender) %&gt;%\n  unite(Gen_Sub, c(Gender, Subject))\n\nstu3$GENDER &lt;- factor(stu3$GENDER, levels = c(\"Male\", \"Female\"))\n\n\nThe following code chunk plots out the\n\n\nShow the code\nggplot(stu3, \n       aes(x = Scores, \n           y = Gen_Sub, \n           color = GENDER)) + \n  stat_halfeye(expand = TRUE) +\n  stat_summary(fun = median, geom = \"text\", aes(label = paste(\"median:\", round(after_stat(x), 0))),\n               position = position_nudge(y=0.15), vjust=-0.5, size = 2.3, color = \"black\")+\n  stat_summary(fun=mean, geom =\"text\", aes(label = paste(\"mean: \", round(after_stat(x), 0))), \n               position = position_nudge(y =-0.2), vjust = 0.5, color= \"black\", size = 2.3)+ \n  stat_summary(fun = mean, geom = \"point\", shape = 16, size = 2, color = \"black\",\n               position = position_nudge(x = 0.0)) +\n  stat_summary(fun = min, geom = \"text\", aes(label = paste(round(after_stat(x), 0))),\n               position = position_nudge(y=-0.25), vjust=-0.5, size = 2, color = \"grey60\")+\n  stat_summary(fun = max, geom = \"text\", aes(label = paste(round(after_stat(x), 0))),\n               position = position_nudge(y=-0.25), vjust=-0.5, size = 2, color = \"grey60\")+\n  labs(title = \"Scores varied widely for each gender across subjects.\\nTop performers for each subject came from male students.\", subtitle = \"Number of observations: 6606\", caption = \"Data from PISA 2022\") + \n  ylab(\"GENDER & SUBJECT\") + \n  xlab(\"SCORES\") + \n  theme_ipsum_rc(plot_title_size = 15, plot_title_margin=4, subtitle_size=10, subtitle_margin=4, axis_text_size=8, axis_title_face= \"bold\", grid_col= \"grey\", plot_margin = margin(10,10,10,10))  +\n    scale_color_manual(breaks = c(\"Male\", \"Female\"),\n                    values=c(\"blue\", \"violetred1\"))"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#chart-3-comparison-of-scores-between-schools",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#chart-3-comparison-of-scores-between-schools",
    "title": "Take-home Exercise 2",
    "section": "4.3 Chart 3: Comparison of Scores Between Schools",
    "text": "4.3 Chart 3: Comparison of Scores Between Schools\nThe following charts were from the original submission.\n\n\n\nChart 3 by Original Author\n\n\n\n4.3.1 Critique\nWhat is the message\n\nThe original author seemed to want to see if there are any differences in subject scores between schools.\n\nAesthetics\n\nSimilarly, the title could be more informative. We can also add a subtitle to provide information on the number of schools.\nWe can put the 3 boxplots side by side for easy comparison.\n\nClarity\n\nIn the original work, the students’ scores were averaged to determine the school’s score.\nBased on the original author’s interpretation, it seemed that if a school’s score was within the box plot and not an outlier, the author interpreted the school as not having a wide disparity between students.\nHowever, we cannot interpret this way due to how the school’s score was being calculated. Since the school score was derived by averaging the students’ scores and arithmetic averages were affected by extreme values, a school with non-outlier scores does not mean that all its students performed well. This is because the school might have some students with very high scores to make up for the under-performing students.\nAs such, when remaking the chart, I would find out the difference between the top score and bottom score for each school. By doing so we get the range of the students’ scores for each school. Then we will compare each school’s range to determine if the school is doing “well”. A wider range means the school’s students had a wider disparity which could warrant further exploration and investigation.\nIn addition, the author chose notched box plots to compare the schools’ scores for different subjects. These plots were also in different tabs, making it hard to compare across subjects.\nThere was a red dot for the average school score but it would be more useful if it has text annotation so that readers can easily read off the value.\n\n\n\n4.3.2 Proposed Sketch\nSince the message that I wanted this chart to show was each school’s range of scores for each subject, I have the following suggestions.\n\n\n\nSuggestions for Chart 3\n\n\n\nFor this chart, I think the tricky bit is that there are many schools (~164 schools) to plot out. Initially, I wanted to do a dumbbell plot (option 1) for each school. So each dot would show to top score and bottom score for each school and the line would show the difference between the top scores and bottom scores. But I realised that it would be quite messy and too much information (i.e., top score, lowest score, difference in scores) in the chart when there are more than 160 schools to plot out.\nHence, I came up with Options 2 and 3. In options 2 and 3, we can use the height of the bar or ‘lollipop’ would show the difference in the top and bottom scores for each school. We can also add a threshold so that we know which schools have a wide range and which schools have students performing at similar levels. For this exercise, we set an arbitrary threshold of 250.\nThe difference between barplot and lollipop plot is that lollipop plot uses less ink so people can focus on the height of the “dot”.\n\nAfter considering the various strengths and weaknesses of the various plots, I decided to remake the original chart into a lollipop plot and this is how I planned for it to look like:\n\n\n\nFinalised Suggestion for Chart 3\n\n\n\n\n4.3.3 Makeover Design\nLet us first prepare the data for each subject. When preparing the data, I removed the “70200” in the school IDs (i.e. CNTSCHID) because it is present in every school IDs and they would clutter and take up extra space when we plot the school IDs on the axis.\n\n\nShow the code\nstu_math_min &lt;- stu %&gt;%\n  group_by(CNTSCHID) %&gt;%\n  summarise_at(vars(MATH),\n               list(min_math = min))\n\nstu_math_max &lt;- stu %&gt;%\n  group_by(CNTSCHID) %&gt;%\n  summarise_at(vars(MATH),\n               list(max_math = max))\n\nstu_math &lt;- full_join(stu_math_max, stu_math_min,\n             by = \"CNTSCHID\")\n\nstu_math$diff &lt;- stu_math$max_math - stu_math$min_math\n\nstu_math$CNTSCHID &lt;- substr(stu_math$CNTSCHID, 6, 8)\n\nstu_math &lt;- stu_math %&gt;% \n  mutate(mycolor = ifelse(diff&gt;250, \"red\", \"blue\"))%&gt;%\n   arrange(diff) %&gt;%    \n  mutate(CNTSCHID=factor(CNTSCHID, levels=CNTSCHID)) \n\nstu_read_min &lt;- stu %&gt;%\n  group_by(CNTSCHID) %&gt;%\n  summarise_at(vars(READ),\n               list(min_read = min))\n\nstu_read_max &lt;- stu %&gt;%\n  group_by(CNTSCHID) %&gt;%\n  summarise_at(vars(READ),\n               list(max_read = max))\n\nstu_read &lt;- full_join(stu_read_max, stu_read_min,\n             by = \"CNTSCHID\")\n\nstu_read$diff &lt;- stu_read$max_read - stu_read$min_read\n\nstu_read$CNTSCHID &lt;- substr(stu_read$CNTSCHID, 6, 8)\n\nstu_read &lt;- stu_read %&gt;% \n  mutate(mycolor = ifelse(diff&gt;250, \"red\", \"blue\"))%&gt;%\n   arrange(diff) %&gt;%    \n  mutate(CNTSCHID=factor(CNTSCHID, levels=CNTSCHID)) \n\nstu_sci_min &lt;- stu %&gt;%\n  group_by(CNTSCHID) %&gt;%\n  summarise_at(vars(SCI),\n               list(min_sci = min))\n\nstu_sci_max &lt;- stu %&gt;%\n  group_by(CNTSCHID) %&gt;%\n  summarise_at(vars(SCI),\n               list(max_sci = max))\n\nstu_sci &lt;- full_join(stu_sci_max, stu_sci_min,\n             by = \"CNTSCHID\")\n\nstu_sci$diff &lt;- stu_sci$max_sci - stu_sci$min_sci\n\nstu_sci$CNTSCHID &lt;- substr(stu_sci$CNTSCHID, 6, 8)\n\nstu_sci &lt;- stu_sci %&gt;% \n  mutate(mycolor = ifelse(diff&gt;250, \"red\", \"blue\"))%&gt;%\n   arrange(diff) %&gt;%    \n  mutate(CNTSCHID=factor(CNTSCHID, levels=CNTSCHID))  \n\n\nThe following code chunk plots out the individual charts for each subject. We will save each subject’s chart as a variable so that we can stitch them together later.\nWhile plotting these charts out,\n\nI rotated the charts using coord_flip() because I realised that it is easier to compare across subjects for each schools when they are in a vertical format.\nI added an abline using geom_hline to make the threshold of 250 points obvious.\nI also removed the plot background colour and grid lines because of the lines in the lollipop charts.\n\n\n\nShow the code\np1 &lt;- ggplot(stu_math, aes(x=CNTSCHID, y = diff)) +\n  geom_segment(aes(x=CNTSCHID, xend=CNTSCHID, y = 250, yend=diff), color = \"grey\") +\n  geom_point(color=stu_math$mycolor, size = 2) +\n  geom_hline(yintercept = 250, color=\"orange\",linewidth = 0.6)+\n  xlab(\"SchoolID\") +\n  ylab(\"Difference in Math Scores\") + coord_flip()+\n  theme(panel.background = element_rect(fill = \"white\", colour = \"grey80\"))\n\n\np2 &lt;- ggplot(stu_read, aes(x=CNTSCHID, y = diff)) +\n  geom_segment(aes(x=CNTSCHID, xend=CNTSCHID, y = 250, yend=diff), color = \"grey\") +\n  geom_point(color=stu_read$mycolor, size = 2) +\n  geom_hline(yintercept = 250, color=\"orange\",linewidth = 0.6)+\n  xlab(\"SchoolID\") +\n  ylab(\"Difference in Reading Scores\") + coord_flip() +\n  theme(panel.background = element_rect(fill = \"white\", colour = \"grey80\"))\n\np3 &lt;- ggplot(stu_sci, aes(x=CNTSCHID, y = diff)) +\n  geom_segment(aes(x=CNTSCHID, xend=CNTSCHID, y = 250, yend=diff), color = \"grey\") +\n  geom_point(color=stu_sci$mycolor, size = 2) +\n  geom_hline(yintercept = 250, color=\"orange\",linewidth = 0.6)+\n  xlab(\"SchoolID\") +\n  ylab(\"Difference in Science Scores\") + coord_flip() + \n  theme(panel.background = element_rect(fill = \"white\", colour = \"grey80\"))\n\n\nWe then stitch the individual charts together using patchwork and added title, subtitle and caption and a theme using plot_annotation().\n\n\nShow the code\npatch1 &lt;- p1 + p2 + p3 \n\npatch1 + plot_annotation(\n  title = \"Most of the schools had a wide disparity between their top scorers and \\nbottom scorers, exceeding the threshold of 250 points.\",\n  subtitle = \"164 Schools participated in study.\",\n  caption = \"Data from PISA 2022\", theme = theme_ipsum_rc())"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#chart-4-comparison-of-scores-and-students-socioeconomic-status",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#chart-4-comparison-of-scores-and-students-socioeconomic-status",
    "title": "Take-home Exercise 2",
    "section": "4.4 Chart 4: Comparison of Scores and Students’ Socioeconomic Status",
    "text": "4.4 Chart 4: Comparison of Scores and Students’ Socioeconomic Status\n\n4.4.1 Critique\nThe following charts were from the original submission.\n\n\n\nChart 4 by Original Author\n\n\nWhat is the message\n\nThe original author seemed to want to see if there are any relationship between subject scores and socioeconomic status (ESCS).\n\nAesthetics\n\nSimilarly, the title could be more informative. We can also add a subtitle to provide information on the number of observations, or additional information about ESCS since it is an index.\nInstead of putting the charts in different tabs, we can put the 3 charts side by side for easy comparison.\n\nClarity\n\nThe author used scatter plot to show the relationship between socioeconomic status (ESCS) and subject scores. A line of best fit was also added to show the relationship between ESCS and subject scores, which was useful to illustrate the relationship.\nHowever, because there are many data points (we have more than 6000 observations), the scatter plots were too dense to interpret.\n\n\n\n4.4.2 Proposed Sketch\nAs such, we suggest the following options:\nOption 1: Hexbin plot to overcome the large number of data points by binning them. A hexbin plot can show the density of data points (using the colour gradient) and help to identify patterns and outliers in the data.\nOption 2: Ridge plot by binning the scores into “Proficiency levels”. Binning the scores makes it easier to identify data clusters and depict patterns. If the higher (i.e. better) proficiency levels tend to fall within the higher ESCS scores, then there could be a relationship between scores and socioeconomic status.\n\n\n\nSuggestions for Chart 4\n\n\nI decided to bind the scores into proficiency levels and plot ridgeplots to compare the students’ proficiency levels across ESCS. The ridgeplots could give a clear overview if ESCS and scores are related by checking if the “ridges” are skewed in a certain direction. This is how I imagined the revamped chart would look like:\n\n\n\nFinalised Suggestion for Chart 4\n\n\n\n\n4.4.3 Makeover Design\nFirst, we will prepare the data by:\n\nbinning the subject scores into various proficiency levels; and\ndefining the factor levels for the subject’s proficiency levels.\n\n\n\nShow the code\nstu_math_b &lt;- stu %&gt;%\n  select(CNTSTUID, MATH, ESCS) \n\nstu_math_b$MATHLevel[stu_math_b$MATH &gt;669.30] &lt;- \"6\"\nstu_math_b$MATHLevel[stu_math_b$MATH &gt; 606.99 & stu_math_b$MATH &lt;= 669.30] &lt;- \"5\"\nstu_math_b$MATHLevel[stu_math_b$MATH &gt; 544.68 & stu_math_b$MATH &lt;= 606.99] &lt;- \"4\"\nstu_math_b$MATHLevel[stu_math_b$MATH &gt; 482.38 & stu_math_b$MATH &lt;= 544.68] &lt;- \"3\"\nstu_math_b$MATHLevel[stu_math_b$MATH &gt; 420.38 & stu_math_b$MATH &lt;= 482.38] &lt;- \"2\"\nstu_math_b$MATHLevel[stu_math_b$MATH &gt; 357.77 & stu_math_b$MATH &lt;= 420.38] &lt;- \"1a\"\nstu_math_b$MATHLevel[stu_math_b$MATH &gt; 295.47 & stu_math_b$MATH &lt;= 357.77] &lt;- \"1b\"\nstu_math_b$MATHLevel[stu_math_b$MATH &gt; 233.17 & stu_math_b$MATH &lt;= 295.47] &lt;- \"1c\"\n\nstu_math_b$MATHLevel&lt;- factor(stu_math_b$MATHLevel, levels=c('1c', '1b', '1a', '2', '3', '4', '5', '6'))\n\nstu_read_b &lt;- stu %&gt;%\n  select(CNTSTUID, READ, ESCS) \n\nstu_read_b$READLevel[stu_read_b$READ &gt; 698.32] &lt;- \"6\"\nstu_read_b$READLevel[stu_read_b$READ &gt; 625.61 & stu_read_b$READ &lt;= 698.32] &lt;- \"5\"\nstu_read_b$READLevel[stu_read_b$READ &gt; 552.89 & stu_read_b$READ &lt;= 625.61] &lt;- \"4\"\nstu_read_b$READLevel[stu_read_b$READ &gt; 480.18 & stu_read_b$READ &lt;= 552.89] &lt;- \"3\"\nstu_read_b$READLevel[stu_read_b$READ &gt; 407.47 & stu_read_b$READ &lt;= 480.18] &lt;- \"2\"\nstu_read_b$READLevel[stu_read_b$READ &gt; 334.75 & stu_read_b$READ &lt;= 407.47] &lt;- \"1a\"\nstu_read_b$READLevel[stu_read_b$READ &gt; 262.04 & stu_read_b$READ &lt;= 334.75] &lt;- \"1b\"\nstu_read_b$READLevel[stu_read_b$READ &gt; 189.33 & stu_read_b$READ &lt;= 262.04] &lt;- \"1c\"\nstu_read_b$READLevel[stu_read_b$READ &lt; 189.33] &lt;- \"0\"\n\nstu_read_b$READLevel&lt;- factor(stu_read_b$READLevel, levels=c('0', '1c', '1b', '1a', '2', '3', '4', '5', '6'))\n\nstu_sci_b &lt;- stu %&gt;%\n  select(CNTSTUID, SCI, ESCS) \n\nstu_sci_b$SCILevel[stu_sci_b$SCI &gt; 707.93] &lt;- \"6\"\nstu_sci_b$SCILevel[stu_sci_b$SCI &gt; 633.33 & stu_sci_b$SCI &lt;= 707.93] &lt;- \"5\"\nstu_sci_b$SCILevel[stu_sci_b$SCI &gt; 558.73 & stu_sci_b$SCI &lt;= 633.33] &lt;- \"4\"\nstu_sci_b$SCILevel[stu_sci_b$SCI &gt; 484.14 & stu_sci_b$SCI &lt;= 558.73] &lt;- \"3\"\nstu_sci_b$SCILevel[stu_sci_b$SCI &gt; 409.54 & stu_sci_b$SCI &lt;= 484.14] &lt;- \"2\"\nstu_sci_b$SCILevel[stu_sci_b$SCI &gt; 334.94 & stu_sci_b$SCI &lt;= 409.54] &lt;- \"1a\"\nstu_sci_b$SCILevel[stu_sci_b$SCI &gt; 260.54 & stu_sci_b$SCI &lt;= 334.94] &lt;- \"1b\"\nstu_sci_b$SCILevel[stu_sci_b$SCI &gt; 185.94 & stu_sci_b$SCI &lt;= 260.54] &lt;- \"1c\"\nstu_sci_b$SCILevel[stu_sci_b$SCI &lt; 185.94] &lt;- \"0\"\n\nstu_sci_b$SCILevel&lt;- factor(stu_sci_b$SCILevel, levels=c('0', '1c', '1b', '1a', '2', '3', '4', '5', '6'))\n\n\nWith the prepared data, we will plot 1 chart for each subject and save each chart as a variable. When working on the chart, I realised that the stat_halfeye() from ggdist (i.e., a raincloud plot without the ‘rain’) is better than ridge chart from ggridge because it shows density with a distribution curve and interval with a boxplot.\n\n\nShow the code\ne1 &lt;- ggplot(stu_math_b, \n       aes(x = ESCS, \n           y = MATHLevel,\n           color = MATHLevel)) + \nstat_halfeye(expand = TRUE) +\n  stat_summary(fun = median, geom = \"text\", aes(label = paste(\"median:\", round(after_stat(x), 1))),\n               position = position_nudge(y=0.15), vjust=-0.5, size = 2.3, color = \"black\")+\n  stat_summary(fun=mean, geom =\"text\", aes(label = paste(\"mean: \", round(after_stat(x), 1))), \n               position = position_nudge(y =-0.2), vjust = 0.5, color= \"black\", size = 2.3)+ \n  stat_summary(fun = mean, geom = \"point\", shape = 16, size = 2, color = \"black\",\n               position = position_nudge(x = 0.0)) +\n  labs(title = \"Maths\")+\n  ylab(\"Proficiency Level\") + \n  xlab(\"ESCS\") + \n  theme_ipsum_rc(plot_margin = margin(4, 4, 4, 4))  \n\n\ne2 &lt;- ggplot(stu_read_b, \n       aes(x = ESCS, \n           y = READLevel,\n           color = READLevel)) + \nstat_halfeye(expand = TRUE) +\n  stat_summary(fun = median, geom = \"text\", aes(label = paste(\"median:\", round(after_stat(x), 1))),\n               position = position_nudge(y=0.15), vjust=-0.5, size = 2.3, color = \"black\")+\n  stat_summary(fun=mean, geom =\"text\", aes(label = paste(\"mean: \", round(after_stat(x), 1))), \n               position = position_nudge(y =-0.2), vjust = 0.5, color= \"black\", size = 2.3)+ \n  stat_summary(fun = mean, geom = \"point\", shape = 16, size = 2, color = \"black\",\n               position = position_nudge(x = 0.0)) +\n  labs(title = \"Reading\")+\n  ylab(\"Proficiency Level\") + \n  xlab(\"ESCS\") + \n  theme_ipsum_rc(plot_margin = margin(4, 4, 4, 4)) \n\n\ne3 &lt;- ggplot(stu_sci_b, \n       aes(x = ESCS, \n           y = SCILevel,\n           color = SCILevel)) + \nstat_halfeye(expand = TRUE) +\n  stat_summary(fun = median, geom = \"text\", aes(label = paste(\"median:\", round(after_stat(x), 1))),\n               position = position_nudge(y=0.15), vjust=-0.5, size = 2.3, color = \"black\")+\n  stat_summary(fun=mean, geom =\"text\", aes(label = paste(\"mean: \", round(after_stat(x), 1))), \n               position = position_nudge(y =-0.2), vjust = 0.5, color= \"black\", size = 2.3)+ \n  stat_summary(fun = mean, geom = \"point\", shape = 16, size = 2, color = \"black\",\n               position = position_nudge(x = 0.0)) +\n  labs(title = \"Science\")+\n  ylab(\"Proficiency Level\") + \n  xlab(\"ESCS\") + \n  theme_ipsum_rc(plot_margin = margin(4, 4, 4, 4)) \n\n\nWe will now use patchwork to stitch these 3 charts into 1 plot. Then, we add title, subtitle, caption and a theme to the plot.\n\n\nShow the code\npatch2 &lt;- e1 + e2 + e3 \n\npatch2 + plot_annotation(\n  title = \"Students with higher Proficiency Levels seemed to have higher ESCS index.\",\n  subtitle = \"Number of observations: 6606.\\nNote: The higher the value of ESCS, the higher the socio-economic status.\", caption = \"Data from PISA 2022\") & theme_ipsum_rc(plot_margin = margin(4, 4, 4, 4), grid_col = \"azure2\", axis_text_size = 9) & theme(legend.position = \"none\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SiHui Learns Visual Analytics!",
    "section": "",
    "text": "Welcome to SiHui Learns Visual Analytics! This website documents my learning journey for ISSS608 Visual Analytics and Applications course. Hopefully I can surf through this course smoothly like this penguin!"
  },
  {
    "objectID": "index.html#hands-on-exercises",
    "href": "index.html#hands-on-exercises",
    "title": "SiHui Learns Visual Analytics!",
    "section": "Hands-On Exercises",
    "text": "Hands-On Exercises\n\n\n\n\n\n\n\n\nHands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods\n\n\n\n\n\n\nGoh Si Hui\n\n\nJan 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHands-On Exercise 2: Beyond ggplot2 Fundamentals\n\n\n\n\n\n\nGoh Si Hui\n\n\nJan 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHands-On Exercise 3a: Programming Interactive Data Visualisation with R\n\n\n\n\n\n\nGoh Si Hui\n\n\nJan 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHands-On Exercise 3b: Programming Animated Statistical Graphics with R\n\n\n\n\n\n\nGoh Si Hui\n\n\nJan 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHands-on Exercise 4a - Visualising Distribution\n\n\n\n\n\n\nGoh Si Hui\n\n\nFeb 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHands-on Exercise 4b - Visual Statistical Analysis\n\n\n\n\n\n\nGoh Si Hui\n\n\nFeb 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHands-on Exercise 4C - Visualising Uncertainty\n\n\n\n\n\n\nGoh Si Hui\n\n\nFeb 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHands-on Exercise 4d - Funnel Plots for Fair Comparisons\n\n\n\n\n\n\nGoh Si Hui\n\n\nFeb 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHands-on Exercise 5a - Creating Ternary Plot with R\n\n\n\n\n\n\nGoh Si Hui\n\n\nFeb 7, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHands-on Exercise 5b - Visual Correlation Analysis\n\n\n\n\n\n\nGoh Si Hui\n\n\nFeb 8, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data\n\n\n\n\n\n\nGoh Si Hui\n\n\nFeb 8, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHands-on Exercise 5d - Visual Multivariate Analysis with Parallel Coordinates Plot\n\n\n\n\n\n\nGoh Si Hui\n\n\nFeb 8, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHands-on Exercise 5e - Treemap Visualisation with R\n\n\n\n\n\n\nGoh Si Hui\n\n\nFeb 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nTake-home Exercise 6 - Visualing and Analysing Time-Oriented Data\n\n\n\n\n\n\nGoh Si Hui\n\n\nFeb 20, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#in-class-exercises",
    "href": "index.html#in-class-exercises",
    "title": "SiHui Learns Visual Analytics!",
    "section": "In-Class Exercises",
    "text": "In-Class Exercises\n\n\n\n\n\n\n\n\nIn-class Exercise 1: Now You See it!\n\n\n\n\n\n\nGoh Si Hui\n\n\nJan 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nIn-class Exercise 2: Horizon Plot\n\n\n\n\n\n\nGoh Si Hui\n\n\nFeb 24, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#take-home-exercises",
    "href": "index.html#take-home-exercises",
    "title": "SiHui Learns Visual Analytics!",
    "section": "Take-Home Exercises",
    "text": "Take-Home Exercises\n\n\n\n\n\n\n\n\nTake-home Exercise 1\n\n\n\n\n\n\nGoh Si Hui\n\n\nJan 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nTake-home Exercise 2\n\n\n\n\n\n\nGoh Si Hui\n\n\nJan 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nTake-home Exercise 3: Be Weatherwise or Otherwise\n\n\n\n\n\n\nGoh Si Hui\n\n\nFeb 12, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#experiments",
    "href": "index.html#experiments",
    "title": "SiHui Learns Visual Analytics!",
    "section": "Experiments",
    "text": "Experiments\nAdditional practices and experiments can be found here!\n\n\n\n\n\n\n\n\nPost-Lesson Thoughts 1: Annotations\n\n\n\n\n\n\nGoh Si Hui\n\n\nJan 13, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#loading-r-packages",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#loading-r-packages",
    "title": "In-class Exercise 1: Now You See it!",
    "section": "1.1 Loading R packages",
    "text": "1.1 Loading R packages\nFor this in-class exercise, we will be using the following two packages:\n\ntidyverse : to load the core tidyverse packages, which includes ggplot2.\nhaven : to read and write various data formats used by other statistical packages by wrapping the ReadStat C library. It is part of the tidyverse family too! haven currently supports SAS, SPSS and Stata. We will need haven to import the PISA 2022’s student questionnaire data file because it is in SAS file type.\n\nThe code chunk below uses p_load() of pacman package to check if tidyverse packages are installed in the computer. If they are, they will be launched in R. Otherwise, pacman will install the relevant packages before launching them.\n\npacman::p_load(tidyverse, haven)",
    "crumbs": [
      "In-class Exercises",
      "In-class Exercise 1: Now You See it!"
    ]
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#importing-the-data-into-r",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#importing-the-data-into-r",
    "title": "In-class Exercise 1: Now You See it!",
    "section": "1.2 Importing the Data into R",
    "text": "1.2 Importing the Data into R\nFor this in-class exercise, we are using PISA 2022 database’s student questionnaire data file. As the data file is in SAS file format, we will use haven’s read_sas() function to import the data into R environment. Then we will filter the data to those data from Singapore.\n\nImport the DataFilter the Imported Data\n\n\n\nstu_qqq &lt;- read_sas(\"data/cy08msp_stu_qqq.sas7bdat\")\n\n\n\n\nstu_qqq_SG &lt;- stu_qqq %&gt;% \n  filter(CNT == \"SGP\")",
    "crumbs": [
      "In-class Exercises",
      "In-class Exercise 1: Now You See it!"
    ]
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#saving-the-data-into-rds-format",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#saving-the-data-into-rds-format",
    "title": "In-class Exercise 1: Now You See it!",
    "section": "1.3 Saving the data into RDS format",
    "text": "1.3 Saving the data into RDS format\nLet us save the filtered data into an R data format (RDS) so that we can easily retrieve in future without importing the stu_qqq dataset again (Note: this dataset is more than 3GB!)\n\nwrite_rds(stu_qqq_SG, \"data/stu_qqq_SG.rds\")",
    "crumbs": [
      "In-class Exercises",
      "In-class Exercise 1: Now You See it!"
    ]
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#reading-the-rds-data",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#reading-the-rds-data",
    "title": "In-class Exercise 1: Now You See it!",
    "section": "1.4 Reading the RDS data",
    "text": "1.4 Reading the RDS data\nWe will read the filtered data using the following code.\n\nstu_qqq_SG &lt;- read_rds(\"data/stu_qqq_SG.rds\")\n\nLet us check the RDS data using glimpse() of dplyr to learn about the associated attribute information in the dataframe.\n\nglimpse(stu_qqq_SG)\n\nRows: 6,606\nColumns: 1,279\n$ CNT          &lt;chr&gt; \"SGP\", \"SGP\", \"SGP\", \"SGP\", \"SGP\", \"SGP\", \"SGP\", \"SGP\", \"…\n$ CNTRYID      &lt;dbl&gt; 702, 702, 702, 702, 702, 702, 702, 702, 702, 702, 702, 70…\n$ CNTSCHID     &lt;dbl&gt; 70200052, 70200134, 70200112, 70200004, 70200152, 7020004…\n$ CNTSTUID     &lt;dbl&gt; 70200001, 70200002, 70200003, 70200004, 70200005, 7020000…\n$ CYC          &lt;chr&gt; \"08MS\", \"08MS\", \"08MS\", \"08MS\", \"08MS\", \"08MS\", \"08MS\", \"…\n$ NatCen       &lt;chr&gt; \"070200\", \"070200\", \"070200\", \"070200\", \"070200\", \"070200…\n$ STRATUM      &lt;chr&gt; \"SGP01\", \"SGP01\", \"SGP01\", \"SGP01\", \"SGP01\", \"SGP01\", \"SG…\n$ SUBNATIO     &lt;chr&gt; \"7020000\", \"7020000\", \"7020000\", \"7020000\", \"7020000\", \"7…\n$ REGION       &lt;dbl&gt; 70200, 70200, 70200, 70200, 70200, 70200, 70200, 70200, 7…\n$ OECD         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ ADMINMODE    &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ LANGTEST_QQQ &lt;dbl&gt; 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 31…\n$ LANGTEST_COG &lt;dbl&gt; 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 31…\n$ LANGTEST_PAQ &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ Option_CT    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ Option_FL    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ Option_ICTQ  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ Option_WBQ   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ Option_PQ    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ Option_TQ    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ Option_UH    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ BOOKID       &lt;dbl&gt; 4, 45, 8, 40, 42, 15, 13, 39, 14, 7, 20, 17, 38, 24, 19, …\n$ ST001D01T    &lt;dbl&gt; 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 1…\n$ ST003D02T    &lt;dbl&gt; 10, 6, 7, 2, 9, 9, 3, 4, 8, 6, 10, 7, 9, 11, 5, 10, 11, 4…\n$ ST003D03T    &lt;dbl&gt; 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 200…\n$ ST004D01T    &lt;dbl&gt; 1, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 1, 1, 2, 1, 2, 1, 2, 1, …\n$ ST250Q01JA   &lt;dbl&gt; 2, 1, 1, 2, 2, 2, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, …\n$ ST250Q02JA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST250Q03JA   &lt;dbl&gt; 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST250Q04JA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST250Q05JA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST250D06JA   &lt;chr&gt; \"7020002\", \"7020001\", \"7020001\", \"7020002\", \"7020002\", \"7…\n$ ST250D07JA   &lt;chr&gt; \"7020002\", \"7020001\", \"7020002\", \"7020002\", \"7020002\", \"7…\n$ ST251Q01JA   &lt;dbl&gt; 2, 1, 2, 1, 2, 2, 2, 1, 3, 3, 1, 2, 2, 1, 2, 2, 1, 2, 3, …\n$ ST251Q02JA   &lt;dbl&gt; 1, 4, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, …\n$ ST251Q03JA   &lt;dbl&gt; 3, 3, 3, 3, 2, 2, 3, 3, 4, 3, 2, 2, 3, 2, 3, 3, 2, 3, 4, …\n$ ST251Q04JA   &lt;dbl&gt; 3, 3, 3, 3, 2, 3, 3, 3, 4, 3, 2, 2, 3, 2, 3, 3, 2, 3, 4, …\n$ ST251Q06JA   &lt;dbl&gt; 3, 4, 2, 2, 1, 2, 2, 3, 4, 1, 3, 3, 1, 2, 2, 4, 4, 1, 2, …\n$ ST251Q07JA   &lt;dbl&gt; 3, 2, 1, 1, 4, 1, 4, 1, 4, 3, 1, 4, 1, 1, 4, 4, 1, 4, 1, …\n$ ST251D08JA   &lt;chr&gt; \"9999997\", \"9999997\", \"9999997\", \"9999997\", \"9999997\", \"9…\n$ ST251D09JA   &lt;chr&gt; \"9999997\", \"9999997\", \"9999997\", \"9999997\", \"9999997\", \"9…\n$ ST253Q01JA   &lt;dbl&gt; 7, 8, 7, 6, 7, 7, 8, 8, 8, 7, 7, 8, 5, 7, 7, 8, 5, 7, 7, …\n$ ST254Q01JA   &lt;dbl&gt; 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 1, 3, 2, 2, 2, 2, …\n$ ST254Q02JA   &lt;dbl&gt; 1, 2, 2, 1, 3, 2, 2, 5, 2, 2, 1, 3, 2, 1, 2, 2, 2, 2, 2, …\n$ ST254Q03JA   &lt;dbl&gt; 3, 2, 2, 2, 2, 2, 3, 3, 3, 4, 2, 3, 2, 2, 3, 3, 2, 3, 2, …\n$ ST254Q04JA   &lt;dbl&gt; 2, 3, 2, 1, 1, 2, 2, 3, 3, 2, 1, 3, 2, 2, 3, 3, 2, 3, 2, …\n$ ST254Q05JA   &lt;dbl&gt; 1, 5, 1, 1, NA, 1, 1, 5, 2, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1,…\n$ ST254Q06JA   &lt;dbl&gt; 3, 2, 3, 3, 4, 2, 4, 3, 4, 4, 3, 3, 2, 3, 3, 4, 3, 4, 3, …\n$ ST255Q01JA   &lt;dbl&gt; 7, 4, 4, 3, 2, 2, 4, 5, 7, 4, 3, 7, 4, 4, 2, 4, 5, 4, 4, …\n$ ST256Q01JA   &lt;dbl&gt; 2, 4, 5, 2, 4, 1, 1, 3, 4, 4, 1, 4, 2, 2, 2, 3, 2, 2, 1, …\n$ ST256Q02JA   &lt;dbl&gt; 2, 5, 2, 1, 1, 2, 1, 5, 4, 2, 2, 4, 1, 2, 2, 2, 3, 5, 1, …\n$ ST256Q03JA   &lt;dbl&gt; 4, 5, 2, 1, 1, 2, 2, 5, 5, 1, 2, 4, 1, 3, 1, 4, 4, 5, 1, …\n$ ST256Q06JA   &lt;dbl&gt; 4, 3, 3, 2, 2, 5, 2, 4, 4, 1, 2, 4, 2, 2, 2, 3, 2, 2, 3, …\n$ ST256Q07JA   &lt;dbl&gt; 3, 5, 5, 2, 2, 5, 1, 4, 3, 1, 2, 4, 1, 2, 2, 2, 1, 5, 1, …\n$ ST256Q08JA   &lt;dbl&gt; 3, 3, 3, 1, 1, 5, 1, 5, 5, 2, 2, 4, 1, 1, 1, 3, 1, 5, 2, …\n$ ST256Q09JA   &lt;dbl&gt; 2, 2, 4, 2, 2, 2, 2, 2, 2, 3, 2, 4, 2, 2, 2, 2, 2, 2, NA,…\n$ ST256Q10JA   &lt;dbl&gt; 4, 4, 5, 2, 4, 1, 4, 4, 4, 3, 2, 4, 4, 1, 2, 4, 3, 3, 3, …\n$ ST230Q01JA   &lt;dbl&gt; 4, 4, 2, 4, 4, 3, 2, 2, 3, 4, 1, 3, 4, 1, 4, 3, 2, 3, 2, …\n$ ST005Q01JA   &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ ST006Q01JA   &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, NA, 2, 2, NA, 2, 1, 2, 2, 2, 2, 2, 2…\n$ ST006Q02JA   &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, NA, 1, 1, 2, 2, 1, 2, 2, 2,…\n$ ST006Q03JA   &lt;dbl&gt; 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, NA, 1, 1, 2, 2, 1, 1, 1, 2,…\n$ ST006Q04JA   &lt;dbl&gt; 2, 1, 2, 2, 1, 1, 2, 1, 1, 2, NA, 2, 1, 2, 2, 2, 1, 2, 2,…\n$ ST006Q05JA   &lt;dbl&gt; 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, NA, 1, 2, 1, 2, 1, 1, 2, 1,…\n$ ST007Q01JA   &lt;dbl&gt; 2, 2, 2, 4, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ ST008Q01JA   &lt;dbl&gt; 2, 2, 2, NA, 2, 2, 2, NA, 2, 2, NA, 2, 1, 2, 2, 2, 2, 2, …\n$ ST008Q02JA   &lt;dbl&gt; 2, 2, 2, NA, 2, 1, 2, NA, 2, 2, NA, 1, 1, 1, 2, 2, 1, 2, …\n$ ST008Q03JA   &lt;dbl&gt; 2, 2, 2, NA, 2, 1, 2, 1, 1, 2, NA, 1, 1, 2, 2, 2, 1, 1, 2…\n$ ST008Q04JA   &lt;dbl&gt; 1, 1, 2, NA, 1, 1, 2, 1, 1, 1, NA, 2, 1, 2, 2, 2, 1, 2, 2…\n$ ST008Q05JA   &lt;dbl&gt; 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, NA, 1, 2, 2, 1, 2, 1, 2, 1,…\n$ ST258Q01JA   &lt;dbl&gt; 1, 1, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 2, 1, 1, …\n$ ST259Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST259Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST019AQ01T   &lt;dbl&gt; 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 2, 2, 1, …\n$ ST019BQ01T   &lt;dbl&gt; 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 2, 2, 2, 1, …\n$ ST019CQ01T   &lt;dbl&gt; 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 2, 2, 2, 1, …\n$ ST021Q01TA   &lt;dbl&gt; NA, NA, NA, NA, NA, 1, NA, 1, NA, NA, NA, NA, NA, 7, NA, …\n$ ST022Q01TA   &lt;dbl&gt; 1, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 1, 1, 2, 2, 2, 2, 1, 1, …\n$ ST226Q01JA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST125Q01NA   &lt;dbl&gt; 3, 5, 8, 4, 8, 3, 4, 8, 8, 4, 8, 2, 7, 2, 3, 5, 8, 8, 3, …\n$ ST126Q01TA   &lt;dbl&gt; 4, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 4, 5, 5, 4, 5, …\n$ ST127Q01TA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST127Q02TA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST127Q03TA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST260Q01JA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, …\n$ ST260Q02JA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST260Q03JA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST261Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 2, NA, NA…\n$ ST261Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 2, NA, NA…\n$ ST261Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 2, NA, NA…\n$ ST261Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 2, NA, NA…\n$ ST261Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1, NA, NA…\n$ ST261Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 2, NA, NA…\n$ ST261Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 2, NA, NA…\n$ ST261Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1, NA, NA…\n$ ST261Q09JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 2, NA, NA…\n$ ST261Q10JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 2, NA, NA…\n$ ST261Q11JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 2, NA, NA…\n$ ST062Q01TA   &lt;dbl&gt; 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST062Q02TA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, …\n$ ST062Q03TA   &lt;dbl&gt; 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST267Q01JA   &lt;dbl&gt; NA, 4, 3, 3, NA, 4, 3, NA, 4, NA, NA, 4, 3, 3, 3, NA, 3, …\n$ ST267Q02JA   &lt;dbl&gt; 2, 3, 3, NA, 3, 3, 1, NA, 4, NA, 3, 4, 1, NA, 4, 2, 1, 4,…\n$ ST267Q03JA   &lt;dbl&gt; NA, NA, 3, 2, NA, 3, 1, 3, NA, 3, NA, 4, NA, 3, NA, 3, NA…\n$ ST267Q04JA   &lt;dbl&gt; 2, 2, NA, NA, 3, NA, 1, 3, 2, 2, 2, NA, 3, 1, 4, 1, 2, NA…\n$ ST267Q05JA   &lt;dbl&gt; 3, 3, 3, 3, 3, 3, NA, 3, NA, NA, 3, 4, NA, NA, NA, 4, 3, …\n$ ST267Q06JA   &lt;dbl&gt; NA, NA, NA, 3, 4, 4, NA, 3, 4, 3, NA, NA, 2, 3, 3, 4, 4, …\n$ ST267Q07JA   &lt;dbl&gt; 3, 4, 4, NA, NA, NA, NA, NA, 4, 3, 3, 4, 1, 3, 4, NA, NA,…\n$ ST267Q08JA   &lt;dbl&gt; 2, NA, NA, 1, 1, NA, 1, 3, NA, 2, 2, NA, NA, NA, NA, NA, …\n$ ST034Q01TA   &lt;dbl&gt; 4, 4, 3, 2, 4, 4, 2, 1, 3, 3, 3, 4, 2, 4, 3, 4, 3, 4, 3, …\n$ ST034Q02TA   &lt;dbl&gt; NA, 2, 2, 3, 1, 2, 2, NA, 1, 2, NA, 1, 3, 1, 4, 2, 3, 2, …\n$ ST034Q03TA   &lt;dbl&gt; 2, NA, NA, 3, 1, 2, 3, 3, 2, NA, 2, 1, 3, 1, 2, NA, 2, NA…\n$ ST034Q04TA   &lt;dbl&gt; 4, 3, 2, 2, 4, NA, 3, 2, NA, 3, 3, 4, 2, 4, 2, 4, NA, 4, …\n$ ST034Q05TA   &lt;dbl&gt; 2, 2, 2, 2, NA, 2, NA, 3, 2, 2, 2, NA, 3, 1, NA, 2, 2, 3,…\n$ ST034Q06TA   &lt;dbl&gt; 3, 3, 3, NA, 4, 4, 3, 4, 3, 3, 3, 4, NA, NA, 3, 4, 3, 4, …\n$ ST038Q03NA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 2, 3, 1, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, …\n$ ST038Q04NA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 4, 3, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, …\n$ ST038Q05NA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST038Q06NA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, …\n$ ST038Q07NA   &lt;dbl&gt; 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, …\n$ ST038Q08NA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, …\n$ ST038Q09JA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST038Q10JA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, …\n$ ST038Q11JA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST265Q01JA   &lt;dbl&gt; 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 3, 1, 2, 1, 2, 1, 2, …\n$ ST265Q02JA   &lt;dbl&gt; 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 2, 1, 3, 1, 2, 1, 2, 1, 2, …\n$ ST265Q03JA   &lt;dbl&gt; 2, 1, 2, 2, 1, 2, 3, 1, 1, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, …\n$ ST265Q04JA   &lt;dbl&gt; 2, 1, 2, 2, 1, 2, 3, 1, 1, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, …\n$ ST266Q01JA   &lt;dbl&gt; 2, 1, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 2, 1, 1, 2, 2, 2, 2, …\n$ ST266Q02JA   &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ ST266Q03JA   &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ ST266Q04JA   &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ ST266Q05JA   &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ ST294Q01JA   &lt;dbl&gt; 5, 4, 6, 1, 1, 1, 1, 6, 6, 1, 5, 1, 1, 1, 4, 3, 6, 6, 1, …\n$ ST294Q02JA   &lt;dbl&gt; 1, 5, 2, 1, 4, 6, 1, 6, 1, 1, 1, 1, 6, 1, 5, 1, 1, 1, 6, …\n$ ST294Q03JA   &lt;dbl&gt; 6, 2, 1, 6, 1, 3, 4, 1, 1, 1, 1, 1, 6, 1, 6, 1, 3, 1, 1, …\n$ ST294Q04JA   &lt;dbl&gt; 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST294Q05JA   &lt;dbl&gt; 1, 3, 1, 6, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 3, …\n$ ST295Q01JA   &lt;dbl&gt; 6, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 1, 6, 6, 6, 3, 6, 6, …\n$ ST295Q02JA   &lt;dbl&gt; 5, 4, 3, 6, 5, 6, 1, 6, 6, 4, 6, 6, 6, 1, 5, 6, 6, 6, 6, …\n$ ST295Q03JA   &lt;dbl&gt; 6, 2, 1, 6, 6, 4, 5, 1, 1, 5, 3, 3, 6, 1, 6, 1, 4, 6, 1, …\n$ ST295Q04JA   &lt;dbl&gt; 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST295Q05JA   &lt;dbl&gt; 2, 3, 3, 1, 5, 2, 3, 1, 4, 6, 2, 3, 6, 3, 3, 1, 3, 4, 1, …\n$ ST326Q01JA   &lt;dbl&gt; 3, 2, 3, 2, 6, 3, 3, 8, 4, 3, 4, 4, 3, 7, 5, 3, 7, 6, 2, …\n$ ST326Q02JA   &lt;dbl&gt; 4, 2, 2, 3, 5, 2, 2, 5, 5, 5, 5, 3, 5, 2, 3, 3, 6, 4, 4, …\n$ ST326Q03JA   &lt;dbl&gt; 2, 3, 2, 2, 4, 1, 1, NA, 4, 4, 6, 5, 8, 1, 4, 3, 9, 4, 2,…\n$ ST326Q04JA   &lt;dbl&gt; 1, 1, 2, 1, 5, 1, 2, 3, 1, 2, 1, 2, 1, 2, 3, 2, 3, 2, 1, …\n$ ST326Q05JA   &lt;dbl&gt; NA, 1, 5, 1, 5, 2, 4, 5, 3, 5, 5, 5, 1, 9, 3, 1, 4, 6, 5,…\n$ ST326Q06JA   &lt;dbl&gt; 8, 1, 7, 1, 2, 5, 9, 7, 4, 5, 4, 7, 2, 9, 5, 1, 7, 7, 6, …\n$ ST326Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST326Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST326Q09JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST326Q10JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST326Q11JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST326Q12JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST322Q01JA   &lt;dbl&gt; 5, 6, 3, 5, 4, 6, 5, 5, 4, NA, 4, 5, 5, 1, 2, 5, NA, 4, 5…\n$ ST322Q02JA   &lt;dbl&gt; 5, 1, 3, 5, 5, 5, 5, 5, NA, 4, 2, 5, 5, NA, NA, 5, 4, 3, …\n$ ST322Q03JA   &lt;dbl&gt; NA, 5, 2, 5, NA, 4, 1, 5, 3, 5, NA, 4, 3, 4, 4, NA, 4, 4,…\n$ ST322Q04JA   &lt;dbl&gt; 2, 5, 2, 1, 5, NA, NA, 5, 3, 3, 4, 5, NA, 5, 5, 2, 2, NA,…\n$ ST322Q06JA   &lt;dbl&gt; 1, 1, NA, NA, 4, 6, 1, NA, 6, 4, 1, NA, 1, 1, 4, 1, 3, 4,…\n$ ST322Q07JA   &lt;dbl&gt; 1, NA, 2, 1, 4, 2, 2, 1, 1, 5, 2, 3, 1, 1, 1, 1, 5, 3, 1,…\n$ ST307Q01JA   &lt;dbl&gt; 4, NA, NA, NA, 4, NA, 4, 5, NA, NA, 5, NA, 4, NA, NA, 4, …\n$ ST307Q02JA   &lt;dbl&gt; NA, 5, 4, NA, NA, 3, NA, NA, 5, NA, NA, 5, NA, 4, NA, NA,…\n$ ST307Q03JA   &lt;dbl&gt; 4, 4, 3, 4, 3, NA, NA, 4, 2, 3, 4, 5, NA, NA, 4, 5, NA, N…\n$ ST307Q04JA   &lt;dbl&gt; 2, NA, 3, 2, NA, NA, NA, 3, 4, 4, NA, NA, NA, 1, 4, 2, 3,…\n$ ST307Q05JA   &lt;dbl&gt; NA, 5, NA, NA, NA, NA, NA, NA, NA, NA, 3, 4, NA, 4, 4, NA…\n$ ST307Q06JA   &lt;dbl&gt; 2, NA, 2, 2, 5, 3, 1, 3, NA, NA, NA, NA, 2, NA, 3, NA, NA…\n$ ST307Q07JA   &lt;dbl&gt; 2, NA, NA, 2, NA, NA, 5, NA, 4, 4, 2, 1, 2, 1, NA, 2, NA,…\n$ ST307Q08JA   &lt;dbl&gt; NA, 3, NA, 4, NA, 4, 5, NA, NA, 4, NA, NA, NA, NA, NA, 4,…\n$ ST307Q09JA   &lt;dbl&gt; NA, NA, 3, NA, 4, 5, NA, NA, 4, NA, 4, 4, 4, 5, 3, NA, 3,…\n$ ST307Q10JA   &lt;dbl&gt; NA, 3, NA, NA, 5, 4, 2, 1, NA, 3, NA, NA, 3, NA, NA, NA, …\n$ ST309Q01JA   &lt;dbl&gt; NA, NA, 5, NA, 4, 4, 5, 4, 5, NA, 4, NA, 5, NA, 5, NA, 4,…\n$ ST309Q02JA   &lt;dbl&gt; 3, 5, NA, NA, 5, NA, NA, NA, 4, NA, NA, 2, 5, NA, NA, NA,…\n$ ST309Q03JA   &lt;dbl&gt; NA, 3, 3, NA, NA, NA, 2, 4, 1, 4, 2, 1, 1, 5, 2, NA, NA, …\n$ ST309Q04JA   &lt;dbl&gt; 3, NA, NA, NA, NA, 4, 5, NA, NA, NA, NA, NA, NA, 4, NA, N…\n$ ST309Q05JA   &lt;dbl&gt; NA, 4, 3, NA, 4, NA, NA, NA, 5, 4, NA, NA, NA, NA, 4, 1, …\n$ ST309Q06JA   &lt;dbl&gt; NA, NA, 4, 3, NA, 4, NA, NA, 4, NA, NA, 1, 3, NA, NA, 3, …\n$ ST309Q07JA   &lt;dbl&gt; 3, NA, NA, 4, NA, NA, 2, 1, NA, 4, 3, 1, NA, 4, 4, 3, NA,…\n$ ST309Q08JA   &lt;dbl&gt; 4, 3, NA, 4, 5, NA, NA, 4, NA, 4, 4, NA, NA, 2, 5, NA, 3,…\n$ ST309Q09JA   &lt;dbl&gt; 2, NA, 1, 4, NA, 2, NA, 2, NA, NA, 3, NA, 2, 4, NA, 4, NA…\n$ ST309Q10JA   &lt;dbl&gt; NA, 5, NA, 3, 4, 4, 4, NA, NA, 3, NA, 5, NA, NA, NA, 3, 4…\n$ ST301Q01JA   &lt;dbl&gt; 5, NA, NA, 4, 5, 4, 4, NA, 3, 5, NA, NA, 3, NA, NA, NA, N…\n$ ST301Q02JA   &lt;dbl&gt; NA, NA, NA, NA, 1, NA, NA, NA, NA, 4, 4, 5, NA, NA, NA, 5…\n$ ST301Q03JA   &lt;dbl&gt; NA, NA, 3, NA, 4, 4, 1, NA, 2, NA, 2, 4, NA, NA, 4, 2, 3,…\n$ ST301Q04JA   &lt;dbl&gt; 5, NA, NA, 4, 5, NA, NA, 5, NA, 4, NA, 5, NA, 4, NA, 4, N…\n$ ST301Q05JA   &lt;dbl&gt; NA, 4, NA, NA, 4, 4, NA, NA, 5, NA, 4, 4, 3, NA, NA, 4, 3…\n$ ST301Q06JA   &lt;dbl&gt; 5, 5, 3, 4, NA, NA, NA, NA, 3, 4, 4, 5, 2, 4, 5, NA, 3, 4…\n$ ST301Q07JA   &lt;dbl&gt; 5, NA, 3, 4, NA, 4, NA, 5, NA, NA, NA, NA, 2, 4, NA, 4, N…\n$ ST301Q08JA   &lt;dbl&gt; NA, 2, NA, 3, NA, NA, 4, 2, 3, 2, NA, NA, NA, 2, 2, NA, 3…\n$ ST301Q09JA   &lt;dbl&gt; NA, 3, 4, NA, NA, 4, 5, 5, NA, NA, 5, NA, NA, 4, 5, NA, N…\n$ ST301Q10JA   &lt;dbl&gt; 5, 4, 3, NA, NA, NA, 4, 5, NA, NA, NA, NA, 3, NA, 4, NA, …\n$ ST343Q01JA   &lt;dbl&gt; 4, NA, 3, NA, NA, NA, 5, NA, 5, NA, NA, 5, 4, NA, NA, NA,…\n$ ST343Q02JA   &lt;dbl&gt; NA, 2, NA, 3, NA, 1, NA, 4, NA, 3, NA, 2, NA, 4, NA, NA, …\n$ ST343Q03JA   &lt;dbl&gt; NA, NA, NA, NA, 4, NA, 3, NA, NA, NA, 4, 5, NA, NA, NA, N…\n$ ST343Q04JA   &lt;dbl&gt; 2, NA, NA, 2, NA, 1, 3, 1, NA, 2, NA, NA, 3, NA, 3, 2, 2,…\n$ ST343Q05JA   &lt;dbl&gt; 3, 3, 3, 2, 2, 1, NA, NA, 1, NA, 2, NA, NA, NA, 5, 2, 4, …\n$ ST343Q06JA   &lt;dbl&gt; 4, 4, 3, 4, 5, NA, NA, 1, 5, 5, 4, 5, 4, 5, NA, 5, 3, NA,…\n$ ST343Q07JA   &lt;dbl&gt; 1, 3, NA, 2, NA, NA, NA, 1, NA, 3, 3, NA, NA, 3, 1, 3, NA…\n$ ST343Q08JA   &lt;dbl&gt; NA, 5, NA, NA, 5, 5, NA, 2, 5, NA, NA, NA, NA, 2, 3, NA, …\n$ ST343Q09JA   &lt;dbl&gt; NA, NA, 4, NA, NA, 5, 2, NA, 5, NA, NA, 5, 3, 3, 3, NA, N…\n$ ST343Q10JA   &lt;dbl&gt; NA, NA, 2, NA, 2, NA, 2, NA, NA, 2, 3, NA, 3, NA, NA, 4, …\n$ ST311Q01JA   &lt;dbl&gt; NA, 2, NA, NA, 2, 1, 5, 2, NA, NA, 2, 1, 2, 2, NA, 2, NA,…\n$ ST311Q02JA   &lt;dbl&gt; 5, 4, 4, 4, NA, 1, NA, NA, NA, NA, NA, NA, NA, NA, 4, NA,…\n$ ST311Q03JA   &lt;dbl&gt; 5, NA, NA, NA, NA, NA, NA, 5, 5, 5, NA, NA, NA, NA, 5, NA…\n$ ST311Q04JA   &lt;dbl&gt; 5, 4, 4, 4, NA, 5, NA, NA, NA, NA, 4, 5, 4, NA, 4, NA, 3,…\n$ ST311Q05JA   &lt;dbl&gt; NA, 2, 2, 2, NA, 1, 3, NA, NA, 1, NA, NA, NA, 2, 1, 2, NA…\n$ ST311Q06JA   &lt;dbl&gt; NA, NA, NA, NA, 3, NA, 3, NA, 5, NA, 3, 5, NA, NA, NA, NA…\n$ ST311Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, 1, 5, NA, NA, NA, 3, 3, 2, NA, 1,…\n$ ST311Q08JA   &lt;dbl&gt; NA, NA, 3, NA, 3, NA, 2, 5, 5, 4, 3, 5, 3, 4, 4, 5, NA, N…\n$ ST311Q09JA   &lt;dbl&gt; 3, 4, 3, 4, 4, NA, NA, NA, 5, 4, NA, NA, 4, NA, NA, 4, NA…\n$ ST311Q10JA   &lt;dbl&gt; 5, NA, NA, 3, 3, 3, NA, 5, 5, 4, 3, NA, NA, 4, NA, NA, 3,…\n$ ST315Q01JA   &lt;dbl&gt; NA, NA, NA, 4, NA, NA, 5, 4, NA, NA, 3, 4, 4, NA, NA, NA,…\n$ ST315Q02JA   &lt;dbl&gt; 4, 3, 3, 2, NA, 3, 1, NA, 5, NA, NA, 5, NA, NA, NA, 5, NA…\n$ ST315Q03JA   &lt;dbl&gt; 3, NA, NA, NA, NA, 5, NA, NA, NA, 4, NA, NA, NA, 2, 2, NA…\n$ ST315Q04JA   &lt;dbl&gt; 3, 3, 3, 4, 3, NA, NA, 1, NA, 4, 4, NA, 3, 2, NA, NA, NA,…\n$ ST315Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 1, 2, NA, 1, 2, 3, 4, NA,…\n$ ST315Q06JA   &lt;dbl&gt; 3, 3, 3, NA, NA, 5, 2, 1, NA, NA, 3, NA, NA, NA, NA, 5, N…\n$ ST315Q07JA   &lt;dbl&gt; NA, 2, NA, NA, 2, NA, 4, 5, 1, NA, 3, NA, NA, 3, NA, 1, N…\n$ ST315Q08JA   &lt;dbl&gt; NA, NA, 2, 1, 2, 3, NA, 1, 3, 4, NA, NA, 3, NA, 4, 5, 2, …\n$ ST315Q09JA   &lt;dbl&gt; NA, 3, NA, NA, 5, 5, 1, NA, 4, 4, 3, 5, NA, 3, 3, 5, 4, 3…\n$ ST315Q10JA   &lt;dbl&gt; 4, NA, 4, 4, 5, NA, NA, NA, NA, NA, NA, 5, 4, NA, 3, NA, …\n$ ST303Q01JA   &lt;dbl&gt; 5, NA, NA, NA, NA, NA, 4, NA, 5, 4, 4, 5, NA, NA, NA, NA,…\n$ ST303Q02JA   &lt;dbl&gt; 5, NA, 3, NA, 5, 4, NA, 5, NA, NA, 4, 5, NA, 4, NA, NA, 3…\n$ ST303Q03JA   &lt;dbl&gt; 5, NA, 2, 3, 4, NA, NA, 5, 5, 4, NA, NA, 3, 4, 3, 5, 3, N…\n$ ST303Q04JA   &lt;dbl&gt; NA, 4, 4, 4, 5, 5, NA, 5, 5, NA, 4, 5, 3, 4, 5, 5, NA, NA…\n$ ST303Q05JA   &lt;dbl&gt; NA, 3, NA, 2, NA, 1, 3, NA, NA, 4, NA, 1, NA, 2, 4, 1, NA…\n$ ST303Q06JA   &lt;dbl&gt; 5, 5, 4, 4, 5, NA, 4, 5, 5, 4, 4, NA, 3, NA, 5, 5, 3, 4, …\n$ ST303Q07JA   &lt;dbl&gt; 1, 4, 3, NA, NA, 2, 2, NA, 3, 3, 3, NA, 3, NA, NA, 1, NA,…\n$ ST303Q08JA   &lt;dbl&gt; NA, 4, NA, 3, 5, 5, 4, 5, NA, NA, NA, 5, 3, 4, 4, NA, 3, …\n$ ST305Q01JA   &lt;dbl&gt; NA, 2, NA, NA, 5, NA, 2, NA, 5, NA, NA, 5, NA, 5, 2, 5, 2…\n$ ST305Q02JA   &lt;dbl&gt; NA, NA, 2, NA, NA, NA, 4, 3, NA, 3, 4, 3, 2, NA, NA, 5, N…\n$ ST305Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, 3, 2, 5, 4, 4, NA, 3, NA, 2, NA, …\n$ ST305Q04JA   &lt;dbl&gt; 3, NA, NA, NA, 5, 2, NA, NA, NA, NA, NA, 5, NA, NA, NA, 1…\n$ ST305Q05JA   &lt;dbl&gt; 4, 4, 3, 3, 2, 3, NA, NA, NA, 4, NA, NA, 2, 5, 2, NA, 3, …\n$ ST305Q06JA   &lt;dbl&gt; 3, NA, 3, 3, NA, 4, NA, NA, 5, NA, 4, NA, NA, 4, NA, 5, 3…\n$ ST305Q07JA   &lt;dbl&gt; 4, 4, NA, 3, NA, NA, NA, 4, 3, 4, 3, 1, NA, NA, 4, NA, 3,…\n$ ST305Q08JA   &lt;dbl&gt; 2, NA, NA, 3, NA, 5, 2, NA, NA, NA, NA, NA, 3, 3, NA, NA,…\n$ ST305Q09JA   &lt;dbl&gt; NA, 3, 2, NA, 3, 1, 4, 1, 3, NA, 3, 4, NA, NA, 2, 3, NA, …\n$ ST305Q10JA   &lt;dbl&gt; NA, 4, 3, 3, 3, NA, NA, 1, NA, 4, NA, NA, 3, 4, NA, NA, N…\n$ ST345Q01JA   &lt;dbl&gt; 3, 4, NA, 3, 5, NA, 3, 5, 4, 4, NA, 3, 5, NA, NA, NA, 4, …\n$ ST345Q02JA   &lt;dbl&gt; 5, NA, 4, 3, 3, NA, 5, NA, NA, NA, NA, 3, 2, NA, 4, NA, 3…\n$ ST345Q03JA   &lt;dbl&gt; 2, 4, NA, NA, NA, 4, NA, 4, NA, NA, NA, NA, 5, 2, NA, NA,…\n$ ST345Q04JA   &lt;dbl&gt; 1, NA, 4, NA, NA, 2, 3, NA, 4, 3, 4, 3, NA, NA, NA, 2, 3,…\n$ ST345Q05JA   &lt;dbl&gt; 3, NA, NA, 3, NA, NA, NA, 4, 4, NA, NA, 3, 1, NA, 3, NA, …\n$ ST345Q06JA   &lt;dbl&gt; NA, 2, NA, NA, 3, 5, NA, NA, 5, 4, NA, NA, NA, 5, 3, 4, N…\n$ ST345Q07JA   &lt;dbl&gt; NA, 5, 3, 3, 5, 5, NA, 3, NA, 5, 5, NA, NA, 1, 5, NA, 5, …\n$ ST345Q08JA   &lt;dbl&gt; NA, NA, 3, 3, NA, NA, NA, NA, NA, 3, 4, 5, NA, NA, NA, 5,…\n$ ST345Q09JA   &lt;dbl&gt; NA, NA, 3, NA, 2, NA, 5, NA, 5, NA, 4, NA, 1, 5, 2, 5, NA…\n$ ST345Q10JA   &lt;dbl&gt; NA, 4, NA, NA, NA, 3, 2, 4, NA, NA, 4, NA, NA, 2, NA, 1, …\n$ ST313Q01JA   &lt;dbl&gt; NA, NA, NA, 3, 2, NA, 5, NA, NA, 4, NA, 5, NA, 4, NA, 3, …\n$ ST313Q02JA   &lt;dbl&gt; 2, NA, 2, NA, NA, 1, 2, 1, 4, NA, NA, 1, 3, NA, NA, 3, NA…\n$ ST313Q03JA   &lt;dbl&gt; 2, 3, 3, 3, NA, NA, NA, NA, 3, NA, NA, NA, 2, 2, NA, NA, …\n$ ST313Q04JA   &lt;dbl&gt; 1, 4, NA, 3, 4, NA, 1, 2, 2, NA, 2, NA, NA, NA, 4, 3, 1, …\n$ ST313Q05JA   &lt;dbl&gt; 3, NA, 3, 3, NA, 1, NA, NA, 4, NA, 4, 4, 1, 5, 3, NA, 3, …\n$ ST313Q06JA   &lt;dbl&gt; NA, 4, 2, NA, 4, 1, 2, 2, NA, 2, 4, 2, 5, 2, 5, NA, 1, 2,…\n$ ST313Q07JA   &lt;dbl&gt; NA, NA, NA, NA, 4, 1, NA, 4, 3, 4, NA, NA, NA, NA, 3, NA,…\n$ ST313Q08JA   &lt;dbl&gt; NA, 4, NA, NA, NA, NA, NA, 2, NA, NA, 2, 1, NA, NA, NA, 3…\n$ ST313Q09JA   &lt;dbl&gt; 1, NA, 2, 3, 4, NA, 3, NA, NA, 4, 3, NA, 3, NA, NA, 3, 1,…\n$ ST313Q10JA   &lt;dbl&gt; NA, 4, NA, NA, NA, 1, NA, NA, NA, 3, NA, NA, NA, 2, 4, NA…\n$ ST263Q02JA   &lt;dbl&gt; 1, 1, 2, 2, 1, 4, 2, 3, 3, 3, 3, 2, 4, 3, 2, 2, 2, 2, 3, …\n$ ST263Q04JA   &lt;dbl&gt; 2, 1, 2, 3, 1, 1, 1, 3, 2, 3, 1, 1, 3, 3, 2, 2, 1, 2, 3, …\n$ ST263Q06JA   &lt;dbl&gt; 2, 1, 3, 2, 1, 1, 1, 3, 2, 3, 1, 1, 3, 3, 2, 2, 1, 2, 3, …\n$ ST263Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST016Q01NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST059Q01TA   &lt;dbl&gt; 4, 10, 8, 10, 5, 7, 20, 18, 6, 3, 9, 10, 5, 12, 5, 7, 8, …\n$ ST059Q02JA   &lt;dbl&gt; 55, 45, 56, 11, 30, 26, 75, 75, 28, 15, 51, 63, 40, 75, 3…\n$ ST296Q01JA   &lt;dbl&gt; 1, 3, 2, 3, 4, 1, 1, 2, 1, 3, 3, 4, 3, 1, 2, 1, 3, 3, 2, …\n$ ST296Q02JA   &lt;dbl&gt; 1, 2, 3, 1, 3, 1, 1, 2, 1, 3, 3, 3, 2, 1, 4, 1, 3, 1, 1, …\n$ ST296Q03JA   &lt;dbl&gt; 2, 3, 3, 2, 4, 1, 1, 2, 1, 2, 3, 4, 3, 1, 3, 1, 3, 3, 2, …\n$ ST296Q04JA   &lt;dbl&gt; 3, 5, 5, 2, 4, 2, 2, 4, 4, 6, 4, 6, 6, 1, 5, 3, 4, 4, 3, …\n$ ST272Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST273Q01JA   &lt;dbl&gt; 3, 4, 3, 2, 4, 3, 2, 3, 2, 3, 3, 3, NA, 2, NA, NA, 4, 4, …\n$ ST273Q02JA   &lt;dbl&gt; NA, 3, 3, NA, NA, 3, 2, 2, 2, 3, 3, NA, 4, 2, 4, 4, NA, 3…\n$ ST273Q03JA   &lt;dbl&gt; NA, 4, NA, NA, 4, NA, NA, 3, 3, 4, NA, 4, 4, NA, 4, 1, 4,…\n$ ST273Q04JA   &lt;dbl&gt; 4, 4, 3, 3, 3, 3, 1, 3, NA, NA, 3, 4, 4, 2, 4, 4, 4, NA, …\n$ ST273Q05JA   &lt;dbl&gt; 4, NA, 4, 2, 3, 3, 2, NA, 3, 3, 3, NA, 4, NA, 2, NA, 4, 4…\n$ ST273Q06JA   &lt;dbl&gt; 3, NA, 3, 4, NA, NA, NA, 3, NA, 3, 3, 4, NA, 2, NA, 1, NA…\n$ ST273Q07JA   &lt;dbl&gt; 3, 4, NA, 4, 2, 3, 4, NA, 3, NA, NA, 4, 4, 2, 2, 1, 4, 4,…\n$ ST270Q01JA   &lt;dbl&gt; 3, 2, 3, 2, 1, 3, 2, 2, 1, 2, 2, 1, 3, 3, 1, 1, 2, 1, 4, …\n$ ST270Q02JA   &lt;dbl&gt; 2, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 1, 3, 1, 1, 1, 2, 1, 2, …\n$ ST270Q03JA   &lt;dbl&gt; 2, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 1, 3, 2, 1, 1, 2, 1, 1, …\n$ ST270Q04JA   &lt;dbl&gt; 3, 1, 3, 2, 1, 2, 3, 2, 1, 2, 2, 1, 4, 2, 1, 1, 2, 1, 3, …\n$ ST285Q01JA   &lt;dbl&gt; NA, 1, NA, 3, 3, 1, 1, 1, NA, NA, 2, 1, 1, 1, 4, 5, NA, 2…\n$ ST285Q02JA   &lt;dbl&gt; 3, NA, 2, 3, 4, NA, NA, 5, 5, 4, NA, NA, 3, NA, 4, 5, NA,…\n$ ST285Q03JA   &lt;dbl&gt; NA, NA, 3, 3, 2, 1, NA, NA, 4, 4, 3, NA, NA, 1, 4, 5, NA,…\n$ ST285Q04JA   &lt;dbl&gt; NA, 2, NA, NA, 5, NA, 3, NA, NA, 4, NA, 5, 3, NA, 4, NA, …\n$ ST285Q05JA   &lt;dbl&gt; 2, 2, 1, NA, NA, 5, NA, NA, NA, NA, 2, NA, NA, 2, NA, NA,…\n$ ST285Q06JA   &lt;dbl&gt; 3, 2, NA, 3, 5, NA, 4, 5, 5, NA, 2, 5, NA, NA, NA, NA, 4,…\n$ ST285Q07JA   &lt;dbl&gt; 1, NA, 3, NA, NA, 5, NA, NA, 5, 2, 2, 5, 4, 2, NA, 5, 4, …\n$ ST285Q08JA   &lt;dbl&gt; 4, NA, 2, 3, NA, 5, 4, 5, 5, NA, NA, 5, 5, NA, NA, NA, 4,…\n$ ST285Q09JA   &lt;dbl&gt; NA, 3, NA, NA, NA, NA, 4, 5, NA, 4, NA, NA, NA, 1, 4, 5, …\n$ ST283Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, 4, 1, NA, NA, 3, 2, 5, NA, NA, NA, 4,…\n$ ST283Q02JA   &lt;dbl&gt; NA, 3, NA, 3, NA, NA, 1, NA, 3, NA, 3, 5, NA, NA, NA, NA,…\n$ ST283Q03JA   &lt;dbl&gt; 3, 3, 1, NA, 5, 4, NA, 4, 5, 3, NA, 5, 3, 1, 4, 2, 4, NA,…\n$ ST283Q04JA   &lt;dbl&gt; 3, 3, 3, 3, NA, NA, NA, 5, NA, 3, NA, 5, NA, NA, 4, 5, NA…\n$ ST283Q05JA   &lt;dbl&gt; 3, 3, NA, 3, 5, NA, 3, NA, 5, 3, NA, NA, 4, NA, NA, NA, N…\n$ ST283Q06JA   &lt;dbl&gt; 2, 2, 1, NA, 5, 3, 1, 4, NA, NA, 2, NA, NA, 1, 4, NA, 4, …\n$ ST283Q07JA   &lt;dbl&gt; 4, NA, NA, 3, 5, 4, 3, 5, 5, NA, 3, NA, 3, 1, 4, NA, NA, …\n$ ST283Q08JA   &lt;dbl&gt; NA, NA, 2, 3, NA, NA, NA, 5, NA, NA, NA, 5, 2, 1, NA, 5, …\n$ ST283Q09JA   &lt;dbl&gt; NA, NA, 2, NA, 5, 4, NA, NA, 5, 3, 3, NA, 3, 1, 4, 5, 4, …\n$ ST275Q01WA   &lt;dbl&gt; NA, 3, 2, NA, NA, 2, 3, NA, 3, NA, 3, NA, NA, 4, 2, 1, 3,…\n$ ST275Q02WA   &lt;dbl&gt; 2, 4, NA, 2, 2, 2, NA, NA, NA, 3, NA, 1, NA, NA, NA, 3, 3…\n$ ST275Q03WA   &lt;dbl&gt; 2, 4, NA, 4, 2, 2, NA, NA, 2, 3, 3, NA, 3, 4, 1, 4, NA, N…\n$ ST275Q04WA   &lt;dbl&gt; 2, NA, 2, 2, NA, NA, 1, 1, 2, NA, NA, 1, NA, NA, 3, NA, N…\n$ ST275Q05WA   &lt;dbl&gt; NA, NA, 1, NA, 1, NA, 3, 1, NA, 2, NA, 1, 3, 2, 1, 1, 1, …\n$ ST275Q06WA   &lt;dbl&gt; 1, 4, NA, 3, 2, NA, NA, 1, 1, NA, NA, 1, NA, NA, 2, NA, N…\n$ ST275Q07WA   &lt;dbl&gt; NA, NA, NA, 1, NA, NA, NA, 1, 1, 2, 4, NA, 3, NA, NA, NA,…\n$ ST275Q08WA   &lt;dbl&gt; 4, NA, 2, NA, 2, 2, 2, NA, NA, NA, 2, 1, 4, 2, NA, NA, NA…\n$ ST275Q09WA   &lt;dbl&gt; NA, 2, 1, NA, NA, 1, 4, 1, NA, 2, 4, NA, 3, 2, NA, 1, 1, …\n$ ST276Q01JA   &lt;dbl&gt; NA, NA, NA, 2, NA, NA, NA, NA, NA, 2, 2, 1, 2, 2, 1, NA, …\n$ ST276Q02JA   &lt;dbl&gt; NA, NA, NA, NA, 2, 2, 2, 1, NA, NA, 2, NA, NA, NA, NA, NA…\n$ ST276Q03JA   &lt;dbl&gt; 4, 3, NA, NA, 2, NA, 2, NA, NA, NA, NA, 1, NA, 3, 1, 2, 3…\n$ ST276Q04JA   &lt;dbl&gt; NA, 2, NA, NA, 2, 2, 2, 1, 1, 2, NA, NA, NA, NA, NA, 2, 2…\n$ ST276Q05JA   &lt;dbl&gt; NA, NA, 2, 2, NA, NA, 2, 1, 2, NA, NA, 1, NA, 2, NA, 2, N…\n$ ST276Q06JA   &lt;dbl&gt; NA, 3, 1, NA, 2, NA, NA, NA, 1, 2, 2, NA, 1, NA, 1, NA, 2…\n$ ST276Q07JA   &lt;dbl&gt; 3, 3, NA, 2, NA, 2, NA, NA, NA, NA, NA, NA, NA, NA, 2, 2,…\n$ ST276Q08JA   &lt;dbl&gt; 3, NA, 3, 2, 2, 2, 4, 4, 4, NA, 3, 1, 4, NA, NA, NA, 4, 4…\n$ ST276Q09JA   &lt;dbl&gt; 3, NA, 4, 2, NA, 2, NA, 4, 1, 3, 2, 1, 3, 2, 3, 4, NA, NA…\n$ ST276Q10JA   &lt;dbl&gt; 3, 4, 3, NA, NA, NA, NA, NA, NA, 2, NA, NA, 3, 2, NA, NA,…\n$ ST268Q01JA   &lt;dbl&gt; 2, 4, 3, 2, 3, 3, 4, 4, 2, 3, 3, 4, 3, 4, 1, 3, 2, 3, 2, …\n$ ST268Q02JA   &lt;dbl&gt; 3, 3, 2, 3, 4, 3, 3, 2, 2, 2, 2, 3, 2, 2, 4, 3, 2, 3, 2, …\n$ ST268Q03JA   &lt;dbl&gt; 3, 3, 3, 3, 4, 3, 4, 3, 3, 3, 3, 4, 2, 4, 2, 2, 2, 3, 3, …\n$ ST268Q04JA   &lt;dbl&gt; 2, 3, 3, 2, 1, 3, 4, 4, 2, 3, 4, 4, 3, 4, 1, 3, 2, 2, 2, …\n$ ST268Q05JA   &lt;dbl&gt; 3, 2, 2, 2, 3, 3, 4, 2, 2, 3, 2, 3, 2, 3, 2, 3, 2, 2, 2, …\n$ ST268Q06JA   &lt;dbl&gt; 3, 3, 2, 2, 2, 2, 3, 2, 2, 3, 3, 3, 2, 4, 1, 2, 2, 2, 3, …\n$ ST268Q07JA   &lt;dbl&gt; 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, …\n$ ST268Q08JA   &lt;dbl&gt; 3, 4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, …\n$ ST268Q09JA   &lt;dbl&gt; 3, 4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, …\n$ ST290Q01WA   &lt;dbl&gt; NA, NA, NA, 3, NA, NA, NA, 3, NA, 3, 4, 4, NA, 4, 2, NA, …\n$ ST290Q02WA   &lt;dbl&gt; 3, 3, 3, 3, 2, NA, NA, NA, 4, NA, 4, 4, 3, 4, NA, NA, NA,…\n$ ST290Q03WA   &lt;dbl&gt; NA, NA, NA, NA, NA, 3, 4, 3, NA, NA, NA, 4, NA, 4, 2, NA,…\n$ ST290Q04WA   &lt;dbl&gt; 3, 3, 2, NA, 2, NA, NA, NA, 3, 3, NA, 4, 2, NA, 2, 3, 2, …\n$ ST290Q05WA   &lt;dbl&gt; 4, 3, 4, NA, 2, 4, NA, NA, NA, 3, 4, NA, 4, NA, NA, NA, N…\n$ ST290Q06WA   &lt;dbl&gt; NA, NA, 3, NA, NA, 2, 4, NA, NA, NA, 4, NA, 4, 4, NA, 4, …\n$ ST290Q07WA   &lt;dbl&gt; NA, 3, 4, 2, NA, 4, 4, 4, 4, 3, NA, NA, 4, NA, 2, 4, 4, N…\n$ ST290Q08WA   &lt;dbl&gt; 2, NA, NA, 2, 2, NA, 3, 4, 4, NA, NA, NA, NA, 4, 2, 4, NA…\n$ ST290Q09WA   &lt;dbl&gt; 4, 3, NA, 2, 4, 4, 4, 4, 4, 3, 4, NA, NA, NA, NA, 4, 4, 4…\n$ ST291Q01JA   &lt;dbl&gt; 3, NA, 2, 2, NA, NA, 3, NA, 4, NA, 4, NA, 3, 4, NA, 4, NA…\n$ ST291Q02JA   &lt;dbl&gt; 3, 3, NA, NA, 2, NA, 3, 4, 4, 3, NA, NA, 4, NA, 2, NA, 2,…\n$ ST291Q03JA   &lt;dbl&gt; 3, 3, 2, 2, 2, 2, NA, 4, 4, NA, 4, NA, NA, NA, NA, NA, 2,…\n$ ST291Q04JA   &lt;dbl&gt; NA, NA, 2, NA, NA, 2, NA, NA, NA, NA, NA, 4, NA, NA, NA, …\n$ ST291Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, 4, 4, NA, NA, NA, 4, 2, 3, 2, 3, …\n$ ST291Q06JA   &lt;dbl&gt; NA, 3, 3, NA, 2, 2, 4, 4, 4, 2, 4, NA, NA, 4, 2, 3, NA, N…\n$ ST291Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, 3, NA, 4, 4, 2, NA, NA, 2, 3, 2, NA, …\n$ ST291Q08JA   &lt;dbl&gt; NA, 3, NA, 2, NA, 2, NA, NA, NA, NA, 3, 4, NA, 2, 2, 2, N…\n$ ST291Q09JA   &lt;dbl&gt; 3, 3, 1, 2, 2, NA, 3, NA, NA, 2, 3, 4, NA, NA, NA, NA, NA…\n$ ST291Q10JA   &lt;dbl&gt; 2, NA, NA, 2, 2, NA, NA, NA, NA, 3, NA, 4, 2, NA, NA, NA,…\n$ ST289Q01WA   &lt;dbl&gt; 2, NA, NA, NA, NA, NA, NA, NA, NA, NA, 5, NA, NA, 2, NA, …\n$ ST289Q02JA   &lt;dbl&gt; 5, NA, 5, NA, NA, NA, NA, NA, 5, NA, NA, 5, 3, 5, NA, NA,…\n$ ST289Q03WA   &lt;dbl&gt; NA, NA, NA, NA, 1, NA, NA, NA, NA, 1, NA, 5, 1, NA, 1, NA…\n$ ST289Q04JA   &lt;dbl&gt; NA, 5, 4, 1, NA, 4, 5, 5, 5, NA, NA, 5, NA, NA, NA, NA, N…\n$ ST289Q05WA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, 5, NA, NA, 4, NA, NA, NA, NA, 4, …\n$ ST289Q06JA   &lt;dbl&gt; NA, NA, NA, 4, NA, 5, NA, 5, 5, NA, 5, NA, NA, 5, NA, 5, …\n$ ST289Q07JA   &lt;dbl&gt; 5, NA, 3, NA, 4, 5, 5, 5, NA, NA, NA, 5, NA, NA, 4, NA, N…\n$ ST289Q08WA   &lt;dbl&gt; NA, 5, NA, NA, 3, NA, NA, 5, NA, 2, NA, NA, 1, NA, 4, 2, …\n$ ST289Q09WA   &lt;dbl&gt; 4, 5, NA, NA, NA, 5, 5, NA, NA, NA, 5, NA, 4, NA, NA, 5, …\n$ ST289Q10WA   &lt;dbl&gt; NA, 5, 5, 5, 4, 5, 5, NA, 5, 5, NA, NA, NA, 5, NA, NA, NA…\n$ ST289Q11WA   &lt;dbl&gt; 1, NA, NA, 4, 2, NA, NA, NA, NA, NA, 1, NA, NA, 2, NA, 2,…\n$ ST289Q14JA   &lt;dbl&gt; NA, 5, 2, 4, NA, NA, NA, 5, 5, 5, 4, 5, 1, NA, 2, NA, 3, …\n$ ST293Q01JA   &lt;dbl&gt; NA, NA, NA, 4, 5, 4, 3, NA, 5, 3, 4, 5, 3, NA, NA, NA, NA…\n$ ST293Q02JA   &lt;dbl&gt; NA, 5, 4, 5, 5, NA, NA, 3, NA, NA, NA, 5, 4, 2, 4, 3, 5, …\n$ ST293Q03JA   &lt;dbl&gt; 4, 5, 3, NA, NA, 5, NA, NA, NA, 3, 5, NA, 4, NA, NA, NA, …\n$ ST293Q04JA   &lt;dbl&gt; NA, NA, 3, NA, NA, NA, NA, 1, 1, NA, NA, NA, 2, NA, 4, NA…\n$ ST293Q05JA   &lt;dbl&gt; 3, 5, NA, NA, 5, 5, 1, 1, 5, 4, 5, 5, NA, 1, NA, NA, 3, 2…\n$ ST293Q06JA   &lt;dbl&gt; 2, NA, 4, NA, NA, NA, NA, NA, NA, NA, 5, 5, NA, 1, 5, 5, …\n$ ST293Q07JA   &lt;dbl&gt; 2, NA, NA, 3, NA, NA, 3, 3, 2, NA, NA, 1, NA, 4, NA, 3, N…\n$ ST293Q08JA   &lt;dbl&gt; NA, 4, NA, 5, 5, 5, 1, NA, 5, 4, NA, NA, 2, NA, 4, 5, 2, …\n$ ST293Q09JA   &lt;dbl&gt; 4, 2, 2, 5, 3, 3, 2, 5, NA, 3, 5, NA, NA, 5, 3, 4, NA, NA…\n$ ST292Q01JA   &lt;dbl&gt; 2, 2, 2, 2, 1, 2, 4, 4, 2, 2, 3, 2, 2, 3, 1, NA, 3, 2, 3,…\n$ ST292Q02JA   &lt;dbl&gt; 2, NA, 3, 2, 1, 4, 4, NA, 3, 2, 3, 2, 3, 3, 1, 3, 3, 3, 3…\n$ ST292Q03JA   &lt;dbl&gt; 3, 3, 3, 2, NA, NA, 4, 4, 3, 2, NA, 3, 2, 3, 1, 4, 3, 3, …\n$ ST292Q04JA   &lt;dbl&gt; 2, 3, 3, NA, 1, 4, 4, 4, 3, 2, 3, 3, 2, 3, 1, 4, 3, 3, 3,…\n$ ST292Q05JA   &lt;dbl&gt; NA, 1, 2, 2, 1, 4, 1, 4, NA, NA, 2, 2, 1, 3, NA, 4, NA, N…\n$ ST292Q06JA   &lt;dbl&gt; 3, 1, NA, 2, 1, 2, NA, 4, 3, 1, 3, NA, NA, NA, 1, 2, 1, 2…\n$ ST297Q01JA   &lt;dbl&gt; 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, …\n$ ST297Q03JA   &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, …\n$ ST297Q05JA   &lt;dbl&gt; 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, …\n$ ST297Q06JA   &lt;dbl&gt; 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, …\n$ ST297Q07JA   &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, …\n$ ST297Q09JA   &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, …\n$ ST334Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST334Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST334Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST334Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST334Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST334Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST334Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST334Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST334Q09JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST334Q10JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST335Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST335Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST335Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST335Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST335Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST335Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST336Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST336Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST336Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST336Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST336Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST336Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST337Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST337Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST337Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST337Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST337Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST337Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST337Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST337Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST338Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST338Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST338Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST338Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST338Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST338Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST338Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST338Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST339Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST339Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST340Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST340Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST340Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST340Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST340Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST340Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST340Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST340Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST340Q09JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST340Q10JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST341Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST341Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST341Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST341Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST341Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST342Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST342Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST342Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST342Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST342Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST342Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST342Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST300Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, 5, NA, 5, NA, NA, 3, 5, NA, NA, NA, N…\n$ ST300Q02JA   &lt;dbl&gt; 5, NA, 5, 3, NA, 5, 5, NA, 5, 5, 5, 5, NA, NA, NA, NA, 5,…\n$ ST300Q03JA   &lt;dbl&gt; NA, NA, NA, 3, NA, 5, NA, NA, NA, NA, NA, 5, 1, NA, 4, NA…\n$ ST300Q04JA   &lt;dbl&gt; NA, 2, 2, 3, 5, NA, NA, NA, 1, NA, 1, NA, 1, NA, 3, NA, N…\n$ ST300Q05JA   &lt;dbl&gt; 4, NA, NA, NA, NA, NA, 4, 5, 5, 5, NA, NA, NA, 1, 3, 1, 2…\n$ ST300Q06JA   &lt;dbl&gt; 2, 3, 2, NA, 5, NA, NA, 5, 5, 5, 3, NA, 1, NA, 2, 1, NA, …\n$ ST300Q07JA   &lt;dbl&gt; 4, 3, NA, NA, 5, NA, 3, 4, NA, 5, NA, NA, NA, 4, NA, 5, 4…\n$ ST300Q08JA   &lt;dbl&gt; NA, NA, 3, NA, 5, NA, NA, 5, 5, NA, 3, 5, 1, 4, NA, 1, 2,…\n$ ST300Q09JA   &lt;dbl&gt; NA, 3, NA, 3, 5, 5, 2, NA, NA, NA, NA, NA, 1, 4, NA, 5, N…\n$ ST300Q10JA   &lt;dbl&gt; 4, 5, 1, 3, NA, 5, 1, NA, NA, 5, NA, 5, NA, 4, 2, NA, 1, …\n$ ST327Q01JA   &lt;dbl&gt; 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST327Q02JA   &lt;dbl&gt; 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST327Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST327Q04JA   &lt;dbl&gt; 2, 1, 3, 3, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST327Q05JA   &lt;dbl&gt; 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 3, 3, 1, 1, 1, 2, 1, 1, 1, …\n$ ST327Q06JA   &lt;dbl&gt; 1, 1, 3, 3, 2, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST327Q07JA   &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 1, 1, 3, 3, 3, 1, 1, 1, 1, 1, NA, 3, 3,…\n$ ST327Q08JA   &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 1, 1, 3, 3, 3, 2, 1, 1, 1, 1, NA, 3, 3,…\n$ ST330Q01WA   &lt;dbl&gt; 3, NA, NA, 3, 3, 3, NA, 3, 3, NA, 3, 3, 3, 1, NA, NA, NA,…\n$ ST330Q02WA   &lt;dbl&gt; 2, NA, 1, NA, 3, NA, 3, NA, 3, 3, NA, NA, 1, 1, 2, NA, NA…\n$ ST330Q03WA   &lt;dbl&gt; NA, NA, 3, 3, NA, 3, 3, NA, NA, NA, NA, 1, 3, NA, 3, 1, 3…\n$ ST330Q04WA   &lt;dbl&gt; NA, 1, NA, NA, 3, 3, NA, 2, NA, 1, NA, NA, NA, NA, NA, 3,…\n$ ST330Q05WA   &lt;dbl&gt; NA, 1, NA, NA, NA, NA, 3, NA, NA, NA, NA, 2, NA, 3, 3, NA…\n$ ST330Q06WA   &lt;dbl&gt; NA, 1, NA, NA, 2, NA, 2, NA, 3, 1, 3, 1, NA, NA, 2, 1, NA…\n$ ST330Q07WA   &lt;dbl&gt; 2, NA, 1, 3, NA, NA, 2, 2, 2, 1, NA, NA, NA, NA, NA, NA, …\n$ ST330Q08WA   &lt;dbl&gt; NA, NA, 3, 3, NA, NA, NA, NA, NA, NA, 3, NA, 3, 1, NA, NA…\n$ ST330Q09WA   &lt;dbl&gt; NA, 1, NA, NA, 2, 3, NA, 2, NA, NA, 2, NA, NA, NA, NA, 1,…\n$ ST330Q11WA   &lt;dbl&gt; 2, 1, NA, 3, NA, 3, NA, NA, NA, 1, 2, NA, NA, 3, NA, NA, …\n$ ST330D10WA   &lt;chr&gt; \"7020003\", \"9999999\", \"7020003\", \"9999999\", \"9999999\", \"9…\n$ ST324Q02JA   &lt;dbl&gt; 3, 3, NA, NA, 4, NA, 3, 2, 3, 3, 2, 3, 4, 3, 3, 4, NA, NA…\n$ ST324Q04JA   &lt;dbl&gt; 3, NA, NA, NA, 4, 3, NA, NA, NA, NA, 3, 4, 3, NA, NA, 3, …\n$ ST324Q05JA   &lt;dbl&gt; NA, NA, 2, 2, NA, 4, NA, 2, 3, 1, 2, 4, NA, 3, 4, NA, 3, …\n$ ST324Q07JA   &lt;dbl&gt; 4, NA, 3, NA, 2, NA, 3, NA, NA, 3, NA, 2, NA, NA, 3, 4, 3…\n$ ST324Q10JA   &lt;dbl&gt; NA, 2, 2, 2, NA, 3, NA, 2, 1, 2, NA, NA, NA, 3, 3, NA, 3,…\n$ ST324Q11JA   &lt;dbl&gt; NA, 1, 2, NA, NA, 2, NA, NA, NA, NA, NA, NA, 2, NA, NA, N…\n$ ST324Q12JA   &lt;dbl&gt; 3, NA, 3, 2, NA, NA, 2, NA, 4, NA, NA, 3, NA, 3, NA, NA, …\n$ ST324Q13JA   &lt;dbl&gt; NA, 1, NA, 2, 4, 3, 1, 2, 4, 3, 3, NA, 3, NA, 3, 1, 2, 3,…\n$ ST324Q14JA   &lt;dbl&gt; 2, 3, NA, 2, 2, NA, 2, 3, NA, NA, 3, NA, 2, 3, NA, 2, 2, …\n$ ST347Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST347Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST348Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST348Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST348Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST348Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST348Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST348Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST348Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST348Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST349Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST350Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST351Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST351Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST351Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST351Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST351Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST351Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST351Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST351Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST352Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST352Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST352Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST352Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST352Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST352Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST352Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST352Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST353Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST353Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST353Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST353Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST353Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST353Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST353Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST353Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST354Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST354Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST354Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST354Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST354Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST354Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST354Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST354Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST354Q09JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST354Q10JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST355Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST355Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST355Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST355Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST355Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST355Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST355Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST355Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST356Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST331Q01JA   &lt;dbl&gt; 7, 8, 8, 8, 10, 6, 7, 10, 7, 8, 10, 9, 7, 6, 8, 4, 7, 8, …\n$ ST331Q02JA   &lt;dbl&gt; 8, 10, 9, 10, 10, 10, 8, 10, 10, 10, 10, 10, 9, 7, 10, 10…\n$ ST331Q03JA   &lt;dbl&gt; 8, 8, 7, 10, 10, 7, 8, 10, 9, 8, 9, 10, 10, 6, 10, 10, 8,…\n$ FL150Q01TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL150Q02TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL150Q03TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL164Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL164Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL164Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL164Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL164Q05HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL164Q06HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL164Q07HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL164Q08HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL164Q09HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL164Q10HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL164Q11HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL164Q12HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL164Q13HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL164Q14HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL164Q15HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL164Q16HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL166Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL166Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL166Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL166Q05HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL166Q06HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL166Q07HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL174Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL174Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL174Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL174Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL174Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL174Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL174Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL167Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL167Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL167Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL167Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL167Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL167Q05HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL167Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL170Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL170Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL170Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL170Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL170Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL170Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL170Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL159Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL159Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL159Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL159Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL160Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL160Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL160Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL160Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL161Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL161Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL161Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL162Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL162Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL162Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL162Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL162Q05HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL162Q06HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL163Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL163Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL163Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL163Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL163Q05HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL171Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL171Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL171Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL171Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL171Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL171Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL171Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL171Q09JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL171Q10JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL171Q11JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL171Q12JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL169Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL169Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL169Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL169Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL169Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL169Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL169Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL169Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL169Q10JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL169Q11JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL172Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL172Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL172Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL172Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ IC170Q01JA   &lt;dbl&gt; 4, 4, 1, 2, 5, 3, 5, 3, 4, 4, 5, 2, 1, 5, 4, 4, 4, 1, 3, …\n$ IC170Q02JA   &lt;dbl&gt; 4, 4, 1, 4, 5, 6, 5, 6, 2, 5, 5, 4, 1, 5, 5, 1, 3, 4, 5, …\n$ IC170Q03JA   &lt;dbl&gt; 1, 4, 5, 1, 6, 3, 1, 5, 1, 6, 6, 4, 4, 5, 5, 1, 1, 4, 1, …\n$ IC170Q04JA   &lt;dbl&gt; 4, 4, 5, 1, 5, 3, 5, 5, 1, 4, 5, 4, 5, 5, 5, 4, 1, 4, 5, …\n$ IC170Q05JA   &lt;dbl&gt; 2, 4, 1, 1, 4, 3, 1, 5, 2, 4, 5, 4, 5, 5, 1, 4, 1, 1, 5, …\n$ IC170Q06JA   &lt;dbl&gt; 2, 4, 5, 1, 6, 3, 3, 5, 3, 1, 3, 4, 2, 5, 5, 2, 2, 4, 1, …\n$ IC170Q07JA   &lt;dbl&gt; 4, 4, 5, 1, 5, 3, 3, 5, 5, 4, 5, 4, 4, 5, 5, 4, 5, 3, 3, …\n$ IC171Q01JA   &lt;dbl&gt; 4, 2, 4, 1, 4, 5, 5, 5, 4, 5, 5, 4, 3, 5, 2, 5, 1, 4, 5, …\n$ IC171Q02JA   &lt;dbl&gt; 5, 4, 4, 5, 4, 5, 5, 5, 4, 5, 5, 4, 4, 5, 5, 5, 5, 4, 5, …\n$ IC171Q03JA   &lt;dbl&gt; 1, 1, 4, 1, 1, 5, 1, 5, 4, 1, 6, 4, 4, 5, 5, 6, 1, 4, 1, …\n$ IC171Q04JA   &lt;dbl&gt; 5, 4, 4, 5, 4, 5, 5, 5, 4, 5, 5, 4, 4, 5, 5, 5, 5, 4, 5, …\n$ IC171Q05JA   &lt;dbl&gt; 2, 1, 3, 1, 1, 1, 4, 5, 3, 1, 4, 4, 2, 5, 5, 1, 1, 4, 3, …\n$ IC171Q06JA   &lt;dbl&gt; 4, 1, 5, 1, 1, 5, 5, 5, 4, 5, 4, 4, 1, 5, 2, 5, 1, 4, 3, …\n$ IC172Q01JA   &lt;dbl&gt; 3, 4, 4, 4, 4, 4, 3, 3, 4, 3, 3, 4, 3, 3, 4, 4, 3, 3, 3, …\n$ IC172Q02JA   &lt;dbl&gt; 3, 4, 4, 4, 4, 4, 3, 3, 4, 3, 3, 4, 3, 3, 4, 4, 3, 3, 2, …\n$ IC172Q03JA   &lt;dbl&gt; 3, 4, 3, 4, 3, 4, 1, 3, 3, 2, 4, 2, 3, 3, 4, 4, 3, 3, 2, …\n$ IC172Q04JA   &lt;dbl&gt; 3, 4, 3, 4, 3, 4, 3, 3, 4, 3, 3, 3, 3, 3, 4, 4, 3, 3, 2, …\n$ IC172Q05JA   &lt;dbl&gt; 3, 4, 3, 4, 4, 4, 3, 3, 4, 3, 3, 4, 3, 3, 4, 4, 3, 3, 3, …\n$ IC172Q06JA   &lt;dbl&gt; 3, 4, 4, 4, 4, 4, 2, 3, 4, 3, 3, 4, 4, 3, 4, 4, 3, 3, 2, …\n$ IC172Q07JA   &lt;dbl&gt; 3, 4, 3, 4, 4, 3, 3, 3, 4, 3, 3, 4, 3, 3, 4, 4, 3, 3, 3, …\n$ IC172Q08JA   &lt;dbl&gt; 3, 4, 3, 4, 4, 3, 2, 3, 3, 3, 3, 4, 2, 3, 4, 4, 3, 3, 3, …\n$ IC172Q09JA   &lt;dbl&gt; 3, 4, 4, 4, 4, 3, 4, 3, 4, 3, 3, 4, 3, 3, 4, 4, 3, 3, 2, …\n$ IC173Q01JA   &lt;dbl&gt; 3, 4, 4, 1, 5, 5, 3, 4, 5, 4, 3, 2, 3, 4, 5, 3, 5, 3, 2, …\n$ IC173Q02JA   &lt;dbl&gt; 1, 2, 5, 1, 1, 3, 5, 4, 3, 1, 1, 3, 1, 2, 2, 2, 2, 4, 1, …\n$ IC173Q03JA   &lt;dbl&gt; 2, 4, 4, 1, 5, 3, 3, 4, 2, 3, 3, 2, 3, 2, 2, 1, 2, 2, 1, …\n$ IC173Q04JA   &lt;dbl&gt; 6, 6, 6, 1, 6, 6, 6, 4, 6, 6, 6, 6, 6, 6, 6, 1, 6, 6, 6, …\n$ IC174Q01JA   &lt;dbl&gt; 2, 3, 1, 3, 3, 2, 1, 1, 2, 3, NA, 4, 1, 1, 3, 2, 2, 1, 1,…\n$ IC174Q02JA   &lt;dbl&gt; 4, 3, 4, 3, 5, 4, 3, 4, 4, 4, NA, 5, 4, 3, 1, 4, 4, 4, 4,…\n$ IC174Q03JA   &lt;dbl&gt; 3, 3, 3, 3, 5, 1, 1, 4, 3, 4, 2, 4, 1, 3, 4, 4, 2, 4, 4, …\n$ IC174Q04JA   &lt;dbl&gt; 2, 3, 4, 3, 3, 3, 3, 4, 3, 4, 2, 5, 3, 3, 2, 1, 2, 3, 1, …\n$ IC174Q05JA   &lt;dbl&gt; 1, 3, 1, 3, 3, 2, 1, 4, 3, 2, 2, 3, 1, 2, 1, 1, 2, 1, 1, …\n$ IC174Q06JA   &lt;dbl&gt; 1, 3, 1, 3, 5, 2, 1, 1, 3, 1, 3, 3, 1, 2, 1, 2, 2, 1, 1, …\n$ IC174Q07JA   &lt;dbl&gt; 2, 3, 1, 3, 5, 2, 5, 4, 3, 4, 3, 5, 4, 2, 1, 3, 2, 1, 1, …\n$ IC174Q08JA   &lt;dbl&gt; 2, 3, 3, 3, 3, 3, 4, 4, 3, 3, 3, 5, 3, 2, 5, 3, 3, 1, 1, …\n$ IC174Q09JA   &lt;dbl&gt; 2, 3, 3, 3, 3, 3, 3, 1, 3, 4, 3, 4, 1, 3, 1, 4, 3, 1, 1, …\n$ IC174Q10JA   &lt;dbl&gt; 2, 3, 3, 3, 3, 3, 1, 4, 3, 3, 1, 4, 1, 1, 1, 1, 2, 1, 1, …\n$ IC175Q01JA   &lt;dbl&gt; 2, 4, 4, 1, 5, 4, 3, 5, 4, 4, 3, 4, 3, 3, 4, 5, 1, 3, 3, …\n$ IC175Q02JA   &lt;dbl&gt; 2, 3, 2, 2, 5, 5, 3, 5, 4, 4, 3, 4, 1, 3, 1, 5, 1, 1, 3, …\n$ IC175Q03JA   &lt;dbl&gt; 2, 3, 1, 1, 1, 3, 1, 5, 2, 4, 1, 4, 1, 3, 4, 5, 1, 1, 2, …\n$ IC175Q05JA   &lt;dbl&gt; 1, 3, 1, 2, 2, 3, 1, 5, 3, 4, 1, 5, 3, 3, 4, 3, 1, 1, 3, …\n$ IC176Q01JA   &lt;dbl&gt; 3, 2, 5, 1, 5, 3, 3, 5, 3, 5, 3, 4, 1, 3, 4, 4, 3, 4, 3, …\n$ IC176Q02JA   &lt;dbl&gt; 3, 2, 4, 1, 5, 4, 1, 5, 5, 5, 5, 5, 4, 3, 5, 5, 3, 3, 3, …\n$ IC176Q03JA   &lt;dbl&gt; 4, 2, 3, 1, 5, 4, 1, 5, 4, 5, 4, 5, 1, 3, 5, 5, 3, 4, 3, …\n$ IC176Q04JA   &lt;dbl&gt; 4, 3, 5, 2, 5, 4, 3, 5, 5, 5, 5, 5, 5, 3, 5, 5, 3, 5, 4, …\n$ IC176Q05JA   &lt;dbl&gt; 4, 2, 4, 2, 5, 4, 3, 5, 5, 4, 4, 5, 3, 3, 3, 5, 3, 5, 3, …\n$ IC176Q06JA   &lt;dbl&gt; 3, 1, 2, 1, 5, 4, 4, 5, 5, 2, 3, 5, 1, 3, 3, 3, 4, 5, 1, …\n$ IC176Q07JA   &lt;dbl&gt; 4, 2, 4, 1, 5, 4, 5, 5, 5, 4, 5, 5, 4, 3, 5, 3, 4, 5, 5, …\n$ IC176Q08JA   &lt;dbl&gt; 4, 3, 1, 3, 5, 3, 4, 5, 4, 5, 5, 4, 1, 3, 5, 4, 4, 5, 3, …\n$ IC184Q01JA   &lt;dbl&gt; 3, 3, 1, 5, 1, 4, 1, 1, 3, 4, 4, 5, 1, 5, 1, 4, 3, 5, 1, …\n$ IC184Q02JA   &lt;dbl&gt; 4, 3, 3, 4, 1, 4, 1, 1, 3, 4, 4, 5, 1, 5, 4, 4, 3, 4, 3, …\n$ IC184Q03JA   &lt;dbl&gt; 1, 3, 3, 1, 1, 3, 3, 1, 3, 6, 6, 4, 3, 5, 1, 1, 3, 3, 1, …\n$ IC184Q04JA   &lt;dbl&gt; 1, 3, 1, 1, 1, 3, 6, 1, 2, 6, 6, 2, 6, 1, 1, 1, 3, 6, 1, …\n$ IC177Q01JA   &lt;dbl&gt; 3, 2, 4, 2, 4, 4, 4, 3, 1, 5, 3, 2, 1, 4, 4, 3, 4, 3, 1, …\n$ IC177Q02JA   &lt;dbl&gt; 2, 3, 2, 2, 6, 4, 1, 3, 2, 5, 3, 3, 2, 2, 4, 3, 3, 5, 3, …\n$ IC177Q03JA   &lt;dbl&gt; 3, 3, 3, 2, 4, 4, 3, 3, 2, 5, 3, 3, 4, 4, 5, 3, 2, 4, 3, …\n$ IC177Q04JA   &lt;dbl&gt; 2, 3, 2, 2, 4, 4, 3, 1, 2, 3, 2, 2, 1, 3, 5, 2, 1, 3, 2, …\n$ IC177Q05JA   &lt;dbl&gt; 2, 3, 2, 2, 4, 4, 2, 1, 2, 2, 2, 4, 2, 2, 5, 1, 4, 4, 1, …\n$ IC177Q06JA   &lt;dbl&gt; 2, 3, 2, 2, 4, 1, 2, 1, 2, 4, 2, 2, 1, 3, 5, 3, 1, 3, 2, …\n$ IC177Q07JA   &lt;dbl&gt; 1, 3, 1, 2, 5, 1, 1, 3, 1, 3, 1, 2, 1, 1, 5, 2, 1, 1, 1, …\n$ IC178Q01JA   &lt;dbl&gt; 3, 3, 5, 1, 4, 4, 6, 3, 1, 6, 3, 3, 1, 5, 5, 4, 4, 4, 1, …\n$ IC178Q02JA   &lt;dbl&gt; 2, 3, 2, 1, 6, 4, 1, 3, 2, 6, 3, 4, 3, 2, 5, 3, 4, 5, 3, …\n$ IC178Q03JA   &lt;dbl&gt; 3, 3, 2, 1, 4, 4, 2, 3, 2, 6, 3, 4, 5, 5, 5, 3, 1, 4, 2, …\n$ IC178Q04JA   &lt;dbl&gt; 2, 3, 2, 1, 4, 4, 2, 1, 2, 2, 2, 3, 1, 2, 5, 2, 1, 3, 2, …\n$ IC178Q05JA   &lt;dbl&gt; 2, 3, 1, 1, 4, 4, 1, 1, 2, 5, 1, 4, 2, 2, 5, 2, 1, 4, 1, …\n$ IC178Q06JA   &lt;dbl&gt; 3, 3, 2, 1, 4, 1, 1, 1, 2, 3, 3, 4, 1, 2, 5, 2, 1, 3, 2, …\n$ IC178Q07JA   &lt;dbl&gt; 1, 3, 1, 1, 1, 1, 1, 3, 1, 2, 1, 2, 1, 1, 5, 2, 1, 1, 1, …\n$ IC179Q01JA   &lt;dbl&gt; 1, 3, 2, 1, 2, 4, 2, 2, 3, 1, 2, 1, 1, 1, 2, 1, 2, 3, 3, …\n$ IC179Q02JA   &lt;dbl&gt; 1, 2, 2, 1, 2, 2, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 2, 3, 1, …\n$ IC179Q03JA   &lt;dbl&gt; 3, 3, 3, 4, 4, 4, 4, 3, 3, 4, 3, 4, 3, 4, 3, 4, 2, 2, 3, …\n$ IC179Q04JA   &lt;dbl&gt; 2, 3, 2, 1, 2, 4, 2, 2, 4, 1, 2, 4, 1, 3, 3, 3, 2, 2, 2, …\n$ IC179Q05JA   &lt;dbl&gt; 2, 3, 2, 1, 4, 4, 2, 2, 4, 1, 3, 4, 1, 3, 4, 3, 2, 2, 3, …\n$ IC179Q06JA   &lt;dbl&gt; 1, 3, 3, 2, 3, 4, 3, 3, 4, 1, 3, 1, 1, 1, 2, 1, 2, 2, 3, …\n$ IC180Q01JA   &lt;dbl&gt; 3, 2, 2, 2, 3, 4, 2, 2, 2, 3, 2, 2, 2, 3, 2, 1, 3, 2, 2, …\n$ IC180Q02JA   &lt;dbl&gt; 3, 3, 3, 2, 3, 2, 3, 3, 2, 3, 3, 4, 3, 3, 4, 4, 2, 3, 3, …\n$ IC180Q03JA   &lt;dbl&gt; 3, 3, 3, 2, 3, 4, 4, 3, 3, 3, 3, 4, 3, 3, 4, 4, 2, 3, 3, …\n$ IC180Q04JA   &lt;dbl&gt; 2, 2, 2, 2, 3, 1, 2, 3, 3, 3, 3, 4, 1, 1, 1, 2, 2, 3, 2, …\n$ IC180Q05JA   &lt;dbl&gt; 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 4, 1, 1, NA, 2, 2, 3, 3,…\n$ IC180Q06JA   &lt;dbl&gt; 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 4, 1, 1, 3, 2, 2, 3, 2, …\n$ IC180Q07JA   &lt;dbl&gt; 3, 2, 2, 2, 2, 4, 1, 3, 3, 3, 2, 2, 3, 2, 1, 4, 2, 3, 2, …\n$ IC180Q08JA   &lt;dbl&gt; 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 2, …\n$ IC181Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ IC181Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ IC181Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ IC181Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ IC182Q01JA   &lt;dbl&gt; 3, 3, 2, 4, 2, 2, 3, 2, 2, 3, 3, 4, 3, 3, 3, 4, 2, 3, 2, …\n$ IC182Q02JA   &lt;dbl&gt; 3, 2, 3, 4, 2, 2, 4, 2, 2, 2, 3, 3, 3, 2, 3, 3, 2, 3, 3, …\n$ IC182Q03JA   &lt;dbl&gt; 3, 3, 3, 4, 3, 3, 4, 3, 3, 3, 3, 4, 3, 3, 4, 4, 2, 3, 3, …\n$ IC183Q01JA   &lt;dbl&gt; 4, 4, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, …\n$ IC183Q02JA   &lt;dbl&gt; 4, 4, 5, 3, 4, 4, 3, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 3, 4, …\n$ IC183Q03JA   &lt;dbl&gt; 4, 4, 2, 3, 4, 4, 3, 4, 4, 3, 3, 4, 3, 4, 4, 4, 4, 4, 4, …\n$ IC183Q04JA   &lt;dbl&gt; 4, 4, 4, 3, 4, 4, 2, 4, 4, 4, 4, 4, 3, 4, 2, 4, 4, 3, 4, …\n$ IC183Q05JA   &lt;dbl&gt; 4, 4, 3, 3, 4, 3, 3, 1, 4, 4, 3, 4, 3, 4, 3, 4, 3, 3, 4, …\n$ IC183Q07JA   &lt;dbl&gt; 4, 4, 3, 3, 4, 4, 3, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, …\n$ IC183Q08JA   &lt;dbl&gt; 4, 4, 2, 3, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 2, 4, 4, 3, 4, …\n$ IC183Q09JA   &lt;dbl&gt; 4, 4, 1, 3, 4, 4, 2, 4, 4, 4, 3, 4, 2, 4, 3, 4, 4, 3, 4, …\n$ IC183Q10JA   &lt;dbl&gt; 4, 2, 1, 3, 4, 4, 2, 4, 3, 2, 3, 4, 1, 4, 2, 4, 1, 4, 2, …\n$ IC183Q12JA   &lt;dbl&gt; 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, …\n$ IC183Q13JA   &lt;dbl&gt; 4, 4, 2, 3, 4, 4, 4, 1, 4, 3, 3, 4, 2, 1, 4, 4, 1, 3, 4, …\n$ IC183Q14JA   &lt;dbl&gt; 2, 4, 1, 3, 5, 2, 2, 1, 2, 5, 2, 2, 1, 1, 1, 2, 1, 2, 1, …\n$ IC183Q15JA   &lt;dbl&gt; 2, 4, 1, 3, 5, 3, 1, 4, 2, 5, 3, 3, 1, 1, 1, 4, 1, 2, 1, …\n$ IC183Q16JA   &lt;dbl&gt; 3, 4, 3, 3, 5, 3, 2, 4, 3, 5, 3, 3, 1, 1, 1, 4, 1, 2, 1, …\n$ WB150Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB151Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB152Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB153Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB153Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB153Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB153Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB153Q05HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB154Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB154Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB154Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB154Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB154Q05HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB154Q06HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB154Q07HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB154Q08HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB154Q09HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB155Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB155Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB155Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB155Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB155Q05HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB155Q06HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB155Q07HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB155Q08HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB155Q09HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB155Q10HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB156Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB158Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB160Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB161Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB162Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB162Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB162Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB162Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB162Q05HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB162Q06HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB162Q07HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB162Q08HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB162Q09HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB163Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB163Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB163Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB163Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB163Q05HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB163Q06HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB163Q07HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB163Q08HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB164Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB165Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB166Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB166Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB166Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB166Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB167Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB168Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB168Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB168Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB168Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB171Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB171Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB171Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB171Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB172Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB173Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB173Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB173Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB173Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB176Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB177Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB177Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB177Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB177Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB032Q01NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB032Q02NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB031Q01NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB178Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB178Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB178Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB178Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB178Q05HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB178Q06HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB178Q07HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA001Q01TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA001Q02TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA001Q03TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA003Q01TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA003Q02TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA003Q03TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA003Q05IA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA003Q18WA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA003Q19WA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA003Q20WA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA003Q11JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA003Q12JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA003Q13JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA003Q14JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA003Q15JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA003Q16JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA003Q17JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA196Q01WA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA196Q02WA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA196Q03WA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA196Q04WA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA197Q01WA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA197Q02WA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA197Q03WA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA197Q04WA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA197Q05WA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA008Q01TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA008Q02TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA008Q03TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA008Q04TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA008Q05TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA008Q06NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA008Q07NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA008Q08NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA008Q09NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA008Q10NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA009Q01NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA009Q02NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA009Q03NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA009Q04NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA009Q05NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA009Q06NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA009Q07NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA009Q08NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA009Q09NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA009Q10NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA009Q11NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA007Q01TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA007Q02TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA007Q03TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA007Q04TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA007Q05TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA007Q06TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA007Q07TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA007Q09NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA007Q11NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA007Q12NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA007Q13NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA007Q14NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA007Q15NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA005Q01TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA006Q01TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA006Q02TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA006Q03TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA006Q04TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA006Q05TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA006Q06TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA006Q07TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA006Q08TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA006Q09TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA006Q10TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA006Q11TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA006Q12HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA006Q13HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA006Q14HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA166Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA167Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA167Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA167Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA167Q05HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA183Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA183Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA183Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA183Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA183Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA183Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA183Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA183Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA018Q01NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA018Q02NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA018Q03NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA177Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA177Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA177Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA177Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA177Q05HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA177Q06HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA177Q07HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA177Q08HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA180Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA182Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA175Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA175Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA175Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA175Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA185Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA185Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA185Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA185Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA185Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA185Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA185Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA185Q09JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA185Q10JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA186Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA186Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA186Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA186Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA186Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA186Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA186Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA186Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA187Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA187Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA188Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA188Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA188Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA188Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA188Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA188Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA188Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA188Q09JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA188Q10JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA189Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA189Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA189Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA189Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA189Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA189Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA189Q09JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA189Q10JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA194Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA195Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA041Q01TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA042Q01TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ EFFORT1      &lt;dbl&gt; 7, 8, 7, NA, 10, NA, 7, 10, 7, 7, 10, 9, 7, 6, 8, NA, 6, …\n$ EFFORT2      &lt;dbl&gt; 9, 8, 8, NA, 10, NA, 8, 10, 10, 10, 10, 10, 10, 7, 10, NA…\n$ OCOD1        &lt;chr&gt; \"9701\", \"31\", \"9701\", \"41\", \"23\", \"9701\", \"11\", \"23\", \"1\"…\n$ OCOD2        &lt;chr&gt; \"83\", \"21\", \"9704\", \"9705\", \"83\", \"34\", \"31\", \"21\", \"14\",…\n$ OCOD3        &lt;chr&gt; \"2634\", \"9705\", \"9704\", \"2411\", \"21\", \"22\", \"2512\", \"2310…\n$ PROGN        &lt;chr&gt; \"07020002\", \"07020002\", \"07020002\", \"07020002\", \"07020002…\n$ AGE          &lt;dbl&gt; 15.50, 15.83, 15.75, 16.17, 15.58, 15.58, 16.08, 16.00, 1…\n$ GRADE        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ ISCEDP       &lt;dbl&gt; 344, 344, 344, 344, 344, 344, 344, 344, 344, 344, 344, 34…\n$ IMMIG        &lt;dbl&gt; 1, 1, 1, 1, 1, 3, 1, 3, 1, 1, 1, 1, 1, 3, 1, 2, 3, 3, 1, …\n$ COBN_S       &lt;chr&gt; \"070200\", \"070200\", \"070200\", \"070200\", \"070200\", \"970200…\n$ COBN_M       &lt;chr&gt; \"070200\", \"070200\", \"970200\", \"070200\", \"070200\", \"970200…\n$ COBN_F       &lt;chr&gt; \"070200\", \"070200\", \"070200\", \"070200\", \"070200\", \"970200…\n$ LANGN        &lt;dbl&gt; 998, 998, 998, 998, 998, 998, 998, 998, 998, 998, 998, 99…\n$ REPEAT       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ MISSSC       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, …\n$ SKIPPING     &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, …\n$ TARDYSD      &lt;dbl&gt; 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ EXERPRAC     &lt;dbl&gt; 1, 4, 2, 5, 9, 1, 2, 0, 3, 5, 1, 2, 5, 2, 4, 0, 2, 3, 2, …\n$ STUDYHMW     &lt;dbl&gt; 4, 7, 3, 5, 7, 10, 0, 10, 5, 3, 5, 5, 10, 0, 8, 5, 5, 5, …\n$ WORKPAY      &lt;dbl&gt; 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ WORKHOME     &lt;dbl&gt; 10, 2, 0, 10, 5, 5, 7, 0, 0, 4, 2, 2, 10, 0, 10, 0, 5, 5,…\n$ EXPECEDU     &lt;dbl&gt; 7, 7, 6, NA, 6, 7, 9, 9, 7, 6, 7, 8, 9, 9, 9, 9, 7, 7, 7,…\n$ MATHPREF     &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, …\n$ MATHEASE     &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, …\n$ MATHMOT      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ DURECEC      &lt;dbl&gt; 3, 2, NA, 3, NA, 4, 3, NA, NA, 3, NA, 5, NA, 5, 3, 2, NA,…\n$ BSMJ         &lt;dbl&gt; 85.85, NA, NA, 76.65, 79.49, 76.98, 74.66, 85.41, 24.79, …\n$ SISCO        &lt;dbl&gt; 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ RELATST      &lt;dbl&gt; -0.2606, 1.2437, 0.7190, -0.2194, 1.0726, 1.1296, -1.0841…\n$ BELONG       &lt;dbl&gt; 0.2442, -0.0437, -0.6137, -1.1147, 2.1143, 0.5159, -0.718…\n$ BULLIED      &lt;dbl&gt; -1.2280, -0.2016, -1.2280, -1.2280, -1.2280, -1.2280, 1.0…\n$ FEELSAFE     &lt;dbl&gt; -0.7560, 1.1246, 0.4417, -0.7560, 1.1246, 0.1413, 0.0936,…\n$ SCHRISK      &lt;dbl&gt; -0.6386, 0.1810, -0.6386, -0.6386, -0.6386, 0.1810, 0.181…\n$ PERSEVAGR    &lt;dbl&gt; 0.4369, 0.4540, -0.4017, 0.5617, -0.5954, -0.1554, 0.4375…\n$ CURIOAGR     &lt;dbl&gt; 2.7951, 0.3058, -0.6563, 0.1778, 0.3406, 0.3247, 0.0134, …\n$ COOPAGR      &lt;dbl&gt; -0.0319, 0.1187, -0.6986, 0.0849, 0.8806, 4.8203, -0.7118…\n$ EMPATAGR     &lt;dbl&gt; 1.3979, 0.1290, -0.2087, -0.1344, -0.5172, -0.5881, -1.26…\n$ ASSERAGR     &lt;dbl&gt; -0.2970, -0.1734, -0.4816, -0.1538, 0.2470, -0.8113, -0.1…\n$ STRESAGR     &lt;dbl&gt; 0.9777, -0.7402, -0.2053, 0.2201, -0.9940, 0.2183, 0.4922…\n$ EMOCOAGR     &lt;dbl&gt; 1.2321, -0.5609, 0.5777, -0.0563, -0.8597, NA, 0.8646, 1.…\n$ GROSAGR      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ INFOSEEK     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FAMSUP       &lt;dbl&gt; -0.3780, -0.5969, -1.0537, -0.8521, 1.7459, 1.7327, -0.88…\n$ DISCLIM      &lt;dbl&gt; 0.3884, 1.1687, 0.2002, -0.1219, 0.1166, -0.1006, -0.8709…\n$ TEACHSUP     &lt;dbl&gt; -0.5635, 0.1475, -0.5635, -0.1002, 1.5558, 0.1475, -0.332…\n$ COGACRCO     &lt;dbl&gt; -0.4092, -0.7102, -0.6541, -0.0820, 0.5051, 0.2635, 0.142…\n$ COGACMCO     &lt;dbl&gt; 0.1666, 0.0024, -0.5604, 0.1868, 2.3426, 0.7819, -0.5560,…\n$ EXPOFA       &lt;dbl&gt; 0.3356, -1.2147, 0.6908, -0.2194, 0.5478, 0.5515, -0.1287…\n$ EXPO21ST     &lt;dbl&gt; -0.7746, -0.5240, 0.0949, 0.4959, 0.4121, 0.4936, 0.2869,…\n$ MATHEFF      &lt;dbl&gt; 0.1429, -0.2874, 0.2226, -0.9344, -0.9322, 0.2637, 1.2000…\n$ MATHEF21     &lt;dbl&gt; 0.4317, 0.7644, -0.4779, -0.5515, -0.5730, -0.2489, 1.401…\n$ FAMCON       &lt;dbl&gt; 1.3180, 4.7588, 0.4736, 0.0594, 0.8973, 2.2322, 3.6192, 4…\n$ ANXMAT       &lt;dbl&gt; 0.3729, 0.6647, 0.0510, 0.6387, 2.5026, -0.7358, -0.9755,…\n$ MATHPERS     &lt;dbl&gt; -0.1305, 0.6178, -0.3993, 1.8158, 1.1353, 0.9918, -1.5183…\n$ CREATEFF     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ CREATSCH     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ CREATFAM     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ CREATAS      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ CREATOOS     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ CREATOP      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ OPENART      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ IMAGINE      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ SCHSUST      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ LEARRES      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PROBSELF     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FAMSUPSL     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FEELLAH      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ SDLEFF       &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ MISCED       &lt;dbl&gt; 8, 7, 4, 6, 7, 7, 6, 9, 8, 8, 4, 9, 10, 6, 4, 9, 8, 8, 6,…\n$ FISCED       &lt;dbl&gt; 7, 7, 4, 6, 7, 9, 2, 8, 8, 7, 4, 9, 10, 9, 6, 4, 9, 8, 6,…\n$ HISCED       &lt;dbl&gt; 8, 7, 4, 6, 7, 9, 6, 9, 8, 8, 4, 9, 10, 9, 6, 9, 9, 8, 6,…\n$ PAREDINT     &lt;dbl&gt; 16.0, 14.5, 12.0, 12.0, 14.5, 16.0, 12.0, 16.0, 16.0, 16.…\n$ BMMJ1        &lt;dbl&gt; 17.00, 37.83, 17.00, 43.33, 75.54, 17.00, 70.34, 75.54, 6…\n$ BFMJ2        &lt;dbl&gt; 30.34, 77.10, NA, NA, 30.34, 57.64, 40.54, 80.78, 43.85, …\n$ HISEI        &lt;dbl&gt; 30.34, 77.10, 17.00, 43.33, 75.54, 57.64, 70.34, 80.78, 6…\n$ ICTRES       &lt;dbl&gt; 0.1940, 0.6249, -0.3987, -0.9028, 0.2514, -0.4733, 0.9904…\n$ HOMEPOS      &lt;dbl&gt; 0.7524, 0.7842, 0.0666, -0.9300, -0.8949, -0.5988, 0.0975…\n$ ESCS         &lt;dbl&gt; 0.1836, 0.8261, -1.0357, -0.9606, 0.0856, 0.1268, -0.0154…\n$ FCFMLRTY     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FLSCHOOL     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FLMULTSB     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FLFAMILY     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ACCESSFP     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FLCONFIN     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FLCONICT     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ACCESSFA     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ATTCONFM     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FRINFLFM     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ICTSCH       &lt;dbl&gt; 0.4062, 0.4062, 0.4062, 0.4062, -1.6647, -0.8411, 0.4062,…\n$ ICTAVSCH     &lt;dbl&gt; 7, 7, 7, 7, 5, 6, 7, 6, 7, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, …\n$ ICTHOME      &lt;dbl&gt; 0.3346, 0.3346, 0.3346, 0.3346, 0.3346, 0.3346, 0.3346, 0…\n$ ICTAVHOM     &lt;dbl&gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6, 6, 6, 5, 6, 6, 6, …\n$ ICTQUAL      &lt;dbl&gt; 0.3623, 2.8889, 1.2313, 2.8889, 1.8870, 1.6615, -0.2086, …\n$ ICTSUBJ      &lt;dbl&gt; -0.1315, 0.5048, 0.9454, -2.0101, 0.6775, 0.6012, 0.5918,…\n$ ICTENQ       &lt;dbl&gt; -0.3767, 0.3109, -0.1658, 0.3109, 1.0316, -0.0477, -0.021…\n$ ICTFEED      &lt;dbl&gt; -0.4038, 0.4297, -0.4292, -0.6744, 0.2776, 0.9257, -0.354…\n$ ICTOUT       &lt;dbl&gt; 0.2260, -0.8080, 0.1088, -1.2894, 2.9804, 0.3464, -0.4834…\n$ ICTWKDY      &lt;dbl&gt; -0.4469, 0.4182, -0.3710, -0.5032, 1.3145, 0.4565, -0.250…\n$ ICTWKEND     &lt;dbl&gt; -0.3452, 0.3311, -0.7926, -3.5000, 0.8948, 0.4976, -1.085…\n$ ICTREG       &lt;dbl&gt; -0.1855, 0.9444, 0.2941, -0.6148, 0.8255, 1.9301, 0.3353,…\n$ ICTINFO      &lt;dbl&gt; 0.2929, -0.4797, 0.0811, -0.8360, 0.4191, 0.2147, 0.1939,…\n$ ICTDISTR     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ICTEFFIC     &lt;dbl&gt; 0.5764, 0.7781, -0.8446, -0.5172, 1.0613, 0.3941, -0.5344…\n$ STUBMI       &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ BODYIMA      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ SOCONPA      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ LIFESAT      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PSYCHSYM     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ SOCCON       &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ EXPWB        &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ CURSUPP      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PQMIMP       &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PQMCAR       &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PARINVOL     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PQSCHOOL     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PASCHPOL     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ATTIMMP      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PAREXPT      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ CREATHME     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ CREATACT     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ CREATOPN     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ CREATOR      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ W_FSTUWT     &lt;dbl&gt; 5.35943, 6.74318, 6.77500, 5.43200, 5.82353, 5.69931, 4.9…\n$ W_FSTURWT1   &lt;dbl&gt; 8.20073, 9.88533, 10.16250, 7.98824, 9.00000, 2.68850, 2.…\n$ W_FSTURWT2   &lt;dbl&gt; 2.63139, 10.36607, 10.16250, 2.66275, 8.48571, 8.76685, 2…\n$ W_FSTURWT3   &lt;dbl&gt; 8.03388, 10.33466, 10.16250, 8.31429, 2.82857, 8.34850, 2…\n$ W_FSTURWT4   &lt;dbl&gt; 8.18690, 3.44489, 3.38750, 7.98824, 9.00000, 8.40156, 7.3…\n$ W_FSTURWT5   &lt;dbl&gt; 7.89416, 3.29511, 3.38750, 2.56226, 8.48571, 2.89878, 7.5…\n$ W_FSTURWT6   &lt;dbl&gt; 8.04452, 3.44489, 3.38750, 2.56226, 8.48571, 9.60179, 2.4…\n$ W_FSTURWT7   &lt;dbl&gt; 2.67796, 3.29830, 3.38750, 2.66275, 9.00000, 2.92228, 7.5…\n$ W_FSTURWT8   &lt;dbl&gt; 2.72897, 10.36607, 10.16250, 2.66275, 3.00000, 8.76685, 7…\n$ W_FSTURWT9   &lt;dbl&gt; 7.74437, 3.29511, 3.38750, 7.98824, 2.82857, 3.05511, 2.4…\n$ W_FSTURWT10  &lt;dbl&gt; 2.67796, 9.89489, 10.16250, 2.66275, 8.48571, 2.89878, 2.…\n$ W_FSTURWT11  &lt;dbl&gt; 2.73358, 3.29830, 3.38750, 8.31429, 2.82857, 3.02482, 7.3…\n$ W_FSTURWT12  &lt;dbl&gt; 8.18690, 9.88533, 10.16250, 2.66275, 2.82857, 2.80052, 7.…\n$ W_FSTURWT13  &lt;dbl&gt; 8.20073, 10.33466, 10.16250, 7.68679, 9.00000, 8.02740, 7…\n$ W_FSTURWT14  &lt;dbl&gt; 2.58146, 10.36607, 10.16250, 7.68679, 8.48571, 8.34850, 7…\n$ W_FSTURWT15  &lt;dbl&gt; 2.68151, 9.89489, 10.16250, 7.98824, 3.00000, 2.58510, 2.…\n$ W_FSTURWT16  &lt;dbl&gt; 2.63139, 3.45536, 3.38750, 8.31429, 2.82857, 8.40156, 7.5…\n$ W_FSTURWT17  &lt;dbl&gt; 2.73358, 3.45536, 3.38750, 2.66275, 3.00000, 8.69635, 2.4…\n$ W_FSTURWT18  &lt;dbl&gt; 7.89416, 9.88533, 10.16250, 2.77143, 3.00000, 2.89878, 7.…\n$ W_FSTURWT19  &lt;dbl&gt; 2.72897, 3.29830, 3.38750, 7.68679, 9.00000, 2.78283, 2.5…\n$ W_FSTURWT20  &lt;dbl&gt; 8.03388, 3.44489, 3.38750, 2.77143, 3.00000, 8.34850, 2.5…\n$ W_FSTURWT21  &lt;dbl&gt; 7.89416, 9.88533, 3.38750, 2.66275, 9.00000, 2.92228, 2.4…\n$ W_FSTURWT22  &lt;dbl&gt; 2.63139, 10.36607, 3.38750, 7.98824, 8.48571, 8.06550, 2.…\n$ W_FSTURWT23  &lt;dbl&gt; 8.03388, 10.33466, 3.38750, 2.77143, 2.82857, 7.73009, 2.…\n$ W_FSTURWT24  &lt;dbl&gt; 7.88647, 3.44489, 10.16250, 2.66275, 9.00000, 9.16534, 7.…\n$ W_FSTURWT25  &lt;dbl&gt; 7.89416, 3.29511, 10.16250, 7.68679, 8.48571, 2.89878, 7.…\n$ W_FSTURWT26  &lt;dbl&gt; 8.36312, 3.44489, 10.16250, 7.68679, 8.48571, 8.76685, 2.…\n$ W_FSTURWT27  &lt;dbl&gt; 2.67796, 3.29830, 10.16250, 7.98824, 9.00000, 2.92228, 7.…\n$ W_FSTURWT28  &lt;dbl&gt; 2.62882, 10.36607, 3.38750, 7.98824, 3.00000, 8.76685, 7.…\n$ W_FSTURWT29  &lt;dbl&gt; 8.03388, 3.29511, 10.16250, 2.66275, 2.82857, 2.80052, 2.…\n$ W_FSTURWT30  &lt;dbl&gt; 2.78196, 9.89489, 3.38750, 7.98824, 8.48571, 2.89878, 2.4…\n$ W_FSTURWT31  &lt;dbl&gt; 2.73358, 3.29830, 10.16250, 2.77143, 2.82857, 2.78283, 7.…\n$ W_FSTURWT32  &lt;dbl&gt; 8.18690, 9.88533, 3.38750, 7.98824, 2.82857, 2.80052, 7.5…\n$ W_FSTURWT33  &lt;dbl&gt; 8.20073, 10.33466, 3.38750, 2.56226, 9.00000, 8.02740, 7.…\n$ W_FSTURWT34  &lt;dbl&gt; 2.67796, 10.36607, 3.38750, 2.56226, 8.48571, 8.34850, 7.…\n$ W_FSTURWT35  &lt;dbl&gt; 2.68151, 9.89489, 3.38750, 2.66275, 3.00000, 2.80052, 2.5…\n$ W_FSTURWT36  &lt;dbl&gt; 2.73358, 3.45536, 10.16250, 2.77143, 2.82857, 8.40156, 7.…\n$ W_FSTURWT37  &lt;dbl&gt; 2.63139, 3.45536, 10.16250, 7.98824, 3.00000, 9.48693, 2.…\n$ W_FSTURWT38  &lt;dbl&gt; 7.89416, 9.88533, 3.38750, 8.31429, 3.00000, 2.89878, 7.3…\n$ W_FSTURWT39  &lt;dbl&gt; 2.62882, 3.29830, 10.16250, 2.56226, 9.00000, 2.78283, 2.…\n$ W_FSTURWT40  &lt;dbl&gt; 8.03388, 3.44489, 10.16250, 8.31429, 3.00000, 9.07446, 2.…\n$ W_FSTURWT41  &lt;dbl&gt; 8.20073, 9.88533, 3.38750, 2.66275, 2.82857, 9.07446, 7.5…\n$ W_FSTURWT42  &lt;dbl&gt; 2.63139, 10.36607, 3.38750, 7.98824, 3.00000, 2.78283, 7.…\n$ W_FSTURWT43  &lt;dbl&gt; 8.03388, 10.33466, 3.38750, 2.77143, 9.00000, 2.92228, 7.…\n$ W_FSTURWT44  &lt;dbl&gt; 8.18690, 3.44489, 10.16250, 2.66275, 2.82857, 2.89878, 2.…\n$ W_FSTURWT45  &lt;dbl&gt; 7.89416, 3.29511, 10.16250, 7.68679, 3.00000, 8.40156, 2.…\n$ W_FSTURWT46  &lt;dbl&gt; 8.04452, 3.44489, 10.16250, 7.68679, 3.00000, 2.57670, 7.…\n$ W_FSTURWT47  &lt;dbl&gt; 2.67796, 3.29830, 10.16250, 7.98824, 2.82857, 8.34850, 2.…\n$ W_FSTURWT48  &lt;dbl&gt; 2.72897, 10.36607, 3.38750, 7.98824, 8.48571, 2.78283, 2.…\n$ W_FSTURWT49  &lt;dbl&gt; 7.74437, 3.29511, 10.16250, 2.66275, 9.00000, 8.02740, 7.…\n$ W_FSTURWT50  &lt;dbl&gt; 2.67796, 9.89489, 3.38750, 7.98824, 3.00000, 8.40156, 7.5…\n$ W_FSTURWT51  &lt;dbl&gt; 2.73358, 3.29830, 10.16250, 2.77143, 9.00000, 8.06550, 2.…\n$ W_FSTURWT52  &lt;dbl&gt; 8.18690, 9.88533, 3.38750, 7.98824, 9.00000, 8.69635, 2.4…\n$ W_FSTURWT53  &lt;dbl&gt; 8.20073, 10.33466, 3.38750, 2.56226, 2.82857, 3.05511, 2.…\n$ W_FSTURWT54  &lt;dbl&gt; 2.58146, 10.36607, 3.38750, 2.56226, 3.00000, 2.92228, 2.…\n$ W_FSTURWT55  &lt;dbl&gt; 2.68151, 9.89489, 3.38750, 2.66275, 8.48571, 9.48693, 7.3…\n$ W_FSTURWT56  &lt;dbl&gt; 2.63139, 3.45536, 10.16250, 2.77143, 9.00000, 2.89878, 2.…\n$ W_FSTURWT57  &lt;dbl&gt; 2.73358, 3.45536, 10.16250, 7.98824, 8.48571, 2.80052, 7.…\n$ W_FSTURWT58  &lt;dbl&gt; 7.89416, 9.88533, 3.38750, 8.31429, 8.48571, 8.40156, 2.5…\n$ W_FSTURWT59  &lt;dbl&gt; 2.72897, 3.29830, 10.16250, 2.56226, 2.82857, 8.76685, 7.…\n$ W_FSTURWT60  &lt;dbl&gt; 8.03388, 3.44489, 10.16250, 8.31429, 8.48571, 2.92228, 7.…\n$ W_FSTURWT61  &lt;dbl&gt; 7.89416, 9.88533, 10.16250, 7.98824, 2.82857, 8.34850, 7.…\n$ W_FSTURWT62  &lt;dbl&gt; 2.63139, 10.36607, 10.16250, 2.66275, 3.00000, 3.02482, 7…\n$ W_FSTURWT63  &lt;dbl&gt; 8.03388, 10.33466, 10.16250, 8.31429, 9.00000, 3.20060, 7…\n$ W_FSTURWT64  &lt;dbl&gt; 7.88647, 3.44489, 3.38750, 7.98824, 2.82857, 2.67580, 2.5…\n$ W_FSTURWT65  &lt;dbl&gt; 7.89416, 3.29511, 3.38750, 2.56226, 3.00000, 8.40156, 2.4…\n$ W_FSTURWT66  &lt;dbl&gt; 8.36312, 3.44489, 3.38750, 2.56226, 3.00000, 2.78283, 7.5…\n$ W_FSTURWT67  &lt;dbl&gt; 2.67796, 3.29830, 3.38750, 2.66275, 2.82857, 8.34850, 2.4…\n$ W_FSTURWT68  &lt;dbl&gt; 2.62882, 10.36607, 10.16250, 2.66275, 8.48571, 2.78283, 2…\n$ W_FSTURWT69  &lt;dbl&gt; 8.03388, 3.29511, 3.38750, 7.98824, 9.00000, 8.69635, 7.5…\n$ W_FSTURWT70  &lt;dbl&gt; 2.78196, 9.89489, 10.16250, 2.66275, 3.00000, 8.40156, 7.…\n$ W_FSTURWT71  &lt;dbl&gt; 2.73358, 3.29830, 3.38750, 8.31429, 9.00000, 8.76685, 2.5…\n$ W_FSTURWT72  &lt;dbl&gt; 8.18690, 9.88533, 10.16250, 2.66275, 9.00000, 8.69635, 2.…\n$ W_FSTURWT73  &lt;dbl&gt; 8.20073, 10.33466, 10.16250, 7.68679, 2.82857, 3.05511, 2…\n$ W_FSTURWT74  &lt;dbl&gt; 2.67796, 10.36607, 10.16250, 7.68679, 3.00000, 2.92228, 2…\n$ W_FSTURWT75  &lt;dbl&gt; 2.68151, 9.89489, 10.16250, 7.98824, 8.48571, 8.69635, 7.…\n$ W_FSTURWT76  &lt;dbl&gt; 2.73358, 3.45536, 3.38750, 8.31429, 9.00000, 2.89878, 2.4…\n$ W_FSTURWT77  &lt;dbl&gt; 2.63139, 3.45536, 3.38750, 2.66275, 8.48571, 2.58510, 7.5…\n$ W_FSTURWT78  &lt;dbl&gt; 7.89416, 9.88533, 10.16250, 2.77143, 8.48571, 8.40156, 2.…\n$ W_FSTURWT79  &lt;dbl&gt; 2.62882, 3.29830, 3.38750, 7.68679, 2.82857, 8.76685, 7.3…\n$ W_FSTURWT80  &lt;dbl&gt; 8.03388, 3.44489, 3.38750, 2.77143, 8.48571, 2.68850, 7.3…\n$ UNIT         &lt;dbl&gt; 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 2, 2, 2, 1, 1, 1, 2, 2, …\n$ WVARSTRR     &lt;dbl&gt; 70, 53, 2, 27, 35, 50, 65, 1, 63, 4, 13, 65, 42, 68, 15, …\n$ PV1MATH      &lt;dbl&gt; 639.004, 697.191, 693.710, 427.317, 436.462, 569.982, 771…\n$ PV2MATH      &lt;dbl&gt; 601.251, 754.277, 654.450, 410.376, 453.450, 539.609, 672…\n$ PV3MATH      &lt;dbl&gt; 621.480, 671.940, 696.938, 423.586, 392.315, 531.648, 653…\n$ PV4MATH      &lt;dbl&gt; 631.596, 657.300, 646.187, 388.935, 439.986, 534.368, 734…\n$ PV5MATH      &lt;dbl&gt; 579.276, 621.126, 678.119, 330.962, 443.125, 465.815, 727…\n$ PV6MATH      &lt;dbl&gt; 591.791, 655.729, 644.019, 379.988, 452.648, 528.509, 729…\n$ PV7MATH      &lt;dbl&gt; 600.709, 747.934, 720.531, 398.535, 396.970, 514.326, 597…\n$ PV8MATH      &lt;dbl&gt; 587.322, 694.365, 671.425, 422.127, 459.945, 521.029, 772…\n$ PV9MATH      &lt;dbl&gt; 618.131, 742.732, 694.085, 375.354, 438.166, 472.382, 694…\n$ PV10MATH     &lt;dbl&gt; 581.973, 656.934, 668.304, 453.348, 448.084, 503.387, 725…\n$ PV1READ      &lt;dbl&gt; 676.298, 625.585, 620.116, 381.495, 448.199, 469.441, 744…\n$ PV2READ      &lt;dbl&gt; 692.247, 686.716, 559.078, 400.815, 560.636, 500.350, 679…\n$ PV3READ      &lt;dbl&gt; 690.981, 663.147, 554.767, 374.911, 365.478, 375.703, 635…\n$ PV4READ      &lt;dbl&gt; 643.067, 567.435, 587.026, 367.484, 469.970, 377.452, 725…\n$ PV5READ      &lt;dbl&gt; 627.908, 614.500, 591.806, 336.009, 503.664, 470.781, 731…\n$ PV6READ      &lt;dbl&gt; 684.676, 604.745, 570.547, 324.630, 481.215, 415.448, 684…\n$ PV7READ      &lt;dbl&gt; 661.380, 669.375, 599.078, 396.242, 436.800, 448.547, 646…\n$ PV8READ      &lt;dbl&gt; 674.070, 623.735, 545.610, 374.723, 531.226, 434.381, 756…\n$ PV9READ      &lt;dbl&gt; 666.282, 649.579, 610.466, 314.704, 480.997, 411.703, 653…\n$ PV10READ     &lt;dbl&gt; 657.387, 571.261, 590.758, 342.956, 478.578, 410.846, 784…\n$ PV1SCIE      &lt;dbl&gt; 710.634, 670.646, 666.095, 340.308, 456.333, 475.158, 693…\n$ PV2SCIE      &lt;dbl&gt; 618.739, 748.839, 604.771, 329.889, 453.400, 470.030, 626…\n$ PV3SCIE      &lt;dbl&gt; 591.623, 635.443, 704.217, 411.353, 498.937, 461.218, 627…\n$ PV4SCIE      &lt;dbl&gt; 659.770, 639.735, 687.659, 327.974, 532.324, 504.199, 676…\n$ PV5SCIE      &lt;dbl&gt; 635.892, 608.385, 690.974, 292.183, 508.231, 486.930, 661…\n$ PV6SCIE      &lt;dbl&gt; 646.901, 670.662, 617.175, 355.423, 504.461, 493.011, 618…\n$ PV7SCIE      &lt;dbl&gt; 603.569, 734.807, 692.886, 400.182, 404.572, 469.950, 602…\n$ PV8SCIE      &lt;dbl&gt; 621.352, 639.748, 630.900, 317.518, 549.457, 464.012, 653…\n$ PV9SCIE      &lt;dbl&gt; 659.674, 716.768, 656.620, 298.893, 411.062, 440.113, 645…\n$ PV10SCIE     &lt;dbl&gt; 649.719, 655.670, 649.087, 362.702, 473.613, 495.410, 662…\n$ PV1MCCR      &lt;dbl&gt; 649.392, 636.431, 645.218, 437.613, 474.516, 471.365, 718…\n$ PV2MCCR      &lt;dbl&gt; 575.372, 674.370, 680.260, 498.958, 521.987, 546.743, 708…\n$ PV3MCCR      &lt;dbl&gt; 603.792, 716.787, 711.792, 402.737, 434.209, 581.279, 707…\n$ PV4MCCR      &lt;dbl&gt; 656.162, 616.569, 633.124, 407.151, 432.725, 567.093, 720…\n$ PV5MCCR      &lt;dbl&gt; 625.136, 701.626, 711.941, 450.975, 392.901, 498.145, 736…\n$ PV6MCCR      &lt;dbl&gt; 582.174, 686.793, 749.576, 487.698, 444.589, 519.416, 670…\n$ PV7MCCR      &lt;dbl&gt; 665.104, 665.775, 692.915, 391.175, 382.733, 521.639, 749…\n$ PV8MCCR      &lt;dbl&gt; 599.694, 729.194, 648.755, 426.641, 497.778, 513.726, 729…\n$ PV9MCCR      &lt;dbl&gt; 616.165, 715.697, 721.097, 366.883, 439.754, 519.125, 696…\n$ PV10MCCR     &lt;dbl&gt; 628.181, 688.830, 657.165, 435.455, 486.126, 544.408, 779…\n$ PV1MCQN      &lt;dbl&gt; 615.137, 661.706, 569.292, 403.429, 432.794, 526.659, 703…\n$ PV2MCQN      &lt;dbl&gt; 538.617, 762.401, 647.507, 419.847, 494.698, 580.938, 707…\n$ PV3MCQN      &lt;dbl&gt; 587.580, 712.182, 660.431, 418.996, 429.147, 577.745, 703…\n$ PV4MCQN      &lt;dbl&gt; 686.280, 685.954, 569.983, 375.158, 464.070, 622.873, 631…\n$ PV5MCQN      &lt;dbl&gt; 589.078, 722.402, 637.599, 433.093, 446.069, 557.871, 673…\n$ PV6MCQN      &lt;dbl&gt; 554.608, 677.709, 652.780, 453.028, 441.193, 488.815, 645…\n$ PV7MCQN      &lt;dbl&gt; 682.886, 691.586, 636.887, 435.661, 417.452, 602.295, 740…\n$ PV8MCQN      &lt;dbl&gt; 574.889, 688.120, 611.227, 441.660, 474.220, 578.260, 757…\n$ PV9MCQN      &lt;dbl&gt; 601.470, 704.436, 685.701, 430.417, 443.916, 541.486, 721…\n$ PV10MCQN     &lt;dbl&gt; 627.403, 692.040, 604.016, 382.344, 476.528, 572.958, 715…\n$ PV1MCSS      &lt;dbl&gt; 686.193, 664.374, 655.070, 397.737, 429.063, 466.035, 637…\n$ PV2MCSS      &lt;dbl&gt; 656.043, 660.937, 612.146, 336.048, 437.136, 498.416, 712…\n$ PV3MCSS      &lt;dbl&gt; 650.782, 692.654, 646.904, 438.551, 451.101, 502.442, 702…\n$ PV4MCSS      &lt;dbl&gt; 643.126, 654.111, 590.767, 347.416, 464.708, 517.999, 702…\n$ PV5MCSS      &lt;dbl&gt; 675.389, 644.941, 727.077, 420.278, 338.774, 452.092, 663…\n$ PV6MCSS      &lt;dbl&gt; 591.001, 699.918, 648.958, 401.609, 453.538, 494.653, 632…\n$ PV7MCSS      &lt;dbl&gt; 684.203, 653.602, 623.166, 413.761, 366.873, 463.116, 727…\n$ PV8MCSS      &lt;dbl&gt; 617.908, 629.891, 677.449, 299.027, 469.776, 498.289, 719…\n$ PV9MCSS      &lt;dbl&gt; 601.076, 696.030, 654.643, 373.784, 438.499, 441.962, 635…\n$ PV10MCSS     &lt;dbl&gt; 645.941, 663.634, 649.460, 363.817, 484.175, 411.098, 774…\n$ PV1MCUD      &lt;dbl&gt; 597.328, 655.345, 658.932, 393.019, 429.756, 491.327, 650…\n$ PV2MCUD      &lt;dbl&gt; 564.341, 732.821, 648.412, 393.366, 491.067, 522.747, 721…\n$ PV3MCUD      &lt;dbl&gt; 655.238, 737.658, 672.524, 384.864, 445.708, 486.597, 663…\n$ PV4MCUD      &lt;dbl&gt; 638.884, 651.654, 614.591, 379.824, 510.568, 524.480, 680…\n$ PV5MCUD      &lt;dbl&gt; 604.706, 690.450, 664.442, 501.245, 403.908, 483.108, 753…\n$ PV6MCUD      &lt;dbl&gt; 576.996, 666.422, 687.078, 453.339, 412.346, 505.925, 667…\n$ PV7MCUD      &lt;dbl&gt; 672.527, 673.451, 708.829, 425.168, 419.173, 501.566, 746…\n$ PV8MCUD      &lt;dbl&gt; 599.424, 728.294, 640.191, 407.016, 576.384, 506.734, 801…\n$ PV9MCUD      &lt;dbl&gt; 604.423, 701.038, 732.526, 421.738, 392.309, 499.849, 638…\n$ PV10MCUD     &lt;dbl&gt; 664.795, 650.797, 619.894, 421.415, 461.595, 442.838, 734…\n$ PV1MPEM      &lt;dbl&gt; 604.382, 705.040, 676.642, 401.548, 437.563, 528.852, 692…\n$ PV2MPEM      &lt;dbl&gt; 575.460, 710.217, 705.385, 389.686, 474.378, 604.877, 678…\n$ PV3MPEM      &lt;dbl&gt; 534.443, 713.023, 585.184, 390.502, 433.958, 498.130, 711…\n$ PV4MPEM      &lt;dbl&gt; 571.301, 679.747, 670.486, 434.343, 442.276, 511.479, 711…\n$ PV5MPEM      &lt;dbl&gt; 675.638, 661.754, 645.880, 366.385, 522.369, 555.593, 706…\n$ PV6MPEM      &lt;dbl&gt; 566.880, 718.268, 760.958, 401.969, 417.482, 555.839, 749…\n$ PV7MPEM      &lt;dbl&gt; 582.805, 613.478, 731.917, 496.875, 444.665, 553.932, 690…\n$ PV8MPEM      &lt;dbl&gt; 558.696, 643.541, 676.092, 353.233, 464.823, 571.562, 708…\n$ PV9MPEM      &lt;dbl&gt; 662.795, 640.522, 684.182, 476.154, 434.014, 520.155, 719…\n$ PV10MPEM     &lt;dbl&gt; 640.998, 655.268, 702.866, 342.948, 458.216, 609.277, 671…\n$ PV1MPFS      &lt;dbl&gt; 518.732, 763.661, 690.547, 421.798, 454.383, 493.759, 702…\n$ PV2MPFS      &lt;dbl&gt; 557.279, 729.497, 728.787, 467.856, 401.375, 538.097, 662…\n$ PV3MPFS      &lt;dbl&gt; 497.254, 714.971, 633.737, 414.444, 453.331, 485.088, 670…\n$ PV4MPFS      &lt;dbl&gt; 615.386, 753.899, 703.501, 445.029, 414.743, 465.568, 658…\n$ PV5MPFS      &lt;dbl&gt; 615.007, 719.492, 654.486, 389.460, 443.411, 493.251, 653…\n$ PV6MPFS      &lt;dbl&gt; 591.702, 715.191, 734.709, 414.555, 391.559, 545.850, 699…\n$ PV7MPFS      &lt;dbl&gt; 595.836, 702.035, 737.481, 489.032, 386.903, 504.961, 697…\n$ PV8MPFS      &lt;dbl&gt; 540.481, 704.257, 685.489, 363.830, 452.824, 532.093, 692…\n$ PV9MPFS      &lt;dbl&gt; 600.664, 664.705, 665.867, 419.104, 478.373, 568.115, 656…\n$ PV10MPFS     &lt;dbl&gt; 613.118, 705.987, 727.280, 394.856, 407.059, 485.863, 698…\n$ PV1MPIN      &lt;dbl&gt; 602.757, 733.566, 682.130, 407.066, 414.746, 459.876, 691…\n$ PV2MPIN      &lt;dbl&gt; 571.184, 744.273, 692.729, 381.339, 399.365, 490.634, 671…\n$ PV3MPIN      &lt;dbl&gt; 646.605, 758.913, 647.770, 364.773, 447.814, 386.584, 737…\n$ PV4MPIN      &lt;dbl&gt; 679.914, 695.003, 629.600, 406.470, 454.758, 499.082, 731…\n$ PV5MPIN      &lt;dbl&gt; 685.582, 714.181, 693.276, 433.901, 442.892, 441.024, 737…\n$ PV6MPIN      &lt;dbl&gt; 637.760, 716.221, 660.979, 406.423, 463.269, 544.965, 763…\n$ PV7MPIN      &lt;dbl&gt; 645.213, 663.813, 684.474, 489.700, 423.967, 475.647, 782…\n$ PV8MPIN      &lt;dbl&gt; 577.579, 662.428, 656.617, 432.277, 464.304, 504.819, 769…\n$ PV9MPIN      &lt;dbl&gt; 661.673, 640.743, 687.070, 512.069, 435.375, 459.819, 686…\n$ PV10MPIN     &lt;dbl&gt; 670.254, 768.695, 648.410, 374.502, 495.469, 481.600, 708…\n$ PV1MPRE      &lt;dbl&gt; 537.068, 706.337, 630.753, 378.730, 364.784, 523.219, 756…\n$ PV2MPRE      &lt;dbl&gt; 614.320, 672.767, 694.543, 400.807, 399.972, 536.264, 700…\n$ PV3MPRE      &lt;dbl&gt; 583.272, 651.949, 604.546, 407.607, 452.831, 454.319, 687…\n$ PV4MPRE      &lt;dbl&gt; 620.093, 620.759, 614.087, 336.451, 394.357, 501.514, 701…\n$ PV5MPRE      &lt;dbl&gt; 634.054, 645.072, 603.798, 317.742, 409.755, 498.824, 724…\n$ PV6MPRE      &lt;dbl&gt; 602.552, 677.174, 644.046, 349.040, 428.787, 523.705, 680…\n$ PV7MPRE      &lt;dbl&gt; 595.217, 634.813, 710.851, 450.198, 375.015, 497.360, 697…\n$ PV8MPRE      &lt;dbl&gt; 603.353, 648.907, 656.938, 392.060, 414.975, 508.547, 736…\n$ PV9MPRE      &lt;dbl&gt; 611.942, 641.203, 690.323, 447.422, 441.178, 499.691, 734…\n$ PV10MPRE     &lt;dbl&gt; 663.352, 644.001, 664.134, 382.088, 421.531, 580.387, 727…\n$ SENWT        &lt;dbl&gt; 0.63867, 0.80357, 0.80736, 0.64732, 0.69397, 0.67917, 0.5…\n$ VER_DAT      &lt;chr&gt; \"01MAY23:14:19:45\", \"01MAY23:14:19:44\", \"01MAY23:14:19:45…\n$ i            &lt;dbl&gt; 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 21…",
    "crumbs": [
      "In-class Exercises",
      "In-class Exercise 1: Now You See it!"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html",
    "title": "Hands-On Exercise 3a: Programming Interactive Data Visualisation with R",
    "section": "",
    "text": "[Updated with in-class notes on 3 Feb 2024]",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-On Exercise 3a: Programming Interactive Data Visualisation with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#installing-and-launching-r-packages",
    "title": "Hands-On Exercise 3a: Programming Interactive Data Visualisation with R",
    "section": "2.1 Installing and Launching R packages",
    "text": "2.1 Installing and Launching R packages\nFor this exercise, other than tidyverse, we will use the following packages:\n\nggiraph: to make interactive ggplot2 plots\nplotly: to plot interactive statistical graphs\nDT: to create interactive tables using the JavaScript library DataTables\npatchwork: to combine multiple ggplot2 graphs into one figure [Note: We have introduced patchwork in Hands-on Exercise 2!]\ncrosstalk: to implement cross-widget interactions.\n\nThe code chunk below uses p_load() of pacman package to check if the abovementioned packages are installed in the computer. If they are, they will be launched in R. Otherwise, pacman will install the relevant packages before launching them.\n\n\nShow the code\npacman::p_load(tidyverse, ggiraph, plotly, DT, patchwork, crosstalk)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-On Exercise 3a: Programming Interactive Data Visualisation with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#importing-the-data",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#importing-the-data",
    "title": "Hands-On Exercise 3a: Programming Interactive Data Visualisation with R",
    "section": "2.2 Importing the Data",
    "text": "2.2 Importing the Data\nFor this exercise, we will be using the Exam_data.csv provided by the course instructor and we have used it in Hands-on Exercises 1 and 2. It consists of year end examination grades of a cohort of primary 3 students from a local school. It is in csv file format.\nWe use read_csv() function of readr to import the Exam_data csv file into R and save it as a tibble data frame called exam_data. Then we will use datatable() of DT to have an overview of the imported data.\n\n\nShow the code\n#import the data into R\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")\n\n#to display the imported data\ndatatable(exam_data, caption = \"Table 1: Exam Data of Primary 3 Students\")\n\n\n\n\n\n\n:::\nFrom the above output, we note that:\n\nThere are a total of seven attributes in the exam_data tibble data frame.\nFour of these attributes are categorical data: ID, CLASS, GENDER and RACE.\nThree of these attributes are continuous data: MATHS, ENGLISH and SCIENCE.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-On Exercise 3a: Programming Interactive Data Visualisation with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#making-use-of-tooltips",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#making-use-of-tooltips",
    "title": "Hands-On Exercise 3a: Programming Interactive Data Visualisation with R",
    "section": "3.1 Making use of Tooltips",
    "text": "3.1 Making use of Tooltips\nLet us introduce ggiraph by creating a graph with tooltip effect. The following code chunk consists two parts: (i) a ggplot object will be created using geom_dotplot_interactive(), (ii) girafe() of ggiraph will be used to create an interactive svg object.\n\n\nShow the code\np1 &lt;- ggplot(data = exam_data, aes(x = MATHS)) + \n  geom_dotplot_interactive(aes(tooltip = ID),\n                           stackgroups = TRUE, \n                           binwidth = 1,\n                           method = \"histodot\") +\n  scale_y_continuous(NULL, breaks = NULL)\n\ngirafe(ggobj = p1, \n       width_svg = 6,\n       height_svg = 6 * 0.618)\n\n\n\n\n\n\n\n\n\n\n\n\nAbout the arguments used\n\n\n\n\ntooltip: this is the interactive parameter provided by ggiraph. if this argument is used, a tooltip is shown when the element is hovered. There are other interactive parameters such as onclick, hover_css and selected_css that we will explore later on! In the above code chunk, we want the tooltip to show the student ID, so tooltip = ID is used.\nstackgroups: this is a geom_dotplot argument from ggplot2. if TRUE, dots would be stacked across groups.\nbinwidth: this is a geom_dotplot argument from ggplot2. When method is “dotdensity”, this specifies maximum bin width. When method is “histodot”, this specifies bin width.\nmethod: this is a geom_dotplot argument from ggplot2. “dotdensity” (default) for dot-density binning, or “histodot” for fixed bin widths (like stat_bin).\nwidth_svg and height_svg: this argument specify the width and height of the graphics region in inches. The default values are 6 and 5 inches. This will define the aspect ratio of the graphic as it will be used to define viewbox attribute of the SVG result.\n\n\n\n\n3.1.1 Displaying Multiple Information on Tooltip\nIn the above example, we only displayed student ID using tooltip. We can make use of tooltip to display more than 1 piece of information.\nThe following code chunk is an example. First, we will create a new column in exam_data to contain the information we want the tooltip to display. Then we will plot the graph using ggplot and make it interactive using ggiraph (as seen in the earlier code chunk).\n\n\nShow the code\nexam_data$tooltip &lt;- c(paste0(\"Name = \", exam_data$ID, \n                              \"\\n Class = \", exam_data$CLASS))\n\np2 &lt;- ggplot(data = exam_data, \n            aes(x = MATHS)) + \n  geom_dotplot_interactive(\n    aes(tooltip = exam_data$tooltip),\n    stackgroups = TRUE, \n    binwidth = 1.2, \n    method = \"histodot\") + \n  scale_y_continuous(NULL, breaks = NULL) \n\ngirafe(ggobj = p2, \n       width_svg = 8, \n       height_svg = 8 * 0.618)\n\n\n\n\n\n\nBy hovering the mouse pointer on an data point of interest, the student’s ID and Class will be displayed.\n\n\n3.1.2 Customising Tooltip Style\nWe can also customise the tooltip (e.g. fonts, the color of the tooltip) using opts_tooltip() of ggiraph. This function customises tooltip rendering by adding css declarations.\n\n\nShow the code\ntooltip_css &lt;- \"background-color:white; #&lt;&lt; font-style:bold; color:black;\"#&lt;&lt;\n\nexam_data$tooltip &lt;- c(paste0(\"Name = \", exam_data$ID, \n                              \"\\n Class = \", exam_data$CLASS))\n\np2 &lt;- ggplot(data = exam_data, \n            aes(x = MATHS)) + \n  geom_dotplot_interactive(\n    aes(tooltip = exam_data$tooltip),\n    stackgroups = TRUE, \n    binwidth = 1.2, \n    method = \"histodot\") + \n  scale_y_continuous(NULL, breaks = NULL) + \n  theme_minimal(base_family = \"Open Sans\")\n\ngirafe(ggobj = p2, \n       width_svg = 8, \n       height_svg = 8 * 0.618, \n       options = list(  #&lt;&lt;\n         opts_tooltip(  #&lt;&lt;\n           css = tooltip_css)) #&lt;&lt; \n       )\n\n\n\n\n\n\n\n\n3.1.3 Displaying Statistics on Tooltip\nWe can also use tooltip to display statistics. In the following code chunk, a function is used to compute 90% confidence interval of the mean. The derived statistics is then displayed on the tooltip.\n\n\nShow the code\ntooltip_stat &lt;- function(y, ymax, accuracy = .01) {\n  mean &lt;- scales::number(y, accuracy = accuracy)\n  sem &lt;- scales:: number(ymax-y, accuracy = accuracy)\n  paste(\"Mean Maths Scores: \", mean, \"+/-\", sem)\n}\n\np3 &lt;- ggplot(data = exam_data,\n             aes(x = RACE),) + \n               stat_summary(aes(y = MATHS, tooltip = after_stat(tooltip_stat(y, ymax))),\n                                           fun.data = \"mean_se\",\n                                           geom = GeomInteractiveCol,\n                                           fill = \"pink\") + \n               stat_summary(aes(y = MATHS),\n                            fun.data = mean_se, \n                            geom = \"errorbar\", width = 0.2, linewidth = 0.2)\n\ngirafe( ggobj = p3,\n        width_svg = 8,\n        height_svg = 8 * 0.618)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-On Exercise 3a: Programming Interactive Data Visualisation with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#making-use-of-hover-effect",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#making-use-of-hover-effect",
    "title": "Hands-On Exercise 3a: Programming Interactive Data Visualisation with R",
    "section": "3.2 Making Use of Hover Effect",
    "text": "3.2 Making Use of Hover Effect\nThe code chunk below shows the second interactive feature of ggiraph: data_id. By making use the data_id parameter, we can specify the effect when we mouse over the various graph components.For example, in the following chart, when we mouse over dots, we see other dots (i.e. students) from the same class being highlighted.\n\n\nShow the code\np4 &lt;- ggplot(data=exam_data, \n             aes(x = MATHS)) + \n  geom_dotplot_interactive(\n    aes(data_id = CLASS, \n        tooltip = exam_data$tooltip), \n    stackgroups = TRUE, \n    binwidth = 1, \n    method = \"histodot\") + \n  scale_y_continuous(NULL, breaks = NULL)+ theme_minimal()\n\ngirafe(ggobj = p4,\n       width_svg = 6,\n       height_svg = 6*0.618)\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe default value of the hover css is hover_css = “fill:orange;”\nI also added the tooltip function in the above chart, just to show that we can have both tooltip and hover effect together!\n\n\n\n\n3.2.1 Styling Hover Effect\nWe can also style the hover effect. For example, in the following code chunk, css codes are used to change the highlighting effect by specifying the options argument in girafe().\n\n\nShow the code\np4 &lt;- ggplot(data=exam_data, \n             aes(x = MATHS)) + \n  geom_dotplot_interactive(\n    aes(data_id = CLASS, \n        tooltip = exam_data$tooltip), \n    stackgroups = TRUE, \n    binwidth = 1, \n    method = \"histodot\") + \n  scale_y_continuous(NULL, breaks = NULL) +\n  theme_minimal() \n\ngirafe(ggobj = p4,\n       width_svg = 6,\n       height_svg = 6*0.618,\n       options = list(\n         opts_hover(css = \"fill:#FF33A2;\"),\n         opts_hover_inv(css = \"opacity:0.2;\")\n       ))\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDifferent from the tooltip customisation, in the above example, the css customisation requests are encoded directly.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-On Exercise 3a: Programming Interactive Data Visualisation with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#making-use-of-click-effect",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#making-use-of-click-effect",
    "title": "Hands-On Exercise 3a: Programming Interactive Data Visualisation with R",
    "section": "3.3 Making Use of Click Effect",
    "text": "3.3 Making Use of Click Effect\nThe onclick argument of ggiraph provides hotlink interactivity on the web. The following code chunk is an example of onclick. We would add a column onclick in the exam_data dataset first, then plot the chart and onclick = was assigned to the exam_data$onlick.\n\n\nShow the code\nexam_data$onclick &lt;- sprintf(\"window.open(\\\"%s%s\\\")\",\n                             \"https://www.moe.gov.sg/schoolfinder?journey=Primary%20school\", \n                             as.character(exam_data$ID))\n\np5 &lt;- ggplot(data = exam_data, aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(onclick = onclick),\n    stackgroups = TRUE,\n    binwidth = 1, \n    method = \"histodot\") + \n  scale_y_continuous(NULL, breaks = NULL)\n\ngirafe(ggobj = p5, \n       width_svg = 6,\n       height_svg = 6 * 0.618)\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe click actions must be a string column in the dataset containing valid javascript instructions in order for the onclick function to work.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-On Exercise 3a: Programming Interactive Data Visualisation with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#coordinated-multiple-views-using-ggiraph",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#coordinated-multiple-views-using-ggiraph",
    "title": "Hands-On Exercise 3a: Programming Interactive Data Visualisation with R",
    "section": "3.4 Coordinated Multiple Views using ggiraph",
    "text": "3.4 Coordinated Multiple Views using ggiraph\nWe can also use ggiraph to coordinate the views when we have multiple plots. This means that when a datapoint of one of a dotplot is selected, the corresponding data point on the second data visualisation will be highlighted too.\nTo build a coordinated multiple views, we will: 1. Use the appropriate interactive functions of ggiraph to create the multiple views 2. Use patchwork inside girafe() function to create interactive coordinate multiple views.\n\n\nShow the code\np6 &lt;- p4 + coord_cartesian(xlim = c(0,100)) + theme(panel.grid.major.x = element_blank(),\n  panel.grid.minor.x = element_blank())\n\np7 &lt;- ggplot(data=exam_data, \n             aes(x = ENGLISH)) + \n  geom_dotplot_interactive(\n    aes(data_id = CLASS, \n        tooltip = exam_data$tooltip), \n    stackgroups = TRUE, \n    binwidth = 1, \n    method = \"histodot\") + \n  scale_y_continuous(NULL, breaks = NULL)+ \n  theme_minimal() + theme(panel.grid.major.x = element_blank(),\n  panel.grid.minor.x = element_blank())\n\ngirafe(code = print(p6 + p7),\n       width_svg = 6,\n       height_svg = 3,\n       options = list(\n          opts_hover(css = \"fill:#FF33A2;\"),\n         opts_hover_inv(css = \"opacity:0.2;\")\n       ))\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe data_id aesthetic is critical to link observations between plots and the tooltip aesthetic is optional and good to have when mouse over a point.\nNote that when using patchwork within girafe(), we need to have “code = print()”. Otherwise, the combined plot won’t appear.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-On Exercise 3a: Programming Interactive Data Visualisation with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#coordinated-multiple-views-using-plotly",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#coordinated-multiple-views-using-plotly",
    "title": "Hands-On Exercise 3a: Programming Interactive Data Visualisation with R",
    "section": "4.1 Coordinated Multiple Views using plotly",
    "text": "4.1 Coordinated Multiple Views using plotly\nWe can also create coordinated linked plots using plotly with the following steps:\n\nhighlight_key() of plotly package is used as shared data. It creates an object class crosstalk shared data frame\ncreate two scatterplots using ggplot2 functions\nsubplot() of plotly package is used to place the two scatterplots side by side\n\n\n\nShow the code\nd &lt;- highlight_key(exam_data)\n\np9 &lt;- ggplot(data = d, \n             aes(x = MATHS, \n                 y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim = c(0,100),\n                  ylim = c(0,100))\n\np10 &lt;- ggplot(data=d,\n              aes(x = MATHS,\n                  y = SCIENCE)) +\n    geom_point(size=1) +\n  coord_cartesian(xlim = c(0,100),\n                  ylim = c(0,100))\n\nsubplot(ggplotly(p9),\n        ggplotly(p10))\n\n\n\n\n\n\n\n\n\n\n\n\nTry it!\n\n\n\nClick on a data point of one of the scatterplot and see how the corresponding point on the other scatterplot is selected.\n\n\n\n\n\n\n\n\nNote\n\n\n\nAs ggplotly cannot use crosstalk, we use subplot() of plotly r to combine the two plots together.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-On Exercise 3a: Programming Interactive Data Visualisation with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#interactive-data-table-dt-package",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#interactive-data-table-dt-package",
    "title": "Hands-On Exercise 3a: Programming Interactive Data Visualisation with R",
    "section": "5.1 Interactive Data Table: DT package",
    "text": "5.1 Interactive Data Table: DT package\nDT package is a wrapper of the Javascript Library DataTables. Data objects in R can be rendered as HTML tables and DataTables provides filtering, pagination, sorting and many other features in the tables.\nThe following is an example of displaying exam_data using DT package.\n\n\nShow the code\ndatatable(exam_data, class=\"compact\")",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-On Exercise 3a: Programming Interactive Data Visualisation with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#linked-brushing-crosstalk-method",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#linked-brushing-crosstalk-method",
    "title": "Hands-On Exercise 3a: Programming Interactive Data Visualisation with R",
    "section": "5.2 Linked brushing: crosstalk method",
    "text": "5.2 Linked brushing: crosstalk method\nAfter we have created the interactive data table, we can make use of plotly and DT packages to implement coordinated brushing using the following code chunk. We will get an interactive chart and data table side by side, and when certain data points are selected on the interactive chart, the data table will be filtered accordingly.\n\n\n\n\n\n\nTry it!\n\n\n\nClick on a row in the data table below and see how the corresponding point on the other scatterplot is selected.\n\n\n\n\nShow the code\nd &lt;- highlight_key(exam_data)\np &lt;- ggplot(d, aes(x=ENGLISH, y=MATHS)) + \n  geom_point(size = 1) +\n  coord_cartesian(xlim = c(0,100), \n                  ylim = c(0,100))\n\ng &lt;- highlight(ggplotly(p),\n               \"plotly_selected\")\n\nbscols(g, datatable(d), widths = 5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbout the arguments used!\n\n\n\nhighlight() is a function of plotly package. It sets a variety of options for brushing (i.e. highlighting) multiple plots. These options are primarily designed for linking multiple plotly graphs, and may not behave as expected when linking plotly to another htmlwidget package via crosstalk. In some cases, other htmlwidgets will respect these options, such as persistent selection in leaflet.\nbscols() is a helper function of crosstalk package. It makes it easy to put HTML elements side by side. It can be called directly from the console but is especially designed to work in an R Markdown document.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-On Exercise 3a: Programming Interactive Data Visualisation with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html",
    "title": "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods",
    "section": "",
    "text": "[Updated charts with annotations and references on 16 Jan 2024]",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#installing-and-launching-r-packages",
    "title": "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods",
    "section": "2.1 Installing and Launching R packages",
    "text": "2.1 Installing and Launching R packages\nThe code chunk below uses p_load() of pacman package to check if tidyverse packages are installed in the computer. If they are, they will be launched in R. Otherwise, pacman will install the relevant packages before launching them.\n\n\nShow the code\npacman::p_load(tidyverse)\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that pacman package has already been installed before using the above code chunk. If you have not yet installed pacman please install it via Rstudios’ “Tools” &gt; “Install Packages” before using the above code chunk.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#importing-the-data-into-r",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#importing-the-data-into-r",
    "title": "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods",
    "section": "2.2 Importing the data into R",
    "text": "2.2 Importing the data into R\nWe use read_csv() function of readr to import the data, glimpse() of dplyr to learn about the associated attribute information in the dataframe, and summary() of base R to get the summary statistics of the data.\n\nCodeDataSummary of Data\n\n\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")\n\n\n\n\nglimpse(exam_data)\n\nRows: 322\nColumns: 7\n$ ID      &lt;chr&gt; \"Student321\", \"Student305\", \"Student289\", \"Student227\", \"Stude…\n$ CLASS   &lt;chr&gt; \"3I\", \"3I\", \"3H\", \"3F\", \"3I\", \"3I\", \"3I\", \"3I\", \"3I\", \"3H\", \"3…\n$ GENDER  &lt;chr&gt; \"Male\", \"Female\", \"Male\", \"Male\", \"Male\", \"Female\", \"Male\", \"M…\n$ RACE    &lt;chr&gt; \"Malay\", \"Malay\", \"Chinese\", \"Chinese\", \"Malay\", \"Malay\", \"Chi…\n$ ENGLISH &lt;dbl&gt; 21, 24, 26, 27, 27, 31, 31, 31, 33, 34, 34, 36, 36, 36, 37, 38…\n$ MATHS   &lt;dbl&gt; 9, 22, 16, 77, 11, 16, 21, 18, 19, 49, 39, 35, 23, 36, 49, 30,…\n$ SCIENCE &lt;dbl&gt; 15, 16, 16, 31, 25, 16, 25, 27, 15, 37, 42, 22, 32, 36, 35, 45…\n\n\n\n\n\nsummary(exam_data)\n\n      ID               CLASS              GENDER              RACE          \n Length:322         Length:322         Length:322         Length:322        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n    ENGLISH          MATHS          SCIENCE     \n Min.   :21.00   Min.   : 9.00   Min.   :15.00  \n 1st Qu.:59.00   1st Qu.:58.00   1st Qu.:49.25  \n Median :70.00   Median :74.00   Median :65.00  \n Mean   :67.18   Mean   :69.33   Mean   :61.16  \n 3rd Qu.:78.00   3rd Qu.:85.00   3rd Qu.:74.75  \n Max.   :96.00   Max.   :99.00   Max.   :96.00  \n\n\n\n\n\nFrom the above output, we note that exam_data has 7 columns:\n\nID is a unique identifier for students.\nCLASS represents the class that the student is in.\nGENDER tells us the student’s gender.\nRACE tells us the student’s race.\nENGLISH is the score that the student got for English subject.\nMATHS is the score that the student got for Mathematics subject.\nSCIENCE is the score that the student got for Science subject.\n\nThere are 322 rows in total. For English, the minimum score (i.e., the lowest score gotten by a student) is 21, median is 70, and the maximum (or rather the highest score gotten by a student) is 96. For Mathematics, the lowest score gotten by a student is 9, median is 74 and the highest score gotten by a student is 99. For Science, the lowest score gotten by a student is 15, median is 65 and the highest score gotten by a student is 96.\n\n2.2.1 Check for Missing Data\nIt is good to check if there are any missing data in our imported dataset so that we are aware if there are any missing data at the onset and also decide how to manage the missing data subsequently.\n\n\nShow the code\nexam_data %&gt;% \n  map(is.na) %&gt;%\n  map(sum)\n\n\n$ID\n[1] 0\n\n$CLASS\n[1] 0\n\n$GENDER\n[1] 0\n\n$RACE\n[1] 0\n\n$ENGLISH\n[1] 0\n\n$MATHS\n[1] 0\n\n$SCIENCE\n[1] 0\n\n\nFrom the above result, we see that there are no missing data in exam_data.\nNow that we have some idea of the data imported in, let us move to the next section!",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#principles-of-grammar-of-graphics",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#principles-of-grammar-of-graphics",
    "title": "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods",
    "section": "3.1 Principles of Grammar of Graphics",
    "text": "3.1 Principles of Grammar of Graphics\nThere are two principles in Grammar of Graphics, they are:\n\nGraphics = distinct layers of grammatical elements; and\nMeaningful plots through aesthetic mapping\n\nA good grammar of graphics will allow us to gain insight into the composition of complicated graphics, and reveal unexpected connections between seemingly different graphics (Cox, 1978). It also provides a strong foundation for understanding a diverse range of graphics. Furthermore, it may also help guide us on what a well-formed or correct graphic looks like, but there will still be many grammatically correct but nonsensical graphics.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#a-layered-grammar-of-graphics",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#a-layered-grammar-of-graphics",
    "title": "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods",
    "section": "4.1 A Layered Grammar of Graphics",
    "text": "4.1 A Layered Grammar of Graphics\nThe R package, ggplot2 is an implementation of Leland Wilkinson’s Grammar of Graphics, and is developed by Hadley Wickham. Hadley’s layered grammar of graphics uses several layered components to describe any graphic or visualization. The figure below shows the seven grammars of ggplot2.\n\nHere is what each layer means:\n\nData refers to the dataset being plotted.\nAesthetics take the attributes of the data and use them to influence visual characteristics, such as position, colours, size, shape, or transparency.\nGeometrics refers to the visual elements used for our data, such as point, bar or line.\nFacets split the data into subsets to create multiple variations of the same graph (paneling, multiple plots).\nStatistics refers to statistical transformations that summarise data (e.g. mean, confidence intervals).\nCoordinate systems define the plane on which data are mapped on the graphic.\nThemes modify all non-data components of a plot, such as main title, sub-title, y-aixs title, or legend background.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#comparison-between-r-graphics-and-ggplot2",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#comparison-between-r-graphics-and-ggplot2",
    "title": "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods",
    "section": "4.2 Comparison between R Graphics and ggplot2",
    "text": "4.2 Comparison between R Graphics and ggplot2\n\nR Graphicsggplot2\n\n\n\nhist(exam_data$MATHS)\n\n\n\n\n\n\n\n\n\n\n\nggplot(data= exam_data, aes(x = MATHS)) + \n  geom_histogram(bins = 10, \n                 boundary = 100, \n                 color = \"black\", \n                 fill = \"grey\") + \n  ggtitle(\"Distribution of Maths Scores\")\n\n\n\n\n\n\n\n\n\n\n\nYou may ask, why should we use ggplot2 rather than R Graphics, especially when the code chunk for R Graphics is relatively simple. As pointed out by Hadley Wickham, the creator of ggplot2:\n\n\n\n\n\n\nImportant\n\n\n\nThe transferable skills from ggplot2 are not the idiosyncrasies of plotting syntax, but a powerful way of thinking about visualisation, as a way of mapping between variables and the visual properties of geometric objects that you can perceive.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#data-1",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#data-1",
    "title": "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods",
    "section": "5.1 Data",
    "text": "5.1 Data\nAs seen from the figure in Section 4.1, the first layer or element of a plot begins with data. Let us call the ggplot() function using the following code chunk.\n\nggplot(data = exam_data)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nNote that a blank canvas appears.\nggplot() initialises a ggplot object.\nThe data argument defines the dataset to be used for plotting. In our case, it is exam_data.\nIf the dataset is not already a dataframe, it will be converted to one by fortify().",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#aesthetic-mappings",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#aesthetic-mappings",
    "title": "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods",
    "section": "5.2 Aesthetic Mappings",
    "text": "5.2 Aesthetic Mappings\nThe aesthetic mappings take attributes of the data and and use them to influence visual characteristics, such as position, colour, size, shape, or transparency. Each visual characteristic can thus encode an aspect of the data and be used to convey information.\nAll aesthetics of a plot are specified in the aes() function call.\n\n\n\n\n\n\nNote\n\n\n\nIn the later part of this lesson, you will see that each geom layer can have its own aes specification.\n\n\nThe code chunk below adds the aesthetics element into the plot.\n\nggplot(data = exam_data, \n       aes(x = MATHS))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nggplot includes the x-axis and the axis’ label.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geoms",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geoms",
    "title": "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods",
    "section": "5.3 Geoms",
    "text": "5.3 Geoms\nGeoms (Geometric objects) are the actual marks we put on a plot. Examples include:\n\ngeom_point for drawing individual points (e.g., scatter plots)\ngeom_line for drawing lines (e.g., for line charts)\ngeom_smooth for drawing smoothed lines (e.g., for simple trends or approximations)\ngeom_bar for drawing bars (e.g., for bar charts)\ngeom_histogram for drawing binned values (e.g., for histograms)\ngeom_polygon for drawing arbitrary shapes\ngeom_map for drawing polygons in the shape of a map! (You can access the data to use for these maps by using the map_data() function).\n\n\n\n\n\n\nA plot must have at least one geom; there is no upper limit. You can add a geom to a plot using the + operator.\n\n5.3.1 Geometric Objects: geom_bar()\nThe code chunk below plots a bar chart using geom_bar().\n\n\nShow the code\nexam_data$RACE &lt;- as.factor(exam_data$RACE)\n\nRACE_count &lt;- exam_data %&gt;% \n  count(exam_data$RACE, sort=TRUE)\n\nggplot(data = exam_data, \n       aes(x = RACE)) + \n  geom_bar() +\n  labs(title=\"Race Distribution of Primary 3 Students\", caption = \"Hands-on Exercise 1\") + \n  xlab(\"Race\") +\n  ylab(\"Number of Students\")+\n  geom_text(stat = \"count\", aes(label = after_stat(count)), vjust= -0.5)\n\n\n\n\n\n\n\n\n\n\n\n5.3.2 Geometric Objects: geom_dotplot()\nIn a dot plot, the width of a dot corresponds to the bin width (or maximum width, depending on the binning algorithm). The dots are stacked, with each dot representing one observation. Note that there are two basic methods for dotplot: dot-density and histodot. The default method is “dotdensity”. When the method is “dotdensity”, the bin width argument specfies maximum bin width. When the method is “histodot”, the binwidth argument specifics bin width.\nIn the code chunk below, geom_dotplot() is used to plot a dot plot. We also adjusted the size of the dots using the dotsize= argument.\n\nmethod=“dotdensity” (default)method=“histodot”\n\n\n\nggplot(data = exam_data, \n       aes(x = MATHS)) + \n  geom_dotplot(method = \"dotdensity\", dotsize = 0.5)+ \n  labs(title=\"Distribution of Maths Scores for Primary 3 Students\", caption = \"Hands-on Exercise 1\") + \n  xlab(\"Maths Score\") \n\n\n\n\n\n\n\n\n\n\n\nggplot(data = exam_data, \n       aes(x = MATHS)) + \n  geom_dotplot(method = \"histodot\", dotsize = 0.5)+ \n  labs(title=\"Distribution of Maths Scores for Primary 3 Students\", caption = \"Hands-on Exercise 1\") + \n  xlab(\"Maths Score\") \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe y axis scale is not very useful because it is in numbers less than 1. It can be very misleading!\n\n\nAs such, we will use: 1. scale_y_continuous() to turn off the misleading y-axis, 2. binwidth= argument to change the binwidth to 2.5. 3. fill= argument to color the dots based on the student’s gender.\n\nggplot(data = exam_data, \n       aes(x = MATHS, fill=GENDER)) + \n  geom_dotplot(binwidth = 2.5,\n               dotsize = 0.5) + \n  scale_y_continuous(NULL, breaks = NULL)+\n  labs(title=\"Distribution of Maths Scores for Primary 3 Students\", caption = \"Hands-on Exercise 1\") + \n  xlab(\"Maths Score\") \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: Why do we use dotplot over bar charts?\n\n\n\nThere is a big difference between dot plot and bar chart. A value in a bar chart is visualised by the length of the bar but in dot plot, the value is visualised by its poistion on an axis.\nAnother difference between a bar chart and a dot plot is that, since a dot plot uses a simple dot on a numerical axis, it is far easier to add more series (more values per category) without needing to stack these series on top of each other and make them rather unreadable, like in a stacked bar chart. This results in a chart that packs a lot of information in a small space. A multi-series dot plot lets you compare values within a category as easily as between categories.\nSource\n\n\n\n\n5.3.3 Geometric Objects: geom_histogram()\nThe following code chunk uses geom_histogram() to create a simple histogram using values in MATHS fields of exam_data.\n\nggplot(data = exam_data, \n       aes(x = MATHS)) +\n  geom_histogram() \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe default bin is 30\n\n\n\n\n5.3.4 Modifying a geometric object by changing geom()\nIn the code chunk below,\n\nbins arugment is used to change the number of bins to 20.\nfill argument is used to shade the histogram with pink color, and\ncolor argument is used to change the outline of the bars to black.\n\n\nmean_MATHS &lt;- round(mean(exam_data$MATHS),1)\n\n\nggplot(data = exam_data,\n       aes(x = MATHS)) + \n  geom_histogram(bins = 25, \n                 color = \"black\",\n                 fill = \"pink\")+ \n  geom_text(x = mean_MATHS, y = 28, \n            label = paste(\"mean Maths\\n Score =\", mean_MATHS),\n            color = \"blue\") + \n  geom_segment(x = mean_MATHS, xend = mean_MATHS,\n               y = 0, yend = 25, color= \"blue\")\n\n\n\n\n\n\n\n\n\n\n5.3.5 Modifying a geometric object by changing aes()\nThe code chunk below changes the fill colour of the histogram based on the subgroup of aes().\n\nggplot(data = exam_data,\n       aes(x = MATHS,\n           fill = GENDER)) + \n  geom_histogram(bins = 20, \n                 color = \"grey30\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis approach can be used to colour, fill and alpha of the geometric object.\n\n\n\n\n5.3.6 Geometric Objects: geom_density()\ngeom_density computes and plots kernel density estimate, which is a smoothed version of the histogram. It is an useful alternative to histrogram for continuous data that comes from an underlying smooth distribution.\nThe code below plots the distribution of Maths scores in a kernel density estimate plot.\n\nggplot(data = exam_data, \n       aes(x = MATHS)) + \n  geom_density()\n\n\n\n\n\n\n\n\nThe code chunks below plot two kernel density lines by using color and fill arguments of aes().\n\nUsing color argument of aes()Using fill argument of aes()\n\n\n\nggplot(data = exam_data, \n       aes(x = MATHS, \n           color = GENDER)) + \n  geom_density()\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = exam_data, \n       aes(x = MATHS, \n           fill = GENDER)) + \n  geom_density()\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.3.7 Geometric Objects: geom_boxplot()\ngeom_boxplot displays continuous value list. It visualises 5 summary statistics: median, two hinges (first quartile and third quartile), and two whiskers (minimum and maximum)), and all “outlying” points individually.\nThe following code chunk plots boxplots using geom_boxplot().\n\nggplot(data = exam_data,\n       aes(y = MATHS,\n           x = GENDER)) + \n  geom_boxplot()\n\n\n\n\n\n\n\n\nNotches are used in box plots to help visually assess whether the medians of distributions differ. If the notches do not overlap, this is evidence that the medians are different.\nThe code chunk below plots the distribution of Maths scores by gender in notched plot instead of boxplot.\n\nggplot(data = exam_data,\n       aes(y = MATHS,\n           x = GENDER)) + \n  geom_boxplot(notch=TRUE)\n\n\n\n\n\n\n\n\n\n\n5.3.8 Geometric Objects: geom_violin()\ngeom_violin is designed for creating violin plots. Violin plots are a way of comparing multiple data distributions. It is difficult to compare more than few distributions with ordingary density curves because the lines visually interfere with each other. With a violin plot, it’s easier to compare several distributions since they’re placed side by side.\nThe code below plots the distribution of Maths score by gender using violin plot.\n\nggplot(data = exam_data, \n       aes(y = MATHS, \n           x = GENDER)) + \n  geom_violin()\n\n\n\n\n\n\n\n\n\n\n5.3.9 Geometric Objects: geom_point()\ngeom_point() is useful for creating scatterplots.\nThe code chunk below plots a scatterplot showing the Maths and English Scores of students using geom_point().\n\nggplot(data = exam_data,\n       aes(x = MATHS,\n           y = ENGLISH)) +\n  \n  geom_point()\n\n\n\n\n\n\n\n\n\n\n5.3.10 Geometric objects can be combined\nAs mentioned earlier, we need to specify at least 1 geom object and can have more than 1 geom object by combining other geom objects to create a plot. The code chunk below plots the data points on the boxplots using both geom_boxplot() and geom_point().\n\nggplot(data = exam_data, \n       aes(y = MATHS, \n           x = GENDER)) + \n  geom_boxplot() + \n  geom_point(position = \"jitter\", \n             size = 0.5)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#stat",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#stat",
    "title": "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods",
    "section": "5.4 Stat",
    "text": "5.4 Stat\nThe Statistics functions statistically transform data, usually as some form of summary. For example:\n\nfrequency of values of a variable (e.g. for a bar graph)\n\nmean\nconfidence limit\n\nThere are two ways to use these functions:\n\nadd a stat_() function and override the default geom, or\nadd a geom_() function and override the default stat.\n\n\n\n5.4.1 Working with stat()\nThe boxplots below are incomplete because the positions of the means were not shown.\n\nggplot(data = exam_data, \n       aes(y = MATHS, x = GENDER)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n5.4.2 Working with stat() - the stat_summary() method\nTo add the positions of the means, we use stat_summary() function and override the default geom.\n\nggplot(data = exam_data, \n       aes(y = MATHS, x = GENDER)) + \n  geom_boxplot() + \n  stat_summary(geom = \"point\", \n                fun.y = \"mean\",\n                colour = \"red\", \n                size = 4)\n\n\n\n\n\n\n\n\n\n\n5.4.3 Working with stat() - the geom() method\nWe now try adding the mean values and position using geom_() function and override the default stat.\n\nggplot(data = exam_data, \n       aes(y = MATHS, x = GENDER)) + \n  geom_boxplot() + \n  geom_point(stat = \"summary\", \n             fun.y = \"mean\",\n             colour = \"red\",\n             size = 4)\n\n\n\n\n\n\n\n\n\n\n5.4.4 Adding a best fit cure on a scatterplot\nThe scatterplot below shows the relationship of students’ Maths and English scores. We can imporve the interpretability of the graph by adding a best fit curve.\n\nWithout best fit curveWith best fit curve\n\n\n\nggplot(data = exam_data,\n       aes(x = MATHS, y = ENGLISH)) + \n  geom_point() + \n  geom_hline(yintercept = 50, color=\"orange\", size = 1) +\n  geom_vline(xintercept = 50, color=\"orange\", size = 1)\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = exam_data,\n       aes(x = MATHS, y = ENGLISH)) + \n  geom_point() + \n  geom_smooth(size=0.5) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe default method used by geom_smooth is loess.\nFor info here.\n\n\nThe default smoothing method can be overridden by specifying the method parameter of geom_smooth().\n\nggplot(data = exam_data,\n       aes(x = MATHS, y = ENGLISH)) + \n  geom_point() +\n  geom_smooth(method = lm, size = 0.5)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#facets",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#facets",
    "title": "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods",
    "section": "5.5 Facets",
    "text": "5.5 Facets\nFacetting generates small multiples of plots (sometimes also called trellis plot), each displaying a different subset of the data. They are an alternative to aesthetics for displaying additional discrete variables. ggplot2 supports two types of facets, namely: facet_grid() and facet_wrap().\n\n5.5.1 Working with facet_wrap()\nfacet_wrap wraps a 1d sequence of panels into 2d. This is generally a bette use of screen space than facet_grid because most displays are roughly rectangular.\nThe following code chunk plots a trellis plot using facet_wrap().\n\nggplot(data = exam_data,\n       aes(x = MATHS)) + \n  geom_histogram(bins = 20) + \n  facet_wrap(~ CLASS)\n\n\n\n\n\n\n\n\n\n\n5.5.2 Working with facet_grid()\nfacet_grid forms a matrix of panels defined by row and coloumn facetting variables. It is most useful when we have two discrete variables, and all combinations of varialbes exist in the data.\nThe following code chunk plots a trellis plot using facet_grid().\n\nggplot(data = exam_data, \n       aes(x = MATHS)) + \n  geom_histogram(bins = 20) + \n  facet_grid(~ CLASS)\n\n\n\n\n\n\n\n\nWhile it is not better use of screen space, using facet_grid() allows us to compare all the charts side by size with the same y-axis.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#coordinates",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#coordinates",
    "title": "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods",
    "section": "5.6 Coordinates",
    "text": "5.6 Coordinates\nThe Coordinates function map the position of objects onto the plane of the plot. There are several different possible coordinate systems to use:\n\ncoord_cartesian(): the default cartesian coordinate systems, where you specify x and y values (e.g. allows you to zoom in or out).\ncoord_flip(): a cartesian system with the x and y flipped.\ncoord_fixed(): a cartesian system with a “fixed” aspect ratio (e.g. 1.78 for a “widescreen” plot).\ncoord_quickmap(): a coordinate system that approximates a good aspect ratio for maps.\n\n\n5.6.1 Working with Coordinates\nBy default, the bar charts of ggplot2 is in vertical form. We can flip it into a horizontal bar chart using coord_flip().\n\nVertical Bar ChartHorizontal Bar Chart\n\n\n\nggplot(data = exam_data, \n       aes(x = RACE)) +\n  geom_bar() \n\n\n\n\n\n\n\n\n\n\n\nggplot(data = exam_data, \n       aes(x = RACE)) +\n  geom_bar() + \n  coord_flip()\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.6.2 Changing the ranges for x- and y- axis\nWe can fix the x- and y-axis ranges for the following scatterplot so that both axes have the same range (0,100) for better interpretability.\n\nBefore Changing the RangesAfter Changing the Ranges\n\n\n\nggplot(data = exam_data, \n       aes(x = MATHS, y = ENGLISH)) +\n  geom_point() + \n  geom_smooth(method = lm, size = 0.5)\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = exam_data, \n       aes(x = MATHS, y = ENGLISH)) +\n  geom_point() + \n  geom_smooth(method = lm, size = 0.5) + \n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#themes",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#themes",
    "title": "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods",
    "section": "5.7 Themes",
    "text": "5.7 Themes\nThemes control elements of the graph not related to the data. For example:\n\nbackground colour\nfont size\ngridlines\ncolour of labels\n\nBuilt-in themes include: theme_gray() (default theme), theme_bw(), theme_classic().\nA list of theme can be found here. Each theme element can be conceived of as either a line (e.g. x-axis), a rectangle (e.g. graph background), or text (e.g. axis title). You can also edit the themes’ indivudal settings using theme().\nThe following code chunks illustrates the different themes that we can use.\n\ntheme_gray (default theme)theme_classictheme_minimal\n\n\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_gray()\n\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_minimal()",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Extra/Post-Lesson_01/Post-Lesson_01.html",
    "href": "Extra/Post-Lesson_01/Post-Lesson_01.html",
    "title": "Post-Lesson Thoughts 1: Annotations",
    "section": "",
    "text": "[Updated with references on 16 Jan 2024]",
    "crumbs": [
      "Extra Stuff",
      "Post-Lesson Thoughts 1: Annotations"
    ]
  },
  {
    "objectID": "Extra/Post-Lesson_01/Post-Lesson_01.html#import-packages",
    "href": "Extra/Post-Lesson_01/Post-Lesson_01.html#import-packages",
    "title": "Post-Lesson Thoughts 1: Annotations",
    "section": "2.1 Import Packages",
    "text": "2.1 Import Packages\n\npacman::p_load(tidyverse)",
    "crumbs": [
      "Extra Stuff",
      "Post-Lesson Thoughts 1: Annotations"
    ]
  },
  {
    "objectID": "Extra/Post-Lesson_01/Post-Lesson_01.html#import-data",
    "href": "Extra/Post-Lesson_01/Post-Lesson_01.html#import-data",
    "title": "Post-Lesson Thoughts 1: Annotations",
    "section": "2.2 Import Data",
    "text": "2.2 Import Data\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")",
    "crumbs": [
      "Extra Stuff",
      "Post-Lesson Thoughts 1: Annotations"
    ]
  },
  {
    "objectID": "Extra/Post-Lesson_01/Post-Lesson_01.html#using-geom_text",
    "href": "Extra/Post-Lesson_01/Post-Lesson_01.html#using-geom_text",
    "title": "Post-Lesson Thoughts 1: Annotations",
    "section": "3.1 Using geom_text()",
    "text": "3.1 Using geom_text()\nFor example, we want to find out how the students did overall for math as compared to the mean Math score. So we will first calculate the mean Math score.\n\nmean_MATHS &lt;- round(mean(exam_data$MATHS),1)\n\n\nggplot(exam_data, aes(x = MATHS)) +\n  geom_histogram() + \n  geom_text(x = mean_MATHS, y = 20, \n            label = paste(\"mean\\n\", mean_MATHS),\n            color = \"blue\")\n\n\n\n\n\n\n\n\nNow we realised that the number is just floating around, so let us draw a line using geom_segment.\n\nggplot(exam_data, aes(x = MATHS)) +\n  geom_histogram() + \n  geom_text(x = mean_MATHS, y = 20, \n            label = paste(\"mean\\n\", mean_MATHS),\n            color = \"blue\") + \n  geom_segment(x = mean_MATHS, xend = mean_MATHS,\n               y = 0, yend = 18, color= \"blue\")",
    "crumbs": [
      "Extra Stuff",
      "Post-Lesson Thoughts 1: Annotations"
    ]
  },
  {
    "objectID": "Extra/Post-Lesson_01/Post-Lesson_01.html#using-geom_label",
    "href": "Extra/Post-Lesson_01/Post-Lesson_01.html#using-geom_label",
    "title": "Post-Lesson Thoughts 1: Annotations",
    "section": "3.2 Using geom_label()",
    "text": "3.2 Using geom_label()\n\nggplot(exam_data, aes(x = MATHS)) +\n  geom_histogram() + \n  geom_label(x = mean_MATHS, y = 20, \n            label = paste(\"mean=\", mean_MATHS),\n            color = \"blue\", fill = \"lightblue\") + \n  geom_segment(x = mean_MATHS, xend = mean_MATHS,\n               y = 0, yend = 18, color= \"blue\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nCurrently geom_label() does not support the check_overlap argument or the angle aesthetic. Also, it is considerably slower than geom_text(). The fill aesthetic controls the background colour of the label.",
    "crumbs": [
      "Extra Stuff",
      "Post-Lesson Thoughts 1: Annotations"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html",
    "title": "Hands-On Exercise 2: Beyond ggplot2 Fundamentals",
    "section": "",
    "text": "In this exercise, we will be introduced to the following ggplot2 extensions to create more elegant and effective statistical graphics:\n\nggrepel: allows us to control the placement of annotation on a graph\nggthemes and hrbrthemes: allows us to create professional publication quality figures\npatchwork: allow us to plot composite figure by combininig ggplot2 graphs",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-On Exercise 2: Beyond ggplot2 Fundamentals"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#installing-and-launching-r-packages",
    "title": "Hands-On Exercise 2: Beyond ggplot2 Fundamentals",
    "section": "2.1 Installing and Launching R packages",
    "text": "2.1 Installing and Launching R packages\nFor this exercise, other than tidyverse, we will use the following packages:\n\nggrepel: an R package that provides geoms for ggplot2 to repel overlapping text labels\nggthemes: an R package that provides extra themes, geoms and scales for ggplot2.\nhrbrthemes: an R package that providestypography-centric themes and theme components for ggplot2\npatchwork: an R package for preparing composite figures created using ggplot2.\n\nThe code chunk below uses p_load() of pacman package to check if the abovementioned packages are installed in the computer. If they are, they will be launched in R. Otherwise, pacman will install the relevant packages before launching them.\n\n\nShow the code\npacman::p_load(tidyverse, ggrepel, patchwork, ggthemes, hrbrthemes)\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that pacman package has already been installed before using the above code chunk. If you have not yet installed pacman please install it via Rstudios’ “Tools” &gt; “Install Packages” before using the above code chunk.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-On Exercise 2: Beyond ggplot2 Fundamentals"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#importing-the-data",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#importing-the-data",
    "title": "Hands-On Exercise 2: Beyond ggplot2 Fundamentals",
    "section": "2.2 Importing the Data",
    "text": "2.2 Importing the Data\nFor this exercise, we will be using the Exam_data provided by the course instructor.It consists of year end examination grades of a cohort of primary 3 students from a local school. It is in csv file format.\nWe use read_csv() function of readr to import the Exam_data csv file into R then we will use glimpse() of dplyr to learn about the associated attribute information in the dataframe.\n\nCodeData\n\n\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")\n\n\n\n\nglimpse(exam_data)\n\nRows: 322\nColumns: 7\n$ ID      &lt;chr&gt; \"Student321\", \"Student305\", \"Student289\", \"Student227\", \"Stude…\n$ CLASS   &lt;chr&gt; \"3I\", \"3I\", \"3H\", \"3F\", \"3I\", \"3I\", \"3I\", \"3I\", \"3I\", \"3H\", \"3…\n$ GENDER  &lt;chr&gt; \"Male\", \"Female\", \"Male\", \"Male\", \"Male\", \"Female\", \"Male\", \"M…\n$ RACE    &lt;chr&gt; \"Malay\", \"Malay\", \"Chinese\", \"Chinese\", \"Malay\", \"Malay\", \"Chi…\n$ ENGLISH &lt;dbl&gt; 21, 24, 26, 27, 27, 31, 31, 31, 33, 34, 34, 36, 36, 36, 37, 38…\n$ MATHS   &lt;dbl&gt; 9, 22, 16, 77, 11, 16, 21, 18, 19, 49, 39, 35, 23, 36, 49, 30,…\n$ SCIENCE &lt;dbl&gt; 15, 16, 16, 31, 25, 16, 25, 27, 15, 37, 42, 22, 32, 36, 35, 45…\n\n\n\n\n\nFrom the output from glimpse(), we note that:\n\nThere are a total of seven attributes in the exam_data tibble data frame.\nFour of these attributes are categorical data: ID, CLASS, GENDER and RACE .\nThree of these attributes are continuous data: MATHS, ENGLISH and SCIENCE.\n\nWe will use the summarize()to get the summary statistics of the continuous data: MATHS, ENGLISH and SCIENCE.\n\nSummary Statistics for MATHS ScoresSummary Statistics for ENGLISH ScoresSummary Statistics for SCIENCE Scores\n\n\n\nexam_data %&gt;% \n  summarize(min = min(MATHS),\n            q1 = quantile(MATHS, 0.25),\n            median = median(MATHS),\n            mean = mean(MATHS),\n            q3 = quantile(MATHS, 0.75),\n            max = max(MATHS))\n\n# A tibble: 1 × 6\n    min    q1 median  mean    q3   max\n  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     9    58     74  69.3    85    99\n\n\n\n\n\nexam_data %&gt;% \n  summarize(min = min(ENGLISH),\n            q1 = quantile(ENGLISH, 0.25),\n            median = median(ENGLISH),\n            mean = mean(ENGLISH),\n            q3 = quantile(ENGLISH, 0.75),\n            max = max(ENGLISH))\n\n# A tibble: 1 × 6\n    min    q1 median  mean    q3   max\n  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    21    59     70  67.2    78    96\n\n\n\n\n\nexam_data %&gt;% \n  summarize(min = min(SCIENCE),\n            q1 = quantile(SCIENCE, 0.25),\n            median = median(SCIENCE),\n            mean = mean(SCIENCE),\n            q3 = quantile(SCIENCE, 0.75),\n            max = max(SCIENCE))\n\n# A tibble: 1 × 6\n    min    q1 median  mean    q3   max\n  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    15  49.2     65  61.2  74.8    96\n\n\n\n\n\n\nIn Hands-on Exercise 1, I used the summary() function of Basic R package to calculate the summary statistics for the continous attribute in the exam_data dataset. For this exercise, I used the summarize() function of dplyr package. While the dplyr package requires more codes to calculate the summary statistics as compared to the summary() function, it offers more flexibility and also returns a tibble dataframe. Hence, whether to use summary() or summarize() depends on user preference and what we want to do with the output.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-On Exercise 2: Beyond ggplot2 Fundamentals"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#using-ggrepel",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#using-ggrepel",
    "title": "Hands-On Exercise 2: Beyond ggplot2 Fundamentals",
    "section": "3.1 Using ggrepel",
    "text": "3.1 Using ggrepel\nggrepel is an extension of ggplot2 package. It provides geoms of ggplot2 to repel overlapping text by replacing geom_text() with geom_text_repel() and replacing geom_label() with geom_label_repel().\nLet us try it for the above chart!\n\n\nShow the code\nggplot(data = exam_data, \n       aes(x= MATHS, y= ENGLISH)) +\n  geom_point() + \n  geom_smooth(method = lm, \n              size=0.5)+\n  coord_cartesian(xlim = c(0,100), \n                  ylim = c(0,100))+\n  geom_label_repel(aes(label = ID), \n             fontface = \"bold\") +\n  ggtitle(\"English Scores versus Maths Scores for Primary 3 Students (with ggrepel)\") \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn the above output, we end up with only 5 labelled points, even though we did not limit or specify the number of labels. This is because ggrepel will discard some text labels if they overlap too many other things (default limit is 10). So if a text label overlaps 10 other text labels or data points, then it will be discarded.\nWe can expect to see a warning if some data points could not be labeled due to too many overlaps.\nSet max.overlaps = Inf to override this behavior and always show all labels, regardless of whether or not a text label overlaps too many other things.\nUse options(ggrepel.max.overlaps = Inf) to set this globally for your entire session. The global option can be overridden by providing the max.overlaps argument to geom_text_repel().",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-On Exercise 2: Beyond ggplot2 Fundamentals"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#other-examples-of-ggrepel",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#other-examples-of-ggrepel",
    "title": "Hands-On Exercise 2: Beyond ggplot2 Fundamentals",
    "section": "3.2 Other examples of ggrepel",
    "text": "3.2 Other examples of ggrepel\nThere are other exciting customisations that we can do for ggreply. Here are some examples that I think will be useful for my personal work.\n\n3.2.1 Hide Some of the labels (or show only certain labels)\nt For example, we only want to label and highlight those students who score less than 30 for both English and Maths amongst those students who score less than 50 for both English and Maths (assuming 50 is the passing score), we will: 1. Subset the tibble dataframe to contain only those who did not meet the passing score of English and Maths. 2. Create a new column to set the IDs of those students who score more than 30 for English and Maths to an empty string “” to hide them, and those who score less than 30 for both English and Maths would show their ID. 3. Then we plot the scatterplot using geom_point and also indicate to color those points with students who score less than 30 for both English and Maths in red.\n\n\nShow the code\nexam_data2 &lt;- subset(exam_data, MATHS &lt; 50 & ENGLISH &lt; 50)\n\nexam_data2$ID_select &lt;- ifelse(exam_data2$MATHS &lt;30 & exam_data2$ENGLISH &lt;30, exam_data2$ID, \"\")\n\nggplot(exam_data2, \n       aes(MATHS, ENGLISH, label = ID_select)) +\n  geom_text_repel() + \n  geom_point(color = ifelse(exam_data2$MATHS &lt; 30 & exam_data2$ENGLISH &lt; 30, \"red\", \"grey50\"))+\n  coord_cartesian(xlim = c(0,50), \n                  ylim = c(0,50))\n\n\n\n\n\n\n\n\n\n\n\n3.2.2 Using ggrepel with stat_summary()\nWe can use stat_summary() with geom = “text_repel”. The position_nudge_repel() function nudges the text label’s position, but it also remembers the original position of the data point.\n\n\nShow the code\nggplot(exam_data, aes(factor(CLASS), MATHS)) + \n  stat_summary(\n    fill = \"darkseagreen\",\n    color = \"black\", \n    fun = \"mean\", \n    geom = \"col\") + stat_summary(\n      aes(label = round(stat(y))),\n      fun = \"mean\",\n      geom = \"text_repel\",\n      min.segment.length = 0, \n      position = position_nudge_repel(y = -2)) +\n  labs(title = \"Mean Maths Scores Across Classes\")\n\n\n\n\n\n\n\n\n\nYou can learn more about ggrepel here!",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-On Exercise 2: Beyond ggplot2 Fundamentals"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#ggthemes",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#ggthemes",
    "title": "Hands-On Exercise 2: Beyond ggplot2 Fundamentals",
    "section": "4.1 ggthemes",
    "text": "4.1 ggthemes\nggthemes provides ggplots2 with themes that replicate the look of plots by Edward Tufte, Stephen Few, Fivethirtyeight, The Economist, ‘Stata’, ‘Excel’, and The Wall Street Journal, among others.\n\ntheme_economist()theme_solarized()\n\n\n\n\nShow the code\nggplot(exam_data, \n       aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color = \"black\",\n                 fill = \"darkslategrey\") + \n  geom_vline(aes(xintercept=mean(MATHS)),\n            color=\"deepskyblue\", linetype=\"dashed\", size=0.7)+\n  annotate(\"text\",\n           x = mean(exam_data$MATHS), \n           y = 40, \n           label = paste(\"Mean=\", round(mean(exam_data$MATHS))),\n           col = \"black\") + \n  theme_economist() + \n  ggtitle(\"Distribution of Maths Scores\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nggplot(exam_data, \n       aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color = \"black\",\n                 fill = \"darkslategrey\") + \n  geom_vline(aes(xintercept=mean(MATHS)),\n            color=\"deepskyblue\", linetype=\"dashed\", size=0.7)+\n  annotate(\"text\",\n           x = mean(exam_data$MATHS), \n           y = 40, \n           label = paste(\"Mean=\", round(mean(exam_data$MATHS))),\n           col = \"black\") + \n  theme_solarized() + \n  ggtitle(\"Distribution of Maths Scores\")",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-On Exercise 2: Beyond ggplot2 Fundamentals"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#hrbrthemes",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#hrbrthemes",
    "title": "Hands-On Exercise 2: Beyond ggplot2 Fundamentals",
    "section": "4.2 hrbrthemes",
    "text": "4.2 hrbrthemes\nhrbrthemes package provides a base theme that focuses on typographic elements, including where various labels are placed as well as the fonts that are used.\n\ntheme_ipsum_rc()theme_ft_rc()\n\n\n\n\nShow the code\nggplot(exam_data, \n       aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color = \"black\",\n                 fill = \"darkslategrey\") + \n  geom_vline(aes(xintercept=mean(MATHS)),\n            color=\"deepskyblue\", linetype=\"dashed\", linewidth=0.7)+\n  annotate(\"text\",\n           x = mean(exam_data$MATHS), \n           y = 40, \n           label = paste(\"Mean=\", round(mean(exam_data$MATHS))),\n           col = \"black\") + \n  ggtitle(\"Distribution of Maths Scores\") + \n  theme_ipsum_rc()\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nggplot(exam_data, aes(MATHS, ENGLISH)) +\n  geom_point(color = ft_cols$yellow, show.legend = FALSE) + \n  labs(title=\"Maths Scores Versus English Scores for Primary 3 Students\",\n       subtitle = \"Passing Score for Maths and English is 50 marks\",\n       caption = \"VAA Hands-on Exercise 2\")+ \n  geom_vline(aes(xintercept=50, color=ft_cols$peach), show.legend = FALSE) + \n  geom_hline(aes(yintercept=50, color=ft_cols$peach), show.legend = FALSE)+\n  coord_cartesian(xlim = c(0,100),\n                  ylim = c(0,100)) + \n  theme_ft_rc(font_rc)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHit a Speed Bump Here! \n\n\n\nWhen trying out this section I encountered error messages stating that I do not have the font family in Windows font database.\nSo here’s what I have tried:\n\nInstall the fonts from hrbrthemes folder (found in “R” folder’s “library” folder) but that did not work.\nSo I found some websites stating that I should import_roboto_condensed() first and also install the fonts on my system before trying to use this theme. So I imported all the fonts needed for hrbrtheme and this method works for me!\n\nReferences used are here, here and here.\n\n\nThe second goal centers around productivity for a production workflow. In fact, this “production workflow” is the context for where the elements of hrbrthemes should be used.\nA “production workflow” is when you intend for the output of your work to be put into a publication of some kind, whether it be a blog post, academic paper, presentation, internal report or industry publication. When you’re cranking through an analysis, the visual elements don’t need to be perfect. They are there to validate/support your work and are more of a starting point for the finished product than anything else. The level of attention to detail on the final graphical products can be a great motivator for your audience to either dive deep into your analysis text or relegate it to the TLDR pile.\nSounds like hrbrtheme package has its own ideas and views on what makes an effective and visually appealing chart! Read more about why they chose certain fonts here!\nLet us try to explore more about hrbrthemes using the following code chunk.\n\n\nShow the code\nggplot(exam_data, aes(MATHS, ENGLISH)) +\n  geom_point(color = ft_cols$yellow, show.legend = FALSE) + \n  labs(title=\"Maths Scores Versus English Scores for Primary 3 Students\",\n       subtitle = \"Passing Score for Maths and English is 50 marks\",\n       caption = \"VAA Hands-on Exercise 2\")+ \n  geom_vline(aes(xintercept=50, color=ft_cols$peach), show.legend = FALSE) + \n  geom_hline(aes(yintercept=50, color=ft_cols$peach), show.legend = FALSE)+\n  coord_cartesian(xlim = c(0,100),\n                  ylim = c(0,100)) + \n  theme_ft_rc(plot_title_size = 15, \n              subtitle_size = 10, \n              axis_title_size = 10, \n              caption_size = 10,\n              base_size = 10,\n              plot_margin = margin(10,10,10,10),\n              grid = \"Y\") \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat the arguments in the above code chunk’s theme_ft_rc means\n\n\n\n\nplot_title_size argument decreased the font size of the chart title from 18 (default) to 15\nsubtitle_size argument decreased the font size of the subtitle from 13 (default) to 10\naxis_title_size argument increased the font size of the axis title from 9 (default) to 10\ncaption_size argument increased the font size from 9 (default) to 10\nbase_size argument decreased the default axis label from 11.5 (default) to 10\nplot_margin argument narrowed the margins of all 4 sides from 30 (default) to 10.\ngrid argument removed the x-axis grid lines\n\nFor more details on the arguments of the theme, take a look here!",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-On Exercise 2: Beyond ggplot2 Fundamentals"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#combining-two-ggplot2-graphs",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#combining-two-ggplot2-graphs",
    "title": "Hands-On Exercise 2: Beyond ggplot2 Fundamentals",
    "section": "5.1 Combining two ggplot2 graphs",
    "text": "5.1 Combining two ggplot2 graphs\nThe figure in the tabset below shows a composite of two histograms created using patchwork. Note how simple the syntax used to create the plot!\n\n\nShow the code\np1 + p2",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-On Exercise 2: Beyond ggplot2 Fundamentals"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#combining-three-ggplot2-graphs",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#combining-three-ggplot2-graphs",
    "title": "Hands-On Exercise 2: Beyond ggplot2 Fundamentals",
    "section": "5.2 Combining three ggplot2 graphs",
    "text": "5.2 Combining three ggplot2 graphs\nIn the code chunk we below, we use tThe pipe sign | will place subplots group (i.e. p1 and p2 next to another plot (i.e. p3) while the division sign / will place p1 and p2 on top of each other.\n\n\nShow the code\n(p1 / p2) | p3",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-On Exercise 2: Beyond ggplot2 Fundamentals"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#creating-a-composite-figure-with-a-tag",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#creating-a-composite-figure-with-a-tag",
    "title": "Hands-On Exercise 2: Beyond ggplot2 Fundamentals",
    "section": "5.3 Creating a composite figure with a tag",
    "text": "5.3 Creating a composite figure with a tag\nIn order to identify subplots in text, patchwork also provides auto-tagging capabilities as shown below.\n\n\nShow the code\n((p1 / p2) | p3) + plot_annotation(tag_levels = \"A\")",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-On Exercise 2: Beyond ggplot2 Fundamentals"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#creating-figure-with-inset_element",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#creating-figure-with-inset_element",
    "title": "Hands-On Exercise 2: Beyond ggplot2 Fundamentals",
    "section": "5.4 Creating Figure with inset_element()",
    "text": "5.4 Creating Figure with inset_element()\nBesides providing functions to place plots next to each other based on the provided layout, with inset_element() of patchwork, we can place one or several plots or graphic elements freely on top or below another plot.\n\n\nShow the code\np3 + inset_element(p2, \n                   left = 0,\n                   bottom = 0.6,\n                   right = 0.4,\n                   top =1, align_to = 'panel')",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-On Exercise 2: Beyond ggplot2 Fundamentals"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#creating-a-composite-figure-using-patchwork-and-ggtheme",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#creating-a-composite-figure-using-patchwork-and-ggtheme",
    "title": "Hands-On Exercise 2: Beyond ggplot2 Fundamentals",
    "section": "5.5 Creating a composite figure using patchwork and ggtheme",
    "text": "5.5 Creating a composite figure using patchwork and ggtheme\nWe can also add a theme from ggtheme to the output created from patchwork, as shown in the code chunk below.\n\npatchwork &lt;- (p1 / p2) | p3\npatchwork & theme_calc()",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-On Exercise 2: Beyond ggplot2 Fundamentals"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03b.html",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03b.html",
    "title": "Hands-On Exercise 3b: Programming Animated Statistical Graphics with R",
    "section": "",
    "text": "In this exercise, we will learn how to create animated data visualisation by using gganimate and plotly r packages. At the same time, we will also learn how to (i) reshape data by using tidyr package, and (ii) process, wrangle and transform data by using dplyr package.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-On Exercise 3b: Programming Animated Statistical Graphics with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03b.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03b.html#installing-and-launching-r-packages",
    "title": "Hands-On Exercise 3b: Programming Animated Statistical Graphics with R",
    "section": "3.1 Installing and Launching R packages",
    "text": "3.1 Installing and Launching R packages\nFor this exercise, other than tidyverse, we will use the following packages:\n\nplotly: an R library for plotting interactive statistical graphs\ngganimate: an ggplot extension for creating animated statistical graphs\ngifski: converts video frames to GIF animations using pngquant’s fancy features for efficient crossframe palettes and temporal dithering. It produces animated GIFs that use thousands of colours per frame.\ngapminder: an excerpt of the data available at Gapminder.org. We want to use its country_colors scheme for this exercise.\n\nThe code chunk below uses p_load() of pacman package to check if the abovementioned packages are installed in the computer. If they are, they will be launched in R. Otherwise, pacman will install the relevant packages before launching them.\n\n\nShow the code\npacman::p_load(readxl, gifski, gapminder, plotly, gganimate, tidyverse)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-On Exercise 3b: Programming Animated Statistical Graphics with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03b.html#importing-the-data",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03b.html#importing-the-data",
    "title": "Hands-On Exercise 3b: Programming Animated Statistical Graphics with R",
    "section": "3.2 Importing the Data",
    "text": "3.2 Importing the Data\nIn this hands-on exercise, the Data worksheet from GlobalPopulation Excel workbook will be used.We will first use read_xls() of readxl package to import the excel sheet, then use mutate_each_() of dplyr package to convert all character data type into factor, then we will use mutate() to convert data values of the Year field into integer.\n\nUsing mutate_each_()Using mutate_at()Using across()\n\n\n\ncol &lt;- c(\"Country\", \"Continent\")\n\nglobalpop &lt;- read_xls(\"data/GlobalPopulation.xls\",\n                      sheet = \"Data\") %&gt;%\n  mutate_each_(funs(factor(.)), col) %&gt;%\n  mutate(Year = as.integer(Year))\n\n\n\nUnfortunately, mutate_each_() was deprecated in dplyr 0.7.0. and funs()was deprecated in dplyr 0.8.0. In view of this, we will re-write the code by using mutate_at() as shown in the code chunk below.\n\ncol &lt;- c(\"Country\", \"Continent\")\n\nglobalpop &lt;- read_xls(\"data/GlobalPopulation.xls\",\n                      sheet = \"Data\") %&gt;%\n  mutate_at(col, as.factor) %&gt;%\n  mutate(Year = as.integer(Year))\n\n\n\nWe can also use across() instead of mutate_at() to derive the same output.\n\ncol &lt;- c(\"Country\", \"Continent\")\n\nglobalpop &lt;- read_xls(\"data/GlobalPopulation.xls\",\n                      sheet = \"Data\") %&gt;%\n  mutate(across(col, as.factor)) %&gt;%\n  mutate(Year = as.integer(Year))",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-On Exercise 3b: Programming Animated Statistical Graphics with R"
    ]
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html",
    "title": "Take-home Exercise 1",
    "section": "",
    "text": "Programme for International Student Assessment (PISA) is a study conducted by the Organisation for Economic Co-operation and Development (OECD) that measures 15-year-olds’ ability to use their reading, mathematics and science knowledge and skills to meet real life challenges. It was first performed in 2000 and then repeated every three years. The results of the 2022 PISA were released in December 2023.\nIn this take-home exercise, I will be using appropriate Exploratory Data Analysis (EDA) methods and ggplot2 functions to reveal:\n\nthe distribution of Singapore students’ performance in mathematics, reading and science, and\nthe relationship between Singapore students’ performance with students’ schools, gender and socioeconomic status.\n\n\n\nWe can download the PISA 2022 dataset with the full set of responses from individual students, school principals, teachers and parents from PISA 2022 Database.\nThe main data files for 2022 PISA are:\n\nstudent-questionnaire data file (which also includes estimates of student performance and parent-questionnaire data);\nschool-questionnaire data file;\nteacher-questionnaire data file;\ncognitive item data file; and\nquestionnaire timing data.\n\nThese data files are in SAS and SPSS formats. For the purpose of this exercise, I will be using thestudent-questionnaire data file only."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#about-pisa-data-files",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#about-pisa-data-files",
    "title": "Take-home Exercise 1",
    "section": "",
    "text": "We can download the PISA 2022 dataset with the full set of responses from individual students, school principals, teachers and parents from PISA 2022 Database.\nThe main data files for 2022 PISA are:\n\nstudent-questionnaire data file (which also includes estimates of student performance and parent-questionnaire data);\nschool-questionnaire data file;\nteacher-questionnaire data file;\ncognitive item data file; and\nquestionnaire timing data.\n\nThese data files are in SAS and SPSS formats. For the purpose of this exercise, I will be using thestudent-questionnaire data file only."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#loading-r-packages",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#loading-r-packages",
    "title": "Take-home Exercise 1",
    "section": "2.1 Loading R packages",
    "text": "2.1 Loading R packages\nFor this exercise, we will be using the following packages:\n\ntidyverse : to load the core tidyverse packages, which includes ggplot2 and dplyr.\nhaven : to read and write various data formats used by other statistical packages by wrapping the ReadStat C library. It is part of the tidyverse family too! haven currently supports SAS, SPSS and Stata. We will need haven to import the PISA 2022’s student questionnaire data file because it is in SAS file type.\npatchwork: to create composition of ggplot2 plots using arithmetic operators.\nggrepel: to repel overlapping text labels away from each other.\nggdist: provides stats and geoms for visualising distributions and uncertainty.\nggridges: provides geoms to plot ridgeline plots, which are partially overlapping line plots that create the impression of a mountain range.\nknitr: provides a general-purpose tool for dynamic report generation in R. We will use this to mainly help us generate simple tables.\n\nThe code chunk below uses p_load() of pacman package to check if the abovementioned packages are installed in the computer. If they are, they will be launched in R. Otherwise, pacman will install the relevant packages before launching them.\n\n\nShow the code\npacman::p_load(tidyverse, haven, patchwork, ggrepel, ggdist, ggridges, knitr)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#importing-the-data-into-r",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#importing-the-data-into-r",
    "title": "Take-home Exercise 1",
    "section": "2.2 Importing the Data into R",
    "text": "2.2 Importing the Data into R\nFor this exercise, we are using PISA 2022 database’s student questionnaire data file. As the data file is in SAS file format, we will use haven’s read_sas() function to import the data into R environment. Then we will filter the data to those data from Singapore since this exercise is only focusing on responses from Singapore students.\n\nImport the Data (All Countries)Filter the Imported Data (Singapore only)\n\n\n\nstu_qqq &lt;- read_sas(\"data/cy08msp_stu_qqq.sas7bdat\")\n\n\n\n\nstu_qqq_SG &lt;- stu_qqq %&gt;% \n  filter(CNT == \"SGP\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#saving-stu_qqq_sg-into-rds-format",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#saving-stu_qqq_sg-into-rds-format",
    "title": "Take-home Exercise 1",
    "section": "2.3 Saving stu_qqq_SG into RDS format",
    "text": "2.3 Saving stu_qqq_SG into RDS format\nLet us save the filtered data into an R data format (RDS) so that we can easily retrieve in future without importing the stu_qqq dataset again because the entire stu_qqq file is very big (more than 3GB)!\n\n\nShow the code\nwrite_rds(stu_qqq_SG, \"data/stu_qqq_SG.rds\")\n\n\nWe will read the stu_qqq_SG.rdsusing the following code chunk.\n\n\nShow the code\nstu_qqq_SG &lt;- read_rds(\"data/stu_qqq_SG.rds\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#filtering-the-columns-for-this-exercise",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#filtering-the-columns-for-this-exercise",
    "title": "Take-home Exercise 1",
    "section": "3.1 Filtering the columns for this exercise",
    "text": "3.1 Filtering the columns for this exercise\nThere are a lot of columns in stu_qqq_SG so let us filter out the columns in stu_qqq_SG that we want to explore for this exercise. We selected columns that were related to the students demographics (such as school, gender, age), indices related to socio-economic status and the columns containing PV values for Maths, Reading and Science.\n\n\nShow the code\nstu2 &lt;- stu_qqq_SG %&gt;% \n  select(CNTSTUID,CNTSCHID, ST004D01T, PROGN, AGE, ST003D02T, ST003D03T, ESCS, PAREDINT, HISEI, HOMEPOS,MISCED,FISCED,HISCED,LEARRES, SCHSUST,FAMSUP, WORKHOME,WORKPAY, STUDYHMW, EXERPRAC, LANGN, PQSCHOOL, ICTOUT, ICTAVHOM,  ICTHOME, ICTAVSCH, ICTSCH, ICTRES, COBN_S,COBN_M, COBN_F, OCOD1, OCOD2, PA042Q01TA, PA194Q01JA, PA195Q01JA, PA041Q01TA,  ends_with(\"MATH\"), ends_with(\"READ\"), ends_with(\"SCIE\"), )\n\nglimpse(stu2)\n\n\nRows: 6,606\nColumns: 68\n$ CNTSTUID   &lt;dbl&gt; 70200001, 70200002, 70200003, 70200004, 70200005, 70200006,…\n$ CNTSCHID   &lt;dbl&gt; 70200052, 70200134, 70200112, 70200004, 70200152, 70200043,…\n$ ST004D01T  &lt;dbl&gt; 1, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 1, 1, 2, 1, 2, 1, 2, 1, 1,…\n$ PROGN      &lt;chr&gt; \"07020002\", \"07020002\", \"07020002\", \"07020002\", \"07020002\",…\n$ AGE        &lt;dbl&gt; 15.50, 15.83, 15.75, 16.17, 15.58, 15.58, 16.08, 16.00, 15.…\n$ ST003D02T  &lt;dbl&gt; 10, 6, 7, 2, 9, 9, 3, 4, 8, 6, 10, 7, 9, 11, 5, 10, 11, 4, …\n$ ST003D03T  &lt;dbl&gt; 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006,…\n$ ESCS       &lt;dbl&gt; 0.1836, 0.8261, -1.0357, -0.9606, 0.0856, 0.1268, -0.0154, …\n$ PAREDINT   &lt;dbl&gt; 16.0, 14.5, 12.0, 12.0, 14.5, 16.0, 12.0, 16.0, 16.0, 16.0,…\n$ HISEI      &lt;dbl&gt; 30.34, 77.10, 17.00, 43.33, 75.54, 57.64, 70.34, 80.78, 65.…\n$ HOMEPOS    &lt;dbl&gt; 0.7524, 0.7842, 0.0666, -0.9300, -0.8949, -0.5988, 0.0975, …\n$ MISCED     &lt;dbl&gt; 8, 7, 4, 6, 7, 7, 6, 9, 8, 8, 4, 9, 10, 6, 4, 9, 8, 8, 6, 6…\n$ FISCED     &lt;dbl&gt; 7, 7, 4, 6, 7, 9, 2, 8, 8, 7, 4, 9, 10, 9, 6, 4, 9, 8, 6, 7…\n$ HISCED     &lt;dbl&gt; 8, 7, 4, 6, 7, 9, 6, 9, 8, 8, 4, 9, 10, 9, 6, 9, 9, 8, 6, 7…\n$ LEARRES    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ SCHSUST    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ FAMSUP     &lt;dbl&gt; -0.3780, -0.5969, -1.0537, -0.8521, 1.7459, 1.7327, -0.8815…\n$ WORKHOME   &lt;dbl&gt; 10, 2, 0, 10, 5, 5, 7, 0, 0, 4, 2, 2, 10, 0, 10, 0, 5, 5, 0…\n$ WORKPAY    &lt;dbl&gt; 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ STUDYHMW   &lt;dbl&gt; 4, 7, 3, 5, 7, 10, 0, 10, 5, 3, 5, 5, 10, 0, 8, 5, 5, 5, 10…\n$ EXERPRAC   &lt;dbl&gt; 1, 4, 2, 5, 9, 1, 2, 0, 3, 5, 1, 2, 5, 2, 4, 0, 2, 3, 2, 2,…\n$ LANGN      &lt;dbl&gt; 998, 998, 998, 998, 998, 998, 998, 998, 998, 998, 998, 998,…\n$ PQSCHOOL   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ ICTOUT     &lt;dbl&gt; 0.2260, -0.8080, 0.1088, -1.2894, 2.9804, 0.3464, -0.4834, …\n$ ICTAVHOM   &lt;dbl&gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6, 6, 6, 5, 6, 6, 6, 6,…\n$ ICTHOME    &lt;dbl&gt; 0.3346, 0.3346, 0.3346, 0.3346, 0.3346, 0.3346, 0.3346, 0.3…\n$ ICTAVSCH   &lt;dbl&gt; 7, 7, 7, 7, 5, 6, 7, 6, 7, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,…\n$ ICTSCH     &lt;dbl&gt; 0.4062, 0.4062, 0.4062, 0.4062, -1.6647, -0.8411, 0.4062, -…\n$ ICTRES     &lt;dbl&gt; 0.1940, 0.6249, -0.3987, -0.9028, 0.2514, -0.4733, 0.9904, …\n$ COBN_S     &lt;chr&gt; \"070200\", \"070200\", \"070200\", \"070200\", \"070200\", \"970200\",…\n$ COBN_M     &lt;chr&gt; \"070200\", \"070200\", \"970200\", \"070200\", \"070200\", \"970200\",…\n$ COBN_F     &lt;chr&gt; \"070200\", \"070200\", \"070200\", \"070200\", \"070200\", \"970200\",…\n$ OCOD1      &lt;chr&gt; \"9701\", \"31\", \"9701\", \"41\", \"23\", \"9701\", \"11\", \"23\", \"1\", …\n$ OCOD2      &lt;chr&gt; \"83\", \"21\", \"9704\", \"9705\", \"83\", \"34\", \"31\", \"21\", \"14\", \"…\n$ PA042Q01TA &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ PA194Q01JA &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ PA195Q01JA &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ PA041Q01TA &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ PV1MATH    &lt;dbl&gt; 639.004, 697.191, 693.710, 427.317, 436.462, 569.982, 771.6…\n$ PV2MATH    &lt;dbl&gt; 601.251, 754.277, 654.450, 410.376, 453.450, 539.609, 672.8…\n$ PV3MATH    &lt;dbl&gt; 621.480, 671.940, 696.938, 423.586, 392.315, 531.648, 653.7…\n$ PV4MATH    &lt;dbl&gt; 631.596, 657.300, 646.187, 388.935, 439.986, 534.368, 734.8…\n$ PV5MATH    &lt;dbl&gt; 579.276, 621.126, 678.119, 330.962, 443.125, 465.815, 727.5…\n$ PV6MATH    &lt;dbl&gt; 591.791, 655.729, 644.019, 379.988, 452.648, 528.509, 729.8…\n$ PV7MATH    &lt;dbl&gt; 600.709, 747.934, 720.531, 398.535, 396.970, 514.326, 597.2…\n$ PV8MATH    &lt;dbl&gt; 587.322, 694.365, 671.425, 422.127, 459.945, 521.029, 772.2…\n$ PV9MATH    &lt;dbl&gt; 618.131, 742.732, 694.085, 375.354, 438.166, 472.382, 694.3…\n$ PV10MATH   &lt;dbl&gt; 581.973, 656.934, 668.304, 453.348, 448.084, 503.387, 725.2…\n$ PV1READ    &lt;dbl&gt; 676.298, 625.585, 620.116, 381.495, 448.199, 469.441, 744.5…\n$ PV2READ    &lt;dbl&gt; 692.247, 686.716, 559.078, 400.815, 560.636, 500.350, 679.8…\n$ PV3READ    &lt;dbl&gt; 690.981, 663.147, 554.767, 374.911, 365.478, 375.703, 635.1…\n$ PV4READ    &lt;dbl&gt; 643.067, 567.435, 587.026, 367.484, 469.970, 377.452, 725.5…\n$ PV5READ    &lt;dbl&gt; 627.908, 614.500, 591.806, 336.009, 503.664, 470.781, 731.1…\n$ PV6READ    &lt;dbl&gt; 684.676, 604.745, 570.547, 324.630, 481.215, 415.448, 684.6…\n$ PV7READ    &lt;dbl&gt; 661.380, 669.375, 599.078, 396.242, 436.800, 448.547, 646.0…\n$ PV8READ    &lt;dbl&gt; 674.070, 623.735, 545.610, 374.723, 531.226, 434.381, 756.8…\n$ PV9READ    &lt;dbl&gt; 666.282, 649.579, 610.466, 314.704, 480.997, 411.703, 653.7…\n$ PV10READ   &lt;dbl&gt; 657.387, 571.261, 590.758, 342.956, 478.578, 410.846, 784.7…\n$ PV1SCIE    &lt;dbl&gt; 710.634, 670.646, 666.095, 340.308, 456.333, 475.158, 693.8…\n$ PV2SCIE    &lt;dbl&gt; 618.739, 748.839, 604.771, 329.889, 453.400, 470.030, 626.9…\n$ PV3SCIE    &lt;dbl&gt; 591.623, 635.443, 704.217, 411.353, 498.937, 461.218, 627.3…\n$ PV4SCIE    &lt;dbl&gt; 659.770, 639.735, 687.659, 327.974, 532.324, 504.199, 676.7…\n$ PV5SCIE    &lt;dbl&gt; 635.892, 608.385, 690.974, 292.183, 508.231, 486.930, 661.8…\n$ PV6SCIE    &lt;dbl&gt; 646.901, 670.662, 617.175, 355.423, 504.461, 493.011, 618.3…\n$ PV7SCIE    &lt;dbl&gt; 603.569, 734.807, 692.886, 400.182, 404.572, 469.950, 602.0…\n$ PV8SCIE    &lt;dbl&gt; 621.352, 639.748, 630.900, 317.518, 549.457, 464.012, 653.9…\n$ PV9SCIE    &lt;dbl&gt; 659.674, 716.768, 656.620, 298.893, 411.062, 440.113, 645.4…\n$ PV10SCIE   &lt;dbl&gt; 649.719, 655.670, 649.087, 362.702, 473.613, 495.410, 662.5…\n\n\nFrom the above output, we see that there are some rows with missing values. As PISA is an international survey, there could be some columns (i.e. questions) which are not applicable to Singapore and the entire column would be NA. Let us find out which are the columns that have the entire column filled with NA values in stu2 dataframe using the following code chunk.\n\n\nShow the code\nnames(which(colSums(is.na(stu2))== nrow(stu2)))\n\n\n[1] \"LEARRES\"    \"SCHSUST\"    \"PQSCHOOL\"   \"PA042Q01TA\" \"PA194Q01JA\"\n[6] \"PA195Q01JA\" \"PA041Q01TA\"\n\n\nFrom the above output, we see that there are 7 columns which contain all NA values. A quick check with the codebook showed these columns are:\n\nLEARRES: Types of learning resources used while school was closed (WLE)\nSCHSUST: School actions/activities to sustain learning (WLE)\nPQSCHOOL: School quality (WLE)\nPA042Q01TA: What is your annual household income?\nPA194Q01JA: How many [digital devices] with screens are there in your home?\nPA195Q01JA: How many books are there in your home?\nPA041Q01TA: In the last twelve months, about how much would you have paid to educational providers for services?\n\nLet us remove these columns from the stu2 dataframe. From the output we see that the number of columns dropped from 68 to 61, which is slightly more manageable than the earlier dataframe. The number of rows remained the same.\n\n\nShow the code\nstu2 &lt;- stu2[, (colSums(is.na(stu2))&lt;nrow(stu2))]\nglimpse(stu2)\n\n\nRows: 6,606\nColumns: 61\n$ CNTSTUID  &lt;dbl&gt; 70200001, 70200002, 70200003, 70200004, 70200005, 70200006, …\n$ CNTSCHID  &lt;dbl&gt; 70200052, 70200134, 70200112, 70200004, 70200152, 70200043, …\n$ ST004D01T &lt;dbl&gt; 1, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 1, 1, 2, 1, 2, 1, 2, 1, 1, …\n$ PROGN     &lt;chr&gt; \"07020002\", \"07020002\", \"07020002\", \"07020002\", \"07020002\", …\n$ AGE       &lt;dbl&gt; 15.50, 15.83, 15.75, 16.17, 15.58, 15.58, 16.08, 16.00, 15.6…\n$ ST003D02T &lt;dbl&gt; 10, 6, 7, 2, 9, 9, 3, 4, 8, 6, 10, 7, 9, 11, 5, 10, 11, 4, 7…\n$ ST003D03T &lt;dbl&gt; 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, …\n$ ESCS      &lt;dbl&gt; 0.1836, 0.8261, -1.0357, -0.9606, 0.0856, 0.1268, -0.0154, 1…\n$ PAREDINT  &lt;dbl&gt; 16.0, 14.5, 12.0, 12.0, 14.5, 16.0, 12.0, 16.0, 16.0, 16.0, …\n$ HISEI     &lt;dbl&gt; 30.34, 77.10, 17.00, 43.33, 75.54, 57.64, 70.34, 80.78, 65.1…\n$ HOMEPOS   &lt;dbl&gt; 0.7524, 0.7842, 0.0666, -0.9300, -0.8949, -0.5988, 0.0975, 0…\n$ MISCED    &lt;dbl&gt; 8, 7, 4, 6, 7, 7, 6, 9, 8, 8, 4, 9, 10, 6, 4, 9, 8, 8, 6, 6,…\n$ FISCED    &lt;dbl&gt; 7, 7, 4, 6, 7, 9, 2, 8, 8, 7, 4, 9, 10, 9, 6, 4, 9, 8, 6, 7,…\n$ HISCED    &lt;dbl&gt; 8, 7, 4, 6, 7, 9, 6, 9, 8, 8, 4, 9, 10, 9, 6, 9, 9, 8, 6, 7,…\n$ FAMSUP    &lt;dbl&gt; -0.3780, -0.5969, -1.0537, -0.8521, 1.7459, 1.7327, -0.8815,…\n$ WORKHOME  &lt;dbl&gt; 10, 2, 0, 10, 5, 5, 7, 0, 0, 4, 2, 2, 10, 0, 10, 0, 5, 5, 0,…\n$ WORKPAY   &lt;dbl&gt; 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ STUDYHMW  &lt;dbl&gt; 4, 7, 3, 5, 7, 10, 0, 10, 5, 3, 5, 5, 10, 0, 8, 5, 5, 5, 10,…\n$ EXERPRAC  &lt;dbl&gt; 1, 4, 2, 5, 9, 1, 2, 0, 3, 5, 1, 2, 5, 2, 4, 0, 2, 3, 2, 2, …\n$ LANGN     &lt;dbl&gt; 998, 998, 998, 998, 998, 998, 998, 998, 998, 998, 998, 998, …\n$ ICTOUT    &lt;dbl&gt; 0.2260, -0.8080, 0.1088, -1.2894, 2.9804, 0.3464, -0.4834, 2…\n$ ICTAVHOM  &lt;dbl&gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6, 6, 6, 5, 6, 6, 6, 6, …\n$ ICTHOME   &lt;dbl&gt; 0.3346, 0.3346, 0.3346, 0.3346, 0.3346, 0.3346, 0.3346, 0.33…\n$ ICTAVSCH  &lt;dbl&gt; 7, 7, 7, 7, 5, 6, 7, 6, 7, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, …\n$ ICTSCH    &lt;dbl&gt; 0.4062, 0.4062, 0.4062, 0.4062, -1.6647, -0.8411, 0.4062, -0…\n$ ICTRES    &lt;dbl&gt; 0.1940, 0.6249, -0.3987, -0.9028, 0.2514, -0.4733, 0.9904, 1…\n$ COBN_S    &lt;chr&gt; \"070200\", \"070200\", \"070200\", \"070200\", \"070200\", \"970200\", …\n$ COBN_M    &lt;chr&gt; \"070200\", \"070200\", \"970200\", \"070200\", \"070200\", \"970200\", …\n$ COBN_F    &lt;chr&gt; \"070200\", \"070200\", \"070200\", \"070200\", \"070200\", \"970200\", …\n$ OCOD1     &lt;chr&gt; \"9701\", \"31\", \"9701\", \"41\", \"23\", \"9701\", \"11\", \"23\", \"1\", \"…\n$ OCOD2     &lt;chr&gt; \"83\", \"21\", \"9704\", \"9705\", \"83\", \"34\", \"31\", \"21\", \"14\", \"3…\n$ PV1MATH   &lt;dbl&gt; 639.004, 697.191, 693.710, 427.317, 436.462, 569.982, 771.62…\n$ PV2MATH   &lt;dbl&gt; 601.251, 754.277, 654.450, 410.376, 453.450, 539.609, 672.81…\n$ PV3MATH   &lt;dbl&gt; 621.480, 671.940, 696.938, 423.586, 392.315, 531.648, 653.74…\n$ PV4MATH   &lt;dbl&gt; 631.596, 657.300, 646.187, 388.935, 439.986, 534.368, 734.81…\n$ PV5MATH   &lt;dbl&gt; 579.276, 621.126, 678.119, 330.962, 443.125, 465.815, 727.52…\n$ PV6MATH   &lt;dbl&gt; 591.791, 655.729, 644.019, 379.988, 452.648, 528.509, 729.83…\n$ PV7MATH   &lt;dbl&gt; 600.709, 747.934, 720.531, 398.535, 396.970, 514.326, 597.22…\n$ PV8MATH   &lt;dbl&gt; 587.322, 694.365, 671.425, 422.127, 459.945, 521.029, 772.27…\n$ PV9MATH   &lt;dbl&gt; 618.131, 742.732, 694.085, 375.354, 438.166, 472.382, 694.39…\n$ PV10MATH  &lt;dbl&gt; 581.973, 656.934, 668.304, 453.348, 448.084, 503.387, 725.29…\n$ PV1READ   &lt;dbl&gt; 676.298, 625.585, 620.116, 381.495, 448.199, 469.441, 744.53…\n$ PV2READ   &lt;dbl&gt; 692.247, 686.716, 559.078, 400.815, 560.636, 500.350, 679.85…\n$ PV3READ   &lt;dbl&gt; 690.981, 663.147, 554.767, 374.911, 365.478, 375.703, 635.12…\n$ PV4READ   &lt;dbl&gt; 643.067, 567.435, 587.026, 367.484, 469.970, 377.452, 725.55…\n$ PV5READ   &lt;dbl&gt; 627.908, 614.500, 591.806, 336.009, 503.664, 470.781, 731.16…\n$ PV6READ   &lt;dbl&gt; 684.676, 604.745, 570.547, 324.630, 481.215, 415.448, 684.68…\n$ PV7READ   &lt;dbl&gt; 661.380, 669.375, 599.078, 396.242, 436.800, 448.547, 646.02…\n$ PV8READ   &lt;dbl&gt; 674.070, 623.735, 545.610, 374.723, 531.226, 434.381, 756.80…\n$ PV9READ   &lt;dbl&gt; 666.282, 649.579, 610.466, 314.704, 480.997, 411.703, 653.76…\n$ PV10READ  &lt;dbl&gt; 657.387, 571.261, 590.758, 342.956, 478.578, 410.846, 784.71…\n$ PV1SCIE   &lt;dbl&gt; 710.634, 670.646, 666.095, 340.308, 456.333, 475.158, 693.80…\n$ PV2SCIE   &lt;dbl&gt; 618.739, 748.839, 604.771, 329.889, 453.400, 470.030, 626.98…\n$ PV3SCIE   &lt;dbl&gt; 591.623, 635.443, 704.217, 411.353, 498.937, 461.218, 627.38…\n$ PV4SCIE   &lt;dbl&gt; 659.770, 639.735, 687.659, 327.974, 532.324, 504.199, 676.79…\n$ PV5SCIE   &lt;dbl&gt; 635.892, 608.385, 690.974, 292.183, 508.231, 486.930, 661.84…\n$ PV6SCIE   &lt;dbl&gt; 646.901, 670.662, 617.175, 355.423, 504.461, 493.011, 618.39…\n$ PV7SCIE   &lt;dbl&gt; 603.569, 734.807, 692.886, 400.182, 404.572, 469.950, 602.07…\n$ PV8SCIE   &lt;dbl&gt; 621.352, 639.748, 630.900, 317.518, 549.457, 464.012, 653.97…\n$ PV9SCIE   &lt;dbl&gt; 659.674, 716.768, 656.620, 298.893, 411.062, 440.113, 645.47…\n$ PV10SCIE  &lt;dbl&gt; 649.719, 655.670, 649.087, 362.702, 473.613, 495.410, 662.55…"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#checking-for-missing-values",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#checking-for-missing-values",
    "title": "Take-home Exercise 1",
    "section": "3.2 Checking for missing values",
    "text": "3.2 Checking for missing values\nWe use the following code chunk to check for missing value(s) and drop rows where there are missing value(s).\n\nColumns with Missing Value(s)Dropping Rows with Missing Value(s)\n\n\n\nmissingv &lt;- stu2 %&gt;% \n  map(is.na) %&gt;%\n  map(sum) \n  \nmissingv &lt;- as_tibble(missingv)\n\nkable(missingv)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCNTSTUID\nCNTSCHID\nST004D01T\nPROGN\nAGE\nST003D02T\nST003D03T\nESCS\nPAREDINT\nHISEI\nHOMEPOS\nMISCED\nFISCED\nHISCED\nFAMSUP\nWORKHOME\nWORKPAY\nSTUDYHMW\nEXERPRAC\nLANGN\nICTOUT\nICTAVHOM\nICTHOME\nICTAVSCH\nICTSCH\nICTRES\nCOBN_S\nCOBN_M\nCOBN_F\nOCOD1\nOCOD2\nPV1MATH\nPV2MATH\nPV3MATH\nPV4MATH\nPV5MATH\nPV6MATH\nPV7MATH\nPV8MATH\nPV9MATH\nPV10MATH\nPV1READ\nPV2READ\nPV3READ\nPV4READ\nPV5READ\nPV6READ\nPV7READ\nPV8READ\nPV9READ\nPV10READ\nPV1SCIE\nPV2SCIE\nPV3SCIE\nPV4SCIE\nPV5SCIE\nPV6SCIE\nPV7SCIE\nPV8SCIE\nPV9SCIE\nPV10SCIE\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n47\n57\n310\n40\n74\n97\n57\n105\n51\n51\n46\n47\n0\n157\n131\n138\n119\n131\n40\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\nFor the purpose of this exercise, we will use the following code chunk to drop rows with missing value(s).\n\nstu2 &lt;- stu2 %&gt;%\n  drop_na()\n\nglimpse(stu2)\n\nRows: 6,094\nColumns: 61\n$ CNTSTUID  &lt;dbl&gt; 70200001, 70200002, 70200003, 70200004, 70200005, 70200006, …\n$ CNTSCHID  &lt;dbl&gt; 70200052, 70200134, 70200112, 70200004, 70200152, 70200043, …\n$ ST004D01T &lt;dbl&gt; 1, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 1, 1, 2, 1, 2, 1, 2, 1, 1, …\n$ PROGN     &lt;chr&gt; \"07020002\", \"07020002\", \"07020002\", \"07020002\", \"07020002\", …\n$ AGE       &lt;dbl&gt; 15.50, 15.83, 15.75, 16.17, 15.58, 15.58, 16.08, 16.00, 15.6…\n$ ST003D02T &lt;dbl&gt; 10, 6, 7, 2, 9, 9, 3, 4, 8, 6, 10, 7, 9, 11, 5, 10, 11, 4, 7…\n$ ST003D03T &lt;dbl&gt; 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, …\n$ ESCS      &lt;dbl&gt; 0.1836, 0.8261, -1.0357, -0.9606, 0.0856, 0.1268, -0.0154, 1…\n$ PAREDINT  &lt;dbl&gt; 16.0, 14.5, 12.0, 12.0, 14.5, 16.0, 12.0, 16.0, 16.0, 16.0, …\n$ HISEI     &lt;dbl&gt; 30.34, 77.10, 17.00, 43.33, 75.54, 57.64, 70.34, 80.78, 65.1…\n$ HOMEPOS   &lt;dbl&gt; 0.7524, 0.7842, 0.0666, -0.9300, -0.8949, -0.5988, 0.0975, 0…\n$ MISCED    &lt;dbl&gt; 8, 7, 4, 6, 7, 7, 6, 9, 8, 8, 4, 9, 10, 6, 4, 9, 8, 8, 6, 6,…\n$ FISCED    &lt;dbl&gt; 7, 7, 4, 6, 7, 9, 2, 8, 8, 7, 4, 9, 10, 9, 6, 4, 9, 8, 6, 7,…\n$ HISCED    &lt;dbl&gt; 8, 7, 4, 6, 7, 9, 6, 9, 8, 8, 4, 9, 10, 9, 6, 9, 9, 8, 6, 7,…\n$ FAMSUP    &lt;dbl&gt; -0.3780, -0.5969, -1.0537, -0.8521, 1.7459, 1.7327, -0.8815,…\n$ WORKHOME  &lt;dbl&gt; 10, 2, 0, 10, 5, 5, 7, 0, 0, 4, 2, 2, 10, 0, 10, 0, 5, 5, 0,…\n$ WORKPAY   &lt;dbl&gt; 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ STUDYHMW  &lt;dbl&gt; 4, 7, 3, 5, 7, 10, 0, 10, 5, 3, 5, 5, 10, 0, 8, 5, 5, 5, 10,…\n$ EXERPRAC  &lt;dbl&gt; 1, 4, 2, 5, 9, 1, 2, 0, 3, 5, 1, 2, 5, 2, 4, 0, 2, 3, 2, 2, …\n$ LANGN     &lt;dbl&gt; 998, 998, 998, 998, 998, 998, 998, 998, 998, 998, 998, 998, …\n$ ICTOUT    &lt;dbl&gt; 0.2260, -0.8080, 0.1088, -1.2894, 2.9804, 0.3464, -0.4834, 2…\n$ ICTAVHOM  &lt;dbl&gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6, 6, 6, 5, 6, 6, 6, 6, …\n$ ICTHOME   &lt;dbl&gt; 0.3346, 0.3346, 0.3346, 0.3346, 0.3346, 0.3346, 0.3346, 0.33…\n$ ICTAVSCH  &lt;dbl&gt; 7, 7, 7, 7, 5, 6, 7, 6, 7, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, …\n$ ICTSCH    &lt;dbl&gt; 0.4062, 0.4062, 0.4062, 0.4062, -1.6647, -0.8411, 0.4062, -0…\n$ ICTRES    &lt;dbl&gt; 0.1940, 0.6249, -0.3987, -0.9028, 0.2514, -0.4733, 0.9904, 1…\n$ COBN_S    &lt;chr&gt; \"070200\", \"070200\", \"070200\", \"070200\", \"070200\", \"970200\", …\n$ COBN_M    &lt;chr&gt; \"070200\", \"070200\", \"970200\", \"070200\", \"070200\", \"970200\", …\n$ COBN_F    &lt;chr&gt; \"070200\", \"070200\", \"070200\", \"070200\", \"070200\", \"970200\", …\n$ OCOD1     &lt;chr&gt; \"9701\", \"31\", \"9701\", \"41\", \"23\", \"9701\", \"11\", \"23\", \"1\", \"…\n$ OCOD2     &lt;chr&gt; \"83\", \"21\", \"9704\", \"9705\", \"83\", \"34\", \"31\", \"21\", \"14\", \"3…\n$ PV1MATH   &lt;dbl&gt; 639.004, 697.191, 693.710, 427.317, 436.462, 569.982, 771.62…\n$ PV2MATH   &lt;dbl&gt; 601.251, 754.277, 654.450, 410.376, 453.450, 539.609, 672.81…\n$ PV3MATH   &lt;dbl&gt; 621.480, 671.940, 696.938, 423.586, 392.315, 531.648, 653.74…\n$ PV4MATH   &lt;dbl&gt; 631.596, 657.300, 646.187, 388.935, 439.986, 534.368, 734.81…\n$ PV5MATH   &lt;dbl&gt; 579.276, 621.126, 678.119, 330.962, 443.125, 465.815, 727.52…\n$ PV6MATH   &lt;dbl&gt; 591.791, 655.729, 644.019, 379.988, 452.648, 528.509, 729.83…\n$ PV7MATH   &lt;dbl&gt; 600.709, 747.934, 720.531, 398.535, 396.970, 514.326, 597.22…\n$ PV8MATH   &lt;dbl&gt; 587.322, 694.365, 671.425, 422.127, 459.945, 521.029, 772.27…\n$ PV9MATH   &lt;dbl&gt; 618.131, 742.732, 694.085, 375.354, 438.166, 472.382, 694.39…\n$ PV10MATH  &lt;dbl&gt; 581.973, 656.934, 668.304, 453.348, 448.084, 503.387, 725.29…\n$ PV1READ   &lt;dbl&gt; 676.298, 625.585, 620.116, 381.495, 448.199, 469.441, 744.53…\n$ PV2READ   &lt;dbl&gt; 692.247, 686.716, 559.078, 400.815, 560.636, 500.350, 679.85…\n$ PV3READ   &lt;dbl&gt; 690.981, 663.147, 554.767, 374.911, 365.478, 375.703, 635.12…\n$ PV4READ   &lt;dbl&gt; 643.067, 567.435, 587.026, 367.484, 469.970, 377.452, 725.55…\n$ PV5READ   &lt;dbl&gt; 627.908, 614.500, 591.806, 336.009, 503.664, 470.781, 731.16…\n$ PV6READ   &lt;dbl&gt; 684.676, 604.745, 570.547, 324.630, 481.215, 415.448, 684.68…\n$ PV7READ   &lt;dbl&gt; 661.380, 669.375, 599.078, 396.242, 436.800, 448.547, 646.02…\n$ PV8READ   &lt;dbl&gt; 674.070, 623.735, 545.610, 374.723, 531.226, 434.381, 756.80…\n$ PV9READ   &lt;dbl&gt; 666.282, 649.579, 610.466, 314.704, 480.997, 411.703, 653.76…\n$ PV10READ  &lt;dbl&gt; 657.387, 571.261, 590.758, 342.956, 478.578, 410.846, 784.71…\n$ PV1SCIE   &lt;dbl&gt; 710.634, 670.646, 666.095, 340.308, 456.333, 475.158, 693.80…\n$ PV2SCIE   &lt;dbl&gt; 618.739, 748.839, 604.771, 329.889, 453.400, 470.030, 626.98…\n$ PV3SCIE   &lt;dbl&gt; 591.623, 635.443, 704.217, 411.353, 498.937, 461.218, 627.38…\n$ PV4SCIE   &lt;dbl&gt; 659.770, 639.735, 687.659, 327.974, 532.324, 504.199, 676.79…\n$ PV5SCIE   &lt;dbl&gt; 635.892, 608.385, 690.974, 292.183, 508.231, 486.930, 661.84…\n$ PV6SCIE   &lt;dbl&gt; 646.901, 670.662, 617.175, 355.423, 504.461, 493.011, 618.39…\n$ PV7SCIE   &lt;dbl&gt; 603.569, 734.807, 692.886, 400.182, 404.572, 469.950, 602.07…\n$ PV8SCIE   &lt;dbl&gt; 621.352, 639.748, 630.900, 317.518, 549.457, 464.012, 653.97…\n$ PV9SCIE   &lt;dbl&gt; 659.674, 716.768, 656.620, 298.893, 411.062, 440.113, 645.47…\n$ PV10SCIE  &lt;dbl&gt; 649.719, 655.670, 649.087, 362.702, 473.613, 495.410, 662.55…\n\n\n\n\n\nAfter dropping the rows with missing value(s), we saw that the number of rows dropped from 6,606 to 6,094."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#checking-data-type",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#checking-data-type",
    "title": "Take-home Exercise 1",
    "section": "3.3 Checking data type",
    "text": "3.3 Checking data type\nFrom the above output, we also noted that some variables are in character data type. Since these columns contain levels (e.g. survey scales) or they actually take on a limited number of different values (e.g. student ID, school ID), we convert them into factor data type using as.factor().\n\n\nShow the code\nstu2 &lt;- stu2 %&gt;%\n  mutate_at(c('CNTSTUID', 'CNTSCHID', 'ST004D01T', 'PROGN','ST003D02T','ST003D03T','MISCED','FISCED','HISCED', \n              'WORKHOME','WORKPAY', 'STUDYHMW','EXERPRAC','LANGN','ICTAVHOM','ICTHOME','ICTAVSCH','ICTSCH',\n              'COBN_S','COBN_M','COBN_F','OCOD1','OCOD2'), as.factor) \n\nglimpse(stu2)\n\n\nRows: 6,094\nColumns: 61\n$ CNTSTUID  &lt;fct&gt; 70200001, 70200002, 70200003, 70200004, 70200005, 70200006, …\n$ CNTSCHID  &lt;fct&gt; 70200052, 70200134, 70200112, 70200004, 70200152, 70200043, …\n$ ST004D01T &lt;fct&gt; 1, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 1, 1, 2, 1, 2, 1, 2, 1, 1, …\n$ PROGN     &lt;fct&gt; 07020002, 07020002, 07020002, 07020002, 07020002, 07020002, …\n$ AGE       &lt;dbl&gt; 15.50, 15.83, 15.75, 16.17, 15.58, 15.58, 16.08, 16.00, 15.6…\n$ ST003D02T &lt;fct&gt; 10, 6, 7, 2, 9, 9, 3, 4, 8, 6, 10, 7, 9, 11, 5, 10, 11, 4, 7…\n$ ST003D03T &lt;fct&gt; 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, …\n$ ESCS      &lt;dbl&gt; 0.1836, 0.8261, -1.0357, -0.9606, 0.0856, 0.1268, -0.0154, 1…\n$ PAREDINT  &lt;dbl&gt; 16.0, 14.5, 12.0, 12.0, 14.5, 16.0, 12.0, 16.0, 16.0, 16.0, …\n$ HISEI     &lt;dbl&gt; 30.34, 77.10, 17.00, 43.33, 75.54, 57.64, 70.34, 80.78, 65.1…\n$ HOMEPOS   &lt;dbl&gt; 0.7524, 0.7842, 0.0666, -0.9300, -0.8949, -0.5988, 0.0975, 0…\n$ MISCED    &lt;fct&gt; 8, 7, 4, 6, 7, 7, 6, 9, 8, 8, 4, 9, 10, 6, 4, 9, 8, 8, 6, 6,…\n$ FISCED    &lt;fct&gt; 7, 7, 4, 6, 7, 9, 2, 8, 8, 7, 4, 9, 10, 9, 6, 4, 9, 8, 6, 7,…\n$ HISCED    &lt;fct&gt; 8, 7, 4, 6, 7, 9, 6, 9, 8, 8, 4, 9, 10, 9, 6, 9, 9, 8, 6, 7,…\n$ FAMSUP    &lt;dbl&gt; -0.3780, -0.5969, -1.0537, -0.8521, 1.7459, 1.7327, -0.8815,…\n$ WORKHOME  &lt;fct&gt; 10, 2, 0, 10, 5, 5, 7, 0, 0, 4, 2, 2, 10, 0, 10, 0, 5, 5, 0,…\n$ WORKPAY   &lt;fct&gt; 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ STUDYHMW  &lt;fct&gt; 4, 7, 3, 5, 7, 10, 0, 10, 5, 3, 5, 5, 10, 0, 8, 5, 5, 5, 10,…\n$ EXERPRAC  &lt;fct&gt; 1, 4, 2, 5, 9, 1, 2, 0, 3, 5, 1, 2, 5, 2, 4, 0, 2, 3, 2, 2, …\n$ LANGN     &lt;fct&gt; 998, 998, 998, 998, 998, 998, 998, 998, 998, 998, 998, 998, …\n$ ICTOUT    &lt;dbl&gt; 0.2260, -0.8080, 0.1088, -1.2894, 2.9804, 0.3464, -0.4834, 2…\n$ ICTAVHOM  &lt;fct&gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6, 6, 6, 5, 6, 6, 6, 6, …\n$ ICTHOME   &lt;fct&gt; 0.3346, 0.3346, 0.3346, 0.3346, 0.3346, 0.3346, 0.3346, 0.33…\n$ ICTAVSCH  &lt;fct&gt; 7, 7, 7, 7, 5, 6, 7, 6, 7, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, …\n$ ICTSCH    &lt;fct&gt; 0.4062, 0.4062, 0.4062, 0.4062, -1.6647, -0.8411, 0.4062, -0…\n$ ICTRES    &lt;dbl&gt; 0.1940, 0.6249, -0.3987, -0.9028, 0.2514, -0.4733, 0.9904, 1…\n$ COBN_S    &lt;fct&gt; 070200, 070200, 070200, 070200, 070200, 970200, 070200, 9702…\n$ COBN_M    &lt;fct&gt; 070200, 070200, 970200, 070200, 070200, 970200, 070200, 9702…\n$ COBN_F    &lt;fct&gt; 070200, 070200, 070200, 070200, 070200, 970200, 070200, 9702…\n$ OCOD1     &lt;fct&gt; 9701, 31, 9701, 41, 23, 9701, 11, 23, 1, 34, 9999, 13, 43, 9…\n$ OCOD2     &lt;fct&gt; 83, 21, 9704, 9705, 83, 34, 31, 21, 14, 31, 24, 33, 0, 9705,…\n$ PV1MATH   &lt;dbl&gt; 639.004, 697.191, 693.710, 427.317, 436.462, 569.982, 771.62…\n$ PV2MATH   &lt;dbl&gt; 601.251, 754.277, 654.450, 410.376, 453.450, 539.609, 672.81…\n$ PV3MATH   &lt;dbl&gt; 621.480, 671.940, 696.938, 423.586, 392.315, 531.648, 653.74…\n$ PV4MATH   &lt;dbl&gt; 631.596, 657.300, 646.187, 388.935, 439.986, 534.368, 734.81…\n$ PV5MATH   &lt;dbl&gt; 579.276, 621.126, 678.119, 330.962, 443.125, 465.815, 727.52…\n$ PV6MATH   &lt;dbl&gt; 591.791, 655.729, 644.019, 379.988, 452.648, 528.509, 729.83…\n$ PV7MATH   &lt;dbl&gt; 600.709, 747.934, 720.531, 398.535, 396.970, 514.326, 597.22…\n$ PV8MATH   &lt;dbl&gt; 587.322, 694.365, 671.425, 422.127, 459.945, 521.029, 772.27…\n$ PV9MATH   &lt;dbl&gt; 618.131, 742.732, 694.085, 375.354, 438.166, 472.382, 694.39…\n$ PV10MATH  &lt;dbl&gt; 581.973, 656.934, 668.304, 453.348, 448.084, 503.387, 725.29…\n$ PV1READ   &lt;dbl&gt; 676.298, 625.585, 620.116, 381.495, 448.199, 469.441, 744.53…\n$ PV2READ   &lt;dbl&gt; 692.247, 686.716, 559.078, 400.815, 560.636, 500.350, 679.85…\n$ PV3READ   &lt;dbl&gt; 690.981, 663.147, 554.767, 374.911, 365.478, 375.703, 635.12…\n$ PV4READ   &lt;dbl&gt; 643.067, 567.435, 587.026, 367.484, 469.970, 377.452, 725.55…\n$ PV5READ   &lt;dbl&gt; 627.908, 614.500, 591.806, 336.009, 503.664, 470.781, 731.16…\n$ PV6READ   &lt;dbl&gt; 684.676, 604.745, 570.547, 324.630, 481.215, 415.448, 684.68…\n$ PV7READ   &lt;dbl&gt; 661.380, 669.375, 599.078, 396.242, 436.800, 448.547, 646.02…\n$ PV8READ   &lt;dbl&gt; 674.070, 623.735, 545.610, 374.723, 531.226, 434.381, 756.80…\n$ PV9READ   &lt;dbl&gt; 666.282, 649.579, 610.466, 314.704, 480.997, 411.703, 653.76…\n$ PV10READ  &lt;dbl&gt; 657.387, 571.261, 590.758, 342.956, 478.578, 410.846, 784.71…\n$ PV1SCIE   &lt;dbl&gt; 710.634, 670.646, 666.095, 340.308, 456.333, 475.158, 693.80…\n$ PV2SCIE   &lt;dbl&gt; 618.739, 748.839, 604.771, 329.889, 453.400, 470.030, 626.98…\n$ PV3SCIE   &lt;dbl&gt; 591.623, 635.443, 704.217, 411.353, 498.937, 461.218, 627.38…\n$ PV4SCIE   &lt;dbl&gt; 659.770, 639.735, 687.659, 327.974, 532.324, 504.199, 676.79…\n$ PV5SCIE   &lt;dbl&gt; 635.892, 608.385, 690.974, 292.183, 508.231, 486.930, 661.84…\n$ PV6SCIE   &lt;dbl&gt; 646.901, 670.662, 617.175, 355.423, 504.461, 493.011, 618.39…\n$ PV7SCIE   &lt;dbl&gt; 603.569, 734.807, 692.886, 400.182, 404.572, 469.950, 602.07…\n$ PV8SCIE   &lt;dbl&gt; 621.352, 639.748, 630.900, 317.518, 549.457, 464.012, 653.97…\n$ PV9SCIE   &lt;dbl&gt; 659.674, 716.768, 656.620, 298.893, 411.062, 440.113, 645.47…\n$ PV10SCIE  &lt;dbl&gt; 649.719, 655.670, 649.087, 362.702, 473.613, 495.410, 662.55…"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#renaming-variable-names",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#renaming-variable-names",
    "title": "Take-home Exercise 1",
    "section": "3.4 Renaming Variable Names",
    "text": "3.4 Renaming Variable Names\nLet us rename the gender and school variable name to make it more intuitive, and change the gender’s code 1 to female and 2 to male using the following code chunk.\n\n\nShow the code\nstu2 &lt;- stu2 %&gt;% \n  rename(GENDER = ST004D01T) %&gt;%\n  mutate(GENDER = ifelse(GENDER == 1, 'FEMALE', 'MALE'))\n\nstu2 &lt;- stu2 %&gt;% \n  rename(SCHOOL = CNTSCHID) \n\nkable(head(stu2)) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCNTSTUID\nSCHOOL\nGENDER\nPROGN\nAGE\nST003D02T\nST003D03T\nESCS\nPAREDINT\nHISEI\nHOMEPOS\nMISCED\nFISCED\nHISCED\nFAMSUP\nWORKHOME\nWORKPAY\nSTUDYHMW\nEXERPRAC\nLANGN\nICTOUT\nICTAVHOM\nICTHOME\nICTAVSCH\nICTSCH\nICTRES\nCOBN_S\nCOBN_M\nCOBN_F\nOCOD1\nOCOD2\nPV1MATH\nPV2MATH\nPV3MATH\nPV4MATH\nPV5MATH\nPV6MATH\nPV7MATH\nPV8MATH\nPV9MATH\nPV10MATH\nPV1READ\nPV2READ\nPV3READ\nPV4READ\nPV5READ\nPV6READ\nPV7READ\nPV8READ\nPV9READ\nPV10READ\nPV1SCIE\nPV2SCIE\nPV3SCIE\nPV4SCIE\nPV5SCIE\nPV6SCIE\nPV7SCIE\nPV8SCIE\nPV9SCIE\nPV10SCIE\n\n\n\n\n70200001\n70200052\nFEMALE\n07020002\n15.50\n10\n2006\n0.1836\n16.0\n30.34\n0.7524\n8\n7\n8\n-0.3780\n10\n0\n4\n1\n998\n0.2260\n6\n0.3346\n7\n0.4062\n0.1940\n070200\n070200\n070200\n9701\n83\n639.004\n601.251\n621.480\n631.596\n579.276\n591.791\n600.709\n587.322\n618.131\n581.973\n676.298\n692.247\n690.981\n643.067\n627.908\n684.676\n661.380\n674.070\n666.282\n657.387\n710.634\n618.739\n591.623\n659.770\n635.892\n646.901\n603.569\n621.352\n659.674\n649.719\n\n\n70200002\n70200134\nMALE\n07020002\n15.83\n6\n2006\n0.8261\n14.5\n77.10\n0.7842\n7\n7\n7\n-0.5969\n2\n0\n7\n4\n998\n-0.8080\n6\n0.3346\n7\n0.4062\n0.6249\n070200\n070200\n070200\n31\n21\n697.191\n754.277\n671.940\n657.300\n621.126\n655.729\n747.934\n694.365\n742.732\n656.934\n625.585\n686.716\n663.147\n567.435\n614.500\n604.745\n669.375\n623.735\n649.579\n571.261\n670.646\n748.839\n635.443\n639.735\n608.385\n670.662\n734.807\n639.748\n716.768\n655.670\n\n\n70200003\n70200112\nMALE\n07020002\n15.75\n7\n2006\n-1.0357\n12.0\n17.00\n0.0666\n4\n4\n4\n-1.0537\n0\n0\n3\n2\n998\n0.1088\n6\n0.3346\n7\n0.4062\n-0.3987\n070200\n970200\n070200\n9701\n9704\n693.710\n654.450\n696.938\n646.187\n678.119\n644.019\n720.531\n671.425\n694.085\n668.304\n620.116\n559.078\n554.767\n587.026\n591.806\n570.547\n599.078\n545.610\n610.466\n590.758\n666.095\n604.771\n704.217\n687.659\n690.974\n617.175\n692.886\n630.900\n656.620\n649.087\n\n\n70200004\n70200004\nMALE\n07020002\n16.17\n2\n2006\n-0.9606\n12.0\n43.33\n-0.9300\n6\n6\n6\n-0.8521\n10\n6\n5\n5\n998\n-1.2894\n6\n0.3346\n7\n0.4062\n-0.9028\n070200\n070200\n070200\n41\n9705\n427.317\n410.376\n423.586\n388.935\n330.962\n379.988\n398.535\n422.127\n375.354\n453.348\n381.495\n400.815\n374.911\n367.484\n336.009\n324.630\n396.242\n374.723\n314.704\n342.956\n340.308\n329.889\n411.353\n327.974\n292.183\n355.423\n400.182\n317.518\n298.893\n362.702\n\n\n70200005\n70200152\nFEMALE\n07020002\n15.58\n9\n2006\n0.0856\n14.5\n75.54\n-0.8949\n7\n7\n7\n1.7459\n5\n0\n7\n9\n998\n2.9804\n6\n0.3346\n5\n-1.6647\n0.2514\n070200\n070200\n070200\n23\n83\n436.462\n453.450\n392.315\n439.986\n443.125\n452.648\n396.970\n459.945\n438.166\n448.084\n448.199\n560.636\n365.478\n469.970\n503.664\n481.215\n436.800\n531.226\n480.997\n478.578\n456.333\n453.400\n498.937\n532.324\n508.231\n504.461\n404.572\n549.457\n411.062\n473.613\n\n\n70200006\n70200043\nFEMALE\n07020002\n15.58\n9\n2006\n0.1268\n16.0\n57.64\n-0.5988\n7\n9\n9\n1.7327\n5\n0\n10\n1\n998\n0.3464\n6\n0.3346\n6\n-0.8411\n-0.4733\n970200\n970200\n970200\n9701\n34\n569.982\n539.609\n531.648\n534.368\n465.815\n528.509\n514.326\n521.029\n472.382\n503.387\n469.441\n500.350\n375.703\n377.452\n470.781\n415.448\n448.547\n434.381\n411.703\n410.846\n475.158\n470.030\n461.218\n504.199\n486.930\n493.011\n469.950\n464.012\n440.113\n495.410"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#checking-the-number-of-unique-students-their-age-and-schools",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#checking-the-number-of-unique-students-their-age-and-schools",
    "title": "Take-home Exercise 1",
    "section": "3.5 Checking the number of unique students, their age and schools",
    "text": "3.5 Checking the number of unique students, their age and schools\nLet us check number of unique students, their age and the schools the students were in using the following code chunk.\n\nNumber of Unique StudentsStudents’ AgeNumber of Unique Schools\n\n\n\nn_distinct(stu2$CNTSTUID)\n\n[1] 6094\n\n\nFrom the output, we noted that there are 6,094 unique students and the stu2 dataframe has 6094 observations, meaning that each observation is a unique student.\n\n\n\nsummary(stu2$AGE)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  15.33   15.50   15.75   15.78   16.00   16.33 \n\n\nFrom the above output we noted that there was a maximum age of 16.33 years old.\nLet us check the students’ birth year to ensure that all of them are born in 2006 (i.e. 15 years old).\n\nunique(stu2$ST003D03T)\n\n[1] 2006\nLevels: 2006\n\n\nFrom the above output, we noted that all students were born in 2006 so we will retain those students who were older than 15 years old for now, until we have more information on how this age being derived.\n\n\n\nunique(stu2$SCHOOL)\n\n  [1] 70200052 70200134 70200112 70200004 70200152 70200043 70200049 70200107\n  [9] 70200012 70200061 70200110 70200108 70200067 70200113 70200093 70200035\n [17] 70200027 70200102 70200163 70200097 70200139 70200081 70200040 70200140\n [25] 70200008 70200066 70200118 70200078 70200135 70200161 70200046 70200116\n [33] 70200144 70200136 70200127 70200092 70200158 70200156 70200100 70200147\n [41] 70200137 70200060 70200086 70200031 70200062 70200145 70200096 70200148\n [49] 70200103 70200003 70200059 70200021 70200001 70200098 70200038 70200154\n [57] 70200026 70200085 70200153 70200013 70200159 70200053 70200146 70200117\n [65] 70200084 70200044 70200090 70200089 70200121 70200165 70200020 70200036\n [73] 70200065 70200037 70200162 70200149 70200048 70200023 70200132 70200119\n [81] 70200064 70200091 70200099 70200080 70200160 70200122 70200017 70200128\n [89] 70200105 70200055 70200068 70200042 70200082 70200030 70200123 70200079\n [97] 70200057 70200056 70200131 70200130 70200143 70200025 70200006 70200063\n[105] 70200033 70200058 70200070 70200029 70200016 70200071 70200074 70200129\n[113] 70200101 70200069 70200114 70200028 70200050 70200007 70200022 70200083\n[121] 70200024 70200125 70200039 70200034 70200019 70200077 70200075 70200051\n[129] 70200072 70200104 70200141 70200142 70200076 70200018 70200009 70200106\n[137] 70200032 70200124 70200002 70200111 70200054 70200045 70200047 70200126\n[145] 70200041 70200088 70200014 70200133 70200109 70200095 70200138 70200155\n[153] 70200151 70200087 70200015 70200011 70200010 70200164 70200094 70200157\n[161] 70200120 70200005 70200115 70200073\n164 Levels: 70200001 70200002 70200003 70200004 70200005 70200006 ... 70200165\n\n\nFrom the above output, we noted that there were 164 schools in this dataset."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#computing-mean-pv-scores-for-each-student",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#computing-mean-pv-scores-for-each-student",
    "title": "Take-home Exercise 1",
    "section": "3.6 Computing Mean PV Scores for each student",
    "text": "3.6 Computing Mean PV Scores for each student\nCurrently there are 10 PV Scores for each subject (i.e., Maths, Reading and Science). We will calculate each student’s mean PV scores for each subject using the following code chunk.\n\nCodesOutput\n\n\n\nstu2$AVG_PVMATH &lt;- rowMeans(stu2[,c(\"PV1MATH\", \"PV2MATH\", \"PV3MATH\", \"PV4MATH\", \"PV5MATH\", \n                             \"PV6MATH\", \"PV7MATH\", \"PV8MATH\", \"PV9MATH\", \"PV10MATH\")])\n\nstu2$AVG_PVREAD &lt;- rowMeans(stu2[,c(\"PV1READ\", \"PV2READ\", \"PV3READ\", \"PV4READ\", \"PV5READ\", \n                             \"PV6READ\", \"PV7READ\", \"PV8READ\", \"PV9READ\", \"PV10READ\")])\n\nstu2$AVG_PVSCI &lt;- rowMeans(stu2[,c(\"PV1SCIE\", \"PV2SCIE\", \"PV3SCIE\", \"PV4SCIE\", \"PV5SCIE\", \n                             \"PV6SCIE\", \"PV7SCIE\", \"PV8SCIE\", \"PV9SCIE\", \"PV10SCIE\")])\n\n\n\n\nkable(head(stu2))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCNTSTUID\nSCHOOL\nGENDER\nPROGN\nAGE\nST003D02T\nST003D03T\nESCS\nPAREDINT\nHISEI\nHOMEPOS\nMISCED\nFISCED\nHISCED\nFAMSUP\nWORKHOME\nWORKPAY\nSTUDYHMW\nEXERPRAC\nLANGN\nICTOUT\nICTAVHOM\nICTHOME\nICTAVSCH\nICTSCH\nICTRES\nCOBN_S\nCOBN_M\nCOBN_F\nOCOD1\nOCOD2\nPV1MATH\nPV2MATH\nPV3MATH\nPV4MATH\nPV5MATH\nPV6MATH\nPV7MATH\nPV8MATH\nPV9MATH\nPV10MATH\nPV1READ\nPV2READ\nPV3READ\nPV4READ\nPV5READ\nPV6READ\nPV7READ\nPV8READ\nPV9READ\nPV10READ\nPV1SCIE\nPV2SCIE\nPV3SCIE\nPV4SCIE\nPV5SCIE\nPV6SCIE\nPV7SCIE\nPV8SCIE\nPV9SCIE\nPV10SCIE\nAVG_PVMATH\nAVG_PVREAD\nAVG_PVSCI\n\n\n\n\n70200001\n70200052\nFEMALE\n07020002\n15.50\n10\n2006\n0.1836\n16.0\n30.34\n0.7524\n8\n7\n8\n-0.3780\n10\n0\n4\n1\n998\n0.2260\n6\n0.3346\n7\n0.4062\n0.1940\n070200\n070200\n070200\n9701\n83\n639.004\n601.251\n621.480\n631.596\n579.276\n591.791\n600.709\n587.322\n618.131\n581.973\n676.298\n692.247\n690.981\n643.067\n627.908\n684.676\n661.380\n674.070\n666.282\n657.387\n710.634\n618.739\n591.623\n659.770\n635.892\n646.901\n603.569\n621.352\n659.674\n649.719\n605.2533\n667.4296\n639.7873\n\n\n70200002\n70200134\nMALE\n07020002\n15.83\n6\n2006\n0.8261\n14.5\n77.10\n0.7842\n7\n7\n7\n-0.5969\n2\n0\n7\n4\n998\n-0.8080\n6\n0.3346\n7\n0.4062\n0.6249\n070200\n070200\n070200\n31\n21\n697.191\n754.277\n671.940\n657.300\n621.126\n655.729\n747.934\n694.365\n742.732\n656.934\n625.585\n686.716\n663.147\n567.435\n614.500\n604.745\n669.375\n623.735\n649.579\n571.261\n670.646\n748.839\n635.443\n639.735\n608.385\n670.662\n734.807\n639.748\n716.768\n655.670\n689.9528\n627.6078\n672.0703\n\n\n70200003\n70200112\nMALE\n07020002\n15.75\n7\n2006\n-1.0357\n12.0\n17.00\n0.0666\n4\n4\n4\n-1.0537\n0\n0\n3\n2\n998\n0.1088\n6\n0.3346\n7\n0.4062\n-0.3987\n070200\n970200\n070200\n9701\n9704\n693.710\n654.450\n696.938\n646.187\n678.119\n644.019\n720.531\n671.425\n694.085\n668.304\n620.116\n559.078\n554.767\n587.026\n591.806\n570.547\n599.078\n545.610\n610.466\n590.758\n666.095\n604.771\n704.217\n687.659\n690.974\n617.175\n692.886\n630.900\n656.620\n649.087\n676.7768\n582.9252\n660.0384\n\n\n70200004\n70200004\nMALE\n07020002\n16.17\n2\n2006\n-0.9606\n12.0\n43.33\n-0.9300\n6\n6\n6\n-0.8521\n10\n6\n5\n5\n998\n-1.2894\n6\n0.3346\n7\n0.4062\n-0.9028\n070200\n070200\n070200\n41\n9705\n427.317\n410.376\n423.586\n388.935\n330.962\n379.988\n398.535\n422.127\n375.354\n453.348\n381.495\n400.815\n374.911\n367.484\n336.009\n324.630\n396.242\n374.723\n314.704\n342.956\n340.308\n329.889\n411.353\n327.974\n292.183\n355.423\n400.182\n317.518\n298.893\n362.702\n401.0528\n361.3969\n343.6425\n\n\n70200005\n70200152\nFEMALE\n07020002\n15.58\n9\n2006\n0.0856\n14.5\n75.54\n-0.8949\n7\n7\n7\n1.7459\n5\n0\n7\n9\n998\n2.9804\n6\n0.3346\n5\n-1.6647\n0.2514\n070200\n070200\n070200\n23\n83\n436.462\n453.450\n392.315\n439.986\n443.125\n452.648\n396.970\n459.945\n438.166\n448.084\n448.199\n560.636\n365.478\n469.970\n503.664\n481.215\n436.800\n531.226\n480.997\n478.578\n456.333\n453.400\n498.937\n532.324\n508.231\n504.461\n404.572\n549.457\n411.062\n473.613\n436.1151\n475.6763\n479.2390\n\n\n70200006\n70200043\nFEMALE\n07020002\n15.58\n9\n2006\n0.1268\n16.0\n57.64\n-0.5988\n7\n9\n9\n1.7327\n5\n0\n10\n1\n998\n0.3464\n6\n0.3346\n6\n-0.8411\n-0.4733\n970200\n970200\n970200\n9701\n34\n569.982\n539.609\n531.648\n534.368\n465.815\n528.509\n514.326\n521.029\n472.382\n503.387\n469.441\n500.350\n375.703\n377.452\n470.781\n415.448\n448.547\n434.381\n411.703\n410.846\n475.158\n470.030\n461.218\n504.199\n486.930\n493.011\n469.950\n464.012\n440.113\n495.410\n518.1055\n431.4652\n476.0031\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote: For the purpose of this exercise, we calculated the individual students’ subject scores. PISA advised that students’ scores should not be interpreted at the individual level. For more details, please refer to this website."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#distribution-of-students-scores-for-maths-reading-and-science",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#distribution-of-students-scores-for-maths-reading-and-science",
    "title": "Take-home Exercise 1",
    "section": "4.1 Distribution of Students’ Scores for Maths, Reading and Science",
    "text": "4.1 Distribution of Students’ Scores for Maths, Reading and Science\nLet us take a look at the distribution of the Students’ Scores for the three subjects: Maths, Reading and Science using ridgeplot.\n\nPreparing the DataPlot\n\n\nAs the current dataframe is large, we will create another dataframe that only contain the student ID and the average subject scores, then pivot longer the table.\n\nstu2s &lt;- stu2 %&gt;%\n  select(CNTSTUID, AVG_PVMATH, AVG_PVREAD, AVG_PVSCI) %&gt;%\n  pivot_longer(-CNTSTUID) %&gt;%\n  rename(subj = name, scores = value)\n\nmed_math &lt;- round(median(stu2$AVG_PVMATH), 0)\nmed_read &lt;- round(median(stu2$AVG_PVREAD), 0)\nmed_sci &lt;- round(median(stu2$AVG_PVSCI), 0)\n\n\n\n\nggplot(stu2s, \n       aes(x = scores,\n           y = subj,\n           fill = factor(stat(quantile)))) + \n  #geom_text(aes(label = med_math), color = \"black\") + \n  stat_density_ridges(geom = \"density_ridges_gradient\",\n                      calc_ecdf = TRUE,\n                      quantiles = c(0.025, 0.975)) +\n  scale_fill_manual(name = \"Probability\",\n    values = c(\"#FF0000A0\", \"#A0A0A0A0\", \"#0000FFA0\"),\n    labels = c(\"(0, 0.025]\", \"(0.025, 0.975]\", \"(0.975, 1]\")) + \n  theme_ridges()+\n  labs(title = \"Distribution of Average Subject Scores\", caption = \"Data from PISA 2022\",\n       x= \"Scores\", y = \"Subjects\") + theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nFrom the above plot, despite the median scores for each subject being relatively similar to other subjects, we see that there was a wide range of scores for each subject. This means that for a particular subject, some students could score as low as 300 points while some students can score as high as 750 points.\nLet us explore further in the subsequent plots!"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#distribution-of-scores-by-gender",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#distribution-of-scores-by-gender",
    "title": "Take-home Exercise 1",
    "section": "4.2 Distribution of Scores By Gender",
    "text": "4.2 Distribution of Scores By Gender\nLet us take a look if there is any difference in scores by gender using ggdist’s raincloud plots. However, as there are a large number of data points, we would not be plotting the dots because the dots would end up become very small. There are other suggestions at the ggdist website on handling data with large samples.\nWe will first prepare one plot for each subject using the following code chunks.\n\nMaths Scores Between GenderReading Scores Between GenderScience Scores Between Gender\n\n\n\ng1 &lt;- ggplot(stu2, aes(x = GENDER, y = AVG_PVMATH, fill = GENDER)) +\n  stat_halfeye(adjust = 0.5,\n               width = 0.3, \n               justification = -0.1,\n               point_color = NA,\n               scale = 1.5) +\n  geom_boxplot(width = 0.05,\n               outlier.shape = NA) + coord_flip() +\n   stat_summary(fun = median, geom = \"text\", aes(label = round(after_stat(y), )),\n               position = position_nudge(x = 0.05), vjust = -0.5,size=3) +  \n  labs(title = \"Distribution of Maths Scores\",\n       y = \"Scores\", x = \"Gender\") +\n  theme_minimal() + scale_y_continuous(limits = c(100,900)) + theme(plot.title = element_text(size = 10))\n\nggsave(\"g1.png\", plot = g1)\ng1\n\n\n\n\n\n\n\ng2 &lt;- ggplot(stu2, aes(x = GENDER, y = AVG_PVREAD, fill = GENDER)) +\n  stat_halfeye(adjust = 0.5,\n               width = 0.3, \n               justification = -0.1,\n               point_color = NA,\n               scale = 1.5) +\n  geom_boxplot(width = 0.05,\n               outlier.shape = NA) + coord_flip() +\n   stat_summary(fun = median, geom = \"text\", aes(label = round(after_stat(y), )),\n               position = position_nudge(x = 0.05), vjust = -0.5,size=3) +  \n labs(title = \"Distribution of Reading Scores\",\n       y = \"Scores\", x = \"Gender\") + theme_minimal()+\n  scale_y_continuous(limits = c(100,900)) + theme(plot.title = element_text(size = 10))\n\n\ng2\n\n\n\n\n\n\n\ng3 &lt;- ggplot(stu2, aes(x = GENDER, y = AVG_PVSCI, fill=GENDER)) +\n  stat_halfeye(adjust = 0.5,\n               width = 0.3, \n               justification = -0.1,\n               point_color = NA,\n               scale = 1.5) +\n  geom_boxplot(width = 0.05,\n               outlier.shape = NA) +coord_flip() +\n stat_summary(fun = median, geom = \"text\", aes(label = round(after_stat(y), )),\n               position = position_nudge(x = 0.05), vjust = -0.5,size=3) +  \n labs(title = \"Distribution of Science Scores\",\n       y = \"Scores\", x = \"Gender\") + theme_minimal() + \n  scale_y_continuous(limits = c(100,900)) + theme(plot.title = element_text(size = 10))\n\ng3\n\n\n\n\n\n\n\nWe will use patchwork to combine these three plots into one with the following code chunk.\n\n\nShow the code\npatch1 &lt;- g1 /  g2 / g3\npatch1+ plot_layout(guides = \"collect\") \n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe difference in median scores between Male and Female Students ranges from: Math: 19 points with male students scoring slightly higher than female students Reading: 12 points difference with females students scoring slightly higher than male students Science: 8 points difference with male students scoring slightly higher than female students.\nFor further investigations, we could also test if the difference between gender is statistically significant."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#differences-in-subject-scores-among-schools",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#differences-in-subject-scores-among-schools",
    "title": "Take-home Exercise 1",
    "section": "4.3 Differences in Subject Scores Among Schools",
    "text": "4.3 Differences in Subject Scores Among Schools\nAs there are more than 160 schools in this dataset, we will focus only on the differences in Scores among the top 10% and bottom 10% of schools for each subject.\nFirst, for each subject score, we use the following code chunk to find the minimum and maximum subject scores for each school using group_by and summarise functions.\n\n\nShow the code\nmath_sch &lt;- stu2 %&gt;%\n  group_by(SCHOOL) %&gt;%\n  summarise_at(vars(AVG_PVMATH), \n               list(minscore = min, maxscore = max)) %&gt;%\n  mutate(difference = maxscore - minscore)\n\nread_sch &lt;- stu2 %&gt;%\n  group_by(SCHOOL) %&gt;%\n  summarise_at(vars(AVG_PVREAD), \n               list(minscore = min, maxscore = max)) %&gt;%\n  mutate(difference = maxscore - minscore)\n\nsci_sch &lt;- stu2 %&gt;%\n  group_by(SCHOOL) %&gt;%\n  summarise_at(vars(AVG_PVSCI), \n               list(minscore = min, maxscore = max)) %&gt;%\n  mutate(difference = maxscore - minscore)\n\n\nThen, we will use the the follow code chunks to (i) get the top 5% schools for each subject using slice_max, (ii) create a table with the top 5% schools’ minimum score of the subject, (iii) create a table with the top 5% schools’ maximum score of the subject then pivot the table using pivot_longer.\n\nMathsReadingScience\n\n\n\n\nShow the code\nmath_sch_high5 &lt;- math_sch %&gt;% slice_max(maxscore, prop = 0.05)\n\n\nmath_sch_min &lt;- math_sch_high5 %&gt;%\n  select(-maxscore)\n\n\nmath_sch_max &lt;- math_sch_high5%&gt;%\n  select(-minscore)\n\n\nmath_long &lt;- math_sch_high5%&gt;% \n  select(-difference)%&gt;%\n  pivot_longer(-SCHOOL) \n\n\nh1 &lt;- ggplot(math_long) +\n  geom_segment(data = math_sch_min,\n               aes(x = minscore, y = SCHOOL,\n                   yend = math_sch_max$SCHOOL, xend=math_sch_max$maxscore),\n               color= \"#aeb6bf\",\n               size = 4,\n               alpha = 0.5) +\n  geom_point(aes(x = value, y = SCHOOL, color = name), size = 4, show.legend = TRUE) + \n  labs(title = \"Differences in Students Scores for Top 5% Schools in Maths\", x = \"Maths Scores\", y = \"School\", color = \"Min/Max Scores\") +\n  geom_text(data = math_sch_min,\n            aes(label= paste(\"Diff: \", difference), x= minscore+100, y= SCHOOL),\n             color = \"black\",\n             size = 2.5)+ theme_minimal()+\n  theme(panel.grid.minor.y = element_blank(),\n        panel.grid.minor.x = element_blank(), \n        axis.ticks.x = element_line(color = \"#4a4e4d\"),\n        axis.text.y = element_text(size = 8),\n        axis.title.y = element_text(size = 8), \n        axis.text.x = element_text(size = 8),\n        axis.title.x = element_text(size = 8))\n\nh1 \n\n\n\n\n\n\n\n\n\nShow the code\nread_sch_high5 &lt;- read_sch %&gt;% slice_max(maxscore, prop = 0.05)\n\nread_sch_min &lt;- read_sch_high5 %&gt;%\n  select(-maxscore)\n\nread_sch_max &lt;- read_sch_high5%&gt;%\n  select(-minscore)\n\nread_long &lt;- read_sch_high5%&gt;% \n  select(-difference)%&gt;%\n  pivot_longer(-SCHOOL) \n\nh2 &lt;- ggplot(read_long) +\n  geom_segment(data = read_sch_min,\n               aes(x = minscore, y = SCHOOL,\n                   yend = read_sch_max$SCHOOL, xend=read_sch_max$maxscore),\n               color= \"#aeb6bf\",\n               size = 4,\n               alpha = 0.5) +\n  geom_point(aes(x = value, y = SCHOOL, color = name), size = 4, show.legend = TRUE) + \n  labs(title = \"Differences in Students Scores for Top 5% Schools in Reading\", x = \"Reading Scores\", y = \"School\", color = \"Min/Max Scores\") +\n  geom_text(data = read_sch_min,\n            aes(label= paste(\"Diff: \", difference), x= minscore+100, y= SCHOOL),\n             color = \"black\",\n             size = 2.5)+ theme_minimal()+\n  theme(panel.grid.minor.y = element_blank(),\n        panel.grid.minor.x = element_blank(), \n        axis.ticks.x = element_line(color = \"#4a4e4d\"),\n        axis.text.y = element_text(size = 8),\n        axis.title.y = element_text(size = 8), \n        axis.text.x = element_text(size = 8),\n        axis.title.x = element_text(size = 8))\nh2\n\n\n\n\n\n\n\n\n\nShow the code\nsci_sch_high5 &lt;- sci_sch %&gt;% slice_max(maxscore, prop = 0.05)\n\nsci_sch_min &lt;- sci_sch_high5 %&gt;%\n  select(-maxscore)\n\nsci_sch_max &lt;- sci_sch_high5%&gt;%\n  select(-minscore)\n\nsci_long &lt;- sci_sch_high5%&gt;% \n  select(-difference)%&gt;%\n  pivot_longer(-SCHOOL) \n\nh3 &lt;- ggplot(sci_long) +\n  geom_segment(data = sci_sch_min,\n               aes(x = minscore, y = SCHOOL,\n                   yend = sci_sch_max$SCHOOL, xend=sci_sch_max$maxscore),\n               color= \"#aeb6bf\",\n               size = 4,\n               alpha = 0.5) +\n  geom_point(aes(x = value, y = SCHOOL, color = name), size = 4, show.legend = TRUE) + \n  labs(title = \"Differences in Students Scores for Top 5% Schools in Science\", caption = \"Data from PISA 2022 Study\", x = \"Science Scores\", y = \"School\", color = \"Min/Max Scores\") +\n  geom_text(data = sci_sch_min,\n            aes(label= paste(\"Diff: \", difference), x= minscore+100, y= SCHOOL),\n             color = \"black\",\n             size = 2.5) + theme_minimal()+\n  theme(panel.grid.minor.y = element_blank(),\n        panel.grid.minor.x = element_blank(), \n        axis.ticks.x = element_line(color = \"#4a4e4d\"), \n        axis.text.y = element_text(size = 8),\n        axis.title.y = element_text(size = 8), \n        axis.text.x = element_text(size = 8),\n        axis.title.x = element_text(size = 8))\n\nh3\n\n\n\n\n\n\n\n\n\n\nShow the code\nh1+ h2 + h3 + plot_layout(nrow = 3, guides = \"collect\")\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nAmongst the schools that scored the top 5% for each subject, the difference between their top scorers and low scorers ranges from 161 marks (school 70200003 for Science) to 458 marks (school 70200068 for Reading). In addition, we noted that there were five schools which had top 5% of the scores for at least two subjects: - For Science, Reading and Maths: 70200155, 70200101,70200003, 70200001 - For Reading and Maths: 70200062\nWe can do further exploration with the schools questionnaire data provided by PISA to find out about these schools. For further investigations, we could also test if the difference between the top and low scorers is statistically significant."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#students-scores-and-socioeconomic-factors",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#students-scores-and-socioeconomic-factors",
    "title": "Take-home Exercise 1",
    "section": "4.4 Students’ Scores and Socioeconomic Factors",
    "text": "4.4 Students’ Scores and Socioeconomic Factors\nWe will now explore the students’ scores and the PISA index of economic, social and cultural status (ESCS) using the following code chunks.\n\nMaths Scores and ESCSReading and ESCSScience Scores and ESCS\n\n\n\nz1 &lt;- ggplot(stu2, \n       aes(x = AVG_PVMATH, y = ESCS)) +\n  geom_point() + \n  geom_smooth(method = \"lm\", size=0.5, fill = \"blue\")+ \n   coord_cartesian(xlim=c(150,850),\n                  ylim=c(-3,3)) + theme_minimal()+\n      theme(legend.position=\"none\") + \n  labs(title = \"Relationship between Maths Scores and ESCS\", \n       x = 'Maths Scores', y = \"ESCS\")\n\nz1\n\n\n\n\n\n\n\nz2 &lt;- ggplot(stu2, \n       aes(x = AVG_PVREAD, y = ESCS)) +\n  geom_point()+\n  geom_smooth(method = \"lm\", size=0.5, fill = \"blue\")+ \n   coord_cartesian(xlim=c(150,850),\n                  ylim=c(-3,3)) + theme_minimal()+\n      theme(legend.position=\"none\") +   \n  labs(title = \"Relationship between Reading Scores and ESCS\", \n       x = 'Reading Scores', y = \"ESCS\")\n\n\nz2\n\n\n\n\n\n\n\nz3 &lt;- ggplot(stu2, \n       aes(x = AVG_PVSCI, y = ESCS)) +\n  geom_point() + \n    geom_smooth(method = \"lm\", size=0.5, fill = \"blue\")+ \n   coord_cartesian(xlim=c(150,850),\n                  ylim=c(-3,3)) + theme_minimal()+\n      theme(legend.position=\"none\") + \n  labs(title = \"Relationship between Science Scores and ESCS\", \n       caption = \"Data from PISA 2022\", \n       x = 'Science Scores', y = \"ESCS\")\nz3\n\n\n\n\n\n\n\n\n\nShow the code\npatch3 &lt;- z1 +  z2 + z3\npatch3 + plot_layout(nrow = 3)\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nBased on the above plot, across all three subjects, it seems that scores seem to have a positive relationship with ESCS. In future analysis, we should investigate the relationship between students’ scores with indices that make up ESCS. We can also explore how scores are related to other PISA items, such as whether the student has to work after school hours (i.e. WORKPAY), and whether the student has to assist in housework or assist in taking care of family members (i.e.WORKHOME)."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html",
    "title": "Hands-on Exercise 4a - Visualising Distribution",
    "section": "",
    "text": "[Updated with In-class notes on 3 Feb 2024 and an interactive raincloud plot!]",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4a - Visualising Distribution"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#installing-and-loading-the-packages",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#installing-and-loading-the-packages",
    "title": "Hands-on Exercise 4a - Visualising Distribution",
    "section": "2.1 Installing and Loading the Packages",
    "text": "2.1 Installing and Loading the Packages\nFor this exercise, other than tidyverse, we will use the following packages:\n\nggridges: a ggplot2 extension specially designed for plotting ridgeline plots\nggdist: a package to visualise distribution and uncertainty.\nDT: to create interactive tables using the JavaScript library DataTables\n\nThe code chunk below uses p_load() of pacman package to check if the abovementioned packages are installed in the computer. If they are, they will be launched in R. Otherwise, pacman will install the relevant packages before launching them.\n\n\nShow the code\npacman::p_load(tidyverse, ggridges, ggdist, DT, colorspace, ggthemes, ggiraph, plotly, patchwork, crosstalk)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4a - Visualising Distribution"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#importing-the-data",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#importing-the-data",
    "title": "Hands-on Exercise 4a - Visualising Distribution",
    "section": "2.2 Importing the Data",
    "text": "2.2 Importing the Data\nFor this exercise, we will be using the Exam_data.csv provided by the course instructor and we have used it in Hands-on Exercises 1 and 2. It consists of year end examination grades of a cohort of primary 3 students from a local school. It is in csv file format.\nWe use read_csv() function of readr to import the Exam_data csv file into R and save it as a tibble data frame called exam_data. Then we will use datatable() of DT to have an overview of the imported data.\n\n\nShow the code\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")\n\ndatatable(exam_data, caption= \"Table 1: Exam Data of Primary 3 Students\")\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nggridge and ggdist are slightly different.\nggridge allow us to examine the distribution of the data while ggdist allow us to examine both distribution and uncertainty (hence the box plot).\nggridge also does not require time-series data. It is akin to a histogram.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4a - Visualising Distribution"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#varying-fill-colours-along-the-x-axis",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#varying-fill-colours-along-the-x-axis",
    "title": "Hands-on Exercise 4a - Visualising Distribution",
    "section": "3.1 Varying fill colours along the x axis",
    "text": "3.1 Varying fill colours along the x axis\nSometimes we would like to have the area under a ridgeline filled with colors that vary in some form along the x axis. This effect can be achieved by using either geom_ridgeline_gradient() or geom_density_ridges_gradient(). Both geoms work just like geom_ridgeline() and geom_density_ridges(), except that they allow for varying fill colors. However, they do not allow for alpha transparency in the fill. For technical reasons, we can have changing fill colors or transparency but not both.\n\n\nShow the code\nggplot(exam_data, aes(x = ENGLISH, y = CLASS, fill = stat(x))) +\n  geom_density_ridges_gradient(\n    scale = 3, \n    rel_min_height = 0.01) +\n  scale_fill_viridis_c(name = \"Scores\", option = \"C\") +\n  scale_x_continuous(name = \"English Scores\", \n                     expand = c(0,0)) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) + \ntheme_ridges()\n\n\n\n\n\n\n\n\n\nBy varying the fill colours, we can visualise how the students’ scores are being distributed. For example, we see that the ridges of class 3A is filled with a lot of yellow, meaning that most of the students had relatively higher scores. In comparison, the ridge for class 3I is filled with a lot of purple, meaning that most of the students had relatively lower scores.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4a - Visualising Distribution"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#mapping-the-probabilities-directly-onto-color",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#mapping-the-probabilities-directly-onto-color",
    "title": "Hands-on Exercise 4a - Visualising Distribution",
    "section": "3.2 Mapping the probabilities directly onto color",
    "text": "3.2 Mapping the probabilities directly onto color\nBesides providing additional geom objects to support the need to plot ridgeline plot, ggridges package also provides a stat function called stat_density_ridges() that replaces stat_density() of ggplot2.\nWe can map the probabilities calculated using stat(ecdf) which represents the empirical cumulative density function for the distribution of English Scores.\n\nggplot(exam_data,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = 0.5 - abs(0.5-stat(ecdf)))) +\n  stat_density_ridges(geom = \"density_ridges_gradient\", \n                      calc_ecdf = TRUE) +\n  scale_fill_viridis_c(name = \"Tail probability\",\n                       direction = -1) +\n  theme_ridges()\n\n\n\n\n\n\n\n\n\nIt is important include the argument calc_ecdf = TRUE in stat_density_ridges().",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4a - Visualising Distribution"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#ridgeline-plot-with-quantile-lines",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#ridgeline-plot-with-quantile-lines",
    "title": "Hands-on Exercise 4a - Visualising Distribution",
    "section": "3.3 Ridgeline plot with quantile lines",
    "text": "3.3 Ridgeline plot with quantile lines\nUsing geom_density_ridges_gradient, we can colour the ridgeline plot by quantile via the calculated stat(quantile) aesthetic as shown in the following code chunk.\n\nggplot(exam_data, aes(x = ENGLISH, y = CLASS, fill = factor(stat(quantile)))) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE,\n    quantiles = 4,\n    quantile_lines = TRUE) + \n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n\n\n\n\n\nInstead of using number to define the quantiles, we can also specify quantiles by cut points such as 2.5% and 97.5% tails to colour the ridgeline plot as shown in following code chunk.\n\nggplot(exam_data,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = c(0.025, 0.975)\n    ) +\n  scale_fill_manual(\n    name = \"Probability\",\n    values = c(\"#FF0000A0\", \"#A0A0A0A0\", \"#0000FFA0\"),\n    labels = c(\"(0, 0.025]\", \"(0.025, 0.975]\", \"(0.975, 1]\")\n  ) +\n  theme_ridges()",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4a - Visualising Distribution"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#plotting-a-half-eye-graph",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#plotting-a-half-eye-graph",
    "title": "Hands-on Exercise 4a - Visualising Distribution",
    "section": "4.1 Plotting a Half Eye graph",
    "text": "4.1 Plotting a Half Eye graph\nTo plot a raincloud plot, first we will plot a half-eye graph using stat_halfeye() of ggdist package. This produces a Half Eye visualization, which is contains a half-density and a slab-interval.\n:::{.panel-tabset}",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4a - Visualising Distribution"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#without-slab-interval",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#without-slab-interval",
    "title": "Hands-on Exercise 4a - Visualising Distribution",
    "section": "4.2 Without slab-interval",
    "text": "4.2 Without slab-interval\n\nggplot(exam_data, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA)\n\n\n\n\n\n\n\n\nNote: We remove the slab interval by setting .width = 0and point_colour = NA.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4a - Visualising Distribution"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#with-slab-interval",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#with-slab-interval",
    "title": "Hands-on Exercise 4a - Visualising Distribution",
    "section": "4.3 With Slab Interval",
    "text": "4.3 With Slab Interval\n\nggplot(exam_data, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0.5,\n               point_colour = \"blue\")",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4a - Visualising Distribution"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#adding-the-boxplot",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#adding-the-boxplot",
    "title": "Hands-on Exercise 4a - Visualising Distribution",
    "section": "4.4 Adding the boxplot",
    "text": "4.4 Adding the boxplot\nNext, we will add the second geometry layer using geom_boxplot() of ggplot2. This produces a narrow boxplot. We reduce the width and adjust the opacity.\n\nggplot(exam_data, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .10,\n               outlier.shape = NA,\n               fill = \"skyblue\", alpha = 0.4)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4a - Visualising Distribution"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#adding-dot-plots",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#adding-dot-plots",
    "title": "Hands-on Exercise 4a - Visualising Distribution",
    "section": "4.5 Adding dot plots",
    "text": "4.5 Adding dot plots\nNext, we will add the third geometry layer using stat_dots() of ggdist package. This produces a half-dotplot, which is similar to a histogram that indicates the number of samples (number of dots) in each bin. We select side = “left” to indicate we want it on the left-hand side.\n\nggplot(exam_data, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 2)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4a - Visualising Distribution"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#finishing-touch",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#finishing-touch",
    "title": "Hands-on Exercise 4a - Visualising Distribution",
    "section": "4.6 Finishing Touch",
    "text": "4.6 Finishing Touch\nLastly, we will flip the raincloud chart horizontally using coord_flip() to give it a raincloud appearance. We will also add theme_economist() of ggthemes package to give the raincloud chart a professional publishing standard look.\n\nggplot(exam_data, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 2)+\n  coord_flip() + theme_economist()",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4a - Visualising Distribution"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html",
    "title": "Hands-on Exercise 4b - Visual Statistical Analysis",
    "section": "",
    "text": "In this hands-on exercise, we will learn how to:\n\ncreate visual graphics with rich statistical information using ggstatsplot package\nvisualise model diagnostics using performance package\nvisualise model parameters using parameters package.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4b - Visual Statistical Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#installing-and-loading-the-packages",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#installing-and-loading-the-packages",
    "title": "Hands-on Exercise 4b - Visual Statistical Analysis",
    "section": "2.1 Installing and Loading the Packages",
    "text": "2.1 Installing and Loading the Packages\nFor this exercise, other than tidyverse, we will use the following packages:\n\nggstatplot: an extension of ggplot2 package for creating graphics with details from statistical tests included in the information-rich plots themselves.\nperformance: to provide utilities for computing indices of model quality and goodness of fit.\nparameters: to provide utilities for processing the parameters of various statistical models\ntidyverse: a family of R packages for data science processing\nreadxl: to import excel files into R\n\nThe code chunk below uses p_load() of pacman package to check if the abovementioned packages are installed in the computer. If they are, they will be launched in R. Otherwise, pacman will install the relevant packages before launching them.\n\n\nShow the code\npacman::p_load(ggstatsplot, performance, parameters, tidyverse, readxl, see)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4b - Visual Statistical Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#importing-the-data",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#importing-the-data",
    "title": "Hands-on Exercise 4b - Visual Statistical Analysis",
    "section": "2.2 Importing the Data",
    "text": "2.2 Importing the Data\nFor this exercise, we will be using the Exam_data.csv provided by the course instructor and we have used it in Hands-on Exercises 1 and 2. It consists of year end examination grades of a cohort of primary 3 students from a local school. It is in csv file format.\nWe use read_csv() function of readr to import the Exam_data.csv file into R and save it as a tibble data frame called exam_data. Then we will use datatable() of DT to have an overview of the imported data.\n\n\nShow the code\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")\nglimpse(exam_data)\n\n\nRows: 322\nColumns: 7\n$ ID      &lt;chr&gt; \"Student321\", \"Student305\", \"Student289\", \"Student227\", \"Stude…\n$ CLASS   &lt;chr&gt; \"3I\", \"3I\", \"3H\", \"3F\", \"3I\", \"3I\", \"3I\", \"3I\", \"3I\", \"3H\", \"3…\n$ GENDER  &lt;chr&gt; \"Male\", \"Female\", \"Male\", \"Male\", \"Male\", \"Female\", \"Male\", \"M…\n$ RACE    &lt;chr&gt; \"Malay\", \"Malay\", \"Chinese\", \"Chinese\", \"Malay\", \"Malay\", \"Chi…\n$ ENGLISH &lt;dbl&gt; 21, 24, 26, 27, 27, 31, 31, 31, 33, 34, 34, 36, 36, 36, 37, 38…\n$ MATHS   &lt;dbl&gt; 9, 22, 16, 77, 11, 16, 21, 18, 19, 49, 39, 35, 23, 36, 49, 30,…\n$ SCIENCE &lt;dbl&gt; 15, 16, 16, 31, 25, 16, 25, 27, 15, 37, 42, 22, 32, 36, 35, 45…\n\n\nIn addition, we will be using the ToyotaCorolla.xls for Visualising Models and Parameters. We use read_xls() of readxl package to import the data worksheet of ToyotaCorolla.xls workbook into R.\n\n\nShow the code\ncar_resale &lt;- read_xls(\"data/ToyotaCorolla.xls\", \n                       \"data\")\nglimpse(car_resale)\n\n\nRows: 1,436\nColumns: 38\n$ Id               &lt;dbl&gt; 81, 1, 2, 3, 4, 5, 6, 7, 8, 44, 45, 46, 47, 49, 51, 6…\n$ Model            &lt;chr&gt; \"TOYOTA Corolla 1.6 5drs 1 4/5-Doors\", \"TOYOTA Coroll…\n$ Price            &lt;dbl&gt; 18950, 13500, 13750, 13950, 14950, 13750, 12950, 1690…\n$ Age_08_04        &lt;dbl&gt; 25, 23, 23, 24, 26, 30, 32, 27, 30, 27, 22, 23, 27, 2…\n$ Mfg_Month        &lt;dbl&gt; 8, 10, 10, 9, 7, 3, 1, 6, 3, 6, 11, 10, 6, 11, 11, 11…\n$ Mfg_Year         &lt;dbl&gt; 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002,…\n$ KM               &lt;dbl&gt; 20019, 46986, 72937, 41711, 48000, 38500, 61000, 9461…\n$ Quarterly_Tax    &lt;dbl&gt; 100, 210, 210, 210, 210, 210, 210, 210, 210, 234, 234…\n$ Weight           &lt;dbl&gt; 1180, 1165, 1165, 1165, 1165, 1170, 1170, 1245, 1245,…\n$ Guarantee_Period &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,…\n$ HP_Bin           &lt;chr&gt; \"100-120\", \"&lt; 100\", \"&lt; 100\", \"&lt; 100\", \"&lt; 100\", \"&lt; 100…\n$ CC_bin           &lt;chr&gt; \"1600\", \"&gt;1600\", \"&gt;1600\", \"&gt;1600\", \"&gt;1600\", \"&gt;1600\", …\n$ Doors            &lt;dbl&gt; 5, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 3, 3,…\n$ Gears            &lt;dbl&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,…\n$ Cylinders        &lt;dbl&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,…\n$ Fuel_Type        &lt;chr&gt; \"Petrol\", \"Diesel\", \"Diesel\", \"Diesel\", \"Diesel\", \"Di…\n$ Color            &lt;chr&gt; \"Blue\", \"Blue\", \"Silver\", \"Blue\", \"Black\", \"Black\", \"…\n$ Met_Color        &lt;dbl&gt; 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,…\n$ Automatic        &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Mfr_Guarantee    &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,…\n$ BOVAG_Guarantee  &lt;dbl&gt; 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ ABS              &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Airbag_1         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Airbag_2         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Airco            &lt;dbl&gt; 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Automatic_airco  &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,…\n$ Boardcomputer    &lt;dbl&gt; 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ CD_Player        &lt;dbl&gt; 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,…\n$ Central_Lock     &lt;dbl&gt; 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Powered_Windows  &lt;dbl&gt; 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Power_Steering   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Radio            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Mistlamps        &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Sport_Model      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,…\n$ Backseat_Divider &lt;dbl&gt; 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Metallic_Rim     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Radio_cassette   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Tow_Bar          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4b - Visual Statistical Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#one-sample-test-gghistostats-method",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#one-sample-test-gghistostats-method",
    "title": "Hands-on Exercise 4b - Visual Statistical Analysis",
    "section": "3.1 One-sample test: gghistostats() method",
    "text": "3.1 One-sample test: gghistostats() method\nWe can use gghistostats() to build an visual of one-sample test on English Scores.\n\nset.seed(2024)\n\ngghistostats(data = exam_data, x = ENGLISH, \n             type = \"bayes\",\n             test.value = 60, \n             xlab= \"English Scores\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe default information presented are:\n\nStatistical details\nBayes Factor\nSample Sizes\nDistribution Summary\n\n\n\n\n3.1.1 Unpacking the Bayes Factor\n\nA Bayes factor is the ratio of the likelihood of one particular hypothesis to the likelihood of another. It can be interpreted as a measure of the strength of evidence in favor of one theory among two competing theories.\nBayes factor gives us a way to evaluate the data in favor of a null hypothesis, and to use external information to do so. It tells us what the weight of the evidence is in favor of a given hypothesis.\nWhen we are comparing two hypotheses, H1 (the alternate hypothesis) and H0 (the null hypothesis), the Bayes Factor is often written as B10.\nThe Schwarz criterion is one of the easiest ways to calculate rough approximation of the Bayes Factor.\nA Bayes Factor can be any positive number (i.e., 0 and ∞).\n\n1 indicates the data do not favor either theory more than the other;\nvalues greater than 1 indicate increasing evidence for one theory over the other (e.g., the alternative over a null hypothesis) ; and\nvalues less than 1 the converse (e.g., increasing evidence for the null over the alternative hypothesis).\n\nThus, Bayes factors allow three different types of conclusions:\n\nThere is strong evidence for the alternative (B much greater than 1);\nthere is strong evidence for the null (B close to 0); and\nthe evidence is insensitive (B close to 1).",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4b - Visual Statistical Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#two-sample-mean-test-ggbetweenstats",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#two-sample-mean-test-ggbetweenstats",
    "title": "Hands-on Exercise 4b - Visual Statistical Analysis",
    "section": "3.2 Two-sample mean test: ggbetweenstats()",
    "text": "3.2 Two-sample mean test: ggbetweenstats()\nWe can use ggbetweenstats() to build an visual of two sample means test of math scores by gender.\n\nggbetweenstats(data = exam_data, x = GENDER, \n               y = MATHS, type = \"np\", \n               messages = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe default information presented are:\n\nStatistical details\nBayes Factor\nSample Sizes\nDistribution Summary",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4b - Visual Statistical Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#one-way-anova-test-ggbetweenstats-method",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#one-way-anova-test-ggbetweenstats-method",
    "title": "Hands-on Exercise 4b - Visual Statistical Analysis",
    "section": "3.3 One Way ANOVA Test: ggbetweenstats() method",
    "text": "3.3 One Way ANOVA Test: ggbetweenstats() method\nWe can use ggbetweenstats() to build a visual for one-way ANOVA test on English Score by race.\n\nggbetweenstats(\n  data = exam_data, x = RACE, y = ENGLISH, \n  type = \"p\", mean.ci = TRUE, \n  pairwise_comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\", \n  messages = FALSE)\n\n\n\n\n\n\n\n\nTry: explore the aov object!",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4b - Visual Statistical Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#significant-test-of-correlation-ggscatterstats",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#significant-test-of-correlation-ggscatterstats",
    "title": "Hands-on Exercise 4b - Visual Statistical Analysis",
    "section": "3.4 Significant Test of Correlation: ggscatterstats()",
    "text": "3.4 Significant Test of Correlation: ggscatterstats()\nWe can use ggscatterstats() to build a visual for significant test of correlation between Math Scores and English Scores.\n\nggscatterstats(data = exam_data, \n               x = MATHS,\n               y = ENGLISH, \n               marginal = FALSE)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4b - Visual Statistical Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#significant-test-of-association-dependence-ggbarstats",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#significant-test-of-association-dependence-ggbarstats",
    "title": "Hands-on Exercise 4b - Visual Statistical Analysis",
    "section": "3.5 Significant Test of Association (Dependence): ggbarstats()",
    "text": "3.5 Significant Test of Association (Dependence): ggbarstats()\nWe can use ggbarstats() to build a visual for significant test of association.\nFor our data, we will first bin the Math scores into a 4-class variable using cut()\n\nexam1 &lt;- exam_data %&gt;%\n  mutate(MATHS_bins = cut(MATHS, breaks = c(0, 60,75,85,100)))\n\nThen we will build the visual using ggbarstats().\n\nggbarstats(exam1, x = MATHS_bins, \n           y = GENDER)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4b - Visual Statistical Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#multiple-regression-model-using-lm",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#multiple-regression-model-using-lm",
    "title": "Hands-on Exercise 4b - Visual Statistical Analysis",
    "section": "4.1 Multiple Regression Model using lm()",
    "text": "4.1 Multiple Regression Model using lm()\nThe following code chunk is used to calibrate a multiple linear regression model using lm() of Base Stats of R.\n\nmodel &lt;- lm(Price ~ Age_08_04 + Mfg_Year + KM + \n              Weight + Guarantee_Period, data = car_resale)\nmodel\n\n\nCall:\nlm(formula = Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period, \n    data = car_resale)\n\nCoefficients:\n     (Intercept)         Age_08_04          Mfg_Year                KM  \n      -2.637e+06        -1.409e+01         1.315e+03        -2.323e-02  \n          Weight  Guarantee_Period  \n       1.903e+01         2.770e+01",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4b - Visual Statistical Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#model-diagnostic-checks-for-multicolinearity",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#model-diagnostic-checks-for-multicolinearity",
    "title": "Hands-on Exercise 4b - Visual Statistical Analysis",
    "section": "4.2 Model Diagnostic: Checks for Multicolinearity",
    "text": "4.2 Model Diagnostic: Checks for Multicolinearity\nWe check for multicolinearity using check_collinearity() function of performance package.\n\ncheck_collinearity(model)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n             Term  VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n               KM 1.46 [ 1.37,  1.57]         1.21      0.68     [0.64, 0.73]\n           Weight 1.41 [ 1.32,  1.51]         1.19      0.71     [0.66, 0.76]\n Guarantee_Period 1.04 [ 1.01,  1.17]         1.02      0.97     [0.86, 0.99]\n\nHigh Correlation\n\n      Term   VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n Age_08_04 31.07 [28.08, 34.38]         5.57      0.03     [0.03, 0.04]\n  Mfg_Year 31.16 [28.16, 34.48]         5.58      0.03     [0.03, 0.04]\n\n\n\ncheck_c &lt;- check_collinearity(model)\nplot(check_c)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4b - Visual Statistical Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#model-diagnostic-check-for-normality",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#model-diagnostic-check-for-normality",
    "title": "Hands-on Exercise 4b - Visual Statistical Analysis",
    "section": "4.3 Model Diagnostic: Check for Normality",
    "text": "4.3 Model Diagnostic: Check for Normality\nWe use check_normality() of performance package to check if the model follows the normality assumption.\n\nmodel1 &lt;- lm(Price ~ Age_08_04 + KM + \n              Weight + Guarantee_Period, data = car_resale)\n\ncheck_n &lt;- check_normality(model1)\n\nplot(check_n)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4b - Visual Statistical Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#model-diagnostic-check-for-homogeneity-of-variances",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#model-diagnostic-check-for-homogeneity-of-variances",
    "title": "Hands-on Exercise 4b - Visual Statistical Analysis",
    "section": "4.4 Model Diagnostic: Check for Homogeneity of Variances",
    "text": "4.4 Model Diagnostic: Check for Homogeneity of Variances\nWe check for homogeneity of Variances using check_heterscedasticity of performance package.\n\ncheck_h &lt;- check_heteroscedasticity(model1)\nplot(check_h)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4b - Visual Statistical Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#model-diagnostic-complete-check",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#model-diagnostic-complete-check",
    "title": "Hands-on Exercise 4b - Visual Statistical Analysis",
    "section": "4.5 Model Diagnostic: Complete Check",
    "text": "4.5 Model Diagnostic: Complete Check\nWe can also perform the complete by using check_model().\n\ncheck_model(model1)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4b - Visual Statistical Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#visualising-regression-parameters",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#visualising-regression-parameters",
    "title": "Hands-on Exercise 4b - Visual Statistical Analysis",
    "section": "4.6 Visualising Regression Parameters",
    "text": "4.6 Visualising Regression Parameters\nWe can use two methods to visualise regression parameters: (i) see method and (ii) ggcoefstats() method.\n\nsee methodggcoefstats() method\n\n\n\nplot(parameters(model1))\n\n\n\n\n\n\n\n\n\n\n\nggcoefstats(model1, \n            output = \"plot\")",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4b - Visual Statistical Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04c.html",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04c.html",
    "title": "Hands-on Exercise 4C - Visualising Uncertainty",
    "section": "",
    "text": "In this hands-on exercise, we will learn how to:\n\nplot static error bars using ggplot2\nplot interactive error bars using ggplot2, plotly and DT\ncreate hypothetical outcome plots (HOPs) by using ungeviz package.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4C - Visualising Uncertainty"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04c.html#installing-and-loading-the-packages",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04c.html#installing-and-loading-the-packages",
    "title": "Hands-on Exercise 4C - Visualising Uncertainty",
    "section": "2.1 Installing and Loading the Packages",
    "text": "2.1 Installing and Loading the Packages\nFor this exercise, other than tidyverse, we will use the following packages:\n\ntidyverse: a family of R packages for data science process,\nplotly: to create interactive plot,\ngganimate: to create animation plot,\nDT: to display interactive html table,\ncrosstalk: to implement cross-widget interactions (currently, linked brushing and filtering), and\nggdist: to visualise distribution and uncertainty.\n\nThe code chunk below uses p_load() of pacman package to check if the abovementioned packages are installed in the computer. If they are, they will be launched in R. Otherwise, pacman will install the relevant packages before launching them.\n\n\nShow the code\ndevtools::install_github(\"wilkelab/ungeviz\") #you only need to do this step once \n\n\n\n\nShow the code\npacman::p_load(ungeviz, plotly, crosstalk,\n               DT, ggdist, ggridges,\n               colorspace, gganimate, tidyverse)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4C - Visualising Uncertainty"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04c.html#importing-data-into-r",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04c.html#importing-data-into-r",
    "title": "Hands-on Exercise 4C - Visualising Uncertainty",
    "section": "2.2 Importing Data into R",
    "text": "2.2 Importing Data into R\nWe will use Exam_data.csv for this exercise.\n\n\nShow the code\nexam &lt;- read_csv(\"data/Exam_data.csv\")",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4C - Visualising Uncertainty"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04c.html#standard-error-bars-of-point-estimates",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04c.html#standard-error-bars-of-point-estimates",
    "title": "Hands-on Exercise 4C - Visualising Uncertainty",
    "section": "4.1 Standard Error Bars of Point Estimates",
    "text": "4.1 Standard Error Bars of Point Estimates\nWe can visualise the standard error bars of mean maths score by race using the following code chunk.\nNote that the error bars are computed by using the formula mean+/-se. :::{.callout-important} For geom_point(), it is important to indicate stat=“identity”. :::\n\n\nShow the code\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=RACE, \n        ymin=mean-se, \n        ymax=mean+se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  ggtitle(\"Standard error of mean maths score by race\")",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4C - Visualising Uncertainty"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04c.html#confidence-interval-of-point-estimates",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04c.html#confidence-interval-of-point-estimates",
    "title": "Hands-on Exercise 4C - Visualising Uncertainty",
    "section": "4.2 Confidence Interval of Point Estimates",
    "text": "4.2 Confidence Interval of Point Estimates\nInstead of plotting the standard error bar of point estimates, we can also plot the confidence intervals of mean maths score by race.\n\n\nShow the code\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=reorder(RACE, -mean), \n        ymin=mean-1.96*se, \n        ymax=mean+1.96*se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  labs(x = \"Maths score\",\n       title = \"95% confidence interval of mean maths score by race\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe confidence intervals are computed by using the formula mean+/-1.96*se. The error bars are sorted using the average maths scores. labs() argument of ggplot2 is used to change the x-axis label.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4C - Visualising Uncertainty"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04c.html#interactive-error-bars",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04c.html#interactive-error-bars",
    "title": "Hands-on Exercise 4C - Visualising Uncertainty",
    "section": "4.3 Interactive error bars",
    "text": "4.3 Interactive error bars\nWe can also plot interactive error bars for the 99% confidence interval of mean maths scores by race using the following code chunk.\n\n\nShow the code\nshared_df = SharedData$new(my_sum)\n\nbscols(widths = c(4,8),\n       ggplotly((ggplot(shared_df) +\n                   geom_errorbar(aes(\n                     x=reorder(RACE, -mean),\n                     ymin=mean-2.58*se, \n                     ymax=mean+2.58*se), \n                     width=0.2, \n                     colour=\"black\", \n                     alpha=0.9, \n                     size=0.5) +\n                   geom_point(aes(\n                     x=RACE, \n                     y=mean, \n                     text = paste(\"Race:\", `RACE`, \n                                  \"&lt;br&gt;N:\", `n`,\n                                  \"&lt;br&gt;Avg. Scores:\", round(mean, digits = 2),\n                                  \"&lt;br&gt;95% CI:[\", \n                                  round((mean-2.58*se), digits = 2), \",\",\n                                  round((mean+2.58*se), digits = 2),\"]\")),\n                     stat=\"identity\", \n                     color=\"red\", \n                     size = 1.5, \n                     alpha=1) + \n                   xlab(\"Race\") + \n                   ylab(\"Average Scores\") + \n                   theme_minimal() + \n                   theme(axis.text.x = element_text(\n                     angle = 45, vjust = 0.5, hjust=1)) +\n                   ggtitle(\"99% Confidence interval of average /&lt;br&gt;maths scores by race\")), \n                tooltip = \"text\"), \n       DT::datatable(shared_df, \n                     rownames = FALSE, \n                     class=\"compact\", \n                     width=\"100%\", \n                     options = list(pageLength = 10,\n                                    scrollX=T), \n                     colnames = c(\"No. of pupils\", \n                                  \"Avg Scores\",\n                                  \"Std Dev\",\n                                  \"Std Error\")) %&gt;%\n         formatRound(columns=c('mean', 'sd', 'se'),\n                     digits=2))",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4C - Visualising Uncertainty"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04c.html#about-ggdist",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04c.html#about-ggdist",
    "title": "Hands-on Exercise 4C - Visualising Uncertainty",
    "section": "5.1 About ggdist",
    "text": "5.1 About ggdist\nggdist is an R package that provides a flexible set of ggplot2 geoms and stats designed especially for visualising distributions and uncertainty.\nIt is designed for both frequentist and Bayesian uncertainty visualization, taking the view that uncertainty visualization can be unified through the perspective of distribution visualization:\n\nfor frequentist models, one visualises confidence distributions or bootstrap distributions (see vignette(“freq-uncertainty-vis”));\nfor Bayesian models, one visualises probability distributions (see the tidybayes package, which builds on top of ggdist).",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4C - Visualising Uncertainty"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04c.html#confidence-intervals-of-mean-maths-scores",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04c.html#confidence-intervals-of-mean-maths-scores",
    "title": "Hands-on Exercise 4C - Visualising Uncertainty",
    "section": "5.2 Confidence Intervals of Mean Maths Scores",
    "text": "5.2 Confidence Intervals of Mean Maths Scores\nWe can use stat_pointinterval() or stat_gradientinterval() to build a visual for displaying distribution of maths scores by race\n\nstat_pointinterval()stat_gradientinterval()\n\n\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_pointinterval() +\n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\n\n\n\n\n\n\n\n\n\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_gradientinterval(   \n    fill = \"skyblue\",      \n    show.legend = TRUE     \n  ) +                        \n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Gradient + interval plot\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThese two functions come with many arguments, please refer to the syntax reference for more detail.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4C - Visualising Uncertainty"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04d.html",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04d.html",
    "title": "Hands-on Exercise 4d - Funnel Plots for Fair Comparisons",
    "section": "",
    "text": "In this exercise, we will learn how to plot the following:\n\nfunnel plots using funnelPlotR package,\nstatic funnel plot using ggplot2 package, and\ninteractive funnel plot using both plotly R and ggplot2 packages.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4d - Funnel Plots for Fair Comparisons"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04d.html#installing-and-loading-the-packages",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04d.html#installing-and-loading-the-packages",
    "title": "Hands-on Exercise 4d - Funnel Plots for Fair Comparisons",
    "section": "3.1 Installing and Loading the Packages",
    "text": "3.1 Installing and Loading the Packages\nFor this exercise, other than tidyverse, we will use the following packages:\n\nreadr for importing csv into R.\nFunnelPlotR for creating funnel plot.\nggplot2 for creating funnel plot manually.\nknitr for building static html table.\nplotly for creating interactive funnel plot.\n\n\npacman::p_load(tidyverse, FunnelPlotR, plotly, knitr)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4d - Funnel Plots for Fair Comparisons"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04d.html#importing-data",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04d.html#importing-data",
    "title": "Hands-on Exercise 4d - Funnel Plots for Fair Comparisons",
    "section": "3.2 Importing Data",
    "text": "3.2 Importing Data\nIn this exercise, COVID-19_DKI_Jakarta will be used. The data was downloaded from Open Data Covid-19 Provinsi DKI Jakarta portal. \nWe will use this data to compare the cumulative COVID-19 cases and deaths by sub-districts as at 31 Jul 2021, DKI Jakarta.\nThe following code chunk imports the data into R and save it as a tibble dataframe object called covid19.\n\ncovid19 &lt;- read_csv(\"data/COVID-19_DKI_Jakarta.csv\") %&gt;%\n  mutate_if(is.character, as.factor)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4d - Funnel Plots for Fair Comparisons"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04d.html#about-funnelplotr",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04d.html#about-funnelplotr",
    "title": "Hands-on Exercise 4d - Funnel Plots for Fair Comparisons",
    "section": "4.1 About FunnelPlotR",
    "text": "4.1 About FunnelPlotR\nFunnelPlotR package uses ggplot to generate funnel plots. It requires a numerator (events of interest), denominator (population to be considered) and group. The key arguments selected for customisation are:\n\nlimit: plot limits (95 or 99).\nlabel_outliers: to label outliers (true or false).\nPoisson_limits: to add Poisson limits to the plot.\nOD_adjust: to add overdispersed limits to the plot.\nxrange and yrange: to specify the range to display for axes, acts like a zoom function.\nOther aesthetic components such as graph title, axis labels etc",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4d - Funnel Plots for Fair Comparisons"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04d.html#basic-plot",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04d.html#basic-plot",
    "title": "Hands-on Exercise 4d - Funnel Plots for Fair Comparisons",
    "section": "4.2 Basic Plot",
    "text": "4.2 Basic Plot\n\nfunnel_plot(\n  numerator = covid19$Positive,\n  denominator = covid19$Death,\n  group = covid19$`Sub-district`,\n  title = \"COVID-19 Cases and Deaths \\nby Sub-Districts in Jakarta (31 Jul 2021)\"\n)\n\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 0 are outliers. \nPlot is adjusted for overdispersion. \n\n\n\n\n\n\n\n\nNote\n\n\n\ngroup in this function is dfferent from the scatterplot. Here, it defines the level of the points to be plotted i.e. Sub-district, District or City. If Cityc is chosen, there are only six data points.\nBy default, data_type argument is “SR”.\nlimit: Plot limits, accepted values are: 95 or 99, corresponding to 95% or 99.8% quantiles of the distribution.\n\n\nBut the above chart is not easy to read because all the dots are close to each other.\nSo we will change the data_type from “SR” to “PR” (proportions) and add xrange and yrange to set the range of x-axis and y-axis.\n\nfunnel_plot(\n  numerator = covid19$Death,\n  denominator = covid19$Positive,\n  group = covid19$`Sub-district`,\n  data_type = \"PR\",     \n  xrange = c(0, 6500),  \n  yrange = c(0, 0.05)   \n)\n\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\nWe can further ehnance the chart by adding title and axis labels, and removing the point labels to avoid overly cluttering the chart.\n\nfunnel_plot(\n  numerator = covid19$Death,\n  denominator = covid19$Positive,\n  group = covid19$`Sub-district`,\n  data_type = \"PR\",   \n  xrange = c(0, 6500),  \n  yrange = c(0, 0.05),\n  label = NA,\n  title = \"Cumulative COVID-19 Fatality Rate by \\nCumulative Total Number of COVID-19 Positive Cases\", #&lt;&lt;           \n  x_label = \"Cumulative COVID-19 Positive Cases\", #&lt;&lt;\n  y_label = \"Cumulative Fatality Rate\"  #&lt;&lt;\n)\n\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4d - Funnel Plots for Fair Comparisons"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04d.html#computing-basic-derived-fields",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04d.html#computing-basic-derived-fields",
    "title": "Hands-on Exercise 4d - Funnel Plots for Fair Comparisons",
    "section": "5.1 Computing Basic Derived Fields",
    "text": "5.1 Computing Basic Derived Fields\nTo plot the funnel plot from scratch, we need to calculate the death rate and the standard error of cumulative death rate.\n\ndf &lt;- covid19 %&gt;%\n  mutate(rate = Death / Positive) %&gt;%\n  mutate(rate.se = sqrt((rate*(1-rate)) / (Positive))) %&gt;%\n  filter(rate &gt; 0)\n\nThen we compute the fit.mean using the following code chunk.\n\nfit.mean &lt;- weighted.mean(df$rate, 1/df$rate.se^2)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4d - Funnel Plots for Fair Comparisons"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04d.html#computing-upper-and-lower-limits-for-95-and-99-confiedence-intervals",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04d.html#computing-upper-and-lower-limits-for-95-and-99-confiedence-intervals",
    "title": "Hands-on Exercise 4d - Funnel Plots for Fair Comparisons",
    "section": "5.2 Computing Upper and Lower Limits for 95% and 99% Confiedence Intervals",
    "text": "5.2 Computing Upper and Lower Limits for 95% and 99% Confiedence Intervals\nThe following code chun computes the upper and lowr limits for 95% and 99% Confidence Intervals.\n\nnumber.seq &lt;- seq(1, max(df$Positive), 1)\nnumber.ll95 &lt;- fit.mean - 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul95 &lt;- fit.mean + 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ll999 &lt;- fit.mean - 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul999 &lt;- fit.mean + 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \ndfCI &lt;- data.frame(number.ll95, number.ul95, number.ll999, \n                   number.ul999, number.seq, fit.mean)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4d - Funnel Plots for Fair Comparisons"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04d.html#static-funnel-plot",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04d.html#static-funnel-plot",
    "title": "Hands-on Exercise 4d - Funnel Plots for Fair Comparisons",
    "section": "5.3 Static Funnel Plot",
    "text": "5.3 Static Funnel Plot\nNow we are ready to start plotting!\nWe can create a static funnel plot using a combination of geom_point and geom_line functions, as seen in the following code chunk.\n\np &lt;- ggplot(df, aes(x = Positive, y = rate)) +\n  geom_point(aes(label=`Sub-district`), \n             alpha=0.4) +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_hline(data = dfCI, \n             aes(yintercept = fit.mean), \n             size = 0.4, \n             colour = \"grey40\") +\n  coord_cartesian(ylim=c(0,0.05)) +\n  annotate(\"text\", x = 1, y = -0.13, label = \"95%\", size = 3, colour = \"grey40\") + \n  annotate(\"text\", x = 4.5, y = -0.18, label = \"99%\", size = 3, colour = \"grey40\") + \n  ggtitle(\"Cumulative Fatality Rate by Cumulative Number of COVID-19 Cases\") +\n  xlab(\"Cumulative Number of COVID-19 Cases\") + \n  ylab(\"Cumulative Fatality Rate\") +\n  theme_light() +\n  theme(plot.title = element_text(size=12),\n        legend.position = c(0.91,0.85), \n        legend.title = element_text(size=7),\n        legend.text = element_text(size=7),\n        legend.background = element_rect(colour = \"grey60\", linetype = \"dotted\"),\n        legend.key.height = unit(0.3, \"cm\"))\n\np",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4d - Funnel Plots for Fair Comparisons"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04d.html#interactive-funnel-plot",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04d.html#interactive-funnel-plot",
    "title": "Hands-on Exercise 4d - Funnel Plots for Fair Comparisons",
    "section": "5.4 Interactive Funnel Plot",
    "text": "5.4 Interactive Funnel Plot\nWe can make the funnel plot interactive using ggplotly() of plotly r package.\n\nfp_ggplotly &lt;- ggplotly(p,\n  tooltip = c(\"label\", \n              \"x\", \n              \"y\"))\nfp_ggplotly",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4d - Funnel Plots for Fair Comparisons"
    ]
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html",
    "title": "In-class Exercise 2: Horizon Plot",
    "section": "",
    "text": "First, let us ensure that the required R packages have been installed and import the relevant data for this hands-on exercise.\n\n\nFor this in-class exercise, we will be using the following packages:\n\ntidyverse\nggHoriPlot\nggthemes\n\nThe code chunk below uses p_load() of pacman package to check if the packages are installed in the computer. If they are, they will be launched in R. Otherwise, pacman will install the relevant packages before launching them.\n\npacman::p_load(tidyverse, ggHoriPlot, ggthemes)\n\n\n\n\nWe will use read_csv() to import the csv file into R and use glimpse() to check the imported data.\n\naverp &lt;- read_csv(\"data/AVERP.csv\")\nglimpse(averp)\n\nRows: 7,452\nColumns: 3\n$ Date             &lt;chr&gt; \"1/1/2014\", \"1/2/2014\", \"1/3/2014\", \"1/4/2014\", \"1/5/…\n$ `Consumer Items` &lt;chr&gt; \"Wholemeal Bread (Per 400 Gram)\", \"Wholemeal Bread (P…\n$ Values           &lt;dbl&gt; 2.05, 2.05, 2.04, 2.04, 2.05, 2.05, 2.05, 2.05, 2.04,…\n\n\nFrom the above output, we noted that the Date field was read as character data type. We will use Lubriate package to change the Date field to Date data type.\n\naverp &lt;- averp %&gt;%\n  mutate(Date = dmy(Date)) %&gt;%\n  rename(ConsumerItem = `Consumer Items`)\nglimpse(averp)\n\nRows: 7,452\nColumns: 3\n$ Date         &lt;date&gt; 2014-01-01, 2014-02-01, 2014-03-01, 2014-04-01, 2014-05-…\n$ ConsumerItem &lt;chr&gt; \"Wholemeal Bread (Per 400 Gram)\", \"Wholemeal Bread (Per 4…\n$ Values       &lt;dbl&gt; 2.05, 2.05, 2.04, 2.04, 2.05, 2.05, 2.05, 2.05, 2.04, 2.0…",
    "crumbs": [
      "In-class Exercises",
      "In-class Exercise 2: Horizon Plot"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#adding-interactivity---using-ggirafe",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#adding-interactivity---using-ggirafe",
    "title": "Hands-on Exercise 4a - Visualising Distribution",
    "section": "4.7 Adding Interactivity - Using ggirafe",
    "text": "4.7 Adding Interactivity - Using ggirafe\n\nexam_data$tooltip &lt;- c(paste0(\"Name = \", exam_data$ID, \n                              \"\\n Class = \", exam_data$CLASS))\n\nplot1 &lt;- ggplot(exam_data, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.4,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               position= position_nudge(x = 0.2),\n               outlier.shape = NA) +\n  geom_dotplot_interactive(aes(data_id = CLASS,\n                               tooltip = exam_data$tooltip),\n                           binaxis = \"y\",\n                           stackdir = \"down\",\n                           stackgroups = TRUE, \n                           binwidth = 0.5,\n                           method = \"histodot\"\n                           )+\n  coord_flip() + \n  theme_economist()\n\ngirafe(ggobj = plot1, \n       width_svg = 6,\n       height_svg = 6*0.618,\n       options = list(\n         opts_hover(css = \"fill:#FF33A2;\"),\n         opts_hover_inv(css = \"opacity:0.2\")\n       ))",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4a - Visualising Distribution"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#adding-interactivity---using-ggplotly",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#adding-interactivity---using-ggplotly",
    "title": "Hands-on Exercise 4a - Visualising Distribution",
    "section": "4.8 Adding Interactivity - Using ggplotly",
    "text": "4.8 Adding Interactivity - Using ggplotly\nWork in progress!",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4a - Visualising Distribution"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html",
    "title": "Hands-on Exercise 5a - Creating Ternary Plot with R",
    "section": "",
    "text": "In this exercise, we will learn how to build ternary plot programmatically using R for visualising and analysing population structure of Singapore.\n\n\n\n\n\n\nWhat is ternary plot?\n\n\n\nTernary plots are a way of displaying the distribution and variability of three-part compositional data. For example, in this exercise, we will have proportions of population: (1) aged, (2) economy active and (3) young. The plot is displayed in a triangle with sides scaled from 0 to 1. Each side represents one of the three components. A point is plotted so that a line drawn perpendicular from the point to each leg of the triangle intersect at the component values of the point.\nWe will see more of it later!",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5a - Creating Ternary Plot with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#installing-and-loading-the-packages",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#installing-and-loading-the-packages",
    "title": "Hands-on Exercise 5a - Creating Ternary Plot with R",
    "section": "2.1 Installing and Loading the Packages",
    "text": "2.1 Installing and Loading the Packages\nFor this exercise, other than tidyverse (in particular readr, dplyr and tidyr), we will use the following packages:\n\nggtern: a ggplot extension specially designed to plot ternary diagrams. We will use this to plot static ternary plots.\nplotly R: to create interactive web-based graphs based on plotly’s JavaScript graphing library, plotly.js. We will make use of plotly R library’s ggplotly() function to convert ggplot2 figures into a plotly object.\n\nThe code chunk below uses p_load() of pacman package to check if the abovementioned packages are installed in the computer. If they are, they will be launched in R. Otherwise, pacman will install the relevant packages before launching them.\n\n\nShow the code\npacman::p_load(tidyverse, plotly, ggtern, DT)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5a - Creating Ternary Plot with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#importing-the-data",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#importing-the-data",
    "title": "Hands-on Exercise 5a - Creating Ternary Plot with R",
    "section": "2.2 Importing the Data",
    "text": "2.2 Importing the Data\nFor this exercise, we will be using the Singapore Residents by Planning Area Subzone, Age Group, Sex and Type of Dwelling (June 2000-2018) data from Singstats. The course instructor has provided the downloaded data respopagsex2000to2018_tidy.csv in csv file format.\nThe following code chunk uses read_csv() function of readr package to import the data into R.\n\n\nShow the code\npopdata &lt;- read_csv(\"data/respopagsex2000to2018_tidy.csv\")\n\n\n\nThe DataChecking the Data\n\n\n\ndatatable(popdata)\n\n\n\n\n\n\n\n\nglimpse(popdata)\n\nRows: 108,126\nColumns: 5\n$ PA         &lt;chr&gt; \"Ang Mo Kio\", \"Ang Mo Kio\", \"Ang Mo Kio\", \"Ang Mo Kio\", \"An…\n$ SZ         &lt;chr&gt; \"Ang Mo Kio Town Centre\", \"Ang Mo Kio Town Centre\", \"Ang Mo…\n$ AG         &lt;chr&gt; \"AGE0-4\", \"AGE0-4\", \"AGE0-4\", \"AGE0-4\", \"AGE0-4\", \"AGE0-4\",…\n$ Year       &lt;dbl&gt; 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2011, 2012,…\n$ Population &lt;dbl&gt; 290, 270, 260, 250, 260, 250, 200, 180, 290, 290, 270, 300,…\n\n\n\n\n\nNote that the data has information from 2000 to 2018. In addition, each row tells us the number of residents for a particular population age group in a certain planning area subzone for a particular year. This current format is not useful for the ternary plot that we are going to make.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5a - Creating Ternary Plot with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#preparing-the-data",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#preparing-the-data",
    "title": "Hands-on Exercise 5a - Creating Ternary Plot with R",
    "section": "2.3 Preparing the Data",
    "text": "2.3 Preparing the Data\nAs such, we use the mutate() function of dplyr package to:\n\nchange the year from numerical to character\nderive three new measures: young, active, and old using spread()\nfilter only those data from year 2018 and with values more than 0.\n\n\n\nShow the code\nagpop_mutated &lt;- popdata %&gt;%\n  mutate(`Year` = as.character(Year))%&gt;%\n  spread(AG, Population) %&gt;%\n  mutate(YOUNG = rowSums(.[4:8]))%&gt;%\n  mutate(ACTIVE = rowSums(.[9:16]))  %&gt;%\n  mutate(OLD = rowSums(.[17:21])) %&gt;%\n  mutate(TOTAL = rowSums(.[22:24])) %&gt;%\n  filter(Year == 2018)%&gt;%\n  filter(TOTAL &gt; 0)\n\n\n\nThe DataChecking the Data\n\n\n\ndatatable(agpop_mutated)\n\n\n\n\n\n\n\n\nglimpse(agpop_mutated)\n\nRows: 234\nColumns: 25\n$ PA         &lt;chr&gt; \"Ang Mo Kio\", \"Ang Mo Kio\", \"Ang Mo Kio\", \"Ang Mo Kio\", \"An…\n$ SZ         &lt;chr&gt; \"Ang Mo Kio Town Centre\", \"Cheng San\", \"Chong Boon\", \"Kebun…\n$ Year       &lt;chr&gt; \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"20…\n$ `AGE0-4`   &lt;dbl&gt; 180, 1060, 900, 720, 220, 550, 260, 830, 160, 810, 350, 282…\n$ `AGE05-9`  &lt;dbl&gt; 270, 1080, 900, 850, 310, 630, 340, 930, 160, 1070, 460, 32…\n$ `AGE10-14` &lt;dbl&gt; 320, 1080, 1030, 1010, 380, 670, 430, 930, 220, 1300, 490, …\n$ `AGE15-19` &lt;dbl&gt; 300, 1260, 1220, 1120, 500, 780, 500, 860, 260, 1450, 400, …\n$ `AGE20-24` &lt;dbl&gt; 260, 1400, 1380, 1230, 550, 950, 640, 1020, 350, 1500, 330,…\n$ `AGE25-29` &lt;dbl&gt; 300, 1880, 1760, 1460, 500, 1080, 690, 1400, 340, 1590, 310…\n$ `AGE30-34` &lt;dbl&gt; 270, 1940, 1830, 1330, 300, 990, 440, 1350, 230, 1390, 310,…\n$ `AGE35-39` &lt;dbl&gt; 330, 2300, 1920, 1540, 290, 1100, 400, 1700, 250, 1770, 630…\n$ `AGE40-44` &lt;dbl&gt; 430, 2090, 1900, 1700, 420, 1140, 490, 1720, 260, 1860, 810…\n$ `AGE45-49` &lt;dbl&gt; 470, 2180, 1910, 1830, 550, 1230, 580, 1530, 320, 2000, 830…\n$ `AGE50-54` &lt;dbl&gt; 360, 2160, 2070, 1880, 550, 1350, 640, 1480, 300, 1980, 620…\n$ `AGE55-59` &lt;dbl&gt; 310, 2150, 2140, 1810, 560, 1420, 730, 1720, 360, 2010, 460…\n$ `AGE60-64` &lt;dbl&gt; 300, 2270, 2170, 1750, 450, 1290, 680, 1680, 350, 1980, 390…\n$ `AGE65-69` &lt;dbl&gt; 270, 2130, 2050, 1700, 410, 1150, 500, 1610, 250, 1790, 340…\n$ `AGE70-74` &lt;dbl&gt; 190, 1370, 1570, 1240, 290, 830, 280, 1190, 160, 1090, 220,…\n$ `AGE75-79` &lt;dbl&gt; 150, 980, 1170, 870, 220, 680, 210, 980, 100, 690, 110, 257…\n$ `AGE80-84` &lt;dbl&gt; 60, 560, 640, 540, 140, 360, 180, 560, 70, 390, 80, 1520, 2…\n$ AGE85over  &lt;dbl&gt; 60, 440, 530, 430, 140, 340, 130, 460, 60, 310, 100, 1350, …\n$ YOUNG      &lt;dbl&gt; 1330, 5880, 5430, 4930, 1960, 3580, 2170, 4570, 1150, 6130,…\n$ ACTIVE     &lt;dbl&gt; 2770, 16970, 15700, 13300, 3620, 9600, 4650, 12580, 2410, 1…\n$ OLD        &lt;dbl&gt; 730, 5480, 5960, 4780, 1200, 3360, 1300, 4800, 640, 4270, 8…\n$ TOTAL      &lt;dbl&gt; 4830, 28330, 27090, 23010, 6780, 16540, 8120, 21950, 4200, …",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5a - Creating Ternary Plot with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#checking-the-data-1",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#checking-the-data-1",
    "title": "Hands-on Exercise 5a - Creating Ternary Plot with R",
    "section": "2.7 Checking the Data",
    "text": "2.7 Checking the Data\n\nglimpse(agpop_mutated)\n\nRows: 234\nColumns: 25\n$ PA         &lt;chr&gt; \"Ang Mo Kio\", \"Ang Mo Kio\", \"Ang Mo Kio\", \"Ang Mo Kio\", \"An…\n$ SZ         &lt;chr&gt; \"Ang Mo Kio Town Centre\", \"Cheng San\", \"Chong Boon\", \"Kebun…\n$ Year       &lt;chr&gt; \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"20…\n$ `AGE0-4`   &lt;dbl&gt; 180, 1060, 900, 720, 220, 550, 260, 830, 160, 810, 350, 282…\n$ `AGE05-9`  &lt;dbl&gt; 270, 1080, 900, 850, 310, 630, 340, 930, 160, 1070, 460, 32…\n$ `AGE10-14` &lt;dbl&gt; 320, 1080, 1030, 1010, 380, 670, 430, 930, 220, 1300, 490, …\n$ `AGE15-19` &lt;dbl&gt; 300, 1260, 1220, 1120, 500, 780, 500, 860, 260, 1450, 400, …\n$ `AGE20-24` &lt;dbl&gt; 260, 1400, 1380, 1230, 550, 950, 640, 1020, 350, 1500, 330,…\n$ `AGE25-29` &lt;dbl&gt; 300, 1880, 1760, 1460, 500, 1080, 690, 1400, 340, 1590, 310…\n$ `AGE30-34` &lt;dbl&gt; 270, 1940, 1830, 1330, 300, 990, 440, 1350, 230, 1390, 310,…\n$ `AGE35-39` &lt;dbl&gt; 330, 2300, 1920, 1540, 290, 1100, 400, 1700, 250, 1770, 630…\n$ `AGE40-44` &lt;dbl&gt; 430, 2090, 1900, 1700, 420, 1140, 490, 1720, 260, 1860, 810…\n$ `AGE45-49` &lt;dbl&gt; 470, 2180, 1910, 1830, 550, 1230, 580, 1530, 320, 2000, 830…\n$ `AGE50-54` &lt;dbl&gt; 360, 2160, 2070, 1880, 550, 1350, 640, 1480, 300, 1980, 620…\n$ `AGE55-59` &lt;dbl&gt; 310, 2150, 2140, 1810, 560, 1420, 730, 1720, 360, 2010, 460…\n$ `AGE60-64` &lt;dbl&gt; 300, 2270, 2170, 1750, 450, 1290, 680, 1680, 350, 1980, 390…\n$ `AGE65-69` &lt;dbl&gt; 270, 2130, 2050, 1700, 410, 1150, 500, 1610, 250, 1790, 340…\n$ `AGE70-74` &lt;dbl&gt; 190, 1370, 1570, 1240, 290, 830, 280, 1190, 160, 1090, 220,…\n$ `AGE75-79` &lt;dbl&gt; 150, 980, 1170, 870, 220, 680, 210, 980, 100, 690, 110, 257…\n$ `AGE80-84` &lt;dbl&gt; 60, 560, 640, 540, 140, 360, 180, 560, 70, 390, 80, 1520, 2…\n$ AGE85over  &lt;dbl&gt; 60, 440, 530, 430, 140, 340, 130, 460, 60, 310, 100, 1350, …\n$ YOUNG      &lt;dbl&gt; 1330, 5880, 5430, 4930, 1960, 3580, 2170, 4570, 1150, 6130,…\n$ ACTIVE     &lt;dbl&gt; 2770, 16970, 15700, 13300, 3620, 9600, 4650, 12580, 2410, 1…\n$ OLD        &lt;dbl&gt; 730, 5480, 5960, 4780, 1200, 3360, 1300, 4800, 640, 4270, 8…\n$ TOTAL      &lt;dbl&gt; 4830, 28330, 27090, 23010, 6780, 16540, 8120, 21950, 4200, …"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#the-data",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#the-data",
    "title": "Hands-on Exercise 5a - Creating Ternary Plot with R",
    "section": "2.3 The Data",
    "text": "2.3 The Data\n\ndatatable(popdata)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#checking-the-data",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#checking-the-data",
    "title": "Hands-on Exercise 5a - Creating Ternary Plot with R",
    "section": "2.4 Checking the Data",
    "text": "2.4 Checking the Data\n\nglimpse(popdata)\n\nRows: 108,126\nColumns: 5\n$ PA         &lt;chr&gt; \"Ang Mo Kio\", \"Ang Mo Kio\", \"Ang Mo Kio\", \"Ang Mo Kio\", \"An…\n$ SZ         &lt;chr&gt; \"Ang Mo Kio Town Centre\", \"Ang Mo Kio Town Centre\", \"Ang Mo…\n$ AG         &lt;chr&gt; \"AGE0-4\", \"AGE0-4\", \"AGE0-4\", \"AGE0-4\", \"AGE0-4\", \"AGE0-4\",…\n$ Year       &lt;dbl&gt; 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2011, 2012,…\n$ Population &lt;dbl&gt; 290, 270, 260, 250, 260, 250, 200, 180, 290, 290, 270, 300,…"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#static-ternary-diagram",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#static-ternary-diagram",
    "title": "Hands-on Exercise 5a - Creating Ternary Plot with R",
    "section": "3.1 Static Ternary Diagram",
    "text": "3.1 Static Ternary Diagram\nWe can use ggtern() function of ggtern package to create a simple static ternary plot.\n\n\nShow the code\nggtern(data = agpop_mutated, \n       aes(x = YOUNG, y = ACTIVE, z = OLD)) + \n  geom_point()\n\n\n\n\n\n\n\n\n\nWe can further customise the ternary chart by adding titles using labs() from ggplot2 and themes from ggtern package.\nFor the list of themes provided by ggtern, please refer to here.\n\n\nShow the code\nggtern(agpop_mutated,\n       aes(x=YOUNG, y=ACTIVE, z=OLD)) +\n  geom_point() +\n  labs(title = \"Population Struction 2018\") +\n  theme_rgbg()",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5a - Creating Ternary Plot with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#interactive-ternary-diagram",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#interactive-ternary-diagram",
    "title": "Hands-on Exercise 5a - Creating Ternary Plot with R",
    "section": "3.2 Interactive Ternary Diagram",
    "text": "3.2 Interactive Ternary Diagram\nTo create an interactive ternary plot, we will be using plot_ly() function of Plotly R. It consists several steps: 1. we will first create a function for creating annotation object. 2. Then we will create a function for axis formating 3. Then we will create a plotly visualisation!\nFirst, we will create a function for the label. In this label, we specify the font size, font color, label background color and border width of the label.\n\n\nShow the code\nlabel &lt;- function(txt){\n  list(\n    text = txt,\n    x = 0.1, \n    y = 0.1,\n    ax = 0,\n    ay = 0,\n    xref=\"paper\",\n    yref = \"paper\",\n    align = \"center\",\n    font = list(family = \"serif\", size = 15, color = \"white\"),\n    bgcolor = \"#760241\", bordercolor = \"black\", borderwidth = 2)\n}\n\n\nThen we will create a function for the axis formatting.\n\n\nShow the code\naxis &lt;- function(txt){\n  list(title = txt, tickformat = \".0%\", tickfont = list(size=10))\n}\n\n\nUsing the axis function created, we will create labels for the axes on the ternary plot.\n\n\nShow the code\nternaryaxes &lt;- list(\n  aaxis = axis(\"Young\"),\n  baxis = axis(\"Active\"), \n  caxis = axis(\"Old\")\n)\n\n\nNow we can plot the ternary chart using the scatterternary chart type in plot_ly.\n\n\nShow the code\nplot_ly(\n  agpop_mutated,\n  a = ~YOUNG,\n  b = ~ACTIVE,\n  c = ~OLD,\n  color = I(\"black\"),\n  type = \"scatterternary\"\n) %&gt;%\n  layout(annotations = label(\"Ternary Markers\"),\n         ternary = ternaryaxes)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5a - Creating Ternary Plot with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#what-is-ternary-plot",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#what-is-ternary-plot",
    "title": "Hands-on Exercise 5a - Creating Ternary Plot with R",
    "section": "",
    "text": "Ternary plots are a way of displaying the distribution and variability of three-part compositional data. For example, in this exercise, we will have proportions of population: (1) aged, (2) economy active and (3) young. The plot is displayed in a triangle with sides scaled from 0 to 1. Each side represents one of the three components. A point is plotted so that a line drawn perpendicular from the point to each leg of the triangle intersect at the component values of the point.\nWe will see more of it later!\n:::"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#the-data-1",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#the-data-1",
    "title": "Hands-on Exercise 5a - Creating Ternary Plot with R",
    "section": "2.6 The Data",
    "text": "2.6 The Data\n\ndatatable(agpop_mutated)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html",
    "title": "Hands-on Exercise 5b - Visual Correlation Analysis",
    "section": "",
    "text": "In this exercise, we will learn how to visualise correlation matrix using R. There are 3 main sections of this exercise. First, we will learn how to create a correlation matrix using pairs() of R Graphics. Then we will learn how to plot corrgram using corrplot package of R. Lastly, we will create an interactive correlation matrix using plotly R.\n\n\n\n\n\n\n\nWhy use correlation matrix?\n\n\n\nCorrelation coefficient measures the type and strength of the relationship between 2 variables. The values of a correlation coefficient ranges between -1.0 and 1.0. A correlation coefficient of 1 shows a perfect linear relationship between the two variables, while a -1.0 shows a perfect inverse relationship between the two variables. A correlation coefficient of 0.0 shows no linear relationship between the two variables.\nWhen we have multivariate data, the correlation coefficients are pairwise comparisons displayed in a table form, also known as correlation matrix.\nThere are three main reasons for computing a correlation matrix:\n\nTo reveal the relationship between high-dimensional variables pair-wisely\nTo input into other analyses. For example, correlation matrices can be inputs for exploratory factor analysis, confirmatory factor analysis and linear regression when excluding missing values pairwise.\nAs a diagnostic when checking other analyses. For example, in linear regression, a high amount of correlation suggests that the linear regression’s estimates would be unreliable.\n\nWhen the data is large, both in terms of the number of observations and the number of variables, Corrgram tend to be used to visually explore and analyse the structure and the patterns of relations among variables. It is designed based on two main schemes:\n\nRendering the value of a correlation to depict its sign and magnitude, and\nReordering the variables in a correlation matrix so that “similar” variables are positioned adjacently, facilitating perception.\n\nWe will see more of corrgram later in this exercise!",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5b - Visual Correlation Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#installing-and-loading-the-packages",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#installing-and-loading-the-packages",
    "title": "Hands-on Exercise 5b - Visual Correlation Analysis",
    "section": "2.1 Installing and Loading the Packages",
    "text": "2.1 Installing and Loading the Packages\nFor this exercise, other than tidyverse , we will use the following packages:\n\ncorrplot: provides a visual exploratory tool on correlation matrix that supports automatic variable reordering to help detect hidden patterns among variables.\nggstatsplot: to create correlation matrix\nplotly: makes interactive, publication-quality graphs.\n\nThe code chunk below uses p_load() of pacman package to check if the abovementioned packages are installed in the computer. If they are, they will be launched in R. Otherwise, pacman will install the relevant packages before launching them.\n\n\nShow the code\npacman::p_load(tidyverse, plotly, corrplot, ggpubr, DT, ggstatsplot)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5b - Visual Correlation Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#importing-the-data",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#importing-the-data",
    "title": "Hands-on Exercise 5b - Visual Correlation Analysis",
    "section": "2.2 Importing the Data",
    "text": "2.2 Importing the Data\nFor this exercise, we will be using the Wine Quality Data Set of UCI Machine Learning Repository. The data set consists of 13 variables and 6497 observations. For the purpose of this exercise, we have combined the red wine and white wine data into one data file. It is called wine_quality and is in csv file format.\nThe following code chunk uses read_csv() function of readr package to import the data into R.\n\n\nShow the code\nwine &lt;- read_csv(\"data/wine_quality.csv\")\n\n\n\nThe DataChecking the Data\n\n\n\ndatatable(wine)\n\n\n\n\n\n\n\n\nglimpse(wine)\n\nRows: 6,497\nColumns: 13\n$ `fixed acidity`        &lt;dbl&gt; 7.4, 7.8, 7.8, 11.2, 7.4, 7.4, 7.9, 7.3, 7.8, 7…\n$ `volatile acidity`     &lt;dbl&gt; 0.700, 0.880, 0.760, 0.280, 0.700, 0.660, 0.600…\n$ `citric acid`          &lt;dbl&gt; 0.00, 0.00, 0.04, 0.56, 0.00, 0.00, 0.06, 0.00,…\n$ `residual sugar`       &lt;dbl&gt; 1.9, 2.6, 2.3, 1.9, 1.9, 1.8, 1.6, 1.2, 2.0, 6.…\n$ chlorides              &lt;dbl&gt; 0.076, 0.098, 0.092, 0.075, 0.076, 0.075, 0.069…\n$ `free sulfur dioxide`  &lt;dbl&gt; 11, 25, 15, 17, 11, 13, 15, 15, 9, 17, 15, 17, …\n$ `total sulfur dioxide` &lt;dbl&gt; 34, 67, 54, 60, 34, 40, 59, 21, 18, 102, 65, 10…\n$ density                &lt;dbl&gt; 0.9978, 0.9968, 0.9970, 0.9980, 0.9978, 0.9978,…\n$ pH                     &lt;dbl&gt; 3.51, 3.20, 3.26, 3.16, 3.51, 3.51, 3.30, 3.39,…\n$ sulphates              &lt;dbl&gt; 0.56, 0.68, 0.65, 0.58, 0.56, 0.56, 0.46, 0.47,…\n$ alcohol                &lt;dbl&gt; 9.4, 9.8, 9.8, 9.8, 9.4, 9.4, 9.4, 10.0, 9.5, 1…\n$ quality                &lt;dbl&gt; 5, 5, 5, 6, 5, 5, 5, 7, 7, 5, 5, 5, 5, 5, 5, 5,…\n$ type                   &lt;chr&gt; \"red\", \"red\", \"red\", \"red\", \"red\", \"red\", \"red\"…",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5b - Visual Correlation Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#changing-data-type",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#changing-data-type",
    "title": "Hands-on Exercise 5b - Visual Correlation Analysis",
    "section": "2.3 Changing Data Type",
    "text": "2.3 Changing Data Type\nNotice that quality should be considered a factor, rather than a numerical value, since the number represents the “level” of wine quality.\n\nwine$quality &lt;- as.factor(wine$quality)\nglimpse(wine)\n\nRows: 6,497\nColumns: 13\n$ `fixed acidity`        &lt;dbl&gt; 7.4, 7.8, 7.8, 11.2, 7.4, 7.4, 7.9, 7.3, 7.8, 7…\n$ `volatile acidity`     &lt;dbl&gt; 0.700, 0.880, 0.760, 0.280, 0.700, 0.660, 0.600…\n$ `citric acid`          &lt;dbl&gt; 0.00, 0.00, 0.04, 0.56, 0.00, 0.00, 0.06, 0.00,…\n$ `residual sugar`       &lt;dbl&gt; 1.9, 2.6, 2.3, 1.9, 1.9, 1.8, 1.6, 1.2, 2.0, 6.…\n$ chlorides              &lt;dbl&gt; 0.076, 0.098, 0.092, 0.075, 0.076, 0.075, 0.069…\n$ `free sulfur dioxide`  &lt;dbl&gt; 11, 25, 15, 17, 11, 13, 15, 15, 9, 17, 15, 17, …\n$ `total sulfur dioxide` &lt;dbl&gt; 34, 67, 54, 60, 34, 40, 59, 21, 18, 102, 65, 10…\n$ density                &lt;dbl&gt; 0.9978, 0.9968, 0.9970, 0.9980, 0.9978, 0.9978,…\n$ pH                     &lt;dbl&gt; 3.51, 3.20, 3.26, 3.16, 3.51, 3.51, 3.30, 3.39,…\n$ sulphates              &lt;dbl&gt; 0.56, 0.68, 0.65, 0.58, 0.56, 0.56, 0.46, 0.47,…\n$ alcohol                &lt;dbl&gt; 9.4, 9.8, 9.8, 9.8, 9.4, 9.4, 9.4, 10.0, 9.5, 1…\n$ quality                &lt;fct&gt; 5, 5, 5, 6, 5, 5, 5, 7, 7, 5, 5, 5, 5, 5, 5, 5,…\n$ type                   &lt;chr&gt; \"red\", \"red\", \"red\", \"red\", \"red\", \"red\", \"red\"…\n\n\nSince correlation matrices are only for numerical variables, we can also create another dataframe by dropping the non-numerical variables (i.e. quality and type).\n\nwine2 &lt;- wine %&gt;% \n  select(-c(quality, type))\n\nglimpse(wine2)\n\nRows: 6,497\nColumns: 11\n$ `fixed acidity`        &lt;dbl&gt; 7.4, 7.8, 7.8, 11.2, 7.4, 7.4, 7.9, 7.3, 7.8, 7…\n$ `volatile acidity`     &lt;dbl&gt; 0.700, 0.880, 0.760, 0.280, 0.700, 0.660, 0.600…\n$ `citric acid`          &lt;dbl&gt; 0.00, 0.00, 0.04, 0.56, 0.00, 0.00, 0.06, 0.00,…\n$ `residual sugar`       &lt;dbl&gt; 1.9, 2.6, 2.3, 1.9, 1.9, 1.8, 1.6, 1.2, 2.0, 6.…\n$ chlorides              &lt;dbl&gt; 0.076, 0.098, 0.092, 0.075, 0.076, 0.075, 0.069…\n$ `free sulfur dioxide`  &lt;dbl&gt; 11, 25, 15, 17, 11, 13, 15, 15, 9, 17, 15, 17, …\n$ `total sulfur dioxide` &lt;dbl&gt; 34, 67, 54, 60, 34, 40, 59, 21, 18, 102, 65, 10…\n$ density                &lt;dbl&gt; 0.9978, 0.9968, 0.9970, 0.9980, 0.9978, 0.9978,…\n$ pH                     &lt;dbl&gt; 3.51, 3.20, 3.26, 3.16, 3.51, 3.51, 3.30, 3.39,…\n$ sulphates              &lt;dbl&gt; 0.56, 0.68, 0.65, 0.58, 0.56, 0.56, 0.46, 0.47,…\n$ alcohol                &lt;dbl&gt; 9.4, 9.8, 9.8, 9.8, 9.4, 9.4, 9.4, 10.0, 9.5, 1…\n\n\nNow we are ready to plot the correlation matrix!",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5b - Visual Correlation Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#drawing-half-of-the-matrix",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#drawing-half-of-the-matrix",
    "title": "Hands-on Exercise 5b - Visual Correlation Analysis",
    "section": "3.1 Drawing Half of the Matrix",
    "text": "3.1 Drawing Half of the Matrix\nAs a correlation matrix is symmetric, we can customise the pairs() function to show the upper or lower half of the matrix.\n\nLower Half of the MatrixUpper Half of the Matrix\n\n\n\npairs(wine2, upper.panel = NULL)\n\n\n\n\n\n\n\n\n\n\n\npairs(wine2, lower.panel = NULL)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5b - Visual Correlation Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#including-correlation-coefficients",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#including-correlation-coefficients",
    "title": "Hands-on Exercise 5b - Visual Correlation Analysis",
    "section": "3.2 Including Correlation Coefficients",
    "text": "3.2 Including Correlation Coefficients\nFor easy interpretation, we can also show the correlation coefficient of each pair of variables rather than a scatter plot by creating a panel.cor function. Higher correlations are shown in a larger font.\n\npanel.cor &lt;- function (x,y, digits = 2, prefix = \"\", cex.cor, ...){\n  usr &lt;- par(\"usr\")\n  on.exit(par(usr))\n  par(usr = c(0, 1, 0, 1))\n  r &lt;- abs(cor(x, y, use=\"complete.obs\"))\n  txt &lt;- format(c(r, 0.123456789), digits=digits)[1]\n  txt &lt;- paste(prefix, txt, sep=\"\")\n  if(missing(cex.cor)) cex.cor &lt;- 0.8/strwidth(txt)\n  text(0.5, 0.5, txt, cex = cex.cor * (1 + r) / 2)\n}\n\n\npairs(wine2, upper.panel = panel.cor)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5b - Visual Correlation Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#the-basic-plot",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#the-basic-plot",
    "title": "Hands-on Exercise 5b - Visual Correlation Analysis",
    "section": "4.1 The Basic Plot",
    "text": "4.1 The Basic Plot\nOn of the advantage of using ggcorrmat() over many other methods to visualise a correlation matrix is it’s ability to provide a comprehensive and yet professional statistical report as shown below.\n\nggcorrmat(wine, \n          cor.vars = 1:11)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5b - Visual Correlation Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#further-customising-the-plot",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#further-customising-the-plot",
    "title": "Hands-on Exercise 5b - Visual Correlation Analysis",
    "section": "4.2 Further Customising the plot",
    "text": "4.2 Further Customising the plot\nWe can further customise the correlation matrix by adding additional arguments and also include title and subtitle!\n\nggcorrmat(\n  data = wine, \n  cor.vars = 1:11,\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  title    = \"Correlogram for wine dataset\",\n  subtitle = \"Four pairs are no significant at p &lt; 0.05\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\ncor.vars: List of variables for which the correlation matrix is to be computed and visualized. If NULL (default), all numeric variables from data will be used.\ngcorrplot.args: A list of additional (mostly aesthetic) arguments that will be passed to ggcorrplot::ggcorrplot() function. The list should avoid any of the following arguments since they are already internally being used: corr, method, p.mat, sig.level, ggtheme, colors, lab, pch, legend.title, digits.\n\n\nTo control specific components of the plot such as the font size of x-axis, y-axis and the statistical report, we can add the following code:\n\nggcorrmat(\n  data = wine, \n  cor.vars = 1:11,\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  title    = \"Correlogram for wine dataset\",\n  subtitle = \"Four pairs are no significant at p &lt; 0.05\", \n  ggplot.component = list(\n    theme(text=element_text(size=7),\n      axis.text.x = element_text(size = 8),\n      axis.text.y = element_text(size = 8)))) \n\n\n\n\n\n\n\n\nNotice that the font size of the axes are smaller now.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5b - Visual Correlation Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#multiple-plots",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#multiple-plots",
    "title": "Hands-on Exercise 5b - Visual Correlation Analysis",
    "section": "4.3 Multiple Plots",
    "text": "4.3 Multiple Plots\nTo build facetted correlation matrix, we have to use grouped_ggcorrmat() of gstatsplot instead.\n\ngrouped_ggcorrmat(\n  data = wine,\n  cor.vars = 1:11,\n  grouping.var = type, # to have a plot for red wines and another plot for white wines\n  type = \"robust\",\n  p.adjust.method = \"holm\",\n  plotgrid.args = list(ncol = 2),\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  annotation.args = list(\n    tag_levels = \"a\",\n    title = \"Correlogram for wine dataset\",\n    subtitle = \"The measures are: alcohol, sulphates, fixed acidity, citric acid, chlorides, residual sugar, density, free sulfur dioxide and volatile acidity\",\n    caption = \"Dataset: UCI Machine Learning Repository\"\n  )\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\nTo build a facet plot, the only argument needed is grouping.var.\nBehind group_ggcorrmat(), patchwork package is used to create the multiplot. plotgrid.args argument provides a list of additional arguments passed to patchwork::wrap_plots, except for guides argument which is already separately specified earlier.\nLikewise, annotation.args argument is calling plot annotation arguments of patchwork package.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5b - Visual Correlation Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#observations",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#observations",
    "title": "Hands-on Exercise 5b - Visual Correlation Analysis",
    "section": "5.1 Observations",
    "text": "5.1 Observations\n\nthe default visual object to plot the corrgram is circle\nthe default layout of the corrgram is a symmetric matrix\nthe default colour scheme is diverging blue-red. Blue colours are used to represent pair variables with positive correlation coefficients and red colours are used to represent pair variables with negative correlation coefficients.. The intensity of the colour or also know as saturation is used to represent the strength of the correlation coefficient. Darker colours indicate relatively stronger linear relationship between the paired variables. On the other hand, lighter colours indicates relatively weaker linear relationship.\n\n:::",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5b - Visual Correlation Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#working-with-visual-geometrics",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#working-with-visual-geometrics",
    "title": "Hands-on Exercise 5b - Visual Correlation Analysis",
    "section": "5.2 Working with visual geometrics",
    "text": "5.2 Working with visual geometrics\nIn corrplot package, there are seven visual geometrics (parameter method) can be used to encode the attribute values. They are: circle, square, ellipse, number, shade, color and pie. The default is circle. However, this default setting can be changed by using the method argument as shown in the code chunk below.\n\nellipseshadesquarenumbercolourpie\n\n\n\ncorrplot(wine.cor, \n         method = \"ellipse\") \n\n\n\n\n\n\n\n\n\n\n\ncorrplot(wine.cor, \n         method = \"shade\") \n\n\n\n\n\n\n\n\n\n\n\ncorrplot(wine.cor, \n         method = \"square\") \n\n\n\n\n\n\n\n\n\n\n\ncorrplot(wine.cor, \n         method = \"number\") \n\n\n\n\n\n\n\n\n\n\n\ncorrplot(wine.cor, \n         method = \"color\") \n\n\n\n\n\n\n\n\n\n\n\ncorrplot(wine.cor, \n         method = \"pie\")",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5b - Visual Correlation Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#working-with-layout",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#working-with-layout",
    "title": "Hands-on Exercise 5b - Visual Correlation Analysis",
    "section": "5.3 Working with layout",
    "text": "5.3 Working with layout\ncorrplor() supports three layout types, namely: “full”, “upper” or “lower”. The default is “full” which display full matrix. The default setting can be changed by using the type argument of corrplot().\n\nupperlower\n\n\n\ncorrplot(wine.cor, \n         method = \"number\", \n         type=\"upper\")\n\n\n\n\n\n\n\n\n\n\n\ncorrplot(wine.cor, \n         method = \"number\", \n         type=\"lower\")\n\n\n\n\n\n\n\n\n\n\n\nThe default layout of the corrgram can be further customised. For example, arguments diag and tl.col are used to turn off the diagonal cells and to change the axis text label colour to black colour respectively as shown in the code chunk and figure below.\n\ncorrplot(wine.cor, \n         method = \"square\", \n         type=\"lower\",\n         diag = FALSE,\n         tl.col = \"black\")",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5b - Visual Correlation Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#working-with-a-mixed-layout",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#working-with-a-mixed-layout",
    "title": "Hands-on Exercise 5b - Visual Correlation Analysis",
    "section": "5.4 Working with a mixed layout",
    "text": "5.4 Working with a mixed layout\nIt is possible to design corrgram with mixed visual matrix of one half and numerical matrix on the other half. In order to create a coorgram with mixed layout, the corrplot.mixed(), a wrapped function for mixed visualisation style will be used.\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotice that argument lower and upper are used to define the visualisation method used. In this case ellipse is used to map the lower half of the corrgram and numerical matrix (i.e. number) is used to map the upper half of the corrgram. The argument tl.pos, on the other, is used to specify the placement of the axis label. Lastly, the diag argument is used to specify the glyph on the principal diagonal of the corrgram.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5b - Visual Correlation Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#combining-corrgram-with-significance-test",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#combining-corrgram-with-significance-test",
    "title": "Hands-on Exercise 5b - Visual Correlation Analysis",
    "section": "5.5 Combining corrgram with significance test",
    "text": "5.5 Combining corrgram with significance test\nIn statistical analysis, we are also interested to know which pair of variables their correlation coefficients are statistically significant.\nWith corrplot package, we can combine corrgram with significance test. First, we use the cor.mtest() to compute the p-values and confidence interval for each pair of variables.\n\nwine.sig &lt;- cor.mtest(wine.cor, conf.level=0.95)\n\nWe then use the p.mat argument of corrplot function as shown below.\n\ncorrplot(wine.cor,\n         method = \"number\",\n         type = \"lower\",\n         diag = FALSE,\n         tl.col = \"black\",\n         tl.srt = 45,\n         p.mat = wine.sig$p,\n         sig.level = .05)\n\n\n\n\n\n\n\n\nThe corrgram reveals that not all correlation pairs are statistically significant. For example the correlation between total sulfur dioxide and free surfur dioxide is statistically significant at significant level of 0.05 but not the pair between total sulfur dioxide and citric acid.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5b - Visual Correlation Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#reorder-a-corrgram",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#reorder-a-corrgram",
    "title": "Hands-on Exercise 5b - Visual Correlation Analysis",
    "section": "5.6 Reorder a corrgram",
    "text": "5.6 Reorder a corrgram\nMatrix reorder is important for mining the hiden structure and pattern in a corrgram. By default, the order of attributes of a corrgram is sorted according to the correlation matrix (i.e. “original”). The default setting can be over-write by using the order argument of corrplot(). Currently, corrplot package support four sorting methods, they are:\n\n“AOE” is for the angular order of the eigenvectors. See Michael Friendly (2002) for details.\n“FPC” for the first principal component order.\n“hclust” for hierarchical clustering order, and “hclust.method” for the agglomeration method to be used. “hclust.method” should be one of “ward”, “single”, “complete”, “average”, “mcquitty”, “median” or “centroid”.\n“alphabet” for alphabetical order.\n\n“AOE”, “FPC”, “hclust”, “alphabet”. More algorithms can be found in seriation package.\n\nUsing AOE orderUsing FPC orderUsing hclust orderUsing alphabet order\n\n\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               order=\"AOE\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\n\n\n\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               order=\"FPC\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\n\n\n\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               order=\"hclust\",\n               hclust.method= \"ward.D\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\n\n\n\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               order=\"alphabet\",\n               tl.col = \"black\")",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5b - Visual Correlation Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html",
    "title": "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data",
    "section": "",
    "text": "In this exercise, we will learn how to use R to plot static and interactive heatmap for visualising and analysing multivariate data.\n\n\n\n\n\n\nWhy heatmaps?\n\n\n\nHeatmaps visualise data through variations in colouring. When applied to a tabular format, heatmaps are useful for cross-examining multivariate data, through placing variables in the columns and observation (or records) in rowa and colouring the cells within the table. Heatmaps are good for showing variance across multiple variables, revealing any patterns, displaying whether any variables are similar to each other, and for detecting if any correlations exist in-between them.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html#installing-and-loading-the-packages",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html#installing-and-loading-the-packages",
    "title": "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data",
    "section": "2.1 Installing and Loading the Packages",
    "text": "2.1 Installing and Loading the Packages\nThe code chunk below uses p_load() of pacman package to check if the abovementioned packages are installed in the computer. If they are, they will be launched in R. Otherwise, pacman will install the relevant packages before launching them.\n\n\nShow the code\npacman::p_load(tidyverse, DT, seriation, dendextend, heatmaply)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html#importing-the-data",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html#importing-the-data",
    "title": "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data",
    "section": "2.2 Importing the Data",
    "text": "2.2 Importing the Data\nFor this exercise, we will be using the data from World Happiness 2018 report. The original data set is in Microsoft Excel format. It has been extracted and saved in csv file called WHData-2018.csv.\nIn the code chunk below, read_csv() of readr is used to import WHData-2018.csv into R and parsed it into tibble R data frame format.\n\nhappy &lt;- read_csv(\"data/WHData-2018.csv\")\n\n\nThe DataChecking the Data\n\n\n\ndatatable(happy)\n\n\n\n\n\n\n\n\nglimpse(happy)\n\nRows: 156\nColumns: 12\n$ Country                        &lt;chr&gt; \"Albania\", \"Bosnia and Herzegovina\", \"B…\n$ Region                         &lt;chr&gt; \"Central and Eastern Europe\", \"Central …\n$ `Happiness score`              &lt;dbl&gt; 4.586, 5.129, 4.933, 5.321, 6.711, 5.73…\n$ `Whisker-high`                 &lt;dbl&gt; 4.695, 5.224, 5.022, 5.398, 6.783, 5.81…\n$ `Whisker-low`                  &lt;dbl&gt; 4.477, 5.035, 4.844, 5.244, 6.639, 5.66…\n$ Dystopia                       &lt;dbl&gt; 1.462, 1.883, 1.219, 1.769, 2.494, 1.45…\n$ `GDP per capita`               &lt;dbl&gt; 0.916, 0.915, 1.054, 1.115, 1.233, 1.20…\n$ `Social support`               &lt;dbl&gt; 0.817, 1.078, 1.515, 1.161, 1.489, 1.53…\n$ `Healthy life expectancy`      &lt;dbl&gt; 0.790, 0.758, 0.712, 0.737, 0.854, 0.73…\n$ `Freedom to make life choices` &lt;dbl&gt; 0.419, 0.280, 0.359, 0.380, 0.543, 0.55…\n$ Generosity                     &lt;dbl&gt; 0.149, 0.216, 0.064, 0.120, 0.064, 0.08…\n$ `Perceptions of corruption`    &lt;dbl&gt; 0.032, 0.000, 0.009, 0.039, 0.034, 0.17…",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html#preparing-the-data",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html#preparing-the-data",
    "title": "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data",
    "section": "2.3 Preparing the Data",
    "text": "2.3 Preparing the Data\nFor the purpose of this exercise, we need to change the rows by country name instead of row number.\n\nrow.names(happy) &lt;- happy$Country\ndatatable(happy)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html#transforming-the-data-frame-into-a-matrix",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html#transforming-the-data-frame-into-a-matrix",
    "title": "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data",
    "section": "2.4 Transforming the data frame into a matrix",
    "text": "2.4 Transforming the data frame into a matrix\nThe data was loaded into a data frame, but it has to be a data matrix to make your heatmap.\nThe code chunk below will be used to transform happy data frame into a data matrix.\n\nhappy_matrix &lt;- data.matrix(happy)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html#heatmap-of-r-stats",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html#heatmap-of-r-stats",
    "title": "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data",
    "section": "3.1 heatmap() of R stats",
    "text": "3.1 heatmap() of R stats\nIn this section, we will learn how to plot static heatmaps by using heatmap() of R Stats package.By default, heatmap() plots a cluster heatmap. The arguments Rowv=NA and Colv=NA are used to switch off the option of plotting the row and column dendrograms.\n\nWith dendrogramsWithout dendrograms\n\n\n\nhappy_heatmap &lt;- heatmap(happy_matrix)\n\n\n\n\n\n\n\n\n\n\n\nhappy_heatmap &lt;- heatmap(happy_matrix,Rowv=NA, Colv=NA )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe order of both rows and columns is different compare to the native happy_matrix. This is because heatmap do a reordering using clusterisation: it calculates the distance between each pair of rows and columns and try to order them by similarity. In addition, the corresponding dendrograms are provided beside the heatmap.\n\n\nBy default, red cells denotes small values. As the Happiness Score variable have relatively higher values, this makes other variables with small values all look the same.Thus, we need to normalize this matrix. This is done using the scale argument. It can be applied to rows or to columns following your needs.\n\nNormalise Matrix column-wiseNormalise Matrix row-wise\n\n\n\nhappy_heatmap_c &lt;- heatmap(happy_matrix,\n                      scale=\"column\",\n                      cexRow = 0.6, \n                      cexCol = 0.8,\n                      margins = c(10, 4))\n\n\n\n\n\n\n\n\n\n\n\nhappy_heatmap_r &lt;- heatmap(happy_matrix,\n                      scale=\"row\",\n                      cexRow = 0.6, \n                      cexCol = 0.8,\n                      margins = c(10, 4))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice that the values are scaled now. Also note that margins argument is used to ensure that the entire x-axis labels are displayed completely and, cexRow and cexCol arguments are used to define the font size used for y-axis and x-axis labels respectively.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html#basic-plot",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html#basic-plot",
    "title": "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data",
    "section": "4.1 Basic Plot",
    "text": "4.1 Basic Plot\nUsing the function heatmaply(), we can easily create an interactive heat map by putting the datamatrix into it. For this example, we will put happy_matrix into it and also tell the function to not use the columns “Country”, “Region”, ‘whisker-high’ and ‘whisker-low’(i.e., columns 1, 2, 4 and 5)\n\nheatmaply(happy_matrix[, -c(1,2,4,5)])\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDifferent from heatmap(), for heatmaply() the default horizontal dendrogram is placed on the left hand side of the heatmap. The text label of each raw, on the other hand, is placed on the right hand side of the heat map. When the x-axis marker labels are too long, they will be rotated by 135 degree from the north.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html#data-transformation",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html#data-transformation",
    "title": "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data",
    "section": "4.2 Data Transformation",
    "text": "4.2 Data Transformation\nWhen analysing multivariate data set, it is very common that the variables in the data sets includes values that reflect different types of measurement. In general, these variables’ values have their own range. In order to ensure that all the variables have comparable values, data transformation are commonly used before clustering.\nThree main data transformation methods are supported by heatmaply(), namely: scale, normalise and percentilse.\n\nScaling MethodNormalising MethodPercentilsing Method\n\n\n\nWhen all variables are came from or assumed to come from some normal distribution, then scaling (i.e.: subtract the mean and divide by the standard deviation) would bring them all close to the standard normal distribution.\nIn such a case, each value would reflect the distance from the mean in units of standard deviation.\nThe scale argument in heatmaply() supports column and row scaling.\n\nThe following code chunk scales the variable values columnwise.\n\nheatmaply(happy_matrix[, -c(1,2,4,5)], \n          scale = \"column\", \n          fontsize_row = 4,\n          fontsize_col = 8)\n\n\n\n\n\n\n\n\nWhen variables in the data comes from possibly different (and non-normal) distributions, the normalize function can be used to bring data to the 0 to 1 scale by subtracting the minimum and dividing by the maximum of all observations.\nThis preserves the shape of each variable’s distribution while making them easily comparable on the same “scale”.\nDifferent from Scaling, the normalise method is performed on the input data set i.e. happy_matrix as shown in the code chunk below.\n\n\nheatmaply(normalize(happy_matrix[, -c(1, 2, 4, 5)]), \n          fontsize_row = 4,\n          fontsize_col = 8)\n\n\n\n\n\n\n\n\nThis is similar to ranking the variables, but instead of keeping the rank values, divide them by the maximal rank.\nThis is done by using the ecdf of the variables on their own values, bringing each value to its empirical percentile.\nThe benefit of the percentize function is that each value has a relatively clear interpretation, it is the percent of observations that got that value or below it.\nSimilar to Normalize method, the Percentize method is also performed on the happy_matrix (i.e, input dataset) as shown in the code chunk below.\n\n\nheatmaply(percentize(happy_matrix[, -c(1, 2, 4, 5)]), \n          fontsize_row = 4,\n          fontsize_col = 8)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html#clustering-algorithm",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html#clustering-algorithm",
    "title": "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data",
    "section": "4.3 Clustering Algorithm",
    "text": "4.3 Clustering Algorithm\nheatmaply supports a variety of hierarchical clustering algorithm. The main arguments provided are:\n\ndistfun: function used to compute the distance (dissimilarity) between both rows and columns. Defaults to dist. The options “pearson”, “spearman” and “kendall” can be used to use correlation-based clustering, which uses as.dist(1 - cor(t(x))) as the distance metric (using the specified correlation method).\nhclustfun: function used to compute the hierarchical clustering when Rowv or Colv are not dendrograms. Defaults to hclust.\ndist_method default is NULL, which results in “euclidean” to be used. It can accept alternative character strings indicating the method to be passed to distfun. By default distfun is “dist”” hence this can be one of “euclidean”, “maximum”, “manhattan”, “canberra”, “binary” or “minkowski”.\nhclust_method default is NULL, which results in “complete” method to be used. It can accept alternative character strings indicating the method to be passed to hclustfun. By default hclustfun is hclust hence this can be one of “ward.D”, “ward.D2”, “single”, “complete”, “average” (= UPGMA), “mcquitty” (= WPGMA), “median” (= WPGMC) or “centroid” (= UPGMC).\n\nIn general, a clustering model can be calibrated either manually or statistically.\n\n4.3.1 Manual Approach\nThe following heatmap is plotted using hierarchical clustering algorithm with Euclidean distance and ward.D method.\n\nheatmaply(normalize(happy_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"ward.D\", \n          fontsize_row = 4,\n          fontsize_col = 8)\n\n\n\n\n\n\n\n4.3.2 Statistical Approach\nIn order to determine the best clustering method and number of clusters the dend_expend() and find_k() functions of dendextend package will be used.\nFirst, the dend_expend() will be used to determine the recommended clustering method to be used.\n\n#normalise the data and compute Eucliean distance \nhappy_d &lt;- dist(normalize(happy_matrix[, -c(1, 2, 4, 5)]), method = \"euclidean\")\n\ndend_expend(happy_d)[[3]]\n\n  dist_methods hclust_methods     optim\n1      unknown         ward.D 0.6137851\n2      unknown        ward.D2 0.6289186\n3      unknown         single 0.4774362\n4      unknown       complete 0.6434009\n5      unknown        average 0.6701688\n6      unknown       mcquitty 0.5020102\n7      unknown         median 0.5901833\n8      unknown       centroid 0.6338734\n\n\nThe output table shows that “average” method should be used because it gave the highest optimum value of 0.6701688.\nNext, find_k() is used to determine the optimal number of cluster.\n\nhappy_clust &lt;- hclust(happy_d, method = \"average\")\nnum_k &lt;- find_k(happy_clust)\nplot(num_k)\n\n\n\n\n\n\n\n\nFigure above shows that k=3 would be good since it has the highest average silhouette width.\nWith reference to the statistical analysis results, we can prepare the code chunk as shown below.\n\nheatmaply(normalize(happy_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"average\",\n          k_row = 3, \n          fontsize_row = 4,\n          fontsize_col = 8)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html#seriation",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html#seriation",
    "title": "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data",
    "section": "4.4 Seriation",
    "text": "4.4 Seriation\nOne of the problems with hierarchical clustering is that it doesn’t actually place the rows in a definite order, it merely constrains the space of possible orderings. Take three items A, B and C. If you ignore reflections, there are three possible orderings: ABC, ACB, BAC. If clustering them gives you ((A+B)+C) as a tree, you know that C can’t end up between A and B, but it doesn’t tell you which way to flip the A+B cluster. It doesn’t tell you if the ABC ordering will lead to a clearer-looking heatmap than the BAC ordering.\nheatmaply uses the seriation package to find an optimal ordering of rows and columns. Optimal means to optimize the Hamiltonian path length that is restricted by the dendrogram structure. This, in other words, means to rotate the branches so that the sum of distances between each adjacent leaf (label) will be minimized. This is related to a restricted version of the travelling salesman problem.\nThe first seriation algorithm that we are going to learn is Optimal Leaf Ordering (OLO). This algorithm starts with the output of an agglomerative clustering algorithm and produces a unique ordering, one that flips the various branches of the dendrogram around so as to minimize the sum of dissimilarities between adjacent leaves. Here is the result of applying Optimal Leaf Ordering (i.e. the default option) to the same clustering result as the heatmap above.\n\nheatmaply(normalize(happy_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"OLO\", \n          fontsize_row = 4,\n          fontsize_col = 8)\n\n\n\n\n\nOptimal leaf ordering optimizes the above criterion (in O(n^4)).\nAnother option is “GW” (Gruvaeus and Wainer) which aims for the same goal but uses a potentially faster heuristic.\n\nheatmaply(normalize(happy_matrix[, -c(1,2,4,5)]),\n          seriate = \"GW\", \n          fontsize_row = 4,\n          fontsize_col = 8)\n\n\n\n\n\nThe third option “mean” gives the output we would get by default from heatmap functions in other packages such as gplots::heatmap.2.\n\nheatmaply(normalize(happy_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"mean\", \n          fontsize_row = 4,\n          fontsize_col = 8)\n\n\n\n\n\nThe option “none” gives us the dendrograms without any rotation that is based on the data matrix.\n\nheatmaply(normalize(happy_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\", \n          fontsize_row = 4,\n          fontsize_col = 8)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html#working-with-color-palettes",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html#working-with-color-palettes",
    "title": "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data",
    "section": "4.5 Working with color palettes",
    "text": "4.5 Working with color palettes\nThe default colour palette uses by heatmaply is viridis. heatmaply users, however, can use other colour palettes in order to improve the aestheticness and visual friendliness of the heatmap.\nIn the following code chunk, we use the RdPu colour palette of rcolorbrewer.\n\nheatmaply(normalize(happy_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\",\n          colors = RdPu, \n          fontsize_row = 4,\n          fontsize_col = 8)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html#finishing-touch",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html#finishing-touch",
    "title": "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data",
    "section": "4.6 Finishing Touch",
    "text": "4.6 Finishing Touch\nheatmaply also provides many plotting features to ensure cartographic quality heatmap can be produced.\nIn the code chunk below the following arguments are used:\n\nk_row is used to produce 5 groups.\nmargins is used to change the top margin to 60 and row margin to 200.\nfontsizw_row and fontsize_col are used to change the font size for row and column labels to 4.\nmain is used to write the main title of the plot.\nxlab and ylab are used to write the x-axis and y-axis labels respectively.\n\n\nheatmaply(normalize(happy_matrix[, -c(1, 2, 4, 5)]),\n          Colv=NA,\n          seriate = \"none\",\n          colors = RdPu,\n          k_row = 5,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 5,\n          fontsize_col = 5,\n          main=\"World Happiness Score and Variables by Country, 2018 \\nDataTransformation using Normalise Method\",\n          xlab = \"World Happiness Indicators\",\n          ylab = \"World Countries\"\n          )",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html",
    "title": "Hands-on Exercise 5d - Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "",
    "text": "In this exercise, we will learn how to:\n\nplotting static parallel coordinates plots by using ggparcoord() of GGally package, and\nplotting interactive parallel coordinates plots by using parallelPlot package.\n\n\n\n\n\n\n\nWhat is Parallel Coordinates Plot?\n\n\n\nParallel coordinates plot is a data visualisation specially designed for visualising and analysing multivariate, numerical data. It is ideal for comparing multiple variables together and seeing the relationships between them. For example, the variables contribute to Happiness Index. Parallel coordinates was invented by Alfred Inselberg in the 1970s as a way to visualize high-dimensional data. This data visualisation technique is more often found in academic and scientific communities than in business and consumer data visualizations. As pointed out by Stephen Few(2006), “This certainly isn’t a chart that you would present to the board of directors or place on your Web site for the general public. In fact, the strength of parallel coordinates isn’t in their ability to communicate some truth in the data to others, but rather in their ability to bring meaningful multivariate patterns and comparisons to light when used interactively for analysis.” For example, parallel coordinates plot can be used to characterise clusters detected during customer segmentation.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5d - Visual Multivariate Analysis with Parallel Coordinates Plot"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html#installing-and-loading-the-packages",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html#installing-and-loading-the-packages",
    "title": "Hands-on Exercise 5d - Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "2.1 Installing and Loading the Packages",
    "text": "2.1 Installing and Loading the Packages\nFor this exercise, the GGally, parcoords, parallelPlot and tidyverse packages will be used.\nThe code chunk below uses p_load() of pacman package to check if the abovementioned packages are installed in the computer. If they are, they will be launched in R. Otherwise, pacman will install the relevant packages before launching them.\n\n\nShow the code\npacman::p_load(tidyverse, DT, GGally, parallelPlot)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5d - Visual Multivariate Analysis with Parallel Coordinates Plot"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html#importing-the-data",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html#importing-the-data",
    "title": "Hands-on Exercise 5d - Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "2.2 Importing the Data",
    "text": "2.2 Importing the Data\nFor this exercise, we will be using the data from World Happiness 2018 report. The original data set is in Microsoft Excel format. It has been extracted and saved in csv file called WHData-2018.csv.\nIn the code chunk below, read_csv() of readr is used to import WHData-2018.csv into R and parsed it into tibble R data frame format.\n\nhappy &lt;- read_csv(\"data/WHData-2018.csv\")",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5d - Visual Multivariate Analysis with Parallel Coordinates Plot"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html#plotting-a-simple-parallel-coordinae-plot",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html#plotting-a-simple-parallel-coordinae-plot",
    "title": "Hands-on Exercise 5d - Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "3.1 Plotting a Simple Parallel Coordinae Plot",
    "text": "3.1 Plotting a Simple Parallel Coordinae Plot\n\nggparcoord(data = happy, \n           columns = c(7:12)) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice that only two argument namely data and columns is used. Data argument is used to map the data object (i.e. wh) and columns is used to select the columns for preparing the parallel coordinates plot.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5d - Visual Multivariate Analysis with Parallel Coordinates Plot"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html#plotting-a-parallel-coordinates-plot-with-boxplots",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html#plotting-a-parallel-coordinates-plot-with-boxplots",
    "title": "Hands-on Exercise 5d - Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "3.2 Plotting a parallel coordinates plot with boxplots",
    "text": "3.2 Plotting a parallel coordinates plot with boxplots\nThe earlier chart does not provide us with useful understanding of the World Happiness measures. As such, we will make over the plot using a collection of arguments provided by ggparcoord().\n\nggparcoord(data = happy, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Parallel Coordinates Plot of World Happines Variables\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\ngroupColumn argument is used to group the observations (i.e. parallel lines) by using a single variable (i.e. Region) and colour the parallel coordinates lines by region name.\nscale argument is used to scale the variables in the parallel coordinate plot by using uniminmax method. The method univariately scale each variable so the minimum of the variable is zero and the maximum is one.\nalphaLines argument is used to reduce the intensity of the line colour to 0.2. The permissible value range is between 0 to 1.\nboxplot argument is used to turn on the boxplot by using logical TRUE. The default is FALSE.\ntitle argument is used to provide the parallel coordinates plot a title.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5d - Visual Multivariate Analysis with Parallel Coordinates Plot"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html#parallel-coordinates-with-facet",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html#parallel-coordinates-with-facet",
    "title": "Hands-on Exercise 5d - Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "3.3 Parallel coordinates with facet",
    "text": "3.3 Parallel coordinates with facet\nSince ggparcoord() is developed by extending ggplot2 package, we can combination use some of the ggplot2 function when plotting a parallel coordinates plot.\nIn the code chunk below, facet_wrap() of ggplot2 is used to plot 10 small multiple parallel coordinates plots. Each plot represent one geographical region such as East Asia.\n\nggparcoord(data = happy, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nSome of the variable names overlap on x-axis!",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5d - Visual Multivariate Analysis with Parallel Coordinates Plot"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html#rotaing-x-axis-text-label",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html#rotaing-x-axis-text-label",
    "title": "Hands-on Exercise 5d - Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "3.4 Rotaing x-axis text label",
    "text": "3.4 Rotaing x-axis text label\nTo make the x-axis text label easy to read, let us rotate the labels by 30 degrees. We can rotate axis text labels using axis.text.x as argument to the theme() function. And we specify element_text(angle = 30) to rotate the x-axis text by an angle 30 degree.\n\nggparcoord(data = happy, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30))",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5d - Visual Multivariate Analysis with Parallel Coordinates Plot"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html#adjusting-the-rotated-x-axis-text-label",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html#adjusting-the-rotated-x-axis-text-label",
    "title": "Hands-on Exercise 5d - Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "3.5 Adjusting the Rotated x-axis text label",
    "text": "3.5 Adjusting the Rotated x-axis text label\nRotating x-axis text labels to 30 degrees makes the label overlap with the plot and we can avoid this by adjusting the text location using hjust argument to theme’s text element with element_text(). We use axis.text.x as we want to change the look of x-axis text.\n\nggparcoord(data = happy, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30, hjust=1))",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5d - Visual Multivariate Analysis with Parallel Coordinates Plot"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html#the-basic-plot",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html#the-basic-plot",
    "title": "Hands-on Exercise 5d - Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "4.1 The basic plot",
    "text": "4.1 The basic plot\nFirst we select the columns that we want using the following code chunk.\n\nhappy &lt;- happy %&gt;%\n  select(\"Happiness score\", c(7:12))\n\nThen we plot the interactive parallel coordinates plot using parallelPlot() function.\n\nparallelPlot(happy,\n             width = 320,\n             height = 500)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5d - Visual Multivariate Analysis with Parallel Coordinates Plot"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html#rotate-axis-label",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html#rotate-axis-label",
    "title": "Hands-on Exercise 5d - Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "4.2 Rotate Axis Label",
    "text": "4.2 Rotate Axis Label\nWe will use the following code chunk to rotate the axis label to avoid them from overlapping.\n\nparallelPlot(happy,\n             rotateTitle = TRUE)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5d - Visual Multivariate Analysis with Parallel Coordinates Plot"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html#changing-the-colour-scheme",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html#changing-the-colour-scheme",
    "title": "Hands-on Exercise 5d - Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "4.3 Changing the colour scheme",
    "text": "4.3 Changing the colour scheme\nWe can change the default blue colour scheme by using continousCS argument as shown in the code chunk below.\n\nparallelPlot(happy,\n             continuousCS = \"Reds\",\n             rotateTitle = TRUE)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5d - Visual Multivariate Analysis with Parallel Coordinates Plot"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html#parallel-coordinates-plot-with-histogram",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html#parallel-coordinates-plot-with-histogram",
    "title": "Hands-on Exercise 5d - Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "4.4 Parallel coordinates plot with histogram",
    "text": "4.4 Parallel coordinates plot with histogram\nIn the code chunk below, histoVisibility argument is used to plot histogram along the axis of each variables.\n\nhistoVisibility &lt;- rep(TRUE, ncol(happy))\nparallelPlot(happy,\n             rotateTitle = TRUE,\n             histoVisibility = histoVisibility)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5d - Visual Multivariate Analysis with Parallel Coordinates Plot"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html",
    "title": "Hands-on Exercise 5e - Treemap Visualisation with R",
    "section": "",
    "text": "In this exercise, we will learn how to design treemap using appropriate R packages. The hands-on exercise consists of three main section. First, you will learn how to manipulate transaction data into a treemap strcuture by using selected functions provided in dplyr package. Then, you will learn how to plot static treemap by using treemap package. In the third section, you will learn how to design interactive treemap by using d3treeR package.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5e - Treemap Visualisation with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#installing-and-loading-the-packages",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#installing-and-loading-the-packages",
    "title": "Hands-on Exercise 5e - Treemap Visualisation with R",
    "section": "2.1 Installing and Loading the Packages",
    "text": "2.1 Installing and Loading the Packages\nFor this exercise, the treemap, treemapify and tidyverse packages will be used.\nThe code chunk below uses p_load() of pacman package to check if the abovementioned packages are installed in the computer. If they are, they will be launched in R. Otherwise, pacman will install the relevant packages before launching them.\n\n\nShow the code\npacman::p_load(tidyverse, DT, treemap, treemapify)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5e - Treemap Visualisation with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#importing-the-data",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#importing-the-data",
    "title": "Hands-on Exercise 5e - Treemap Visualisation with R",
    "section": "2.2 Importing the Data",
    "text": "2.2 Importing the Data\nFor this exercise, REALIS2018.csv data will be used. This dataset provides information of private property transaction records in 2018. The dataset is extracted from REALIS portal of Urban Redevelopment Authority (URA).\nIn the code chunk below, read_csv() of readr is used to import WHData-2018.csv into R and parsed it into tibble R data frame format.\n\nrealis2018 &lt;- read_csv(\"data/realis2018.csv\")\n\n\nThe DataChecking the data\n\n\n\ndatatable(realis2018)\n\n\n\n\n\n\n\n\nglimpse(realis2018)\n\nRows: 23,205\nColumns: 20\n$ `Project Name`                &lt;chr&gt; \"ADANA @ THOMSON\", \"ALANA\", \"ALANA\", \"AL…\n$ Address                       &lt;chr&gt; \"8 Old Upper Thomson Road  #05-03\", \"156…\n$ `No. of Units`                &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ `Area (sqm)`                  &lt;dbl&gt; 52, 284, 256, 256, 277, 285, 234, 155, 1…\n$ `Type of Area`                &lt;chr&gt; \"Strata\", \"Strata\", \"Strata\", \"Strata\", …\n$ `Transacted Price ($)`        &lt;dbl&gt; 888888, 2530000, 2390863, 2450000, 19800…\n$ `Nett Price($)`               &lt;chr&gt; \"-\", \"-\", \"2382517\", \"2441654\", \"-\", \"-\"…\n$ `Unit Price ($ psm)`          &lt;dbl&gt; 17094, 8908, 9307, 9538, 7148, 6947, 147…\n$ `Unit Price ($ psf)`          &lt;dbl&gt; 1588, 828, 865, 886, 664, 645, 1371, 149…\n$ `Sale Date`                   &lt;chr&gt; \"4-Jul-18\", \"5-Oct-18\", \"9-Jun-18\", \"14-…\n$ `Property Type`               &lt;chr&gt; \"Apartment\", \"Terrace House\", \"Terrace H…\n$ Tenure                        &lt;chr&gt; \"Freehold\", \"103 Yrs From 12/08/2013\", \"…\n$ `Completion Date`             &lt;chr&gt; \"2018\", \"2018\", \"2018\", \"2018\", \"2008\", …\n$ `Type of Sale`                &lt;chr&gt; \"New Sale\", \"Sub Sale\", \"New Sale\", \"New…\n$ `Purchaser Address Indicator` &lt;chr&gt; \"Private\", \"Private\", \"HDB\", \"N.A\", \"Pri…\n$ `Postal District`             &lt;dbl&gt; 20, 28, 28, 28, 26, 26, 26, 26, 26, 26, …\n$ `Postal Sector`               &lt;dbl&gt; 57, 80, 80, 80, 78, 78, 78, 78, 78, 78, …\n$ `Postal Code`                 &lt;dbl&gt; 573868, 804555, 804529, 804540, 786300, …\n$ `Planning Region`             &lt;chr&gt; \"North East Region\", \"North East Region\"…\n$ `Planning Area`               &lt;chr&gt; \"Ang Mo Kio\", \"Ang Mo Kio\", \"Ang Mo Kio\"…",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5e - Treemap Visualisation with R"
    ]
  },
  {
    "objectID": "index.html#extras",
    "href": "index.html#extras",
    "title": "SiHui Learns Visual Analytics!",
    "section": "Extras",
    "text": "Extras\nAdditional practices and experiments can be found here!\n\n\n\n\n\n\n\n\nPost-Lesson Thoughts 1: Annotations\n\n\n\n\n\n\nGoh Si Hui\n\n\nJan 13, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#data-wrangling",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#data-wrangling",
    "title": "Hands-on Exercise 5e - Treemap Visualisation with R",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\nAs seen from the above,realis2018 is in trasaction record form, which is highly disaggregated and not appropriate to be used to plot a treemap. In this section, we will perform the following steps to manipulate and prepare a data.frtame that is appropriate for treemap visualisation:\n\ngroup transaction reocrds by Project Name, Planning Region, Planning Area, Property Type and Type of Sale; and\ncompute Total Units Sold, Total Area, Median Unit Price and Median Transacted Price by applying appropriate summary statistics on number of units, area(sqm), unit price (\\(psm) and transacted price (\\)) respectively.\n\nTwo key verbs of dplyr package, namely: group_by() and summarize() will be used to perform these steps.\ngroup_by() breaks down a data.frame into specified groups of rows. When you then apply the verbs above on the resulting object they’ll be automatically applied “by group”.\nGrouping affects the verbs as follows:\n\ngrouped select() is the same as ungrouped select(), except that grouping variables are always retained.\ngrouped arrange() is the same as ungrouped; unless you set .by_group = TRUE, in which case it orders first by the grouping variables.\nmutate() and filter() are most useful in conjunction with window functions (like rank(), or min(x) == x).\nsample_n() and sample_frac() sample the specified number/fraction of rows in each group.\nsummarise() computes the summary for each group.\n\nIn our case, group_by() will used together with summarise() to derive the summarised data.frame.\n\nGrouped summaries without the PipeGrouped summaries with the Pipe\n\n\nThe code chunk below shows a typical two-lines code approach to perform the steps.\n\nrealis2018_grouped &lt;- group_by(realis2018, `Project Name`,\n                               `Planning Region`, `Planning Area`, \n                               `Property Type`, `Type of Sale`)\n\nrealis2018_summarised &lt;- summarise(realis2018_grouped, \n                          `Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE),\n                          `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n                          `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE), \n                          `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))\n\nThe code chunk above is not very efficient because we have to give each intermediate data.frame a name, even though we don’t have to care about it.\n\n\nThe code chunk below shows a more efficient way to tackle the same processes by using the pipe operator, %&gt;%:\n\nrealis2018_summarised &lt;- realis2018 %&gt;% \n  group_by(`Project Name`,`Planning Region`, \n           `Planning Area`, `Property Type`, \n           `Type of Sale`) %&gt;%\n  summarise(`Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE), \n            `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n            `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE),\n            `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5e - Treemap Visualisation with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#designing-a-static-treemap",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#designing-a-static-treemap",
    "title": "Hands-on Exercise 5e - Treemap Visualisation with R",
    "section": "3.1 Designing a static treemap",
    "text": "3.1 Designing a static treemap\nIn this section, treemap() of Treemap package is used to plot a treemap showing the distribution of median unit prices and total unit sold of resale condominium by geographic hierarchy in 2017.\nFirst, we will select records belongs to resale condominium property type from realis2018_selected data frame.\n\nrealis2018_selected &lt;- realis2018_summarised %&gt;%\n  filter(`Property Type` == \"Condominium\", `Type of Sale` == \"Resale\")",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5e - Treemap Visualisation with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#using-the-basic-arguments",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#using-the-basic-arguments",
    "title": "Hands-on Exercise 5e - Treemap Visualisation with R",
    "section": "3.2 Using the basic arguments",
    "text": "3.2 Using the basic arguments\nThe code chunk below designed a treemap by using three core arguments of treemap(), namely: index, vSize and vColor.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThings to learn from the three arguments used:\n\nindex\n\nThe index vector must consist of at least two column names or else no hierarchy treemap will be plotted.\nIf multiple column names are provided, such as the code chunk above, the first name is the highest aggregation level, the second name the second highest aggregation level, and so on.\n\nvSize\n\nThe column must not contain negative values. This is because it’s vaues will be used to map the sizes of the rectangles of the treemaps.\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe treemap above was wrongly coloured. For a correctly designed treemap, the colours of the rectagles should be in different intensity showing, in our case, median unit prices.\nFor treemap(), vColor is used in combination with the argument type to determines the colours of the rectangles. Without defining type, like the code chunk above, treemap() assumes type = index, in our case, the hierarchy of planning areas.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5e - Treemap Visualisation with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#working-with-vcolor-and-type-arguments",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#working-with-vcolor-and-type-arguments",
    "title": "Hands-on Exercise 5e - Treemap Visualisation with R",
    "section": "3.3 Working with vColor and type arguments",
    "text": "3.3 Working with vColor and type arguments\nIn the code chunk below, type argument is define as value.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type = \"value\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe rectangles are coloured with different intensity of green, reflecting their respective median unit prices.\nThe legend reveals that the values are binned into ten bins, i.e. 0-5000, 5000-10000, etc. with an equal interval of 5000.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5e - Treemap Visualisation with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#colours-with-treemap-package",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#colours-with-treemap-package",
    "title": "Hands-on Exercise 5e - Treemap Visualisation with R",
    "section": "3.4 Colours with treemap package",
    "text": "3.4 Colours with treemap package\nThere are two arguments that determine the mapping to color palettes: mapping and palette. The only difference between “value” and “manual” is the default value for mapping. The “value” treemap considers palette to be a diverging color palette (say ColorBrewer’s “RdYlBu”), and maps it in such a way that 0 corresponds to the middle color (typically white or yellow), -max(abs(values)) to the left-end color, and max(abs(values)), to the right-end color. The “manual” treemap simply maps min(values) to the left-end color, max(values) to the right-end color, and mean(range(values)) to the middle color.\n\n3.4.1 The “value” type treemap\nThe\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\nThing to learn from the code chunk above:\n\nalthough the colour palette used is RdYlBu but there are no red rectangles in the treemap above. This is because all the median unit prices are positive.\nThe reason why we see only 5000 to 45000 in the legend is because the range argument is by default c(min(values, max(values)) with some pretty rounding.\n\n\n\n3.4.2 The “manual” type treemap\nThe “manual” type does not interpret the values as the “value” type does. Instead, the value range is mapped linearly to the colour palette.\nThe code chunk below shows a manual type treemap.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\nThe colour scheme used is very confusing. This is because mapping = (min(values), mean(range(values)), max(values)). It is not wise to use diverging colour palette such as RdYlBu if the values are all positive or negative.\n\nTo overcome this problem, a single colour palette such as Blues/Greens should be used.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Greens\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5e - Treemap Visualisation with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#treemap-layout",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#treemap-layout",
    "title": "Hands-on Exercise 5e - Treemap Visualisation with R",
    "section": "3.5 Treemap Layout",
    "text": "3.5 Treemap Layout\ntreemap() supports two popular treemap layouts, namely: “squarified” and “pivotSize”. The default is “pivotSize”.\nThe squarified treemap algorithm (Bruls et al., 2000) produces good aspect ratios, but ignores the sorting order of the rectangles (sortID). The ordered treemap, pivot-by-size, algorithm (Bederson et al., 2002) takes the sorting order (sortID) into account while aspect ratios are still acceptable.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5e - Treemap Visualisation with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#working-with-algorithm-argument",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#working-with-algorithm-argument",
    "title": "Hands-on Exercise 5e - Treemap Visualisation with R",
    "section": "3.6 Working with algorithm argument",
    "text": "3.6 Working with algorithm argument\nThe code chunk below plots a squarified treemap by changing the algorithm argument.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"squarified\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5e - Treemap Visualisation with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#using-sortid",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#using-sortid",
    "title": "Hands-on Exercise 5e - Treemap Visualisation with R",
    "section": "3.7 Using sortID",
    "text": "3.7 Using sortID\nWhen “pivotSize” algorithm is used, sortID argument can be used to dertemine the order in which the rectangles are placed from top left to bottom right.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"pivotSize\",\n        sortID = \"Median Transacted Price\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5e - Treemap Visualisation with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#designing-a-basic-treemap",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#designing-a-basic-treemap",
    "title": "Hands-on Exercise 5e - Treemap Visualisation with R",
    "section": "4.1 Designing a basic treemap",
    "text": "4.1 Designing a basic treemap\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`),\n       layout = \"scol\",\n       start = \"bottomleft\") + \n  geom_treemap() +\n  scale_fill_gradient(low = \"light blue\", high = \"blue\")",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5e - Treemap Visualisation with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#defining-hierarchy",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#defining-hierarchy",
    "title": "Hands-on Exercise 5e - Treemap Visualisation with R",
    "section": "4.2 Defining hierarchy",
    "text": "4.2 Defining hierarchy\nGroup by Planning Region\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`),\n       start = \"topleft\") + \n  geom_treemap()\n\n\n\n\n\n\n\n\nGroup by Planning Area\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap()\n\n\n\n\n\n\n\n\nAdding boundary line\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap() +\n  geom_treemap_subgroup2_border(colour = \"gray40\",\n                                size = 2) +\n  geom_treemap_subgroup_border(colour = \"gray20\")",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5e - Treemap Visualisation with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#installing-d3treer-package",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#installing-d3treer-package",
    "title": "Hands-on Exercise 5e - Treemap Visualisation with R",
    "section": "5.1 Installing d3treeR package",
    "text": "5.1 Installing d3treeR package\nWe will first load the devtools library and install the package found in github using the codes below.\n\nlibrary(devtools)\ninstall_github(\"timelyportfolio/d3treeR\")\n\nThen we will launch d3treeR package.\n\nlibrary(d3treeR)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5e - Treemap Visualisation with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#designing-an-interactive-treemap",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#designing-an-interactive-treemap",
    "title": "Hands-on Exercise 5e - Treemap Visualisation with R",
    "section": "5.2 Designing an Interactive Treemap",
    "text": "5.2 Designing an Interactive Treemap\nThe following codes perform two processes:\n\ntreemap() is used to build a treemap by using selected variables in condominium data.frame. The treemap created is save as object called tm.\n\n\ntm &lt;- treemap(realis2018_summarised,\n        index=c(\"Planning Region\", \"Planning Area\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        title=\"Private Residential Property Sold, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\nThen d3tree() is used to build an interactive treemap.\n\n\nd3tree(tm,rootname = \"Singapore\")",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5e - Treemap Visualisation with R"
    ]
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html",
    "title": "Take-home Exercise 3: Be Weatherwise or Otherwise",
    "section": "",
    "text": "In this take-home exercise, I will be using the visual analytics techniques learnt in ISSS 608 to visualise uncertainty methods and create interactive visual analytics to validate the projection that daily mean temperature is projected to increase by 1.4 to 4.6 degrees celsius by the end of the century."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#loading-r-packages",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#loading-r-packages",
    "title": "Take-home Exercise 3: Be Weatherwise or Otherwise",
    "section": "2.1 Loading R packages",
    "text": "2.1 Loading R packages\nFor this exercise, we will be using the following packages:\n\ntidyverse : to load the core tidyverse packages, which includes ggplot2 and dplyr.\nggdist: provides stats and geoms for visualising distributions and uncertainty.\nplotly:\ngganimate\nDT\ncrosstalk\nggstatsplot:\nperformance:\nparameters:\n\nThe code chunk below uses p_load() of pacman package to check if the abovementioned packages are installed in the computer. If they are, they will be launched in R. Otherwise, pacman will install the relevant packages before launching them.\n\n\nShow the code\npacman::p_load(tidyverse, ggdist, plotly, gganimate, DT, crosstalk, ggstatsplot, performance, parameters, patchwork, nortest)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#importing-the-data",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#importing-the-data",
    "title": "Take-home Exercise 3: Be Weatherwise or Otherwise",
    "section": "2.2 Importing the Data",
    "text": "2.2 Importing the Data\nFor this exercise, we will be using the historical daily temperature data from Meteorological Service Singapore. As there are more than 60 weather stations in Singapore, for the purpose of this exercise, we will be using the temperature data from Changi weather station in August 1983, 1993, 2003, 2013 and 2023.\n\nAug 1983Aug 1993Aug 2003Aug 2013Aug 2023\n\n\n\naug1983 &lt;- read_csv(\"data/DAILYDATA_S24_198308.csv\")\nhead(aug1983,10)\n\n# A tibble: 10 × 13\n   Station  Year Month   Day `Daily Rainfall Total (mm)` Highest 30 Min Rainfa…¹\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;                       &lt;dbl&gt; &lt;lgl&gt;                  \n 1 Changi   1983     8     1                         0   NA                     \n 2 Changi   1983     8     2                         6.4 NA                     \n 3 Changi   1983     8     3                         2.8 NA                     \n 4 Changi   1983     8     4                         3.7 NA                     \n 5 Changi   1983     8     5                        18.7 NA                     \n 6 Changi   1983     8     6                         0   NA                     \n 7 Changi   1983     8     7                         0   NA                     \n 8 Changi   1983     8     8                         0   NA                     \n 9 Changi   1983     8     9                         0   NA                     \n10 Changi   1983     8    10                         0   NA                     \n# ℹ abbreviated name: ¹​`Highest 30 Min Rainfall (mm)`\n# ℹ 7 more variables: `Highest 60 Min Rainfall (mm)` &lt;lgl&gt;,\n#   `Highest 120 Min Rainfall (mm)` &lt;lgl&gt;,\n#   `Mean Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Maximum Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Minimum Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Mean Wind Speed (km/h)` &lt;dbl&gt;, `Max Wind Speed (km/h)` &lt;dbl&gt;\n\n\n\n\n\naug1993 &lt;- read_csv(\"data/DAILYDATA_S24_199308.csv\")\nhead(aug1993,10)\n\n# A tibble: 10 × 13\n   Station  Year Month   Day `Daily Rainfall Total (mm)` Highest 30 Min Rainfa…¹\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;                       &lt;dbl&gt; &lt;lgl&gt;                  \n 1 Changi   1993     8     1                         0.4 NA                     \n 2 Changi   1993     8     2                         0   NA                     \n 3 Changi   1993     8     3                         0   NA                     \n 4 Changi   1993     8     4                         0   NA                     \n 5 Changi   1993     8     5                         0   NA                     \n 6 Changi   1993     8     6                         0   NA                     \n 7 Changi   1993     8     7                         0   NA                     \n 8 Changi   1993     8     8                         0   NA                     \n 9 Changi   1993     8     9                         0.4 NA                     \n10 Changi   1993     8    10                         0   NA                     \n# ℹ abbreviated name: ¹​`Highest 30 Min Rainfall (mm)`\n# ℹ 7 more variables: `Highest 60 Min Rainfall (mm)` &lt;lgl&gt;,\n#   `Highest 120 Min Rainfall (mm)` &lt;lgl&gt;,\n#   `Mean Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Maximum Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Minimum Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Mean Wind Speed (km/h)` &lt;dbl&gt;, `Max Wind Speed (km/h)` &lt;dbl&gt;\n\n\n\n\n\naug2003 &lt;- read_csv(\"data/DAILYDATA_S24_200308.csv\")\nhead(aug2003,10)\n\n# A tibble: 10 × 13\n   Station  Year Month   Day `Daily Rainfall Total (mm)` Highest 30 Min Rainfa…¹\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;                       &lt;dbl&gt; &lt;lgl&gt;                  \n 1 Changi   2003     8     1                         0   NA                     \n 2 Changi   2003     8     2                        21.1 NA                     \n 3 Changi   2003     8     3                         0   NA                     \n 4 Changi   2003     8     4                         0   NA                     \n 5 Changi   2003     8     5                        64.4 NA                     \n 6 Changi   2003     8     6                         9.2 NA                     \n 7 Changi   2003     8     7                         0   NA                     \n 8 Changi   2003     8     8                         0   NA                     \n 9 Changi   2003     8     9                         1.3 NA                     \n10 Changi   2003     8    10                         0   NA                     \n# ℹ abbreviated name: ¹​`Highest 30 Min Rainfall (mm)`\n# ℹ 7 more variables: `Highest 60 Min Rainfall (mm)` &lt;lgl&gt;,\n#   `Highest 120 Min Rainfall (mm)` &lt;lgl&gt;,\n#   `Mean Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Maximum Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Minimum Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Mean Wind Speed (km/h)` &lt;dbl&gt;, `Max Wind Speed (km/h)` &lt;dbl&gt;\n\n\n\n\n\naug2013 &lt;- read_csv(\"data/DAILYDATA_S24_201308.csv\")\nhead(aug2013,10)\n\n# A tibble: 10 × 13\n   Station  Year Month   Day `Daily Rainfall Total (mm)` Highest 30 Min Rainfa…¹\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;                       &lt;dbl&gt; &lt;lgl&gt;                  \n 1 Changi   2013     8     1                         8.2 NA                     \n 2 Changi   2013     8     2                         0   NA                     \n 3 Changi   2013     8     3                         0   NA                     \n 4 Changi   2013     8     4                         0   NA                     \n 5 Changi   2013     8     5                        43.2 NA                     \n 6 Changi   2013     8     6                         1   NA                     \n 7 Changi   2013     8     7                        19.2 NA                     \n 8 Changi   2013     8     8                         0.6 NA                     \n 9 Changi   2013     8     9                         1.8 NA                     \n10 Changi   2013     8    10                        40   NA                     \n# ℹ abbreviated name: ¹​`Highest 30 Min Rainfall (mm)`\n# ℹ 7 more variables: `Highest 60 Min Rainfall (mm)` &lt;lgl&gt;,\n#   `Highest 120 Min Rainfall (mm)` &lt;lgl&gt;,\n#   `Mean Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Maximum Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Minimum Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Mean Wind Speed (km/h)` &lt;dbl&gt;, `Max Wind Speed (km/h)` &lt;dbl&gt;\n\n\n\n\n\naug2023 &lt;- read_csv(\"data/DAILYDATA_S24_202308.csv\")\nhead(aug2023,10)\n\n# A tibble: 10 × 13\n   Station  Year Month   Day `Daily Rainfall Total (mm)` Highest 30 min Rainfa…¹\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;                       &lt;dbl&gt;                   &lt;dbl&gt;\n 1 Changi   2023     8     1                         0                       0  \n 2 Changi   2023     8     2                         0                       0  \n 3 Changi   2023     8     3                         9.2                     2.2\n 4 Changi   2023     8     4                         0                       0  \n 5 Changi   2023     8     5                         0                       0  \n 6 Changi   2023     8     6                         0                       0  \n 7 Changi   2023     8     7                         0                       0  \n 8 Changi   2023     8     8                        24.4                    16.4\n 9 Changi   2023     8     9                         0                       0  \n10 Changi   2023     8    10                         0                       0  \n# ℹ abbreviated name: ¹​`Highest 30 min Rainfall (mm)`\n# ℹ 7 more variables: `Highest 60 min Rainfall (mm)` &lt;dbl&gt;,\n#   `Highest 120 min Rainfall (mm)` &lt;dbl&gt;,\n#   `Mean Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Maximum Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Minimum Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Mean Wind Speed (km/h)` &lt;dbl&gt;, `Max Wind Speed (km/h)` &lt;dbl&gt;"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#data-preparation",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#data-preparation",
    "title": "Take-home Exercise 3: Be Weatherwise or Otherwise",
    "section": "2.3 Data Preparation",
    "text": "2.3 Data Preparation\nAs the downloaded data are in different tables, we will join them into one table first.\n\nchangi &lt;- full_join(aug1983, aug1993)\nchangi &lt;- full_join(changi, aug2003)\nchangi &lt;- full_join(changi, aug2003)\nchangi &lt;- full_join(changi, aug2013)\nchangi &lt;- full_join(changi, aug2023)\n\n\ndatatable(changi)\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nMean Daily Temperature: Mean of the temperatures observed at 24 equidistant times in the course of a continuous 24-hour period (normally the mean solar day from midnight to midnight according to the zonal time of the station).\nDaily Maximum Temperature: Maximum temperature in the course of a continuous time interval of 24 hours (usually midnight to midnight local time).\nDaily Minimum Temperature: Minimum temperature in the course of a continuous time interval of 24 hours (usually midnight to midnight local time).\nSource\n\n\nWe will first drop the columns that we do not need (i.e. those related to rainfall and wind) and retain only those related to temperature.\n\nchangitemp &lt;- changi %&gt;%\n  select(Year, Month, Day, `Mean Temperature (degrees celsius)`, `Minimum Temperature (degrees celsius)`, `Maximum Temperature (degrees celsius)`)\n\nWe also use the following code chunk to check for missing data.\n\nany(is.na(changitemp))\n\n[1] FALSE\n\n\nWe will check data type using glimpse()\n\nglimpse(changitemp)\n\nRows: 155\nColumns: 6\n$ Year                                    &lt;dbl&gt; 1983, 1983, 1983, 1983, 1983, …\n$ Month                                   &lt;dbl&gt; 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, …\n$ Day                                     &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,…\n$ `Mean Temperature (degrees celsius)`    &lt;dbl&gt; 28.9, 28.4, 28.5, 26.3, 27.3, …\n$ `Minimum Temperature (degrees celsius)` &lt;dbl&gt; 26.7, 25.1, 25.3, 24.0, 24.6, …\n$ `Maximum Temperature (degrees celsius)` &lt;dbl&gt; 32.1, 32.1, 31.7, 28.9, 31.7, …\n\n\n\nchangitemp &lt;- rename(changitemp, \n       MeanTemp = `Mean Temperature (degrees celsius)`,\n       MinTemp = `Minimum Temperature (degrees celsius)`,\n       MaxTemp = `Maximum Temperature (degrees celsius)`\n       )\n\n\nchangitemp$Day &lt;- as.factor(changitemp$Day)\nchangitemp$Month &lt;- as.factor(changitemp$Month)\nchangitemp$Year &lt;- as.factor(changitemp$Year)\n\n\nchangitemp &lt;- changitemp %&gt;%\n  mutate(diff = round(MaxTemp - MinTemp,1),\n         avgMaxMin = round((MaxTemp + MinTemp)/2, 1)) \n\n\nchangitemp &lt;- changitemp %&gt;% \n  unite(\"YYYYMMDD\", Year:Day, remove = FALSE)\n\nPerform groupby Year to get the mean temperature for the month, min max temperature for the month,\n\nchangitemp_byyear &lt;- changitemp %&gt;%\n  group_by(Year) %&gt;%\n  summarise(\n    avgdailytemp = round(mean(MeanTemp),1),\n    avgmintemp = round(mean(MinTemp),1), \n    avgmaxtemp = round(mean(MaxTemp),1),\n    minMeanTemp = min(MeanTemp),\n    maxMeanTemp = max(MeanTemp), \n    minMinTemp = min(MinTemp),\n    maxMinTemp = max(MinTemp),\n    minMaxTemp = min(MaxTemp),\n    maxMaxTemp = max(MaxTemp)) \n\n\ntemp &lt;- full_join(changitemp, changitemp_byyear)\ndatatable(temp)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#visualising-uncertainty",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#visualising-uncertainty",
    "title": "Take-home Exercise 3: Be Weatherwise or Otherwise",
    "section": "3.1 Visualising Uncertainty",
    "text": "3.1 Visualising Uncertainty\n\ntemp %&gt;%\n  ggplot(aes(x = Year, \n             y = MeanTemp)) +\n  stat_pointinterval(   \n    show.legend = FALSE    \n  ) +                        \n  labs(\n    title = \"Visualising confidence intervals of mean daily temperature (in degrees celsius)\",\n    subtitle = \"Mean point + multiple-interval plot\")\n\n\n\n\n\n\n\n\n\ntemp %&gt;%\n  ggplot(aes(x = Year, \n             y = MeanTemp)) +\n  stat_gradientinterval(   \n    fill = \"orange\",      \n    show.legend = TRUE     \n  ) +                        \n  labs(\n    title = \"Visualising confidence intervals of mean daily temperature (in degrees celsius)\",\n    subtitle = \"Gradient + interval plot\")\n\n\n\n\n\n\n\n\n\nlibrary(ungeviz)\n\nggplot(data = temp, \n       (aes(x = factor(Year), \n            y = MeanTemp))) +\n  geom_point(position = position_jitter(\n    height = 0.3, \n    width = 0.05), \n    size = 0.4, \n    color = \"#0072B2\", \n    alpha = 1/2) +\n  geom_hpline(data = sampler(25, \n                             group = Year), \n              height = 0.6, \n              color = \"#D55E00\") +\n  theme_bw() + \n  transition_states(.draw, 1, 3)\n\n\n\n\n\n\n\n\n\nlibrary(ungeviz)\n\nggplot(data = temp, \n       (aes(x = factor(Year), \n            y = MinTemp))) +\n  geom_point(position = position_jitter(\n    height = 0.3, \n    width = 0.05), \n    size = 0.4, \n    color = \"#0072B2\", \n    alpha = 1/2) +\n  geom_hpline(data = sampler(25, \n                             group = Year), \n              height = 0.6, \n              color = \"#D55E00\") +\n  theme_bw() + \n  transition_states(.draw, 1, 3)\n\n\n\n\n\n\n\n\n\nlibrary(ungeviz)\n\nggplot(data = temp, \n       (aes(x = factor(Year), \n            y = MaxTemp))) +\n  geom_point(position = position_jitter(\n    height = 0.3, \n    width = 0.05), \n    size = 0.4, \n    color = \"#0072B2\", \n    alpha = 1/2) +\n  geom_hpline(data = sampler(25, \n                             group = Year), \n              height = 0.6, \n              color = \"#D55E00\") +\n  theme_bw() + \n  transition_states(.draw, 1, 3)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03b.html",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03b.html",
    "title": "Take-home Exercise 3: Be Weatherwise or Otherwise",
    "section": "",
    "text": "According to Ministry of Sustainability and the Environment, the daily mean temperature is projected to increase by 1.4 to 4.6 Degree Celsius by the end of the century.\nIn this take-home exercise, I will be using the visual analytics techniques learnt in ISSS 608 to visualise uncertainty methods and create interactive visual analytics to validate the projection if the daily mean temperature is increasing from 1983 to 2023.",
    "crumbs": [
      "Take-home Exercises",
      "Take-home Exercise 3: Be Weatherwise or Otherwise"
    ]
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03b.html#loading-r-packages",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03b.html#loading-r-packages",
    "title": "Take-home Exercise 3: Be Weatherwise or Otherwise",
    "section": "2.1 Loading R packages",
    "text": "2.1 Loading R packages\nFor this exercise, we will be using the following packages:\n\ntidyverse : to load the core tidyverse packages, which includes ggplot2 and dplyr.\nggdis: provides stats and geoms for visualising distributions and uncertainty.\nggiraph: to make interactive ggplot2 plots\nplotly: to plot interactive statistical graphs\nDT: to create interactive tables using the JavaScript library DataTables\ncrosstalk: to implement cross-widget interactions\nggstatsplot:an extension of ggplot2 package for creating graphics with details from statistical tests included in the information-rich plots themselves.\nnortest: to test the normality assumption.\n\nThe code chunk below uses p_load() of pacman package to check if the abovementioned packages are installed in the computer. If they are, they will be launched in R. Otherwise, pacman will install the relevant packages before launching them.\n\n\nShow the code\npacman::p_load(tidyverse, ggdist, plotly, gganimate, DT, crosstalk, ggstatsplot, nortest, ggiraph, hrbrthemes)",
    "crumbs": [
      "Take-home Exercises",
      "Take-home Exercise 3: Be Weatherwise or Otherwise"
    ]
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03b.html#importing-the-data",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03b.html#importing-the-data",
    "title": "Take-home Exercise 3: Be Weatherwise or Otherwise",
    "section": "2.2 Importing the Data",
    "text": "2.2 Importing the Data\nFor this exercise, we will be using the historical daily temperature data from Meteorological Service Singapore. As there are more than 60 weather stations in Singapore, for the purpose of this exercise, we will be using the temperature data from Changi weather station in August 1983, 1993, 2003, 2013 and 2023.\n\nAug 1983Aug 1993Aug 2003Aug 2013Aug 2023\n\n\n\naug1983 &lt;- read_csv(\"data/DAILYDATA_S24_198308.csv\")\nhead(aug1983,10)\n\n# A tibble: 10 × 13\n   Station  Year Month   Day `Daily Rainfall Total (mm)` Highest 30 Min Rainfa…¹\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;                       &lt;dbl&gt; &lt;lgl&gt;                  \n 1 Changi   1983     8     1                         0   NA                     \n 2 Changi   1983     8     2                         6.4 NA                     \n 3 Changi   1983     8     3                         2.8 NA                     \n 4 Changi   1983     8     4                         3.7 NA                     \n 5 Changi   1983     8     5                        18.7 NA                     \n 6 Changi   1983     8     6                         0   NA                     \n 7 Changi   1983     8     7                         0   NA                     \n 8 Changi   1983     8     8                         0   NA                     \n 9 Changi   1983     8     9                         0   NA                     \n10 Changi   1983     8    10                         0   NA                     \n# ℹ abbreviated name: ¹​`Highest 30 Min Rainfall (mm)`\n# ℹ 7 more variables: `Highest 60 Min Rainfall (mm)` &lt;lgl&gt;,\n#   `Highest 120 Min Rainfall (mm)` &lt;lgl&gt;,\n#   `Mean Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Maximum Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Minimum Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Mean Wind Speed (km/h)` &lt;dbl&gt;, `Max Wind Speed (km/h)` &lt;dbl&gt;\n\n\n\n\n\naug1993 &lt;- read_csv(\"data/DAILYDATA_S24_199308.csv\")\nhead(aug1993,10)\n\n# A tibble: 10 × 13\n   Station  Year Month   Day `Daily Rainfall Total (mm)` Highest 30 Min Rainfa…¹\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;                       &lt;dbl&gt; &lt;lgl&gt;                  \n 1 Changi   1993     8     1                         0.4 NA                     \n 2 Changi   1993     8     2                         0   NA                     \n 3 Changi   1993     8     3                         0   NA                     \n 4 Changi   1993     8     4                         0   NA                     \n 5 Changi   1993     8     5                         0   NA                     \n 6 Changi   1993     8     6                         0   NA                     \n 7 Changi   1993     8     7                         0   NA                     \n 8 Changi   1993     8     8                         0   NA                     \n 9 Changi   1993     8     9                         0.4 NA                     \n10 Changi   1993     8    10                         0   NA                     \n# ℹ abbreviated name: ¹​`Highest 30 Min Rainfall (mm)`\n# ℹ 7 more variables: `Highest 60 Min Rainfall (mm)` &lt;lgl&gt;,\n#   `Highest 120 Min Rainfall (mm)` &lt;lgl&gt;,\n#   `Mean Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Maximum Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Minimum Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Mean Wind Speed (km/h)` &lt;dbl&gt;, `Max Wind Speed (km/h)` &lt;dbl&gt;\n\n\n\n\n\naug2003 &lt;- read_csv(\"data/DAILYDATA_S24_200308.csv\")\nhead(aug2003,10)\n\n# A tibble: 10 × 13\n   Station  Year Month   Day `Daily Rainfall Total (mm)` Highest 30 Min Rainfa…¹\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;                       &lt;dbl&gt; &lt;lgl&gt;                  \n 1 Changi   2003     8     1                         0   NA                     \n 2 Changi   2003     8     2                        21.1 NA                     \n 3 Changi   2003     8     3                         0   NA                     \n 4 Changi   2003     8     4                         0   NA                     \n 5 Changi   2003     8     5                        64.4 NA                     \n 6 Changi   2003     8     6                         9.2 NA                     \n 7 Changi   2003     8     7                         0   NA                     \n 8 Changi   2003     8     8                         0   NA                     \n 9 Changi   2003     8     9                         1.3 NA                     \n10 Changi   2003     8    10                         0   NA                     \n# ℹ abbreviated name: ¹​`Highest 30 Min Rainfall (mm)`\n# ℹ 7 more variables: `Highest 60 Min Rainfall (mm)` &lt;lgl&gt;,\n#   `Highest 120 Min Rainfall (mm)` &lt;lgl&gt;,\n#   `Mean Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Maximum Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Minimum Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Mean Wind Speed (km/h)` &lt;dbl&gt;, `Max Wind Speed (km/h)` &lt;dbl&gt;\n\n\n\n\n\naug2013 &lt;- read_csv(\"data/DAILYDATA_S24_201308.csv\")\nhead(aug2013,10)\n\n# A tibble: 10 × 13\n   Station  Year Month   Day `Daily Rainfall Total (mm)` Highest 30 Min Rainfa…¹\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;                       &lt;dbl&gt; &lt;lgl&gt;                  \n 1 Changi   2013     8     1                         8.2 NA                     \n 2 Changi   2013     8     2                         0   NA                     \n 3 Changi   2013     8     3                         0   NA                     \n 4 Changi   2013     8     4                         0   NA                     \n 5 Changi   2013     8     5                        43.2 NA                     \n 6 Changi   2013     8     6                         1   NA                     \n 7 Changi   2013     8     7                        19.2 NA                     \n 8 Changi   2013     8     8                         0.6 NA                     \n 9 Changi   2013     8     9                         1.8 NA                     \n10 Changi   2013     8    10                        40   NA                     \n# ℹ abbreviated name: ¹​`Highest 30 Min Rainfall (mm)`\n# ℹ 7 more variables: `Highest 60 Min Rainfall (mm)` &lt;lgl&gt;,\n#   `Highest 120 Min Rainfall (mm)` &lt;lgl&gt;,\n#   `Mean Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Maximum Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Minimum Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Mean Wind Speed (km/h)` &lt;dbl&gt;, `Max Wind Speed (km/h)` &lt;dbl&gt;\n\n\n\n\n\naug2023 &lt;- read_csv(\"data/DAILYDATA_S24_202308.csv\")\nhead(aug2023,10)\n\n# A tibble: 10 × 13\n   Station  Year Month   Day `Daily Rainfall Total (mm)` Highest 30 min Rainfa…¹\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;                       &lt;dbl&gt;                   &lt;dbl&gt;\n 1 Changi   2023     8     1                         0                       0  \n 2 Changi   2023     8     2                         0                       0  \n 3 Changi   2023     8     3                         9.2                     2.2\n 4 Changi   2023     8     4                         0                       0  \n 5 Changi   2023     8     5                         0                       0  \n 6 Changi   2023     8     6                         0                       0  \n 7 Changi   2023     8     7                         0                       0  \n 8 Changi   2023     8     8                        24.4                    16.4\n 9 Changi   2023     8     9                         0                       0  \n10 Changi   2023     8    10                         0                       0  \n# ℹ abbreviated name: ¹​`Highest 30 min Rainfall (mm)`\n# ℹ 7 more variables: `Highest 60 min Rainfall (mm)` &lt;dbl&gt;,\n#   `Highest 120 min Rainfall (mm)` &lt;dbl&gt;,\n#   `Mean Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Maximum Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Minimum Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Mean Wind Speed (km/h)` &lt;dbl&gt;, `Max Wind Speed (km/h)` &lt;dbl&gt;",
    "crumbs": [
      "Take-home Exercises",
      "Take-home Exercise 3: Be Weatherwise or Otherwise"
    ]
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03b.html#data-preparation",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03b.html#data-preparation",
    "title": "Take-home Exercise 3: Be Weatherwise or Otherwise",
    "section": "2.3 Data Preparation",
    "text": "2.3 Data Preparation\nAs the downloaded data are in different tables, we will join them into one table Changi first.\n\n\nShow the code\nchangi &lt;- full_join(aug1983, aug1993)\nchangi &lt;- full_join(changi, aug2003)\nchangi &lt;- full_join(changi, aug2003)\nchangi &lt;- full_join(changi, aug2013)\nchangi &lt;- full_join(changi, aug2023)\n\n\ndatatable(changi, caption = \"Table 1 - Observatons from Changi Weather Station\", class='compact')\n\n\n\n\n\n\nFrom the output above, we see that there are columns such as rainfall and wind that we do not need for this exercise. Since we want to verify if daily mean temperature is indeed rising over the years, we will retain the mean temperature column, which we assumed is the daily mean temperature.\n\n\n\n\n\n\nNote\n\n\n\nAt the time of this take-home exercise, we cannot find information on how Weather.gov.sg calculated “mean temperature” or “daily mean temperature”.\nAccording to this Source, the mean daily temperature is the mean of the temperatures observed at 24 equidistant times in the course of a continuous 24-hour period (normally the mean solar day from midnight to midnight according to the zonal time of the station).The data from Weather.gov.sg also provided us with the daily maximum temperature and daily minimum temperature, which is the maximum and minimum temperature in the course of a continuous time interval of 24 hours. However, if we do not have hourly temperature of the day, we would not be able to verify the mean temperature given and we are still unable to determine the distribution of the temperature throughout the day.Hence, we will drop the minimum and maximum temperature columns.\n\n\nWe will first drop the columns that we do not need (i.e. those related to rainfall and wind) and retain only the Day, Year, Mean Temperature using select. We also check the output using glimpse().\n\n\nShow the code\nchangitemp &lt;- changi %&gt;%\n  select(Day, Year, `Mean Temperature (degrees celsius)`)\n\nglimpse(changitemp)\n\n\nRows: 155\nColumns: 3\n$ Day                                  &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11…\n$ Year                                 &lt;dbl&gt; 1983, 1983, 1983, 1983, 1983, 198…\n$ `Mean Temperature (degrees celsius)` &lt;dbl&gt; 28.9, 28.4, 28.5, 26.3, 27.3, 27.…\n\n\nWe also use the following code chunk to check for missing data.\n\n\nShow the code\nany(is.na(changitemp))\n\n\n[1] FALSE\n\n\nFrom the above output, we verified that there is no missing data.\n\nRename Mean TemperatureChange Variables from character to factor\n\n\n\nchangitemp &lt;- rename(changitemp, \n       DailyTemp = `Mean Temperature (degrees celsius)`)\n\nglimpse(changitemp)\n\nRows: 155\nColumns: 3\n$ Day       &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n$ Year      &lt;dbl&gt; 1983, 1983, 1983, 1983, 1983, 1983, 1983, 1983, 1983, 1983, …\n$ DailyTemp &lt;dbl&gt; 28.9, 28.4, 28.5, 26.3, 27.3, 27.5, 27.9, 28.6, 28.8, 29.0, …\n\n\n\n\n\nchangitemp$Day &lt;- as.factor(changitemp$Day)\nchangitemp$Year &lt;- as.factor(changitemp$Year)\n\nglimpse(changitemp)\n\nRows: 155\nColumns: 3\n$ Day       &lt;fct&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n$ Year      &lt;fct&gt; 1983, 1983, 1983, 1983, 1983, 1983, 1983, 1983, 1983, 1983, …\n$ DailyTemp &lt;dbl&gt; 28.9, 28.4, 28.5, 26.3, 27.3, 27.5, 27.9, 28.6, 28.8, 29.0, …\n\n\n\n\n\nCurrently, we see that the temperatures for all years are in one column. To facilitate the visualisation and filtering of the charts in subsequent steps, we transform the table to make each Year a column using pivot_wider(). By doing so, the temperature for each year is in one column.\n\n\nShow the code\n#Make every year's temperature a column\nchangitemp_transformed &lt;- changitemp %&gt;%\n   pivot_wider(names_from = Year, values_from = DailyTemp)\n\n#Rename the Columns\nchangitemp_transformed &lt;- changitemp_transformed %&gt;%\n  rename(\"Year1983\" = `1983`,\n         \"Year1993\" = `1993`,\n         \"Year2003\" = `2003`,\n         \"Year2013\" = `2013`,\n         \"Year2023\" = `2023`)\n\n#Checking the end product \ndatatable(changitemp_transformed, caption = \"Table 2 - Daily Temperature by Years\", class='compact', rownames = FALSE)",
    "crumbs": [
      "Take-home Exercises",
      "Take-home Exercise 3: Be Weatherwise or Otherwise"
    ]
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03b.html#dotplot-to-have-a-sense-of-the-distribution-of-daily-temperature",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03b.html#dotplot-to-have-a-sense-of-the-distribution-of-daily-temperature",
    "title": "Take-home Exercise 3: Be Weatherwise or Otherwise",
    "section": "3.1 Dotplot to have a sense of the distribution of daily temperature",
    "text": "3.1 Dotplot to have a sense of the distribution of daily temperature\n\np &lt;- ggplot(data=changitemp, \n       aes(x = MeanDailyTemp)) +\n  geom_dotplot_interactive(           \n    aes(data_id = Year),             \n    stackgroups = TRUE,               \n    binwidth = 0.1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618                      \n)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03b.html#boxplot-for-overview-of-highest-and-lowest-mean-daily-temp-in-the-month-of-august",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03b.html#boxplot-for-overview-of-highest-and-lowest-mean-daily-temp-in-the-month-of-august",
    "title": "Take-home Exercise 3: Be Weatherwise or Otherwise",
    "section": "3.2 Boxplot for overview of highest and lowest mean daily temp in the month of August",
    "text": "3.2 Boxplot for overview of highest and lowest mean daily temp in the month of August\n\nd1 &lt;- highlight_key(changitemp) \n\nc1 &lt;- ggplot(d1,\n        aes(Year,\n        MeanDailyTemp))+\n  geom_violin() +\n  geom_boxplot() + \n  geom_jitter(height = 0, width = 0.1) + \n  #Add title, subtitle, x-axis labels \n  labs(title = \"Distribution of Daily Mean Temperature in the Month of August\",\n                     subtitle = \"Years: 1983, 1993, 2003, 2013, 2023 \\n Weather Station: Changi\") +\n  xlab(\"Year\") + \n  ylab(\"Daily Mean Temperature (degrees celsius)\") + \n  #Add a theme \n  theme_ipsum_rc(plot_title_size = 13, plot_title_margin=4, subtitle_size=11, subtitle_margin=4, \n                 axis_title_size = 10, axis_text_size=8, axis_title_face= \"bold\", plot_margin = margin(4, 4, 4, 4)) \n\ngg1 &lt;- highlight(ggplotly(c1),        \n                \"plotly_selected\")  \n\ncrosstalk::bscols(gg1,               \n                  DT::datatable(d1), \n                  widths = 10)  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nd1 &lt;- highlight_key(changitemp) \n\nc1 &lt;- plot_ly(data = d1,\n              x = ~Year,\n              y = ~MeanDailyTemp,\n              line = list(width =1),\n              type = \"violin\",\n              marker = list(opacity = 0.5,\n                            line = list(width = 2)),\n              box = list(visible = T),\n              meanline = list(visible = T,\n                              color = \"skyblue\"))\n\ngg1 &lt;- highlight(ggplotly(c1),        \n                \"plotly_selected\")  \n\ncrosstalk::bscols(gg1,               \n                  DT::datatable(d1), \n                  widths = 6)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03b.html#visualising-uncertainty",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03b.html#visualising-uncertainty",
    "title": "Take-home Exercise 3: Be Weatherwise or Otherwise",
    "section": "4.3 Visualising Uncertainty",
    "text": "4.3 Visualising Uncertainty\nAs we are using point estimates (i.e. median), there are uncertainties surrounding the point estimates since each estimate is derived from a bunch of figures. In our case, each median daily temperature for the year is derived from the 31 daily temperatures in August for the year. As such, it would be more accurate and informative to show the target quantile confidence levels (e.g. 95% or 99%) that the true (unknown) estimate would lie within the interval, given the evidence provided by the observed data.\nFor the following plot, we will use median point estimates instead of mean due to outliers and skewness of data. In addition, using median also allows to user to relate to the above one-way ANOVA analysis\nWith median as the point estimate, quantile intervals are used instead of confidence interval. We use 95% and 99% intervals because they are commonly associated with 5% and 1% error rate, which are commonly used in hypothesis testing.\n\n\nShow the code\n#Base ggplot\np2 &lt;- ggplot(\n  data = changitemp,\n  aes(x = Year,\n      y = DailyTemp)) +\n  \n  #Using stat_pointinterval to plot the points and intervals\n  stat_pointinterval(\n    aes(interval_color = stat(level)),\n    .width = c(0.95, 0.99),\n    .point = median,\n    .interval = qi,\n    point_color = \"black\",\n    show.legend = TRUE) + \n   stat_summary(fun = median, geom = \"text\", aes(label = round(after_stat(y), 1)), position = position_nudge(x = 0.15), vjust = -0.5,size=3)+\n  \n  #Add title, subtitle, x-axis labels \n  labs(title = \"Uncertainty in Median Daily Temperature in the Month of August\",\nsubtitle = \"Years: 1983, 1993, 2003, 2013, 2023 \\nWeather Station: Changi \\nQuantile Intervals: 95% and 99% of Daily Temperature\") +\n  xlab(\"Year\") + \n  ylab(\"Daily Temperature (Degree Celsius)\")+\n  \n  #Add a theme \n  theme_ipsum_rc(plot_title_size = 13, plot_title_margin=4, subtitle_size=11, subtitle_margin=4, \n                 axis_title_size = 8, axis_text_size=8, axis_title_face= \"bold\", plot_margin = margin(4, 4, 4, 4)) \n\np2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe length of the error bars indicates the amount of uncertainty. For those years with more outliers or more varied temperatures, they have higher uncertainities, hence longer length of error bar. For example Year 1993 had a wide range of daily temperatures as compared to other years. In contrast, 2013 had a relatively smaller range of temperatures and less outliers, hence a shorter length of error bar because it had lower uncertainties.",
    "crumbs": [
      "Take-home Exercises",
      "Take-home Exercise 3: Be Weatherwise or Otherwise"
    ]
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03b.html#aug-1983",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03b.html#aug-1983",
    "title": "Take-home Exercise 3: Be Weatherwise or Otherwise",
    "section": "2.3 Aug 1983",
    "text": "2.3 Aug 1983\n\naug1983 &lt;- read_csv(\"data/DAILYDATA_S24_198308.csv\")\nhead(aug1983,10)\n\n# A tibble: 10 × 13\n   Station  Year Month   Day `Daily Rainfall Total (mm)` Highest 30 Min Rainfa…¹\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;                       &lt;dbl&gt; &lt;lgl&gt;                  \n 1 Changi   1983     8     1                         0   NA                     \n 2 Changi   1983     8     2                         6.4 NA                     \n 3 Changi   1983     8     3                         2.8 NA                     \n 4 Changi   1983     8     4                         3.7 NA                     \n 5 Changi   1983     8     5                        18.7 NA                     \n 6 Changi   1983     8     6                         0   NA                     \n 7 Changi   1983     8     7                         0   NA                     \n 8 Changi   1983     8     8                         0   NA                     \n 9 Changi   1983     8     9                         0   NA                     \n10 Changi   1983     8    10                         0   NA                     \n# ℹ abbreviated name: ¹​`Highest 30 Min Rainfall (mm)`\n# ℹ 7 more variables: `Highest 60 Min Rainfall (mm)` &lt;lgl&gt;,\n#   `Highest 120 Min Rainfall (mm)` &lt;lgl&gt;,\n#   `Mean Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Maximum Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Minimum Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Mean Wind Speed (km/h)` &lt;dbl&gt;, `Max Wind Speed (km/h)` &lt;dbl&gt;"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03b.html#aug-1993",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03b.html#aug-1993",
    "title": "Take-home Exercise 3: Be Weatherwise or Otherwise",
    "section": "2.4 Aug 1993",
    "text": "2.4 Aug 1993\n\naug1993 &lt;- read_csv(\"data/DAILYDATA_S24_199308.csv\")\nhead(aug1993,10)\n\n# A tibble: 10 × 13\n   Station  Year Month   Day `Daily Rainfall Total (mm)` Highest 30 Min Rainfa…¹\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;                       &lt;dbl&gt; &lt;lgl&gt;                  \n 1 Changi   1993     8     1                         0.4 NA                     \n 2 Changi   1993     8     2                         0   NA                     \n 3 Changi   1993     8     3                         0   NA                     \n 4 Changi   1993     8     4                         0   NA                     \n 5 Changi   1993     8     5                         0   NA                     \n 6 Changi   1993     8     6                         0   NA                     \n 7 Changi   1993     8     7                         0   NA                     \n 8 Changi   1993     8     8                         0   NA                     \n 9 Changi   1993     8     9                         0.4 NA                     \n10 Changi   1993     8    10                         0   NA                     \n# ℹ abbreviated name: ¹​`Highest 30 Min Rainfall (mm)`\n# ℹ 7 more variables: `Highest 60 Min Rainfall (mm)` &lt;lgl&gt;,\n#   `Highest 120 Min Rainfall (mm)` &lt;lgl&gt;,\n#   `Mean Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Maximum Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Minimum Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Mean Wind Speed (km/h)` &lt;dbl&gt;, `Max Wind Speed (km/h)` &lt;dbl&gt;"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03b.html#aug-2003",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03b.html#aug-2003",
    "title": "Take-home Exercise 3: Be Weatherwise or Otherwise",
    "section": "2.5 Aug 2003",
    "text": "2.5 Aug 2003\n\naug2003 &lt;- read_csv(\"data/DAILYDATA_S24_200308.csv\")\nhead(aug2003,10)\n\n# A tibble: 10 × 13\n   Station  Year Month   Day `Daily Rainfall Total (mm)` Highest 30 Min Rainfa…¹\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;                       &lt;dbl&gt; &lt;lgl&gt;                  \n 1 Changi   2003     8     1                         0   NA                     \n 2 Changi   2003     8     2                        21.1 NA                     \n 3 Changi   2003     8     3                         0   NA                     \n 4 Changi   2003     8     4                         0   NA                     \n 5 Changi   2003     8     5                        64.4 NA                     \n 6 Changi   2003     8     6                         9.2 NA                     \n 7 Changi   2003     8     7                         0   NA                     \n 8 Changi   2003     8     8                         0   NA                     \n 9 Changi   2003     8     9                         1.3 NA                     \n10 Changi   2003     8    10                         0   NA                     \n# ℹ abbreviated name: ¹​`Highest 30 Min Rainfall (mm)`\n# ℹ 7 more variables: `Highest 60 Min Rainfall (mm)` &lt;lgl&gt;,\n#   `Highest 120 Min Rainfall (mm)` &lt;lgl&gt;,\n#   `Mean Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Maximum Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Minimum Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Mean Wind Speed (km/h)` &lt;dbl&gt;, `Max Wind Speed (km/h)` &lt;dbl&gt;"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03b.html#aug-2013",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03b.html#aug-2013",
    "title": "Take-home Exercise 3: Be Weatherwise or Otherwise",
    "section": "2.6 Aug 2013",
    "text": "2.6 Aug 2013\n\naug2013 &lt;- read_csv(\"data/DAILYDATA_S24_201308.csv\")\nhead(aug2013,10)\n\n# A tibble: 10 × 13\n   Station  Year Month   Day `Daily Rainfall Total (mm)` Highest 30 Min Rainfa…¹\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;                       &lt;dbl&gt; &lt;lgl&gt;                  \n 1 Changi   2013     8     1                         8.2 NA                     \n 2 Changi   2013     8     2                         0   NA                     \n 3 Changi   2013     8     3                         0   NA                     \n 4 Changi   2013     8     4                         0   NA                     \n 5 Changi   2013     8     5                        43.2 NA                     \n 6 Changi   2013     8     6                         1   NA                     \n 7 Changi   2013     8     7                        19.2 NA                     \n 8 Changi   2013     8     8                         0.6 NA                     \n 9 Changi   2013     8     9                         1.8 NA                     \n10 Changi   2013     8    10                        40   NA                     \n# ℹ abbreviated name: ¹​`Highest 30 Min Rainfall (mm)`\n# ℹ 7 more variables: `Highest 60 Min Rainfall (mm)` &lt;lgl&gt;,\n#   `Highest 120 Min Rainfall (mm)` &lt;lgl&gt;,\n#   `Mean Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Maximum Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Minimum Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Mean Wind Speed (km/h)` &lt;dbl&gt;, `Max Wind Speed (km/h)` &lt;dbl&gt;"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03b.html#aug-2023",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03b.html#aug-2023",
    "title": "Take-home Exercise 3: Be Weatherwise or Otherwise",
    "section": "2.7 Aug 2023",
    "text": "2.7 Aug 2023\n\naug2023 &lt;- read_csv(\"data/DAILYDATA_S24_202308.csv\")\nhead(aug2023,10)\n\n# A tibble: 10 × 13\n   Station  Year Month   Day `Daily Rainfall Total (mm)` Highest 30 min Rainfa…¹\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;                       &lt;dbl&gt;                   &lt;dbl&gt;\n 1 Changi   2023     8     1                         0                       0  \n 2 Changi   2023     8     2                         0                       0  \n 3 Changi   2023     8     3                         9.2                     2.2\n 4 Changi   2023     8     4                         0                       0  \n 5 Changi   2023     8     5                         0                       0  \n 6 Changi   2023     8     6                         0                       0  \n 7 Changi   2023     8     7                         0                       0  \n 8 Changi   2023     8     8                        24.4                    16.4\n 9 Changi   2023     8     9                         0                       0  \n10 Changi   2023     8    10                         0                       0  \n# ℹ abbreviated name: ¹​`Highest 30 min Rainfall (mm)`\n# ℹ 7 more variables: `Highest 60 min Rainfall (mm)` &lt;dbl&gt;,\n#   `Highest 120 min Rainfall (mm)` &lt;dbl&gt;,\n#   `Mean Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Maximum Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Minimum Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Mean Wind Speed (km/h)` &lt;dbl&gt;, `Max Wind Speed (km/h)` &lt;dbl&gt;"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03b.html#rename-mean-temperature",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03b.html#rename-mean-temperature",
    "title": "Take-home Exercise 3: Be Weatherwise or Otherwise",
    "section": "2.4 Rename Mean Temperature",
    "text": "2.4 Rename Mean Temperature\n\n\nShow the code\nchangitemp &lt;- rename(changitemp, \n       DailyTemp = `Mean Temperature (degrees celsius)`)\n\nglimpse(changitemp)\n\n\nRows: 155\nColumns: 3\n$ Day       &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n$ Year      &lt;dbl&gt; 1983, 1983, 1983, 1983, 1983, 1983, 1983, 1983, 1983, 1983, …\n$ DailyTemp &lt;dbl&gt; 28.9, 28.4, 28.5, 26.3, 27.3, 27.5, 27.9, 28.6, 28.8, 29.0, …"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03b.html#change-variables-from-character-to-factor",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03b.html#change-variables-from-character-to-factor",
    "title": "Take-home Exercise 3: Be Weatherwise or Otherwise",
    "section": "2.5 Change Variables from character to factor",
    "text": "2.5 Change Variables from character to factor\n\n\nShow the code\nchangitemp$Day &lt;- as.factor(changitemp$Day)\nchangitemp$Year &lt;- as.factor(changitemp$Year)\n\nglimpse(changitemp)\n\n\nRows: 155\nColumns: 3\n$ Day       &lt;fct&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n$ Year      &lt;fct&gt; 1983, 1983, 1983, 1983, 1983, 1983, 1983, 1983, 1983, 1983, …\n$ DailyTemp &lt;dbl&gt; 28.9, 28.4, 28.5, 26.3, 27.3, 27.5, 27.9, 28.6, 28.8, 29.0, …"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03b.html#exploring-the-distribution-of-daily-temperature",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03b.html#exploring-the-distribution-of-daily-temperature",
    "title": "Take-home Exercise 3: Be Weatherwise or Otherwise",
    "section": "3.1 Exploring the Distribution of Daily Temperature",
    "text": "3.1 Exploring the Distribution of Daily Temperature\nTo get a sense of the distribution of the daily mean temperature in these five years, we plot a boxplot first.\n\nc2 &lt;- ggplot(data = changitemp, \n             aes(x = Day, \n                 y = DailyTemp,\n                 group = Year,\n                 color = Year),\n             show.legend = FALSE) +\n  geom_line() + \n  geom_point() + \n  geom_smooth(method = \"lm\", size=0.5, fill = \"blue\")+ \n  ylim(0,35) +\n  facet_grid(rows = vars(Year)) + \n  theme_ipsum_rc(plot_title_size = 13, plot_title_margin=4,\n                         subtitle_size=11, subtitle_margin=4, \n                         axis_title_size = 8, axis_text_size=8, axis_title_face=\n                           \"bold\", plot_margin = margin(4, 4, 4, 4)) \n\n\nd1 &lt;- highlight_key(changitemp) \n\nc1 &lt;- ggplot(data = d1,\n         aes(x = Year,\n             y = DailyTemp,\n             fill = Year)) +\n    geom_boxplot() + \n    geom_jitter(color=\"black\", size=0.4, alpha=0.9) +\n    theme_ipsum_rc(plot_title_size = 13, plot_title_margin=4,\n                         subtitle_size=11, subtitle_margin=4, \n                         axis_title_size = 8, axis_text_size=8, axis_title_face=\n                           \"bold\", plot_margin = margin(4, 4, 4, 4))\n\n\ngg1 &lt;- highlight(ggplotly(c1),        \n                \"plotly_selected\")  \n\ngg2 &lt;- highlight(ggplotly(c2),\n                 \"plotly_selected\")\n\n#crosstalk::bscols(widths = c(6,6), gg1, list(gg2,(datatable(d1, class='compact'))))\n\nfig &lt;- subplot(gg1, gg2, widths = c(0.3, 0.7)) %&gt;% \n  layout(title = 'Distribution of Daily Temperature')\n\nfig\n\n\n\n\n\n:::{.callout-note appearance = “simple”} ## Insights\nAt first glance, the distributions of the daily temperature for all five years do not resemble a normal distribution. The distribution patterns were also different for different years since each year has their own peaks.\nThe median is also different for different years, with some years having differences of close to 0.7 degree Celsius. In 1983, the median is 28.5 degree Celsius, it was 28.8 in 1993, 28.3 in 2003, 28.1 in 2013 and 28.8 in 2023.The difference in median between 2013 and 2023 is 0.7.\nIn addition, the temperature ranges for each year is different. For example, it seems that the daily temperature for 2003, 2013 and 2023 have similar temperature range, where the lowest daily temperature is around 26.5 to 26.7 degrees Celsius, and the highest daily temperature is around 29 to 29.5 degree Celsius.Interestingly, 1993 experienced a wide range of daily mean temperature. 1993 had the lowest temperature of 25.5 degrees celsius and also experienced the highest temperature of 30.1 degree celsius. These two temperatures were also the lowest and highest temperature of the distribution.\nAlthough the temperature range for 2023 is similar to 2003, we observe that 2023 has more days where temperature are higher than its median.\nIn the next section, we will perform confirmatory data analysis to confirm if the daily temperature is increasing over the years.\n:::"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03b.html#line-chart-for-daily-temperature",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03b.html#line-chart-for-daily-temperature",
    "title": "Take-home Exercise 3: Be Weatherwise or Otherwise",
    "section": "3.2 Line chart for daily temperature",
    "text": "3.2 Line chart for daily temperature\n\nd1 &lt;- highlight_key(changitemp) \n\nc3 &lt;- plot_ly(data = d1,\n              x = ~Year,\n              y = ~DailyTemp,\n              fill = ~Year,\n              line = list(width =1),\n              type = \"box\",\n              boxpoints = \"all\",\n              jitter = 0.3,\n              marker = list(opacity = 0.5,\n                            line = list(width = 2)),\n              box = list(visible = T),\n              meanline = list(visible = T,\n                              color = \"red\"))\n\n\nd2 &lt;- highlight_key(changitemp_transformed) \n\nc7 &lt;- plot_ly(data = d2,\n        x = ~Day,\n        y = ~Year1983,\n        type = \"scatter\",\n        mode = \"lines+markers\") |&gt;\nlayout(title = \"Temperature observed by Changi Weather Station in August\",\n       xaxis = list(title =\"\"),\n       yaxis = list(title = \"\", range = c(0,35)),\n       updatemenus = list(list(type = 'dropdown',\n                               xref = \"paper\",\n                               yref = \"paper\",\n                               xanchor = \"left\",\n                                x = 0.04, \n                                 y = 0.95,\n                                 buttons = list(\n                                   list(method = \"update\",\n                                        args = list(list(y = list(changitemp_transformed$Year1983)),\n                                                    list(yaxis = list(title = \"Temperature in 1983\", range = c(0,35)))),label = \"1983\"),\n                                   list(method = \"update\",\n                                        args = list(list(y = list(changitemp_transformed$Year1993)),\n                                                    list(yaxis = list(title = \"Temperature in 1993\", range = c(0,35)))),label = \"1993\"),\n                                   list(method = \"update\",\n                                        args = list(list(y = list(changitemp_transformed$Year2003)),\n                                                    list(yaxis = list(title = \"Temperature  in 2003\", range = c(0,35)))),label = \"2003\"),\n                                   list(method = \"update\",\n                                        args = list(list(y = list(changitemp_transformed$Year2013)),\n                                                    list(yaxis = list(title = \"Temperature  in 2013\", range = c(0,35)))),label = \"2013\"),\n                                   list(method = \"update\",\n                                        args = list(list(y = list(changitemp_transformed$Year2023)),\n                                                    list(yaxis = list(title = \"Temperature  in 2023\", range = c(0,35)))),label = \"2023\")\n                                   \n                               ))))\n\n\ngg2 &lt;- highlight(c7,        \n                \"plotly_selected\")  \n\n\n\ncrosstalk::bscols(widths = c(6,6), c3 ,list(gg2, datatable(d2, class='compact')))"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03b.html#normality-test",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03b.html#normality-test",
    "title": "Take-home Exercise 3: Be Weatherwise or Otherwise",
    "section": "4.1 Normality test",
    "text": "4.1 Normality test\nBefore we can perform confirmatory data analysis to find out if the daily temperatures were indeed increasing over the years, we need to decide if parametric or non-parametric test should be used. As such, we will use ad.test() from nortest package to perform Anderson-Darling test with a confidence level of 95% to test the null hypothesis that the daily temperature for each year is normally distributed.\nIn the following code chunk, we will loop through each year (i.e. 1983, 1993, 2003, 2013 and 2023) to create a list called testresultlist containing a list of Anderson-Darling test results. Then we will create a tibble resultlist to contain the Year and the p-value result of the Anderson-Darling test. Then we will display the tibble using datatable() from DT package.\n\n\nShow the code\ntestresultlist &lt;- list()\n\nfor (i in unique(changitemp$Year)){\n  subdf &lt;- subset(x = changitemp, subset=Year==i)\n  testresultlist[[i]] &lt;- ad.test(subdf$DailyTemp)\n}\n\nresultlist &lt;- tibble(Year = unique(changitemp$Year),\n                     p_value = unlist(lapply(testresultlist, `[[`, 2)))\n\ndatatable(resultlist, rownames = FALSE, caption = \"Table 3 - p-value for each Year's Anderson-Darling test\", class='compact')\n\n\n\n\n\n\nBased on the result above, the null hypothesis (i.e. distribution is normally distributed) is rejected because the p-value for Years 1983, 1993, 2013 and 2023 are below the 0.05 critical value. As such, we are unable to confirm normality assumption for the distribution of daily temperature.",
    "crumbs": [
      "Take-home Exercises",
      "Take-home Exercise 3: Be Weatherwise or Otherwise"
    ]
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03b.html#anova-test",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03b.html#anova-test",
    "title": "Take-home Exercise 3: Be Weatherwise or Otherwise",
    "section": "4.2 ANOVA Test",
    "text": "4.2 ANOVA Test\nAs we are comparing the point estimates between more than 2 groups, we will use ggstatsplot’s ggbetweenstats() to visualise the ANOVA Test results. When visualising the ANOVA test results using ggbetweenstats(), non-parametric test is considered, hence type = \"np\" argument because we were unable to confirm the normality assumption for the distribution of daily temperature. In addition, we wanted to make pairwise comparisons e between significant pairs since we are interested to know those pairs with significant difference between them. Hence, pairwise.display = \"s\".\n\n\nShow the code\ntest1 &lt;- ggbetweenstats(\n  data = changitemp,\n  x = Year, \n  y = DailyTemp,\n  type = \"np\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE, \n  title = \"One-Way ANOVA Shows Differences in Daily Temperature Across Years\",\n  caption = \"Data from Weather.gov.sg\", \n  ylab = \"Daily Temperature\", \n  theme = theme_ipsum_rc(plot_title_size = 13, plot_title_margin=4,\n                         subtitle_size=11, subtitle_margin=4, \n                         axis_title_size = 8, axis_text_size=8, axis_title_face=\n                           \"bold\", plot_margin = margin(4, 4, 4, 4))) \n\n\ntest1\n\n\n\n\n\n\n\n\n\nAs noted above, the hypothesis testing is done using Kruskal-Wallis test with 95% confidence level. The hypothesis is:\nH0 : There is no difference between median daily temperatures between years.\nH1 : There is difference between median daily temperatures between years.\n\n\nShow the code\nextract_stats(test1)\n\n\n$subtitle_data\n# A tibble: 1 × 15\n  parameter1 parameter2 statistic df.error  p.value method                      \n  &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt;    &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;                       \n1 DailyTemp  Year            18.8        4 0.000844 Kruskal-Wallis rank sum test\n  effectsize      estimate conf.level conf.low conf.high conf.method         \n  &lt;chr&gt;              &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;               \n1 Epsilon2 (rank)    0.122       0.95   0.0668         1 percentile bootstrap\n  conf.iterations n.obs expression\n            &lt;int&gt; &lt;int&gt; &lt;list&gt;    \n1             100   155 &lt;language&gt;\n\n$caption_data\nNULL\n\n$pairwise_comparisons_data\n# A tibble: 10 × 9\n   group1 group2 statistic p.value alternative distribution p.adjust.method\n   &lt;chr&gt;  &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;        &lt;chr&gt;          \n 1 1983   1993       2.29  0.0553  two.sided   z            FDR            \n 2 1983   2003       0.803 0.469   two.sided   z            FDR            \n 3 1983   2013       0.947 0.429   two.sided   z            FDR            \n 4 1983   2023       2.71  0.0222  two.sided   z            FDR            \n 5 1993   2003       1.49  0.196   two.sided   z            FDR            \n 6 1993   2013       3.24  0.00607 two.sided   z            FDR            \n 7 1993   2023       0.425 0.671   two.sided   z            FDR            \n 8 2003   2013       1.75  0.133   two.sided   z            FDR            \n 9 2003   2023       1.91  0.112   two.sided   z            FDR            \n10 2013   2023       3.66  0.00252 two.sided   z            FDR            \n   test  expression\n   &lt;chr&gt; &lt;list&gt;    \n 1 Dunn  &lt;language&gt;\n 2 Dunn  &lt;language&gt;\n 3 Dunn  &lt;language&gt;\n 4 Dunn  &lt;language&gt;\n 5 Dunn  &lt;language&gt;\n 6 Dunn  &lt;language&gt;\n 7 Dunn  &lt;language&gt;\n 8 Dunn  &lt;language&gt;\n 9 Dunn  &lt;language&gt;\n10 Dunn  &lt;language&gt;\n\n$descriptive_data\nNULL\n\n$one_sample_data\nNULL\n\n$tidy_data\nNULL\n\n$glance_data\nNULL\n\n\n\n\n\n\n\n\nObservations\n\n\n\nSince the p-value (0.0008444241) is less than the critical value of 0.05, there is statistical evidence to reject the null hypothesis. We can conclude that there is difference between median daily temperature.\nIn the above plot, we observed that there are certain pairs of years with p-value less than 0.05. These pairs are: 1983 and 2023, 1993 and 2013, 2013 and 2023. This suggests that the differences between the medians of these pairs are statistically significant.\nLooking at the significant differences between 1983 and 2023 and 2013 and 2023, based on the medians, it seems that there was indeed a temperature rise from 1983 to 2023 and from 2013 and 2023. However, the differences between the medians were small, which were different from the figures quoted. We would need more years of data to ascertain the projectation that daily mean temperature would increase by 1.4 to 4.6 Degree Celsius.",
    "crumbs": [
      "Take-home Exercises",
      "Take-home Exercise 3: Be Weatherwise or Otherwise"
    ]
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03b.html#observations-2",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03b.html#observations-2",
    "title": "Take-home Exercise 3: Be Weatherwise or Otherwise",
    "section": "4.4 Observations",
    "text": "4.4 Observations\nThe length of the error bars indicates the amount of uncertainty. For those years with more outliers or more varied temperatures, they have higher uncertainities, hence longer length of error bar. For example Year 1993 had a wide range of daily temperatures as compared to other years. In contrast, 2013 had a relatively smaller range of temperatures and less outliers, hence a shorter length of error bar because it had lower uncertainties.\n:::"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html",
    "title": "Take-home Exercise 6 - Visualing and Analysing Time-Oriented Data",
    "section": "",
    "text": "In this exercise, we will be learning how to create the following visualisations:\n\na calendar heatmap using ggplot2 functions\na cycle plot using ggplot2 function\na slopegraph",
    "crumbs": [
      "Hands-on Exercises",
      "Take-home Exercise 6 - Visualing and Analysing Time-Oriented Data"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#installing-and-loading-the-packages",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#installing-and-loading-the-packages",
    "title": "Take-home Exercise 6 - Visualing and Analysing Time-Oriented Data",
    "section": "2.1 Installing and Loading the Packages",
    "text": "2.1 Installing and Loading the Packages\nFor this exercise, other than tidyverse, we will use the following packages:\n\nscales\nviridis\nlubridate\nggthemes\ngridExtra\nreadxl\nknitr\ndata.table\nCGPfunctions\n\n\n\nShow the code\npacman::p_load(tidyverse, scales, viridis, lubridate, ggthemes, gridExtra, readxl,knitr, data.table, CGPfunctions)",
    "crumbs": [
      "Hands-on Exercises",
      "Take-home Exercise 6 - Visualing and Analysing Time-Oriented Data"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#importing-the-data",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#importing-the-data",
    "title": "Take-home Exercise 6 - Visualing and Analysing Time-Oriented Data",
    "section": "3.1 Importing the Data",
    "text": "3.1 Importing the Data\nFor the purpose of this hands-on exercise, eventlog.csv file will be used. This data file consists of 199,999 rows of time-series cyber attack records by country.\nFirst, we will use the code chunk below to import eventlog.csv file into R environment and called the data frame as attacks.\n\n\nShow the code\nattacks &lt;- read_csv(\"data/eventlog.csv\")",
    "crumbs": [
      "Hands-on Exercises",
      "Take-home Exercise 6 - Visualing and Analysing Time-Oriented Data"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#examining-the-data-structure",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#examining-the-data-structure",
    "title": "Take-home Exercise 6 - Visualing and Analysing Time-Oriented Data",
    "section": "3.2 Examining the Data Structure",
    "text": "3.2 Examining the Data Structure\nWe will use kable() to review the structure of the imported data frame.\n\n\nShow the code\nkable(head(attacks))\n\n\n\n\n\ntimestamp\nsource_country\ntz\n\n\n\n\n2015-03-12 15:59:16\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:00:48\nFR\nEurope/Paris\n\n\n2015-03-12 16:02:26\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:02:38\nUS\nAmerica/Chicago\n\n\n2015-03-12 16:03:22\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:03:45\nCN\nAsia/Shanghai\n\n\n\n\n\nFrom the above output, we see there there are three columns in the attack dataset: timestamp, source_country and tz.\n\ntimestamp: stores datetime vales in POSIXct format\nsource_country: stores the source of the attack. It is in ISO 3166-1 alpha-2 country code\ntz: stores time zone of the source IP address.",
    "crumbs": [
      "Hands-on Exercises",
      "Take-home Exercise 6 - Visualing and Analysing Time-Oriented Data"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#data-preparation",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#data-preparation",
    "title": "Take-home Exercise 6 - Visualing and Analysing Time-Oriented Data",
    "section": "3.3 Data Preparation",
    "text": "3.3 Data Preparation\n\n3.3.1 Step 1\nBefore we can plot the calendar heatmap, we need to derive two new fields: wkday and hour using a function. To get the hour function, we will make use of hour() from lubridate package. To get the wkday field, we will make use of weekdays(), which is a base R function.\n\nMethod 1: Using lubridate and base R functionMethod 2: Using lubridate package\n\n\n\nmake_hr_wkday &lt;- function(ts,sc,tz){\n  real_times &lt;- ymd_hms(ts,\n                        tz = tz[1],\n                        quiet = TRUE)\n  \n  dt &lt;- data.table(source_country = sc,\n                   wkday = weekdays(real_times),\n                   hour = hour(real_times))\n  return(dt)\n}\n\n\n\n\n\n\n\nAbout the code chunk\n\n\n\n\nymd_hms() and hour() are from lubridate package.\n\nNote! before using ymd_hms(), we should examine the date/time format on our laptops. The date/time format on our laptops should be yyyy-mm-dd hh:mm:ss before using ymd_hms(). If the print date/time format is different, then we should either (1) use different function or (2) change the date/time format of our computer.\n\nweekdays() is a base R function.\n\n\n\n\n\n\nattacks1 &lt;- attacks %&gt;%\n  mutate(wkday = lubridate:: wday(timestamp,\n                       label = TRUE,\n                      abbr = FALSE, # to display the day of the week \n                       week_start = 7), #Sun is the first level \n         hr = hour(timestamp))\n\n\n\n\n\n\n\nAbout the code chunk\n\n\n\n\nhour() and wday() are from lubridate package.\n\nNote: these two functions are relatively more efficient than Base R function.\n\n\n\n\n\n\n\n\n\n3.3.2 Step 2\nWe will now derive the attacks tibble data frame using the following code chunk.\n\nMethod 1Method 2\n\n\nAfter creating the function in earlier section, we will now use it to derive the wkday and hour columns. After that we will also use mutate() to convert wkday and hour fields into factor so that they will be ordered when plotting.\n\nwkday_levels &lt;- c('Saturday', 'Friday', \n                  'Thursday', 'Wednesday', \n                  'Tuesday', 'Monday', \n                  'Sunday')\n\nattacks2 &lt;- attacks %&gt;%\n  group_by(tz) %&gt;%\n  do(make_hr_wkday(.$timestamp,\n                   .$source_country,\n                   .$tz)) %&gt;%\n  ungroup() %&gt;%\n  mutate(wkday = factor(\n    wkday, levels = wkday_levels),\n    hour = factor(hour, levels = 0:23))\n\n\n\nUsing the attacks1 dataframe from previous section’s method 2, we will just select the columns that we want and convert the wkday and hr columns into factors using as.factor().\n\nattacks3 &lt;- attacks1 %&gt;%\n  select(tz, source_country, wkday, hr)\n\nattacks3$wkday &lt;- as.factor(attacks3$wkday)\n\nattacks3$hr &lt;- as.factor(attacks3$hr)\n\n\n\n\n\n\n3.3.3 Step 3 - Building Calendar Heatmaps\n\ngrouped &lt;- attacks2 %&gt;%\n  count(wkday, hour) %&gt;%\n  ungroup %&gt;%\n  na.omit()\n\nggplot(grouped,\n       aes(hour, \n           wkday, \n           fill = n)) +\n  geom_tile(color = \"white\", \n            size = 0.1) +\n  \n  theme_tufte(base_family = \"sans serif\") + \n  coord_equal() +\n  scale_fill_gradient(name = \"# of attacks\",\n                    low = \"lavender\", \n                    high = \"maroon4\") +\n  labs(x = NULL, \n       y = NULL, \n       title = \"Attacks by Weekday and Time of Day\") +\n  \n  theme(axis.ticks = element_blank(),\n      plot.title = element_text(hjust = 0.5),\n      legend.title = element_text(size = 8),\n      legend.text = element_text(size = 6) )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbout the code chunk\n\n\n\n\nA tibble data table called grouped is derived by aggregating the attack by wkday and hour fields\nA new field called n is derived by using group_by() and count() functions.\nna.omit() is used to exclude missing value.\ngeom_tile() is used to plot tiles(grids) at each x and y positions. color and size arguments are used to specify the border color and line size of the tiles. In this case, the border color is white and the line size is 0.1.\ntheme_tufte() of ggthemes package is used to remove unnecessary chart junk.\ncoord_equal() is used to ensure the plot will have an aspect ratio of 1:1\nscale_fill_gradient() function is used to create a two color gradient (low-high)",
    "crumbs": [
      "Hands-on Exercises",
      "Take-home Exercise 6 - Visualing and Analysing Time-Oriented Data"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#building-calendar-heatmaps",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#building-calendar-heatmaps",
    "title": "Take-home Exercise 6 - Visualing and Analysing Time-Oriented Data",
    "section": "3.4 Building Calendar Heatmaps",
    "text": "3.4 Building Calendar Heatmaps"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-multiple-calendar-heatmaps",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-multiple-calendar-heatmaps",
    "title": "Take-home Exercise 6 - Visualing and Analysing Time-Oriented Data",
    "section": "3.4 Plotting Multiple Calendar Heatmaps",
    "text": "3.4 Plotting Multiple Calendar Heatmaps\nWe can also plot multiple calendar heatmaps for the top four countries with the highest number of attacks.\n\n3.4.1 Step 1: Deriving Attack by Country Object\nFirst we need to identify the top 4 countries with the highest number of attacks: - count the number of attacks by country - calculate the percent of attacks by country, and - save the results in a tibble data frame\n\nattacks_by_cty &lt;- count(\n  attacks, source_country) %&gt;%\n  mutate(percent = percent(n/sum(n))) %&gt;%\n  arrange(desc(n))\n\n\n\n3.4.2 Step 2: Preparing the tidy data frame\nNow we will extract the attack records of the top 4 countries from attacks2data frame. Then save the data in a new tibble data frame (i.e. top4_attacks).\n\ntop4 &lt;- attacks_by_cty$source_country[1:4]\n\ntop4_attacks &lt;- attacks2 %&gt;%\n  filter(source_country %in% top4) %&gt;%\n  count(source_country, wkday, hour) %&gt;%\n  ungroup() %&gt;%\n  mutate(source_country = factor(\n    source_country, levels = top4)) %&gt;%\n  na.omit()\n\n\n\n3.4.3 Step 3: Plotting the Multiple Calendar Heatmap using ggplot2 package\nNow, we are ready to plot multiple calendar heatmap!\n\n# New facet label names\ncty.labs &lt;- c(\"CHINA\", \"UNITED STATES\", \"KOREA\", \"NETHERLANDS\")\nnames(cty.labs) &lt;- c(\"CN\", \"US\", \"KR\", \"NL\")\n\n\n#baseplot \nggplot(top4_attacks,\n       aes(hour,\n           wkday,\n           fill = n)) +\n  geom_tile(color = \"white\",\n            size = 0.1) +\n  theme_tufte(base_family = \"sans serif\") + \n  coord_equal() +\n  scale_fill_gradient(name = \"# of attacks\",\n                    low = \"grey80\", \n                    high = \"maroon4\") +\n  #plot for each country using facet_wrap \n  facet_wrap(~source_country, ncol = 2, \n             labeller = labeller(source_country = cty.labs)) + \n  labs(x = NULL, y = NULL,\n       title = \"Attacks on Top 4 Countries by Weekday and Time of Day\") + \n   theme(axis.ticks = element_blank(),\n        axis.text.x = element_text(size = 7),\n        plot.title = element_text(hjust = 0.5),\n        legend.title = element_text(size = 8),\n        legend.text = element_text(size = 6))",
    "crumbs": [
      "Hands-on Exercises",
      "Take-home Exercise 6 - Visualing and Analysing Time-Oriented Data"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#importing-the-data-1",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#importing-the-data-1",
    "title": "Take-home Exercise 6 - Visualing and Analysing Time-Oriented Data",
    "section": "4.1 Importing the Data",
    "text": "4.1 Importing the Data\nFor the purpose of this hands-on exercise, arrivals_by_air.xlsx will be used.\nFirst, we will use the code chunk below to import arrivals_by_air.xlsx file into R environment and call the tibble data frame as air.\n\n\nShow the code\nair &lt;- read_excel(\"data/arrivals_by_air.xlsx\")\nkable(head(air))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMonth-Year\nRepublic of South Africa\nCanada\nUSA\nBangladesh\nBrunei\nChina\nHong Kong SAR (China)\nIndia\nIndonesia\nJapan\nSouth Korea\nKuwait\nMalaysia\nMyanmar\nPakistan\nPhilippines\nSaudi Arabia\nSri Lanka\nTaiwan\nThailand\nUnited Arab Emirates\nVietnam\nBelgium & Luxembourg\nCIS\nFinland\nFrance\nGermany\nIreland\nItaly\nNetherlands\nSpain\nSwitzerland\nUnited Kingdom\nAustralia\nNew Zealand\n\n\n\n\n2000-01-01\n3291\n5545\n25906\n2883\n3749\n33895\n13692\n19235\n65151\n59288\n21457\n507\n27472\n1177\n2150\n8404\n1312\n3922\n15766\n12048\n1318\n1527\n1434\n2703\n1634\n4752\n12739\n1292\n3544\n4962\n925\n3731\n28986\n34616\n5034\n\n\n2000-02-01\n2357\n6120\n28262\n2469\n3236\n34344\n19870\n18975\n37105\n58188\n19634\n199\n29084\n1161\n2496\n9128\n623\n3988\n24861\n12745\n899\n2269\n1596\n1182\n1297\n6391\n13093\n1200\n2897\n5054\n747\n3980\n35148\n26030\n3938\n\n\n2000-03-01\n4036\n6255\n30439\n2904\n3342\n27053\n17086\n21049\n44205\n74426\n20719\n386\n30504\n1355\n2429\n11691\n1578\n4259\n18767\n16971\n1474\n2034\n1548\n1088\n1220\n5528\n13645\n1368\n2717\n4950\n935\n3576\n36117\n31119\n4668\n\n\n2000-04-01\n4241\n4521\n25378\n2843\n5117\n30464\n22346\n26160\n45480\n49985\n17489\n221\n34478\n1593\n2711\n14141\n705\n6579\n22735\n20397\n1284\n2420\n1592\n1012\n1208\n5544\n13366\n1345\n2512\n4149\n941\n3850\n33792\n34824\n6890\n\n\n2000-05-01\n2841\n3914\n26163\n2793\n4152\n30775\n16357\n35869\n38350\n48937\n19398\n164\n34795\n1397\n2594\n13305\n679\n4625\n18399\n15769\n1042\n1833\n1167\n660\n743\n4225\n10878\n1067\n2205\n3643\n764\n3025\n23377\n33139\n7006\n\n\n2000-06-01\n2776\n3487\n28179\n3146\n5018\n26720\n18133\n31314\n47982\n53798\n17522\n440\n34660\n1715\n2924\n10555\n2749\n4740\n21042\n17217\n1545\n2480\n1170\n712\n982\n4047\n9054\n1363\n2196\n3544\n855\n2580\n21769\n35731\n7634",
    "crumbs": [
      "Hands-on Exercises",
      "Take-home Exercise 6 - Visualing and Analysing Time-Oriented Data"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#deriving-month-and-year-fields",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#deriving-month-and-year-fields",
    "title": "Take-home Exercise 6 - Visualing and Analysing Time-Oriented Data",
    "section": "4.2 Deriving month and year fields",
    "text": "4.2 Deriving month and year fields\nWe will now derive two new fields month and year from the Month-Year field.\n\n\nShow the code\nair$month &lt;- factor(month(air$`Month-Year`),\n                    levels = 1:12,\n                    labels = month.abb,\n                    ordered = TRUE)\n\nair$year &lt;- year(ymd(air$`Month-Year`))\n\nkable(head(air))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMonth-Year\nRepublic of South Africa\nCanada\nUSA\nBangladesh\nBrunei\nChina\nHong Kong SAR (China)\nIndia\nIndonesia\nJapan\nSouth Korea\nKuwait\nMalaysia\nMyanmar\nPakistan\nPhilippines\nSaudi Arabia\nSri Lanka\nTaiwan\nThailand\nUnited Arab Emirates\nVietnam\nBelgium & Luxembourg\nCIS\nFinland\nFrance\nGermany\nIreland\nItaly\nNetherlands\nSpain\nSwitzerland\nUnited Kingdom\nAustralia\nNew Zealand\nmonth\nyear\n\n\n\n\n2000-01-01\n3291\n5545\n25906\n2883\n3749\n33895\n13692\n19235\n65151\n59288\n21457\n507\n27472\n1177\n2150\n8404\n1312\n3922\n15766\n12048\n1318\n1527\n1434\n2703\n1634\n4752\n12739\n1292\n3544\n4962\n925\n3731\n28986\n34616\n5034\nJan\n2000\n\n\n2000-02-01\n2357\n6120\n28262\n2469\n3236\n34344\n19870\n18975\n37105\n58188\n19634\n199\n29084\n1161\n2496\n9128\n623\n3988\n24861\n12745\n899\n2269\n1596\n1182\n1297\n6391\n13093\n1200\n2897\n5054\n747\n3980\n35148\n26030\n3938\nFeb\n2000\n\n\n2000-03-01\n4036\n6255\n30439\n2904\n3342\n27053\n17086\n21049\n44205\n74426\n20719\n386\n30504\n1355\n2429\n11691\n1578\n4259\n18767\n16971\n1474\n2034\n1548\n1088\n1220\n5528\n13645\n1368\n2717\n4950\n935\n3576\n36117\n31119\n4668\nMar\n2000\n\n\n2000-04-01\n4241\n4521\n25378\n2843\n5117\n30464\n22346\n26160\n45480\n49985\n17489\n221\n34478\n1593\n2711\n14141\n705\n6579\n22735\n20397\n1284\n2420\n1592\n1012\n1208\n5544\n13366\n1345\n2512\n4149\n941\n3850\n33792\n34824\n6890\nApr\n2000\n\n\n2000-05-01\n2841\n3914\n26163\n2793\n4152\n30775\n16357\n35869\n38350\n48937\n19398\n164\n34795\n1397\n2594\n13305\n679\n4625\n18399\n15769\n1042\n1833\n1167\n660\n743\n4225\n10878\n1067\n2205\n3643\n764\n3025\n23377\n33139\n7006\nMay\n2000\n\n\n2000-06-01\n2776\n3487\n28179\n3146\n5018\n26720\n18133\n31314\n47982\n53798\n17522\n440\n34660\n1715\n2924\n10555\n2749\n4740\n21042\n17217\n1545\n2480\n1170\n712\n982\n4047\n9054\n1363\n2196\n3544\n855\n2580\n21769\n35731\n7634\nJun\n2000",
    "crumbs": [
      "Hands-on Exercises",
      "Take-home Exercise 6 - Visualing and Analysing Time-Oriented Data"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#extracting-the-target-country",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#extracting-the-target-country",
    "title": "Take-home Exercise 6 - Visualing and Analysing Time-Oriented Data",
    "section": "4.3 Extracting the target country",
    "text": "4.3 Extracting the target country\nNext, we will extract the data for the target country (i.e. Vietnam) and data beyond year 2010 using the following code chunk.\n\nviet &lt;- air %&gt;% \n  select(Vietnam, month, year) %&gt;%\n  filter(year &gt;= 2010)",
    "crumbs": [
      "Hands-on Exercises",
      "Take-home Exercise 6 - Visualing and Analysing Time-Oriented Data"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#computing-year-average-arrivals-by-month",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#computing-year-average-arrivals-by-month",
    "title": "Take-home Exercise 6 - Visualing and Analysing Time-Oriented Data",
    "section": "4.4 Computing year average arrivals by month",
    "text": "4.4 Computing year average arrivals by month\nNow we will use group_by() and summarise() of dplyr to compute average arrivals by month across all years.\n\nhline.data &lt;- viet %&gt;%\n  group_by(month) %&gt;%\n  summarise(avgvalue = mean(Vietnam))",
    "crumbs": [
      "Hands-on Exercises",
      "Take-home Exercise 6 - Visualing and Analysing Time-Oriented Data"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-the-cycle-plot",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-the-cycle-plot",
    "title": "Take-home Exercise 6 - Visualing and Analysing Time-Oriented Data",
    "section": "4.5 Plotting the cycle plot",
    "text": "4.5 Plotting the cycle plot\nTo plot a cycle plot for each month, we will make use of geom_line() to plot the number of visitors in each year for every month. Then, we will use geom_hline()to plot out the “average value” for each month. To have a chart for every month, we will make use of facet_grid().\n\nggplot() +\n  geom_line(data = viet,\n            aes(x = year,\n                y = `Vietnam`,\n                group = month),\n            color = \"black\") +\n  geom_hline(aes(yintercept =avgvalue),\n             data = hline.data,\n             linetype = 6,\n             color = \"red\",\n             size = 0.5) +\n  facet_grid(~month) + \n  labs(axis.text.x = element_blank(),\n       title = \"Visitor Arrivals from Vietnam by Air, Jan 2010 - Dec 2019\") +\n  xlab(\"\") +\n  ylab(\"No. of Visitors\") +\n  theme_minimal() +\n  scale_x_binned(guide = guide_axis(angle = 45)) +\n  theme(axis.text.x = element_text(size = rel(0.75)))",
    "crumbs": [
      "Hands-on Exercises",
      "Take-home Exercise 6 - Visualing and Analysing Time-Oriented Data"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#importing-the-data-2",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#importing-the-data-2",
    "title": "Take-home Exercise 6 - Visualing and Analysing Time-Oriented Data",
    "section": "5.1 Importing the Data",
    "text": "5.1 Importing the Data\nFor the purpose of this hands-on exercise, `rice.csv`` will be used.\nFirst, we will use the code chunk below to import rice.csv file into R environment and call the tibble data frame as `rice``.\n\n\nShow the code\nrice &lt;- read_csv(\"data/rice.csv\")\nkable(head(rice))\n\n\n\n\n\nCountry\nYear\nYield\nProduction\n\n\n\n\nChina\n1961\n20787\n56217601\n\n\nChina\n1962\n23700\n65675288\n\n\nChina\n1963\n26833\n76439280\n\n\nChina\n1964\n28289\n85853780\n\n\nChina\n1965\n29667\n90705630\n\n\nChina\n1966\n31445\n98403990",
    "crumbs": [
      "Hands-on Exercises",
      "Take-home Exercise 6 - Visualing and Analysing Time-Oriented Data"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-the-slopegraph",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-the-slopegraph",
    "title": "Take-home Exercise 6 - Visualing and Analysing Time-Oriented Data",
    "section": "5.2 Plotting the slopegraph",
    "text": "5.2 Plotting the slopegraph\nLet us convert the Year to a factor so that the years will be arranged in levels. Then we filter the rows where the Years are 1961 and 1980.\n\n\nShow the code\nrice &lt;- rice %&gt;%\n  mutate(Year = factor(Year)) %&gt;%\n  filter(Year %in% c(1961, 1980)) \n\nglimpse(rice)\n\n\nRows: 22\nColumns: 4\n$ Country    &lt;chr&gt; \"China\", \"China\", \"India\", \"India\", \"Indonesia\", \"Indonesia…\n$ Year       &lt;fct&gt; 1961, 1980, 1961, 1980, 1961, 1980, 1961, 1980, 1961, 1980,…\n$ Yield      &lt;dbl&gt; 20787, 41435, 15419, 20002, 17623, 32928, 48793, 51279, 414…\n$ Production &lt;dbl&gt; 56217601, 142876520, 53494500, 80312000, 12084000, 29651900…\n\n\nUsing the filtered data, we will plot the slopegraph using newggslopegraph(). We will indicate the dataframe argument as rice, which is the tibble dataframe that we have prepared in the earlier step. The Times argument refers to the column inside the dataframe that will be plotted on the x axis. Traditionally this Times argument is some measure of time. However, newggslopegraph() accepts a column of class ordered, factor or character. The Measurement argument refers to the column inside the dataframe that will be plotted on the y axis. Traditionally this Measurement argument is some measure such as a percentage. Currently, the function accepts a column of type integer or number. The Grouping argument refers to a column inside the dataframe that will be used to group and distinguish measurements.\n\n\n\n\n\n\nAbout the code chunk\n\n\n\nIn the following code chunk we also: - Added a title using the argument Title. Note that Title = \"\" will provide an empty title but retain the spacing. - Added a subtitle using the argument SubTitle. Note that SubTitle = \"\" will provide and empty title but retain the spacing. - Added a caption using the argument Caption. Note that Caption = \"\" will provide and empty title but retain the spacing. - tried to space out the data labels by adjusting the DataLabelPadding argument. - changed the theme to wsj using ThemeChoice argument.Note that by default ThemeChoice is set to “bw” and the other choices are “ipsum”, “econ”, “wsj”, “gdocs”, and “tufte”.\n\n\n\nnewggslopegraph(dataframe = rice,\n                Times = Year, \n                Measurement = Yield, \n                Grouping = Country,\n                Title = \"Rice Yield of Top 11 Asian Countries\",\n                SubTitle = \"1961 - 1980\",\n                Caption = \"Prepared by: Goh Si Hui\",\n                DataLabelPadding = 0.07,\n                ThemeChoice = \"wsj\"\n                )",
    "crumbs": [
      "Hands-on Exercises",
      "Take-home Exercise 6 - Visualing and Analysing Time-Oriented Data"
    ]
  },
  {
    "objectID": "Extra/Post-Lesson_02/Post-Lesson_02.html",
    "href": "Extra/Post-Lesson_02/Post-Lesson_02.html",
    "title": "Post-Lesson Thoughts 2: More uses of Plotly!",
    "section": "",
    "text": "1 About Plotly R\nIn Hands-on Exercises and Take Home Exercise 3,\n\n\n2 Creating Dropdown list\n\n\n3 Creating Heatmaps\n\n\n4 Creating Mixed Subplots\n\n\n5"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#installing-the-r-packages",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#installing-the-r-packages",
    "title": "In-class Exercise 2: Horizon Plot",
    "section": "",
    "text": "For this in-class exercise, we will be using the following packages:\n\ntidyverse\nggHoriPlot\nggthemes\n\nThe code chunk below uses p_load() of pacman package to check if the packages are installed in the computer. If they are, they will be launched in R. Otherwise, pacman will install the relevant packages before launching them.\n\npacman::p_load(tidyverse, ggHoriPlot, ggthemes)",
    "crumbs": [
      "In-class Exercises",
      "In-class Exercise 2: Horizon Plot"
    ]
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#importing-the-data",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#importing-the-data",
    "title": "In-class Exercise 2: Horizon Plot",
    "section": "",
    "text": "We will use read_csv() to import the csv file into R and use glimpse() to check the imported data.\n\naverp &lt;- read_csv(\"data/AVERP.csv\")\nglimpse(averp)\n\nRows: 7,452\nColumns: 3\n$ Date             &lt;chr&gt; \"1/1/2014\", \"1/2/2014\", \"1/3/2014\", \"1/4/2014\", \"1/5/…\n$ `Consumer Items` &lt;chr&gt; \"Wholemeal Bread (Per 400 Gram)\", \"Wholemeal Bread (P…\n$ Values           &lt;dbl&gt; 2.05, 2.05, 2.04, 2.04, 2.05, 2.05, 2.05, 2.05, 2.04,…\n\n\nFrom the above output, we noted that the Date field was read as character data type. We will use Lubriate package to change the Date field to Date data type.\n\naverp &lt;- averp %&gt;%\n  mutate(Date = dmy(Date)) %&gt;%\n  rename(ConsumerItem = `Consumer Items`)\nglimpse(averp)\n\nRows: 7,452\nColumns: 3\n$ Date         &lt;date&gt; 2014-01-01, 2014-02-01, 2014-03-01, 2014-04-01, 2014-05-…\n$ ConsumerItem &lt;chr&gt; \"Wholemeal Bread (Per 400 Gram)\", \"Wholemeal Bread (Per 4…\n$ Values       &lt;dbl&gt; 2.05, 2.05, 2.04, 2.04, 2.05, 2.05, 2.05, 2.05, 2.04, 2.0…",
    "crumbs": [
      "In-class Exercises",
      "In-class Exercise 2: Horizon Plot"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07a.html",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07a.html",
    "title": "Take-home Exercise 7a: Choropleth Mapping with R",
    "section": "",
    "text": "In this exercise, we will learn how to plot functional and truthful choropleth maps using tmap package.\n\n\n\n\n\n\nWhat is choropleth mapping?\n\n\n\nChoropleth mapping involves the symbolisation of enumeration units, such as countries, provinces, states, counties or census units, using area patterns or graduated colors. For example, a social scientist may need to use a choropleth map to portray the spatial distribution of aged population of Singapore by Master Plan 2014 Subzone Boundary."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07a.html#installing-and-loading-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07a.html#installing-and-loading-r-packages",
    "title": "Take-home Exercise 7a: Choropleth Mapping with R",
    "section": "2.1 Installing and Loading R Packages",
    "text": "2.1 Installing and Loading R Packages\nFor this exercise, other than tmap, we will use the following packages:\n\nreadr for importing delimited text file\ntidyr for tidying data\ndplyr for wrangling data\nsf for handling geospatial data\n\n\n\n\n\n\n\nNote\n\n\n\nAmong the four packages, readr, tidyr and dplyr are part of tidyverse package.So, we only need to install tidyverse instead of readr, tidyr and dplyr individually.\n\n\n\n\nShow the code\npacman::p_load(tidyverse, sf, tmap)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07a.html#importing-the-data-into-r",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07a.html#importing-the-data-into-r",
    "title": "Take-home Exercise 7a: Choropleth Mapping with R",
    "section": "2.2 Importing the Data into R",
    "text": "2.2 Importing the Data into R\nFor this exercise, we will use two datasets to create the choropleth map:\n\nGeospatial Data: Master Plan 2014 Subzone Boundary (Web) in ESRI shapefile format (MP14_SUBZONE_WEB_PL). It can be downloaded at data.gov.sg.This dataset contains the geographical boundary of Singapore at the planning subzone level. The data is based on URA Master Plan 2014.\nAspatial Data: Singapore Residents by Planning Area/Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 in csv format (respopagesextod2011to2020.csv``). It can be downloaded from [Department of Statistics, Singapore](https://www.singstat.gov.sg/). Although it does not contain any coordinates values, but it’s PA and SZ fields can be used as unique identifiers to geocode toMP14_SUBZONE_WEB_PL` shapefile.\n\n\nGeospatial DataAspatial Data\n\n\nThe code chunk below uses the st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a simple feature data frame called mpsz.\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\",\n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\sihuihui\\VAA\\Hands-on_Ex\\Hands-on_Ex07\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\nmpsz\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\nThe message above also tells us that mpsz’s geometry type is multipolygon, there are a total of 323 multipolygon features and 15 fields in mpsz and it is in svy21 projected coordinates systems.\n\n\nWe will import respopagesextod2011to2020.csv file using read_csv() function of readr package.\n\npopdata &lt;- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\nhead(popdata)\n\n# A tibble: 6 × 7\n  PA         SZ                     AG     Sex   TOD                   Pop  Time\n  &lt;chr&gt;      &lt;chr&gt;                  &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt;\n1 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Males HDB 1- and 2-Room …     0  2011\n2 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Males HDB 3-Room Flats       10  2011\n3 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Males HDB 4-Room Flats       30  2011\n4 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Males HDB 5-Room and Exe…    50  2011\n5 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Males HUDC Flats (exclud…     0  2011\n6 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Males Landed Properties       0  2011\n\n\nFrom the above output, we see that there are 7 columns in the datatable, namely: Planning area (PA), Subzone (SZ), Age Group (AG), Sex, Type of Dwelling (TOD), Population (Pop), Year (Time)."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07a.html#data-preparation",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07a.html#data-preparation",
    "title": "Take-home Exercise 7a: Choropleth Mapping with R",
    "section": "2.3 Data Preparation",
    "text": "2.3 Data Preparation\nBefore a thematic map can be prepared, we need to prepare a data table with year 2020 values. The data table should include the variables: PA, SZ, YOUNG(AG 0 to 4 until AG 20 to 24), ECONOMY ACTIVE (AG 25 to 29 until AG 60 - 64), AGED (AG 65 and above), TOTAL (All AG), DEPENDENCY (ratio between young and aged against economy active group).\n\n2.3.1 Data Wrangling\n\nTransforming the DataData Table\n\n\n\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time == 2020) %&gt;%\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(Pop = sum(Pop)) %&gt;%\n  ungroup() %&gt;%\n  pivot_wider(names_from = AG, \n               values_from = Pop) \n\nglimpse(popdata2020)\n\nRows: 332\nColumns: 21\n$ PA            &lt;chr&gt; \"Ang Mo Kio\", \"Ang Mo Kio\", \"Ang Mo Kio\", \"Ang Mo Kio\", …\n$ SZ            &lt;chr&gt; \"Ang Mo Kio Town Centre\", \"Cheng San\", \"Chong Boon\", \"Ke…\n$ `0_to_4`      &lt;dbl&gt; 170, 1060, 850, 680, 210, 560, 200, 670, 0, 160, 0, 740,…\n$ `10_to_14`    &lt;dbl&gt; 280, 1040, 1020, 960, 400, 640, 390, 930, 0, 210, 0, 119…\n$ `15_to_19`    &lt;dbl&gt; 340, 1160, 1070, 1010, 450, 700, 460, 830, 0, 260, 0, 13…\n$ `20_to_24`    &lt;dbl&gt; 270, 1330, 1310, 1170, 500, 860, 590, 890, 0, 300, 0, 14…\n$ `25_to_29`    &lt;dbl&gt; 260, 1720, 1610, 1410, 500, 970, 680, 1310, 0, 320, 0, 1…\n$ `30_to_34`    &lt;dbl&gt; 310, 2020, 1890, 1420, 340, 1030, 500, 1410, 0, 240, 0, …\n$ `35_to_39`    &lt;dbl&gt; 330, 2150, 1720, 1440, 300, 980, 330, 1420, 0, 250, 0, 1…\n$ `40_to_44`    &lt;dbl&gt; 400, 2080, 1810, 1630, 370, 1010, 430, 1640, 0, 260, 0, …\n$ `45_to_49`    &lt;dbl&gt; 480, 2200, 1820, 1810, 550, 1190, 580, 1580, 0, 320, 0, …\n$ `50_to_54`    &lt;dbl&gt; 380, 2050, 1900, 1720, 540, 1200, 580, 1370, 0, 290, 0, …\n$ `55_to_59`    &lt;dbl&gt; 310, 2130, 2100, 1800, 550, 1390, 660, 1570, 0, 340, 0, …\n$ `5_to_9`      &lt;dbl&gt; 230, 1050, 850, 800, 320, 570, 300, 870, 0, 180, 0, 950,…\n$ `60_to_64`    &lt;dbl&gt; 290, 2110, 2150, 1780, 480, 1280, 720, 1650, 0, 390, 0, …\n$ `65_to_69`    &lt;dbl&gt; 250, 2180, 2100, 1710, 410, 1200, 560, 1530, 0, 270, 0, …\n$ `70_to_74`    &lt;dbl&gt; 240, 1750, 1800, 1450, 360, 970, 390, 1430, 0, 200, 0, 1…\n$ `75_to_79`    &lt;dbl&gt; 130, 960, 1120, 830, 230, 630, 210, 890, 0, 120, 0, 650,…\n$ `80_to_84`    &lt;dbl&gt; 100, 650, 800, 630, 150, 430, 190, 700, 0, 80, 0, 480, 9…\n$ `85_to_89`    &lt;dbl&gt; 30, 340, 430, 350, 100, 250, 110, 360, 0, 50, 0, 250, 50…\n$ `90_and_over` &lt;dbl&gt; 10, 170, 220, 150, 60, 130, 70, 190, 0, 30, 0, 100, 50, …\n\n\n\n\n\nhead(popdata2020, 10)\n\n# A tibble: 10 × 21\n   PA      SZ    `0_to_4` `10_to_14` `15_to_19` `20_to_24` `25_to_29` `30_to_34`\n   &lt;chr&gt;   &lt;chr&gt;    &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1 Ang Mo… Ang …      170        280        340        270        260        310\n 2 Ang Mo… Chen…     1060       1040       1160       1330       1720       2020\n 3 Ang Mo… Chon…      850       1020       1070       1310       1610       1890\n 4 Ang Mo… Kebu…      680        960       1010       1170       1410       1420\n 5 Ang Mo… Semb…      210        400        450        500        500        340\n 6 Ang Mo… Shan…      560        640        700        860        970       1030\n 7 Ang Mo… Tago…      200        390        460        590        680        500\n 8 Ang Mo… Town…      670        930        830        890       1310       1410\n 9 Ang Mo… Yio …        0          0          0          0          0          0\n10 Ang Mo… Yio …      160        210        260        300        320        240\n# ℹ 13 more variables: `35_to_39` &lt;dbl&gt;, `40_to_44` &lt;dbl&gt;, `45_to_49` &lt;dbl&gt;,\n#   `50_to_54` &lt;dbl&gt;, `55_to_59` &lt;dbl&gt;, `5_to_9` &lt;dbl&gt;, `60_to_64` &lt;dbl&gt;,\n#   `65_to_69` &lt;dbl&gt;, `70_to_74` &lt;dbl&gt;, `75_to_79` &lt;dbl&gt;, `80_to_84` &lt;dbl&gt;,\n#   `85_to_89` &lt;dbl&gt;, `90_and_over` &lt;dbl&gt;\n\n\n\n\n\nThen we will group the various AG to create YOUNG, ECONOMY ACTIVE, AGED, TOTAL and also calculate the ratio DEPENDENCY using mutate() of dplyr package.\n\nTransforming the DataData Table\n\n\n\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate(YOUNG = rowSums(.[3:6]) + rowSums(.[14])) %&gt;%\n  mutate(ECONOMYACTIVE = rowSums(.[7:13]) + rowSums(.[15])) %&gt;%\n  mutate(AGED = rowSums(.[16:21])) %&gt;%\n  mutate(TOTAL = rowSums(.[3:21])) %&gt;%\n  mutate(DEPENDENCY = (YOUNG + AGED) / ECONOMYACTIVE) %&gt;%\n  select(PA, SZ, YOUNG, ECONOMYACTIVE, AGED, TOTAL, DEPENDENCY)\n\n\n\n\nhead(popdata2020, 10)\n\n# A tibble: 10 × 7\n   PA         SZ                     YOUNG ECONOMYACTIVE  AGED TOTAL DEPENDENCY\n   &lt;chr&gt;      &lt;chr&gt;                  &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;\n 1 Ang Mo Kio Ang Mo Kio Town Centre  1290          2760   760  4810      0.743\n 2 Ang Mo Kio Cheng San               5640         16460  6050 28150      0.710\n 3 Ang Mo Kio Chong Boon              5100         15000  6470 26570      0.771\n 4 Ang Mo Kio Kebun Bahru             4620         13010  5120 22750      0.749\n 5 Ang Mo Kio Sembawang Hills         1880          3630  1310  6820      0.879\n 6 Ang Mo Kio Shangri-La              3330          9050  3610 15990      0.767\n 7 Ang Mo Kio Tagore                  1940          4480  1530  7950      0.775\n 8 Ang Mo Kio Townsville              4190         11950  5100 21240      0.777\n 9 Ang Mo Kio Yio Chu Kang               0             0     0     0    NaN    \n10 Ang Mo Kio Yio Chu Kang East       1110          2410   750  4270      0.772\n\n\n\n\n\n\n\n2.3.2 Joining the aspatial data and geospatial data\nBefore we can perform the georelational join, we need to convert the values in PA and SZ fields to uppercase. This is because the values of PA and SZ fields are made up of upper- and lowercase. On the other, hand the SUBZONE_N and PLN_AREA_N are in uppercase. We will also retain those rows where ECONOMYACTIVE is more than 0 using filter().\n\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = funs(toupper)) %&gt;%\n  filter(ECONOMYACTIVE &gt; 0)\n\nThen we will use left_join() of dplyr to join the geospatial and aspatial data using planning subzone name (i.e. SUBZONE_N and SZ) as the common identifier.\n\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\n\n\n\n\n\n\nNote\n\n\n\nNote! The mpsz simple feature data frame is the left data table in the left_join. This is to ensure that the output will be a simple features data frame.\n\n\nThen we will write mpsz_pop2020 into an rds file for easy retrieval in future.\n\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07a.html#plotting-a-choropleth-map-quickly-using-qtm",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07a.html#plotting-a-choropleth-map-quickly-using-qtm",
    "title": "Take-home Exercise 7a: Choropleth Mapping with R",
    "section": "3.1 Plotting a choropleth map quickly using qtm()",
    "text": "3.1 Plotting a choropleth map quickly using qtm()\n\nStatic MapInteractive Map\n\n\n\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\n\n\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"view\")\nqtm(mpsz_pop2020, fill = \"DEPENDENCY\")\n\n\n\n\n\n\n\n\nThings to learn from the code chunks above:\ntmap_mode() with “plot” option is used to produce a static map. For interactive mode, “view” option should be used. fill argument is used to map the attribute (i.e. DEPENDENCY)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07a.html#creating-a-choropleth-map-using-tmaps-elements",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07a.html#creating-a-choropleth-map-using-tmaps-elements",
    "title": "Take-home Exercise 7a: Choropleth Mapping with R",
    "section": "3.2 Creating a choropleth map using tmap’s elements",
    "text": "3.2 Creating a choropleth map using tmap’s elements\n\n3.2.1 Drawing a base map\nThe basic building block of tmap is tm_shape() followed by one or more layer elemments such as tm_fill() and tm_polygons().\nIn the code chunk below, tm_shape() is used to define the input data (i.e mpsz_pop2020) and tm_polygons() is used to draw the planning subzone polygons.\n\n\nShow the code\ntmap_mode(\"plot\")\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\n\n\n\n\n\n\n3.2.2 Drawing a choropleth map using tm_polygons()\nTo draw a choropleth map showing the geographical distribution of a selected variable by planning subzone, we need to assign the target variable such as Dependency to tm_polygons().\n\n\nShow the code\ntm_shape(mpsz_pop2020) +\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe default interval binning used to draw the choropleth map is called “pretty”. A detailed discussion of the data classification methods supported by tmap will be provided in the following section.\nThe default colour scheme used is YlOrRd of ColorBrewer. We will learn how to change the colour scheme in the subsequent sections.\nBy default, missing values will be shaded in grey.\n\n\n\n\n\n3.2.3 Drawing a choropleth map using tm_fill() and tm_border()\ntm_polygons() is a wraper of tm_fill() and tm_border(). tm_fill() shades the polygons by using the default colour scheme and tm_borders() adds the borders of the shapefile onto the choropleth map.\nThe following code chunk draws a choropleth map using tm_fill() only.\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice from the above output, there are no borders when only tm_fill() is used.\n\n\nTo add the boundary of the planning subzones, tm_borders() will be used.\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1, alpha = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe added light grey border lines on the choropleth map using the above code chunk.\nThe alpha argument is used to define transparency number between 0 (totally transparent) and 1 (not transparent). By default, the alpha value of the colour used is 1.\nThere are three other arguments for tm_borders(): - col: border color - lwd: border line width. The default is 1. - lty: border line type. The default is “solid”.\n\n\n\n\n3.2.4 Data Classification Methods of tmap\nMost choropleth maps employ some methods of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\ntmap provides a total ten data classification methods, namely: fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, and jenks.\nWe can define a data classification method using the style argument of tm_fill() or tm_polygons().\n\n3.2.4.1 Plotting choropleth maps with built-in classification methods\n\nJenksEqualquantile\n\n\nThe following code chunk uses jenks data classification. We specify the number of classes to be broken into using n = 5 to group the observations into 5 classes.\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          n = 5, \n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\nThe following code chunk uses equal data classification. We specify the number of classes to be broken into using n = 5 to group the observations into 5 classes.\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          n = 5, \n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\nThe following code chunk uses quantile data classification. We specify the number of classes to be broken into using n = 5 to group the observations into 5 classes.\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          n = 5, \n          style = \"quantile\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n:::{.callout-note} From the above charts, we note that the distribution of different data classification methods lead to different visualisation output. For example, using the equal data classification method, it seemed that most areas had a low ratio except for one area due to it having a very dark shade of orange. In comparison, when the same data uses quantile or jenks data classification methods, the distribution seems more even.\n\n\n3.2.4.2 Plotting choropleth map with custom breaks\nFor all the built-in styles, the category breaks are computed internally. In order to override these defaults, the breakpoints can be set explicitly by means of the breaks argument to the tm_fill(). It is important to note that, in tmap the breaks include a minimum and maximum. As a result, in order to end up with n categories, n+1 elements must be specified in the breaks option (the values must be in increasing order).\nBefore we get started, it is always a good practice to get some descriptive statistics on the variable before setting the break points. Code chunk below will be used to compute and display the descriptive statistics of DEPENDENCY field.\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.0000  0.6519  0.7025  0.7742  0.7645 19.0000      92 \n\n\nBased on the above results, we set break point at 0.60, 0.70, 0.80, and 0.90. In addition, we also need to include a minimum and maximum, which we set at 0 and 100. Our breaks vector is thus c(0, 0.60, 0.70, 0.80, 0.90, 1.00).\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n3.2.5 Color Scheme\ntmap supports colour ramps either defined by the user or a set of predefined colour ramps from the RColorBrewer package.\n\n3.2.5.1 Using ColourBrewer Palette\nTo change the colour, we assign the preferred colour to palette argument of tm_fill() as shown in the code chunks below.\n\nUsing BluesUsing Reds\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\nTo reverse the colour shading, add a “-” prefix.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Reds\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.2.6 Map Layouts\nMap layout refers to the combination of all map elements into a cohensive map. Map elements include among others the objects to be mapped, the title, the scale bar, the compass, margins and aspects ratios. Colour settings and data classification methods covered in the previous section relate to the palette and break-points are used to affect how the map looks.\n\n3.2.6.1 Map Legend\nIn tmap, several legend options are provided to change the placement, format and appearance of the legend.In the following code chunk, we added a legend histogram using legend.hist = TRUE. adjusted the length height and width using legend.height and legend.width. We also specified the length position using legend.position.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Greens\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n3.2.6.2 Map Style\ntmap allows a wide variety of layout settings to be changed. They can be called by using tmap_style().\nThe following map uses the classic style.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Quantile classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.4, \n            legend.width = 0.35,)\n\n\n\n\n\n\n\n\n\n\n3.2.6.3 Cartographic Furniture\nBeside map style, tmap also also provides arguments to draw other map furniture such as compass, scale bar and grid lines.\nIn the code chunk below, tm_compass(), tm_scale_bar() and tm_grid() are used to add compass, scale bar and grid lines onto the choropleth map.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.4, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\nTo reset to the default style, we changed the style to white.\n\ntmap_style(\"white\")\n\n\n\n\n3.2.7 Drawing Multiple Small Choropleth Maps\nSmall multiple maps, also referred to as facet maps, are composed of many maps arrange side-by-side, and sometimes stacked vertically. Small multiple maps enable the visualisation of how spatial relationships change with respect to another variable, such as time.\nIn tmap, small multiple maps can be plotted in three ways: - by assigning multiple values to at least one of the asthetic arguments, - by defining a group-by variable in tm_facets(), and - by creating multiple stand-alone maps with tmap_arrange().\n\n3.2.7.1 By assigning multiple values to at least one of the aesthetic arguments\nIn the following code chunk, small multiple choropleth maps are created by defining ncols in tm_fill().\n\ntm_shape(mpsz_pop2020) + \n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\",\n          palette = \"Blues\") +  \n  tm_borders(alpha = 0.5) + \n  tm_layout(main.title = \"Distribution of Young and Old\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.4, \n            legend.width = 0.35,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = TRUE)\n\n\n\n\n\n\n\n\nIn the following example, multiple choropleth maps are created by assigning multiple values to at least one of the aesthetic arguments. We can also define different data classification and different colour schemes by assigning multiple values.\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons(c(\"DEPENDENCY\", \"AGED\"),\n              style = c(\"equal\", \"quantile\"),\n              palette = list(\"Blues\", \"Greens\")) +\n   tm_layout(main.title = \"Distribution of Young and Old\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.4, \n            legend.width = 0.35,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = TRUE)\n\n\n\n\n\n\n\n\n\n\n3.2.7.2 By defining a group-by variable in tm_facets()\nIn the following example, multiple choropleth maps are created using tm_facets().\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Greens\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=FALSE) +\n  tm_layout(main.title = \"Dependency Ratio by Regions\",\n            main.title.size = 1,\n            legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\n\n\n3.2.7.3 By creating multiple stand-alone maps with tmap_arrange()\nIn the following code chunk. multiple choropleth maps are created by first creating multiple stand-alone maps then arranged them together with tmap_arrange(). This is akin to how we use patchwork to arrange several ggplots together.\n\nyoungmap &lt;- tm_shape(mpsz_pop2020) +\n  tm_polygons(\"YOUNG\",\n              style = \"quantile\",\n              palette = \"Reds\") +\n   tm_layout(legend.height = 0.4, \n            legend.width = 0.35,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = TRUE)\n  \n\nagedmap &lt;- tm_shape(mpsz_pop2020) +\n  tm_polygons(\"AGED\",\n              style = \"quantile\",\n              palette = \"Reds\") +\n   tm_layout(legend.height = 0.4, \n            legend.width = 0.35,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = TRUE)\n\ntmap_arrange(youngmap, agedmap, asp = 1, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n3.2.8 Mapping Spatial Object Meeting a Selection Criterion\nInstead of creating small multiple choropleth map, you can also use selection function to map spatial objects meeting the selection criterion.\nIn the following code chunk, we define the selection using tm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ]) to only plot out the Central Region and its dependency ratio.\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07a.html#equal",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07a.html#equal",
    "title": "Take-home Exercise 7a: Choropleth Mapping with R",
    "section": "3.3 Equal",
    "text": "3.3 Equal\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          n = 5, \n          style = \"equal\") +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07a.html#quantile",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07a.html#quantile",
    "title": "Take-home Exercise 7a: Choropleth Mapping with R",
    "section": "3.4 quantile",
    "text": "3.4 quantile\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          n = 5, \n          style = \"quantile\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n3.4.0.1 Plotting choropleth map with custom breaks\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.0000  0.6519  0.7025  0.7742  0.7645 19.0000      92 \n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n3.4.1 Color Scheme\n\n3.4.1.1 Using ColourBrewer Palette\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Reds\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n3.4.2 Map Layouts\n\n3.4.2.1 Map Legend\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Greens\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n3.4.2.2 Map Style\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Quantile classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.4, \n            legend.width = 0.35,)\n\n\n\n\n\n\n\n\n\n\n3.4.2.3 Cartographic Furniture\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.4, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\n\ntmap_style(\"white\")\n\n\n\n\n3.4.3 Drawing Multiple Small Choropleth Maps\n\n3.4.3.1 By assigning multiple values to at least one of the aesthetic arguments\n\ntm_shape(mpsz_pop2020) + \n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\",\n          palette = \"Blues\") +  \n  tm_borders(alpha = 0.5) + \n  tm_layout(main.title = \"Distribution of Young and Old\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.4, \n            legend.width = 0.35,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = TRUE)\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons(c(\"DEPENDENCY\", \"AGED\"),\n              style = c(\"equal\", \"quantile\"),\n              palette = list(\"Blues\", \"Greens\")) +\n   tm_layout(main.title = \"Distribution of Young and Old\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.4, \n            legend.width = 0.35,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = TRUE)\n\n\n\n\n\n\n\n\n\n\n3.4.3.2 By defining a group-by variable in tm_facets()\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Greens\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=FALSE) +\n  tm_layout(main.title = \"Dependency Ratio by Regions\",\n            main.title.size = 1,\n            legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\n\n\n3.4.3.3 By creating multiple stand-alone maps with tmap_arrange()\n\nyoungmap &lt;- tm_shape(mpsz_pop2020) +\n  tm_polygons(\"YOUNG\",\n              style = \"quantile\",\n              palette = \"Reds\") +\n   tm_layout(legend.height = 0.4, \n            legend.width = 0.35,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = TRUE)\n  \n\nagedmap &lt;- tm_shape(mpsz_pop2020) +\n  tm_polygons(\"AGED\",\n              style = \"quantile\",\n              palette = \"Reds\") +\n   tm_layout(legend.height = 0.4, \n            legend.width = 0.35,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = TRUE)\n\ntmap_arrange(youngmap, agedmap, asp = 1, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n3.4.4 Mapping Spatial Object Meeting a Selection Criterion\nInstead of creating small multiple choropleth map, you can also use selection function to map spatial objects meeting the selection criterion.\nIn the following code chunk, we select the Central Region’s dependency ratio.\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07b.html",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07b.html",
    "title": "Hands-on Exercise 7b: Visualising Geospatial Point Data",
    "section": "",
    "text": "In this exercise, we will learn how to: - convert an aspatial data file into simple point feature data frame, and assign it an appropriate projection reference to the newly created simple point feature data frame. - plot interactive proportional symbol maps.\nWe will be creating a proportional symbol map showing the number of Group1 and Group 2 wins by Singapore Pools’ outlets using an R package called tmap.\n\n\n\n\n\n\nWhat is proportional symbol map?\n\n\n\nProportional symbol maps (also known as graduate symbol maps) are a class of maps that use the visual variable of size to represent differences in the magnitude of a discrete, abruptly changing phenomenon, e.g. counts of people. Like choropleth maps, you can create classed or unclassed versions of these maps. The classed ones are known as range-graded or graduated symbols, and the unclassed are called proportional symbols, where the area of the symbols are proportional to the values of the attribute being mapped."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07b.html#installing-and-loading-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07b.html#installing-and-loading-r-packages",
    "title": "Hands-on Exercise 7b: Visualising Geospatial Point Data",
    "section": "2.1 Installing and Loading R Packages",
    "text": "2.1 Installing and Loading R Packages\nFor this exercise, other than tmap, we will use the following packages:\n\ntidyverse for tidying and wrangling data\nsf for handling geospatial data\n\nThe code chunk below uses p_load() of pacman package to check if the abovementioned packages are installed in the computer. If they are, they will be launched in R. Otherwise, pacman will install the relevant packages before launching them.\n\n\nShow the code\npacman::p_load(tidyverse, tmap, sf)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07b.html#importing-the-data",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07b.html#importing-the-data",
    "title": "Hands-on Exercise 7b: Visualising Geospatial Point Data",
    "section": "2.2 Importing the Data",
    "text": "2.2 Importing the Data\nWe will be using the data set SGPools_svy21 for this exercise. The data is in csv file format.\nThe code chunk below uses read_csv() function of readr package to import SGPools_svy21.csv into R as a tibble data frame called sgpools.\n\n\nShow the code\nsgpools &lt;- read_csv(\"data/aspatial/SGPools_svy21.csv\")\nhead(sgpools)\n\n\n# A tibble: 6 × 7\n  NAME            ADDRESS POSTCODE XCOORD YCOORD `OUTLET TYPE` `Gp1Gp2 Winnings`\n  &lt;chr&gt;           &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;\n1 Livewire (Mari… 2 Bayf…    18972 30842. 29599. Branch                        5\n2 Livewire (Reso… 26 Sen…    98138 26704. 26526. Branch                       11\n3 SportsBuzz (Kr… Lotus …   738078 20118. 44888. Branch                        0\n4 SportsBuzz (Po… 1 Sele…   188306 29777. 31382. Branch                       44\n5 Prime Serangoo… Blk 54…   552542 32239. 39519. Branch                        0\n6 Singapore Pool… 1A Woo…   731001 21012. 46987. Branch                        3\n\n\nFrom the above output, we see that the sgpools dataset consists of seven columns. The XCOORD and YCOORD columns are the x-coordinates and y-coordinates of SingPools outlets and branches. They are in Singapore SVY21 Projected Coordinates System.\n\n\n\n\n\n\nObservations\n\n\n\n\nNotice that the sgpools data in tibble data frame and not the common R data frame.\nwe can also use list() instead of glimpse()."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07b.html#creating-an-sf-dataframe-from-an-aspatial-data-frame",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07b.html#creating-an-sf-dataframe-from-an-aspatial-data-frame",
    "title": "Hands-on Exercise 7b: Visualising Geospatial Point Data",
    "section": "3.1 Creating an sf dataframe from an aspatial data frame",
    "text": "3.1 Creating an sf dataframe from an aspatial data frame\nWe will convert sgpools data frame into a simple feature data frame using st_as_sf() of sf package.\n\n\nShow the code\nsgpools_sf &lt;- st_as_sf(sgpools, \n         coords = c(\"XCOORD\", \"YCOORD\"),\n         crs = 3414)\nsgpools_sf \n\n\nSimple feature collection with 306 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 7844.194 ymin: 26525.7 xmax: 45176.57 ymax: 47987.13\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 306 × 6\n   NAME                         ADDRESS POSTCODE `OUTLET TYPE` `Gp1Gp2 Winnings`\n * &lt;chr&gt;                        &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;\n 1 Livewire (Marina Bay Sands)  2 Bayf…    18972 Branch                        5\n 2 Livewire (Resorts World Sen… 26 Sen…    98138 Branch                       11\n 3 SportsBuzz (Kranji)          Lotus …   738078 Branch                        0\n 4 SportsBuzz (PoMo)            1 Sele…   188306 Branch                       44\n 5 Prime Serangoon North        Blk 54…   552542 Branch                        0\n 6 Singapore Pools Woodlands C… 1A Woo…   731001 Branch                        3\n 7 Singapore Pools 64 Circuit … Blk 64…   370064 Branch                       17\n 8 Singapore Pools 88 Circuit … Blk 88…   370088 Branch                       16\n 9 Singapore Pools Anchorvale … Blk 30…   540308 Branch                       21\n10 Singapore Pools Ang Mo Kio … Blk 20…   560202 Branch                       25\n# ℹ 296 more rows\n# ℹ 1 more variable: geometry &lt;POINT [m]&gt;\n\n\nFrom the above output, we noticed that: - a new column called geometry has been added into the data frame. - sgppols_sf is in point feature class. - its epsg ID is 3414.\n\n\n\n\n\n\nAbout the above code chunk\n\n\n\n\nthe coords = argument requires us to provide the column name of the x-coordinates first, followed by the column name of the y-coordinates.\nthe crs = argument requires us to provide the coordinates system in epsg format. From epsg.io, we know that EPSG: 3414 is Singapore SVY21 Projected Coordinate System."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07b.html#interactive-point-symbol-map",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07b.html#interactive-point-symbol-map",
    "title": "Hands-on Exercise 7b: Visualising Geospatial Point Data",
    "section": "4.1 Interactive point symbol map",
    "text": "4.1 Interactive point symbol map\n\ntmap_mode(\"view\")\ntm_basemap(\"CartoDB.Positron\") +\n  tm_shape(sgpools_sf) +\n  tm_bubbles(col = \"red\",\n           size = 1,\n           border.col = \"black\",\n           border.lwd = 1) +\n  tm_layout(title = \"Locations of SGPools Branches and Outlets\",\n    title.size = 1)\n\n\n\n\n\nTo draw a proportional symbol map, we need to assign a numerical variable to the size visual attribute. The code chunks below show that the variable Gp1Gp2 Winnings is assigned to size visual attribute.\n\ntm_basemap(\"CartoDB.Positron\") +\n  tm_shape(sgpools_sf) +\n  tm_bubbles(col = \"red\",\n           size = \"Gp1Gp2 Winnings\",\n           border.col = \"black\",\n           border.lwd = 1) +\n  tm_layout(title = \"Number of Gp1Gp2 Winnings\",\n    title.size = 1)\n\n\n\n\n\nThe proportional symbol map can be further improved using the colour visual attribute. In the code chunk below, OUTLET TYPE variable is used as the colour attribute variable, as such, Singapore Pools Branches and Outlets have different colour representation on the map.\n\ntm_basemap(\"CartoDB.Positron\") +\n  tm_shape(sgpools_sf) +\n  tm_bubbles(col = \"OUTLET TYPE\",\n           size = \"Gp1Gp2 Winnings\",\n           border.col = \"black\",\n           border.lwd = 1) +\n  tm_layout(title = \"Number of Gp1Gp2 Winnings by Outlet Type\",\n    title.size = 1)"
  }
]