[
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#loading-r-packages",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#loading-r-packages",
    "title": "Take-home Exercise 4: Prototyping Time Series Module for Visual Analytics Shiny Application",
    "section": "2.1 Loading R Packages",
    "text": "2.1 Loading R Packages\n\npacman::p_load(tidyverse, lubridate, gridExtra, readxl, knitr, data.table, ggplot2, forecast, MLmetrics, tsbox, xts, plotly, hrbrthemes, astsa, ggfortify)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#importing-data-into-r",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#importing-data-into-r",
    "title": "Take-home Exercise 4: Prototyping Time Series Module for Visual Analytics Shiny Application",
    "section": "2.2 Importing Data Into R",
    "text": "2.2 Importing Data Into R\nTalk about how we get this data: made use of data.gov.sg’s API\n\ndata &lt;- read_csv(\"data/daily_historical.csv\")\n\n\nglimpse(data)\n\nRows: 329,156\nColumns: 13\n$ station                  &lt;chr&gt; \"Macritchie Reservoir\", \"Macritchie Reservoir…\n$ year                     &lt;dbl&gt; 1980, 1980, 1980, 1980, 1980, 1980, 1980, 198…\n$ month                    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ day                      &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14…\n$ daily_rainfall_total     &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 22.6, 49.6, 2.4, 0.0, 0.0…\n$ highest_30_min_rainfall  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ highest_60_min_rainfall  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ highest_120_min_rainfall &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ mean_temperature         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ maximum_temperature      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ minimum_temperature      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ mean_wind_speed          &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ max_wind_speed           &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html",
    "title": "Take-home Exercise 2",
    "section": "",
    "text": "In this take-home exercise, I will be using the data visualisation design principles and best practices learnt in ISSS608 Lesson 1 and 2 to improve on my peer’s visualisations.",
    "crumbs": [
      "Take-home Exercises",
      "Take-home Exercise 2"
    ]
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#loading-r-packages",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#loading-r-packages",
    "title": "Take-home Exercise 2",
    "section": "2.1 Loading R packages",
    "text": "2.1 Loading R packages\nFor this exercise, we will be using the following packages:\n\ntidyverse : to load the core tidyverse packages, which includes ggplot2 and dplyr.\npatchwork: to create composition of ggplot2 plots using arithmetic operators.\nggrepel: to repel overlapping text labels away from each other.\nggdist: provides stats and geoms for visualising distributions and uncertainty.\nggridges: provides geoms to plot ridgeline plots, which are partially overlapping line plots that create the impression of a mountain range.\nknitr: provides a general-purpose tool for dynamic report generation in R. We will use this to mainly help us generate simple tables.\n\nThe code chunk below uses p_load() of pacman package to check if the abovementioned packages are installed in the computer. If they are, they will be launched in R. Otherwise, pacman will install the relevant packages before launching them.\n\n\nShow the code\npacman::p_load(tidyverse, haven, patchwork, ggrepel, ggdist, ggridges, knitr, hrbrthemes)",
    "crumbs": [
      "Take-home Exercises",
      "Take-home Exercise 2"
    ]
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#importing-data-into-r",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#importing-data-into-r",
    "title": "Take-home Exercise 2",
    "section": "2.2 Importing Data into R",
    "text": "2.2 Importing Data into R\nFor this exercise, we are using PISA 2022 database’s student questionnaire data file, which is the same dataset as Take-home exercise 1. As we have already filtered out the PISA data from Singapore and saved it in rds file previously in Take-home exercise 1, let us import the rds file using the following code chunk.\n\n\nShow the code\nstu_qqq_SG &lt;- read_rds(\"data/stu_qqq_SG.rds\")",
    "crumbs": [
      "Take-home Exercises",
      "Take-home Exercise 2"
    ]
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#chart-1-comparison-of-scores-amongst-students",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#chart-1-comparison-of-scores-amongst-students",
    "title": "Take-home Exercise 2",
    "section": "4.1 Chart 1: Comparison of Scores Amongst Students",
    "text": "4.1 Chart 1: Comparison of Scores Amongst Students\nThe following charts were from the original submission.\n\n\n\nChart 1 from Original Author\n\n\n\n4.1.1 Critique\nWhat is the message\n\nBefore I critique, I shall attempt to deduce the author’s message for this chart so that we can make recommendations to improve the chart based on the intended message.\nI think the author was trying to find out if there was any difference in student scores between subjects since a notched box plot was plotted for each subject’s scores.\n\nAesthetics\n\nThe title could be more informative. We can also add a subtitle to provide more information on the sample size.\nWe can put the 3 boxplots side by side for easy comparison.\n\nClarity\n\nThe author used notched box plots to give a summary of the minimum, maximum, median and interquartile range of Math, Reading and Science scores for Singapore students. The box plots also indicated “outliers” with those black dots at the end. However, since the message was on the difference in scores and not on the outliers, we can choose to remove these outliers to reduce ink.\nIn the original chart the notches of the box plots were not very obvious. This could be due to the size of the boxplots being very wide, making it hard to see the notches. Also, these charts were placed in different tabs, making it hard for readers to see if there was a difference in scores between subjects. It might be easier for readers to compare if these plots were placed side by side.\nIn the original chart, there was a red dot for the mean scores of each subject. However, there was no text labels so the readers have to try to interpret and gauge the mean value based on the y-axis. It would be more useful if it has text annotation so that readers can easily read off the mean value.\nOne of the pitfalls of box plot is that it does not show the distribution of the data well. It hides multimodality and other features of distributions. Hence, we can also consider alternatives such as violin plot.\n\n\n\n4.1.2 Proposed Sketch\n\nAs having a clear message for the plot was the first step of of data visualisation design process, I shall assume that the author wanted to show if there was any difference in scores between the different subjects (Maths, Reading and Science) among Singapore students.\nIf we want to show difference in scores between subjects, we can consider the following plots:\n\n\n\n\nSuggestions for Chart 1\n\n\nFirstly, for all the options, I proposed to put the scores for all 3 subjects side by side. Also, rather that stitching 3 different plots together (i.e. 1 plot for 1 subject) using patchwork, I suggested to put all three plots together so that they can be on the same axes and easier for comparison.\nThere are pros and cons for the above 3 options:\n\nViolin plot: shows the distribution and summary stats of the data. However, certain parts of it are a bit redundant because half of the violin would already reveal the distribution. So a violin plot might not be that efficient use of space.\nRidge plot: This brings us to ridge plot, which uses “half” of the violin plot and we rotated the “half-violin” to be horizontal, giving us the image of “ridges”. Ridge plot also can show distribution of the data and makes good use of the space. We can add annotations (e.g. lines and texts) to show the median and mean. However, ridge plot might be better for situations where we have medium to high number of groups to represent. In this case, we have less than 5 groups, so we might want to consider other distribution plots.\nRaincloud plot: We can further enhance the ridge plot and make it into a raincloud plot by adding boxplot and dotplot. This will also show us the distribution and summary statistics of the data.\n\nAfter weighing the above pros and cons, I decided to change the box plots to raincloud plot. Below is how I imagined the plot to look like with a title and subtitle:\n\n\n\nFinalised Suggestion for Chart 1\n\n\n\n\n4.1.3 Makeover Design\nWe will first prepare the data by selecting the columns we need (i.e., student ID, Math, Reading and Science scores) then pivot the table longer using pivot_longer() so that we can have all three subjects in 1 chart.\n\n\nShow the code\nstu1 &lt;- stu %&gt;%\n  select(CNTSTUID, MATH, READ, SCI) %&gt;%\n  pivot_longer(!CNTSTUID, names_to = \"Subject\")\n\n\nUsing the data prepared stu1, we will plot the raincloud plot using the following code chunk.\n\n\nShow the code\nggplot(stu1, \n       aes(x = Subject, \n           y = value,\n           color = Subject)) + \n  stat_halfeye(adjust = 0.5,\n               width = 0.5,\n               justification = -0.1, \n               .width = 0, \n               point_color = NA) +\n  geom_boxplot(width = 0.1, outlier.shape = NA) +\n  stat_dots(side = \"left\",\n            justification = 1.1,\n            binwidth = 0.5,\n            dotsize = 2) + \n  coord_flip() + theme_minimal()+\n  stat_summary(fun = median, geom = \"text\", aes(label = paste(\"median:\", round(after_stat(y), 0))),\n               position = position_nudge(x=0.05), vjust=-0.5, size = 2.5, color = \"black\")+\n  stat_summary(fun=mean, geom =\"text\", aes(label = paste(\"mean: \", round(after_stat(y), 0))), position = position_nudge(x = 0.25), vjust = 4, color= \"black\", size = 2.5)+ \n  stat_summary(fun = mean, geom = \"point\", shape = 16, size = 2, color = \"black\",\n               position = position_nudge(x = 0.0)) +\n    stat_summary(fun = min, geom = \"text\", aes(label = paste(round(after_stat(y), 0))),\n               position = position_nudge(y=-0.25), vjust=-0.5, size = 2, color = \"grey40\")+\n  stat_summary(fun = max, geom = \"text\", aes(label = paste(round(after_stat(y), 0))),\n               position = position_nudge(y=-0.25), vjust=-0.5, size = 2, color = \"grey40\")+\n  labs(title = \"Although the scores varied widely for each subject, \\nthe difference in median and mean scores were small.\", subtitle = \"Number of observations: 6606\", caption = \"Data from PISA 2022\") + \n  xlab(\"SUBJECT\") + \n  ylab(\"SCORES\") + \n  theme_ipsum_rc(plot_title_size = 15, plot_title_margin=4, subtitle_size=12, subtitle_margin=4, axis_text_size=8, axis_title_face= \"bold\", plot_margin = margin(10,10,10,10)) + \n  theme(legend.position = \"none\")",
    "crumbs": [
      "Take-home Exercises",
      "Take-home Exercise 2"
    ]
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#chart-2-comparison-of-scores-between-gender",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#chart-2-comparison-of-scores-between-gender",
    "title": "Take-home Exercise 2",
    "section": "4.2 Chart 2: Comparison of Scores Between Gender",
    "text": "4.2 Chart 2: Comparison of Scores Between Gender\nThe following charts were from the original submission.\n\n\n\nChart 2 from Original Author\n\n\n\n4.2.1 Critique\nWhat is the message\n\nThe original author seemed to want to see if there are any differences in subject scores between gender.\n\nAesthetics\n\nThe title could be more informative. We can also add a subtitle to provide information on the sample size.\nWe can put the 3 boxplots side by side for easy comparison.\n\nClarity\n\nNotched boxplots were used to display the difference in subject scores between gender.\nSimilar to the first chart, the notches of the box plots were not obvious, possibly due to the size of the boxplots being very wide. Also, these charts were in different tabs, making it hard for readers to see if there was a difference in subject scores between genders. It would be easier for readers to compare if these plots were placed side by side.\nThere was a red dot on each boxplot to indicate the mean scores of each subject and gender but it would be more useful if it has text annotation so that readers can easily read off the mean value.\n\n\n\n4.2.2 Proposed Sketch\nThere are several options to display if there was any difference in scores. I thought we can use either:\n\nOption 1: use ridge plot to plot all the subject scores for both gender plotted on 1 chart,\nOption 2: use ridge plot and box plot (i.e. raincloud plot without the “rain”) to display the scores between genders for each subject in separate charts, then make use of patchwork to combine the 3 charts into 1 for ease of comparison.\n\n\n\n\nSuggestions for Chart 2\n\n\nI suggested ridge plot and put all 6 subject scores (i.e. 3 subjects for each gender) into 1 plot so that we can easily compare differences between gender and also within gender. This chart would easily allow us to know which gender performed well for each subject. However, ridge plot does not have information on the summary statistics so I came up with option 2 (i.e. ridge plot and box plot) since ggridges allow us to have such plots too.\nFor option 2, I separated the charts by subjects to test out if it can display the same message effectively. I realised that having a separate chart for each subject helps us to know which gender does well for a particular subject, but it does not allow us to have an “overview” which gender performed better overall.\nI did not suggest raincloud plot because if there are 6 subject scores in 1 plot, it might be too messy to have so many dots on the plot.\nThere are pros and cons for each option.\nFor option 1, having the scores for all subjects and gender in 1 plot means that we will have 6 distributions in 1 plot. It might be easy to pick out any obvious differences. However it might be too overwhelming since there are 6 “ridges” to look and compare at 1 shot, or some readers might not know where to start. So if we use option 1, we would have to have clear annotations and titles to guide the readers.\nFor option 2, it splits the plots by subject so that we can see if there are any differences in scores between gender for each subject. However, we might not be able easily compare differences in scores between subject for each gender (e.g. to find out if female students performed better in Reading as opposed to Maths).\nAfter weighing the above pros and cons, I decided to change the box plots to combine option 1 and 2 ideas. So we will plot ridge plots and box plots for gender and subjects into 1 plot. This is how it looks like:\n\n\n\nFinalised Suggestion for Chart 2\n\n\n\n\n4.2.3 Makeover Design\nLet us prepare the data for this chart. We will first select the columns that we want, then pivot_longer() so that we can plot all the subjects scores for both gender into 1 plot. We then concatenated the Gender and Subject columns into 1 column so that we can use this new column Gen_Sub to lcreate a chart for each Gender’s Subject.\n\n\nShow the code\nstu$CNTSTUID &lt;- as.factor(stu$CNTSTUID)\n\nstu2 &lt;- stu %&gt;%\n  select(CNTSTUID, Gender, MATH, READ, SCI) %&gt;%\n  pivot_longer(cols = MATH:SCI) %&gt;%\n  rename(\"Subject\" = \"name\",\n         \"Scores\" = \"value\")\n\nstu3 &lt;- stu2 %&gt;%\n  mutate(GENDER = Gender) %&gt;%\n  unite(Gen_Sub, c(Gender, Subject))\n\nstu3$GENDER &lt;- factor(stu3$GENDER, levels = c(\"Male\", \"Female\"))\n\n\nThe following code chunk plots out the\n\n\nShow the code\nggplot(stu3, \n       aes(x = Scores, \n           y = Gen_Sub, \n           color = GENDER)) + \n  stat_halfeye(expand = TRUE) +\n  stat_summary(fun = median, geom = \"text\", aes(label = paste(\"median:\", round(after_stat(x), 0))),\n               position = position_nudge(y=0.15), vjust=-0.5, size = 2.3, color = \"black\")+\n  stat_summary(fun=mean, geom =\"text\", aes(label = paste(\"mean: \", round(after_stat(x), 0))), \n               position = position_nudge(y =-0.2), vjust = 0.5, color= \"black\", size = 2.3)+ \n  stat_summary(fun = mean, geom = \"point\", shape = 16, size = 2, color = \"black\",\n               position = position_nudge(x = 0.0)) +\n  stat_summary(fun = min, geom = \"text\", aes(label = paste(round(after_stat(x), 0))),\n               position = position_nudge(y=-0.25), vjust=-0.5, size = 2, color = \"grey60\")+\n  stat_summary(fun = max, geom = \"text\", aes(label = paste(round(after_stat(x), 0))),\n               position = position_nudge(y=-0.25), vjust=-0.5, size = 2, color = \"grey60\")+\n  labs(title = \"Scores varied widely for each gender across subjects.\\nTop performers for each subject came from male students.\", subtitle = \"Number of observations: 6606\", caption = \"Data from PISA 2022\") + \n  ylab(\"GENDER & SUBJECT\") + \n  xlab(\"SCORES\") + \n  theme_ipsum_rc(plot_title_size = 15, plot_title_margin=4, subtitle_size=10, subtitle_margin=4, axis_text_size=8, axis_title_face= \"bold\", grid_col= \"grey\", plot_margin = margin(10,10,10,10))  +\n    scale_color_manual(breaks = c(\"Male\", \"Female\"),\n                    values=c(\"blue\", \"violetred1\"))",
    "crumbs": [
      "Take-home Exercises",
      "Take-home Exercise 2"
    ]
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#chart-3-comparison-of-scores-between-schools",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#chart-3-comparison-of-scores-between-schools",
    "title": "Take-home Exercise 2",
    "section": "4.3 Chart 3: Comparison of Scores Between Schools",
    "text": "4.3 Chart 3: Comparison of Scores Between Schools\nThe following charts were from the original submission.\n\n\n\nChart 3 by Original Author\n\n\n\n4.3.1 Critique\nWhat is the message\n\nThe original author seemed to want to see if there are any differences in subject scores between schools.\n\nAesthetics\n\nSimilarly, the title could be more informative. We can also add a subtitle to provide information on the number of schools.\nWe can put the 3 boxplots side by side for easy comparison.\n\nClarity\n\nIn the original work, the students’ scores were averaged to determine the school’s score.\nBased on the original author’s interpretation, it seemed that if a school’s score was within the box plot and not an outlier, the author interpreted the school as not having a wide disparity between students.\nHowever, we cannot interpret this way due to how the school’s score was being calculated. Since the school score was derived by averaging the students’ scores and arithmetic averages were affected by extreme values, a school with non-outlier scores does not mean that all its students performed well. This is because the school might have some students with very high scores to make up for the under-performing students.\nAs such, when remaking the chart, I would find out the difference between the top score and bottom score for each school. By doing so we get the range of the students’ scores for each school. Then we will compare each school’s range to determine if the school is doing “well”. A wider range means the school’s students had a wider disparity which could warrant further exploration and investigation.\nIn addition, the author chose notched box plots to compare the schools’ scores for different subjects. These plots were also in different tabs, making it hard to compare across subjects.\nThere was a red dot for the average school score but it would be more useful if it has text annotation so that readers can easily read off the value.\n\n\n\n4.3.2 Proposed Sketch\nSince the message that I wanted this chart to show was each school’s range of scores for each subject, I have the following suggestions.\n\n\n\nSuggestions for Chart 3\n\n\n\nFor this chart, I think the tricky bit is that there are many schools (~164 schools) to plot out. Initially, I wanted to do a dumbbell plot (option 1) for each school. So each dot would show to top score and bottom score for each school and the line would show the difference between the top scores and bottom scores. But I realised that it would be quite messy and too much information (i.e., top score, lowest score, difference in scores) in the chart when there are more than 160 schools to plot out.\nHence, I came up with Options 2 and 3. In options 2 and 3, we can use the height of the bar or ‘lollipop’ would show the difference in the top and bottom scores for each school. We can also add a threshold so that we know which schools have a wide range and which schools have students performing at similar levels. For this exercise, we set an arbitrary threshold of 250.\nThe difference between barplot and lollipop plot is that lollipop plot uses less ink so people can focus on the height of the “dot”.\n\nAfter considering the various strengths and weaknesses of the various plots, I decided to remake the original chart into a lollipop plot and this is how I planned for it to look like:\n\n\n\nFinalised Suggestion for Chart 3\n\n\n\n\n4.3.3 Makeover Design\nLet us first prepare the data for each subject. When preparing the data, I removed the “70200” in the school IDs (i.e. CNTSCHID) because it is present in every school IDs and they would clutter and take up extra space when we plot the school IDs on the axis.\n\n\nShow the code\nstu_math_min &lt;- stu %&gt;%\n  group_by(CNTSCHID) %&gt;%\n  summarise_at(vars(MATH),\n               list(min_math = min))\n\nstu_math_max &lt;- stu %&gt;%\n  group_by(CNTSCHID) %&gt;%\n  summarise_at(vars(MATH),\n               list(max_math = max))\n\nstu_math &lt;- full_join(stu_math_max, stu_math_min,\n             by = \"CNTSCHID\")\n\nstu_math$diff &lt;- stu_math$max_math - stu_math$min_math\n\nstu_math$CNTSCHID &lt;- substr(stu_math$CNTSCHID, 6, 8)\n\nstu_math &lt;- stu_math %&gt;% \n  mutate(mycolor = ifelse(diff&gt;250, \"red\", \"blue\"))%&gt;%\n   arrange(diff) %&gt;%    \n  mutate(CNTSCHID=factor(CNTSCHID, levels=CNTSCHID)) \n\nstu_read_min &lt;- stu %&gt;%\n  group_by(CNTSCHID) %&gt;%\n  summarise_at(vars(READ),\n               list(min_read = min))\n\nstu_read_max &lt;- stu %&gt;%\n  group_by(CNTSCHID) %&gt;%\n  summarise_at(vars(READ),\n               list(max_read = max))\n\nstu_read &lt;- full_join(stu_read_max, stu_read_min,\n             by = \"CNTSCHID\")\n\nstu_read$diff &lt;- stu_read$max_read - stu_read$min_read\n\nstu_read$CNTSCHID &lt;- substr(stu_read$CNTSCHID, 6, 8)\n\nstu_read &lt;- stu_read %&gt;% \n  mutate(mycolor = ifelse(diff&gt;250, \"red\", \"blue\"))%&gt;%\n   arrange(diff) %&gt;%    \n  mutate(CNTSCHID=factor(CNTSCHID, levels=CNTSCHID)) \n\nstu_sci_min &lt;- stu %&gt;%\n  group_by(CNTSCHID) %&gt;%\n  summarise_at(vars(SCI),\n               list(min_sci = min))\n\nstu_sci_max &lt;- stu %&gt;%\n  group_by(CNTSCHID) %&gt;%\n  summarise_at(vars(SCI),\n               list(max_sci = max))\n\nstu_sci &lt;- full_join(stu_sci_max, stu_sci_min,\n             by = \"CNTSCHID\")\n\nstu_sci$diff &lt;- stu_sci$max_sci - stu_sci$min_sci\n\nstu_sci$CNTSCHID &lt;- substr(stu_sci$CNTSCHID, 6, 8)\n\nstu_sci &lt;- stu_sci %&gt;% \n  mutate(mycolor = ifelse(diff&gt;250, \"red\", \"blue\"))%&gt;%\n   arrange(diff) %&gt;%    \n  mutate(CNTSCHID=factor(CNTSCHID, levels=CNTSCHID))  \n\n\nThe following code chunk plots out the individual charts for each subject. We will save each subject’s chart as a variable so that we can stitch them together later.\nWhile plotting these charts out,\n\nI rotated the charts using coord_flip() because I realised that it is easier to compare across subjects for each schools when they are in a vertical format.\nI added an abline using geom_hline to make the threshold of 250 points obvious.\nI also removed the plot background colour and grid lines because of the lines in the lollipop charts.\n\n\n\nShow the code\np1 &lt;- ggplot(stu_math, aes(x=CNTSCHID, y = diff)) +\n  geom_segment(aes(x=CNTSCHID, xend=CNTSCHID, y = 250, yend=diff), color = \"grey\") +\n  geom_point(color=stu_math$mycolor, size = 2) +\n  geom_hline(yintercept = 250, color=\"orange\",linewidth = 0.6)+\n  xlab(\"SchoolID\") +\n  ylab(\"Difference in Math Scores\") + coord_flip()+\n  theme(panel.background = element_rect(fill = \"white\", colour = \"grey80\"))\n\n\np2 &lt;- ggplot(stu_read, aes(x=CNTSCHID, y = diff)) +\n  geom_segment(aes(x=CNTSCHID, xend=CNTSCHID, y = 250, yend=diff), color = \"grey\") +\n  geom_point(color=stu_read$mycolor, size = 2) +\n  geom_hline(yintercept = 250, color=\"orange\",linewidth = 0.6)+\n  xlab(\"SchoolID\") +\n  ylab(\"Difference in Reading Scores\") + coord_flip() +\n  theme(panel.background = element_rect(fill = \"white\", colour = \"grey80\"))\n\np3 &lt;- ggplot(stu_sci, aes(x=CNTSCHID, y = diff)) +\n  geom_segment(aes(x=CNTSCHID, xend=CNTSCHID, y = 250, yend=diff), color = \"grey\") +\n  geom_point(color=stu_sci$mycolor, size = 2) +\n  geom_hline(yintercept = 250, color=\"orange\",linewidth = 0.6)+\n  xlab(\"SchoolID\") +\n  ylab(\"Difference in Science Scores\") + coord_flip() + \n  theme(panel.background = element_rect(fill = \"white\", colour = \"grey80\"))\n\n\nWe then stitch the individual charts together using patchwork and added title, subtitle and caption and a theme using plot_annotation().\n\n\nShow the code\npatch1 &lt;- p1 + p2 + p3 \n\npatch1 + plot_annotation(\n  title = \"Most of the schools had a wide disparity between their top scorers and \\nbottom scorers, exceeding the threshold of 250 points.\",\n  subtitle = \"164 Schools participated in study.\",\n  caption = \"Data from PISA 2022\", theme = theme_ipsum_rc())",
    "crumbs": [
      "Take-home Exercises",
      "Take-home Exercise 2"
    ]
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#chart-4-comparison-of-scores-and-students-socioeconomic-status",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#chart-4-comparison-of-scores-and-students-socioeconomic-status",
    "title": "Take-home Exercise 2",
    "section": "4.4 Chart 4: Comparison of Scores and Students’ Socioeconomic Status",
    "text": "4.4 Chart 4: Comparison of Scores and Students’ Socioeconomic Status\n\n4.4.1 Critique\nThe following charts were from the original submission.\n\n\n\nChart 4 by Original Author\n\n\nWhat is the message\n\nThe original author seemed to want to see if there are any relationship between subject scores and socioeconomic status (ESCS).\n\nAesthetics\n\nSimilarly, the title could be more informative. We can also add a subtitle to provide information on the number of observations, or additional information about ESCS since it is an index.\nInstead of putting the charts in different tabs, we can put the 3 charts side by side for easy comparison.\n\nClarity\n\nThe author used scatter plot to show the relationship between socioeconomic status (ESCS) and subject scores. A line of best fit was also added to show the relationship between ESCS and subject scores, which was useful to illustrate the relationship.\nHowever, because there are many data points (we have more than 6000 observations), the scatter plots were too dense to interpret.\n\n\n\n4.4.2 Proposed Sketch\nAs such, we suggest the following options:\nOption 1: Hexbin plot to overcome the large number of data points by binning them. A hexbin plot can show the density of data points (using the colour gradient) and help to identify patterns and outliers in the data.\nOption 2: Ridge plot by binning the scores into “Proficiency levels”. Binning the scores makes it easier to identify data clusters and depict patterns. If the higher (i.e. better) proficiency levels tend to fall within the higher ESCS scores, then there could be a relationship between scores and socioeconomic status.\n\n\n\nSuggestions for Chart 4\n\n\nI decided to bind the scores into proficiency levels and plot ridgeplots to compare the students’ proficiency levels across ESCS. The ridgeplots could give a clear overview if ESCS and scores are related by checking if the “ridges” are skewed in a certain direction. This is how I imagined the revamped chart would look like:\n\n\n\nFinalised Suggestion for Chart 4\n\n\n\n\n4.4.3 Makeover Design\nFirst, we will prepare the data by:\n\nbinning the subject scores into various proficiency levels; and\ndefining the factor levels for the subject’s proficiency levels.\n\n\n\nShow the code\nstu_math_b &lt;- stu %&gt;%\n  select(CNTSTUID, MATH, ESCS) \n\nstu_math_b$MATHLevel[stu_math_b$MATH &gt;669.30] &lt;- \"6\"\nstu_math_b$MATHLevel[stu_math_b$MATH &gt; 606.99 & stu_math_b$MATH &lt;= 669.30] &lt;- \"5\"\nstu_math_b$MATHLevel[stu_math_b$MATH &gt; 544.68 & stu_math_b$MATH &lt;= 606.99] &lt;- \"4\"\nstu_math_b$MATHLevel[stu_math_b$MATH &gt; 482.38 & stu_math_b$MATH &lt;= 544.68] &lt;- \"3\"\nstu_math_b$MATHLevel[stu_math_b$MATH &gt; 420.38 & stu_math_b$MATH &lt;= 482.38] &lt;- \"2\"\nstu_math_b$MATHLevel[stu_math_b$MATH &gt; 357.77 & stu_math_b$MATH &lt;= 420.38] &lt;- \"1a\"\nstu_math_b$MATHLevel[stu_math_b$MATH &gt; 295.47 & stu_math_b$MATH &lt;= 357.77] &lt;- \"1b\"\nstu_math_b$MATHLevel[stu_math_b$MATH &gt; 233.17 & stu_math_b$MATH &lt;= 295.47] &lt;- \"1c\"\n\nstu_math_b$MATHLevel&lt;- factor(stu_math_b$MATHLevel, levels=c('1c', '1b', '1a', '2', '3', '4', '5', '6'))\n\nstu_read_b &lt;- stu %&gt;%\n  select(CNTSTUID, READ, ESCS) \n\nstu_read_b$READLevel[stu_read_b$READ &gt; 698.32] &lt;- \"6\"\nstu_read_b$READLevel[stu_read_b$READ &gt; 625.61 & stu_read_b$READ &lt;= 698.32] &lt;- \"5\"\nstu_read_b$READLevel[stu_read_b$READ &gt; 552.89 & stu_read_b$READ &lt;= 625.61] &lt;- \"4\"\nstu_read_b$READLevel[stu_read_b$READ &gt; 480.18 & stu_read_b$READ &lt;= 552.89] &lt;- \"3\"\nstu_read_b$READLevel[stu_read_b$READ &gt; 407.47 & stu_read_b$READ &lt;= 480.18] &lt;- \"2\"\nstu_read_b$READLevel[stu_read_b$READ &gt; 334.75 & stu_read_b$READ &lt;= 407.47] &lt;- \"1a\"\nstu_read_b$READLevel[stu_read_b$READ &gt; 262.04 & stu_read_b$READ &lt;= 334.75] &lt;- \"1b\"\nstu_read_b$READLevel[stu_read_b$READ &gt; 189.33 & stu_read_b$READ &lt;= 262.04] &lt;- \"1c\"\nstu_read_b$READLevel[stu_read_b$READ &lt; 189.33] &lt;- \"0\"\n\nstu_read_b$READLevel&lt;- factor(stu_read_b$READLevel, levels=c('0', '1c', '1b', '1a', '2', '3', '4', '5', '6'))\n\nstu_sci_b &lt;- stu %&gt;%\n  select(CNTSTUID, SCI, ESCS) \n\nstu_sci_b$SCILevel[stu_sci_b$SCI &gt; 707.93] &lt;- \"6\"\nstu_sci_b$SCILevel[stu_sci_b$SCI &gt; 633.33 & stu_sci_b$SCI &lt;= 707.93] &lt;- \"5\"\nstu_sci_b$SCILevel[stu_sci_b$SCI &gt; 558.73 & stu_sci_b$SCI &lt;= 633.33] &lt;- \"4\"\nstu_sci_b$SCILevel[stu_sci_b$SCI &gt; 484.14 & stu_sci_b$SCI &lt;= 558.73] &lt;- \"3\"\nstu_sci_b$SCILevel[stu_sci_b$SCI &gt; 409.54 & stu_sci_b$SCI &lt;= 484.14] &lt;- \"2\"\nstu_sci_b$SCILevel[stu_sci_b$SCI &gt; 334.94 & stu_sci_b$SCI &lt;= 409.54] &lt;- \"1a\"\nstu_sci_b$SCILevel[stu_sci_b$SCI &gt; 260.54 & stu_sci_b$SCI &lt;= 334.94] &lt;- \"1b\"\nstu_sci_b$SCILevel[stu_sci_b$SCI &gt; 185.94 & stu_sci_b$SCI &lt;= 260.54] &lt;- \"1c\"\nstu_sci_b$SCILevel[stu_sci_b$SCI &lt; 185.94] &lt;- \"0\"\n\nstu_sci_b$SCILevel&lt;- factor(stu_sci_b$SCILevel, levels=c('0', '1c', '1b', '1a', '2', '3', '4', '5', '6'))\n\n\nWith the prepared data, we will plot 1 chart for each subject and save each chart as a variable. When working on the chart, I realised that the stat_halfeye() from ggdist (i.e., a raincloud plot without the ‘rain’) is better than ridge chart from ggridge because it shows density with a distribution curve and interval with a boxplot.\n\n\nShow the code\ne1 &lt;- ggplot(stu_math_b, \n       aes(x = ESCS, \n           y = MATHLevel,\n           color = MATHLevel)) + \nstat_halfeye(expand = TRUE) +\n  stat_summary(fun = median, geom = \"text\", aes(label = paste(\"median:\", round(after_stat(x), 1))),\n               position = position_nudge(y=0.15), vjust=-0.5, size = 2.3, color = \"black\")+\n  stat_summary(fun=mean, geom =\"text\", aes(label = paste(\"mean: \", round(after_stat(x), 1))), \n               position = position_nudge(y =-0.2), vjust = 0.5, color= \"black\", size = 2.3)+ \n  stat_summary(fun = mean, geom = \"point\", shape = 16, size = 2, color = \"black\",\n               position = position_nudge(x = 0.0)) +\n  labs(title = \"Maths\")+\n  ylab(\"Proficiency Level\") + \n  xlab(\"ESCS\") + \n  theme_ipsum_rc(plot_margin = margin(4, 4, 4, 4))  \n\n\ne2 &lt;- ggplot(stu_read_b, \n       aes(x = ESCS, \n           y = READLevel,\n           color = READLevel)) + \nstat_halfeye(expand = TRUE) +\n  stat_summary(fun = median, geom = \"text\", aes(label = paste(\"median:\", round(after_stat(x), 1))),\n               position = position_nudge(y=0.15), vjust=-0.5, size = 2.3, color = \"black\")+\n  stat_summary(fun=mean, geom =\"text\", aes(label = paste(\"mean: \", round(after_stat(x), 1))), \n               position = position_nudge(y =-0.2), vjust = 0.5, color= \"black\", size = 2.3)+ \n  stat_summary(fun = mean, geom = \"point\", shape = 16, size = 2, color = \"black\",\n               position = position_nudge(x = 0.0)) +\n  labs(title = \"Reading\")+\n  ylab(\"Proficiency Level\") + \n  xlab(\"ESCS\") + \n  theme_ipsum_rc(plot_margin = margin(4, 4, 4, 4)) \n\n\ne3 &lt;- ggplot(stu_sci_b, \n       aes(x = ESCS, \n           y = SCILevel,\n           color = SCILevel)) + \nstat_halfeye(expand = TRUE) +\n  stat_summary(fun = median, geom = \"text\", aes(label = paste(\"median:\", round(after_stat(x), 1))),\n               position = position_nudge(y=0.15), vjust=-0.5, size = 2.3, color = \"black\")+\n  stat_summary(fun=mean, geom =\"text\", aes(label = paste(\"mean: \", round(after_stat(x), 1))), \n               position = position_nudge(y =-0.2), vjust = 0.5, color= \"black\", size = 2.3)+ \n  stat_summary(fun = mean, geom = \"point\", shape = 16, size = 2, color = \"black\",\n               position = position_nudge(x = 0.0)) +\n  labs(title = \"Science\")+\n  ylab(\"Proficiency Level\") + \n  xlab(\"ESCS\") + \n  theme_ipsum_rc(plot_margin = margin(4, 4, 4, 4)) \n\n\nWe will now use patchwork to stitch these 3 charts into 1 plot. Then, we add title, subtitle, caption and a theme to the plot.\n\n\nShow the code\npatch2 &lt;- e1 + e2 + e3 \n\npatch2 + plot_annotation(\n  title = \"Students with higher Proficiency Levels seemed to have higher ESCS index.\",\n  subtitle = \"Number of observations: 6606.\\nNote: The higher the value of ESCS, the higher the socio-economic status.\", caption = \"Data from PISA 2022\") & theme_ipsum_rc(plot_margin = margin(4, 4, 4, 4), grid_col = \"azure2\", axis_text_size = 9) & theme(legend.position = \"none\")",
    "crumbs": [
      "Take-home Exercises",
      "Take-home Exercise 2"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SiHui Learns Visual Analytics!",
    "section": "",
    "text": "Welcome to SiHui Learns Visual Analytics! This website documents my learning journey for ISSS608 Visual Analytics and Applications course. Hopefully I can surf through this course smoothly like this penguin!"
  },
  {
    "objectID": "index.html#hands-on-exercises",
    "href": "index.html#hands-on-exercises",
    "title": "SiHui Learns Visual Analytics!",
    "section": "Hands-On Exercises",
    "text": "Hands-On Exercises\n\n\n\n\n\n\n\n\nHands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods\n\n\n\n\n\n\nGoh Si Hui\n\n\nJan 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHands-On Exercise 2: Beyond ggplot2 Fundamentals\n\n\n\n\n\n\nGoh Si Hui\n\n\nJan 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHands-On Exercise 3a: Programming Interactive Data Visualisation with R\n\n\n\n\n\n\nGoh Si Hui\n\n\nJan 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHands-On Exercise 3b: Programming Animated Statistical Graphics with R\n\n\n\n\n\n\nGoh Si Hui\n\n\nJan 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHands-on Exercise 4a - Visualising Distribution\n\n\n\n\n\n\nGoh Si Hui\n\n\nFeb 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHands-on Exercise 4b - Visual Statistical Analysis\n\n\n\n\n\n\nGoh Si Hui\n\n\nFeb 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHands-on Exercise 4C - Visualising Uncertainty\n\n\n\n\n\n\nGoh Si Hui\n\n\nFeb 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHands-on Exercise 4d - Funnel Plots for Fair Comparisons\n\n\n\n\n\n\nGoh Si Hui\n\n\nFeb 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHands-on Exercise 5a - Creating Ternary Plot with R\n\n\n\n\n\n\nGoh Si Hui\n\n\nFeb 7, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHands-on Exercise 5b - Visual Correlation Analysis\n\n\n\n\n\n\nGoh Si Hui\n\n\nFeb 8, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data\n\n\n\n\n\n\nGoh Si Hui\n\n\nFeb 8, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHands-on Exercise 5d - Visual Multivariate Analysis with Parallel Coordinates Plot\n\n\n\n\n\n\nGoh Si Hui\n\n\nFeb 8, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHands-on Exercise 5e - Treemap Visualisation with R\n\n\n\n\n\n\nGoh Si Hui\n\n\nFeb 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nTake-home Exercise 6 - Visualing and Analysing Time-Oriented Data\n\n\n\n\n\n\nGoh Si Hui\n\n\nFeb 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nTake-home Exercise 7a: Choropleth Mapping with R\n\n\n\n\n\n\nGoh Si Hui\n\n\nFeb 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHands-on Exercise 7b: Visualising Geospatial Point Data\n\n\n\n\n\n\nGoh Si Hui\n\n\nFeb 26, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHands-on Exercise 7c: Analytical Mapping\n\n\n\n\n\n\nGoh Si Hui\n\n\nFeb 26, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#in-class-exercises",
    "href": "index.html#in-class-exercises",
    "title": "SiHui Learns Visual Analytics!",
    "section": "In-Class Exercises",
    "text": "In-Class Exercises\n\n\n\n\n\n\n\n\nIn-class Exercise 1: Now You See it!\n\n\n\n\n\n\nGoh Si Hui\n\n\nJan 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nIn-class Exercise 2: Horizon Plot\n\n\n\n\n\n\nGoh Si Hui\n\n\nFeb 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nIn-class Exercise 3\n\n\n\n\n\n\nGoh Si Hui\n\n\nMar 9, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#take-home-exercises",
    "href": "index.html#take-home-exercises",
    "title": "SiHui Learns Visual Analytics!",
    "section": "Take-Home Exercises",
    "text": "Take-Home Exercises\n\n\n\n\n\n\n\n\nTake-home Exercise 1\n\n\n\n\n\n\nGoh Si Hui\n\n\nJan 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nTake-home Exercise 2\n\n\n\n\n\n\nGoh Si Hui\n\n\nJan 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nTake-home Exercise 3: Be Weatherwise or Otherwise\n\n\n\n\n\n\nGoh Si Hui\n\n\nFeb 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nTake-home Exercise 4: Prototyping Time Series Module for Shiny Application\n\n\n\n\n\n\nGoh Si Hui\n\n\nFeb 25, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#extras",
    "href": "index.html#extras",
    "title": "SiHui Learns Visual Analytics!",
    "section": "Extras",
    "text": "Extras\nAdditional practices and experiments can be found here!\n\n\n\n\n\n\n\n\nPost-Lesson Thoughts 1: Annotations\n\n\n\n\n\n\nGoh Si Hui\n\n\nJan 13, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#loading-r-packages",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#loading-r-packages",
    "title": "In-class Exercise 1: Now You See it!",
    "section": "1.1 Loading R packages",
    "text": "1.1 Loading R packages\nFor this in-class exercise, we will be using the following two packages:\n\ntidyverse : to load the core tidyverse packages, which includes ggplot2.\nhaven : to read and write various data formats used by other statistical packages by wrapping the ReadStat C library. It is part of the tidyverse family too! haven currently supports SAS, SPSS and Stata. We will need haven to import the PISA 2022’s student questionnaire data file because it is in SAS file type.\n\nThe code chunk below uses p_load() of pacman package to check if tidyverse packages are installed in the computer. If they are, they will be launched in R. Otherwise, pacman will install the relevant packages before launching them.\n\npacman::p_load(tidyverse, haven)",
    "crumbs": [
      "In-class Exercises",
      "In-class Exercise 1: Now You See it!"
    ]
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#importing-the-data-into-r",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#importing-the-data-into-r",
    "title": "In-class Exercise 1: Now You See it!",
    "section": "1.2 Importing the Data into R",
    "text": "1.2 Importing the Data into R\nFor this in-class exercise, we are using PISA 2022 database’s student questionnaire data file. As the data file is in SAS file format, we will use haven’s read_sas() function to import the data into R environment. Then we will filter the data to those data from Singapore.\n\nImport the DataFilter the Imported Data\n\n\n\nstu_qqq &lt;- read_sas(\"data/cy08msp_stu_qqq.sas7bdat\")\n\n\n\n\nstu_qqq_SG &lt;- stu_qqq %&gt;% \n  filter(CNT == \"SGP\")",
    "crumbs": [
      "In-class Exercises",
      "In-class Exercise 1: Now You See it!"
    ]
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#saving-the-data-into-rds-format",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#saving-the-data-into-rds-format",
    "title": "In-class Exercise 1: Now You See it!",
    "section": "1.3 Saving the data into RDS format",
    "text": "1.3 Saving the data into RDS format\nLet us save the filtered data into an R data format (RDS) so that we can easily retrieve in future without importing the stu_qqq dataset again (Note: this dataset is more than 3GB!)\n\nwrite_rds(stu_qqq_SG, \"data/stu_qqq_SG.rds\")",
    "crumbs": [
      "In-class Exercises",
      "In-class Exercise 1: Now You See it!"
    ]
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#reading-the-rds-data",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#reading-the-rds-data",
    "title": "In-class Exercise 1: Now You See it!",
    "section": "1.4 Reading the RDS data",
    "text": "1.4 Reading the RDS data\nWe will read the filtered data using the following code.\n\nstu_qqq_SG &lt;- read_rds(\"data/stu_qqq_SG.rds\")\n\nLet us check the RDS data using glimpse() of dplyr to learn about the associated attribute information in the dataframe.\n\nglimpse(stu_qqq_SG)\n\nRows: 6,606\nColumns: 1,279\n$ CNT          &lt;chr&gt; \"SGP\", \"SGP\", \"SGP\", \"SGP\", \"SGP\", \"SGP\", \"SGP\", \"SGP\", \"…\n$ CNTRYID      &lt;dbl&gt; 702, 702, 702, 702, 702, 702, 702, 702, 702, 702, 702, 70…\n$ CNTSCHID     &lt;dbl&gt; 70200052, 70200134, 70200112, 70200004, 70200152, 7020004…\n$ CNTSTUID     &lt;dbl&gt; 70200001, 70200002, 70200003, 70200004, 70200005, 7020000…\n$ CYC          &lt;chr&gt; \"08MS\", \"08MS\", \"08MS\", \"08MS\", \"08MS\", \"08MS\", \"08MS\", \"…\n$ NatCen       &lt;chr&gt; \"070200\", \"070200\", \"070200\", \"070200\", \"070200\", \"070200…\n$ STRATUM      &lt;chr&gt; \"SGP01\", \"SGP01\", \"SGP01\", \"SGP01\", \"SGP01\", \"SGP01\", \"SG…\n$ SUBNATIO     &lt;chr&gt; \"7020000\", \"7020000\", \"7020000\", \"7020000\", \"7020000\", \"7…\n$ REGION       &lt;dbl&gt; 70200, 70200, 70200, 70200, 70200, 70200, 70200, 70200, 7…\n$ OECD         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ ADMINMODE    &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ LANGTEST_QQQ &lt;dbl&gt; 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 31…\n$ LANGTEST_COG &lt;dbl&gt; 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 31…\n$ LANGTEST_PAQ &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ Option_CT    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ Option_FL    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ Option_ICTQ  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ Option_WBQ   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ Option_PQ    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ Option_TQ    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ Option_UH    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ BOOKID       &lt;dbl&gt; 4, 45, 8, 40, 42, 15, 13, 39, 14, 7, 20, 17, 38, 24, 19, …\n$ ST001D01T    &lt;dbl&gt; 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 1…\n$ ST003D02T    &lt;dbl&gt; 10, 6, 7, 2, 9, 9, 3, 4, 8, 6, 10, 7, 9, 11, 5, 10, 11, 4…\n$ ST003D03T    &lt;dbl&gt; 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 200…\n$ ST004D01T    &lt;dbl&gt; 1, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 1, 1, 2, 1, 2, 1, 2, 1, …\n$ ST250Q01JA   &lt;dbl&gt; 2, 1, 1, 2, 2, 2, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, …\n$ ST250Q02JA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST250Q03JA   &lt;dbl&gt; 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST250Q04JA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST250Q05JA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST250D06JA   &lt;chr&gt; \"7020002\", \"7020001\", \"7020001\", \"7020002\", \"7020002\", \"7…\n$ ST250D07JA   &lt;chr&gt; \"7020002\", \"7020001\", \"7020002\", \"7020002\", \"7020002\", \"7…\n$ ST251Q01JA   &lt;dbl&gt; 2, 1, 2, 1, 2, 2, 2, 1, 3, 3, 1, 2, 2, 1, 2, 2, 1, 2, 3, …\n$ ST251Q02JA   &lt;dbl&gt; 1, 4, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, …\n$ ST251Q03JA   &lt;dbl&gt; 3, 3, 3, 3, 2, 2, 3, 3, 4, 3, 2, 2, 3, 2, 3, 3, 2, 3, 4, …\n$ ST251Q04JA   &lt;dbl&gt; 3, 3, 3, 3, 2, 3, 3, 3, 4, 3, 2, 2, 3, 2, 3, 3, 2, 3, 4, …\n$ ST251Q06JA   &lt;dbl&gt; 3, 4, 2, 2, 1, 2, 2, 3, 4, 1, 3, 3, 1, 2, 2, 4, 4, 1, 2, …\n$ ST251Q07JA   &lt;dbl&gt; 3, 2, 1, 1, 4, 1, 4, 1, 4, 3, 1, 4, 1, 1, 4, 4, 1, 4, 1, …\n$ ST251D08JA   &lt;chr&gt; \"9999997\", \"9999997\", \"9999997\", \"9999997\", \"9999997\", \"9…\n$ ST251D09JA   &lt;chr&gt; \"9999997\", \"9999997\", \"9999997\", \"9999997\", \"9999997\", \"9…\n$ ST253Q01JA   &lt;dbl&gt; 7, 8, 7, 6, 7, 7, 8, 8, 8, 7, 7, 8, 5, 7, 7, 8, 5, 7, 7, …\n$ ST254Q01JA   &lt;dbl&gt; 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 1, 3, 2, 2, 2, 2, …\n$ ST254Q02JA   &lt;dbl&gt; 1, 2, 2, 1, 3, 2, 2, 5, 2, 2, 1, 3, 2, 1, 2, 2, 2, 2, 2, …\n$ ST254Q03JA   &lt;dbl&gt; 3, 2, 2, 2, 2, 2, 3, 3, 3, 4, 2, 3, 2, 2, 3, 3, 2, 3, 2, …\n$ ST254Q04JA   &lt;dbl&gt; 2, 3, 2, 1, 1, 2, 2, 3, 3, 2, 1, 3, 2, 2, 3, 3, 2, 3, 2, …\n$ ST254Q05JA   &lt;dbl&gt; 1, 5, 1, 1, NA, 1, 1, 5, 2, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1,…\n$ ST254Q06JA   &lt;dbl&gt; 3, 2, 3, 3, 4, 2, 4, 3, 4, 4, 3, 3, 2, 3, 3, 4, 3, 4, 3, …\n$ ST255Q01JA   &lt;dbl&gt; 7, 4, 4, 3, 2, 2, 4, 5, 7, 4, 3, 7, 4, 4, 2, 4, 5, 4, 4, …\n$ ST256Q01JA   &lt;dbl&gt; 2, 4, 5, 2, 4, 1, 1, 3, 4, 4, 1, 4, 2, 2, 2, 3, 2, 2, 1, …\n$ ST256Q02JA   &lt;dbl&gt; 2, 5, 2, 1, 1, 2, 1, 5, 4, 2, 2, 4, 1, 2, 2, 2, 3, 5, 1, …\n$ ST256Q03JA   &lt;dbl&gt; 4, 5, 2, 1, 1, 2, 2, 5, 5, 1, 2, 4, 1, 3, 1, 4, 4, 5, 1, …\n$ ST256Q06JA   &lt;dbl&gt; 4, 3, 3, 2, 2, 5, 2, 4, 4, 1, 2, 4, 2, 2, 2, 3, 2, 2, 3, …\n$ ST256Q07JA   &lt;dbl&gt; 3, 5, 5, 2, 2, 5, 1, 4, 3, 1, 2, 4, 1, 2, 2, 2, 1, 5, 1, …\n$ ST256Q08JA   &lt;dbl&gt; 3, 3, 3, 1, 1, 5, 1, 5, 5, 2, 2, 4, 1, 1, 1, 3, 1, 5, 2, …\n$ ST256Q09JA   &lt;dbl&gt; 2, 2, 4, 2, 2, 2, 2, 2, 2, 3, 2, 4, 2, 2, 2, 2, 2, 2, NA,…\n$ ST256Q10JA   &lt;dbl&gt; 4, 4, 5, 2, 4, 1, 4, 4, 4, 3, 2, 4, 4, 1, 2, 4, 3, 3, 3, …\n$ ST230Q01JA   &lt;dbl&gt; 4, 4, 2, 4, 4, 3, 2, 2, 3, 4, 1, 3, 4, 1, 4, 3, 2, 3, 2, …\n$ ST005Q01JA   &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ ST006Q01JA   &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, NA, 2, 2, NA, 2, 1, 2, 2, 2, 2, 2, 2…\n$ ST006Q02JA   &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, NA, 1, 1, 2, 2, 1, 2, 2, 2,…\n$ ST006Q03JA   &lt;dbl&gt; 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, NA, 1, 1, 2, 2, 1, 1, 1, 2,…\n$ ST006Q04JA   &lt;dbl&gt; 2, 1, 2, 2, 1, 1, 2, 1, 1, 2, NA, 2, 1, 2, 2, 2, 1, 2, 2,…\n$ ST006Q05JA   &lt;dbl&gt; 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, NA, 1, 2, 1, 2, 1, 1, 2, 1,…\n$ ST007Q01JA   &lt;dbl&gt; 2, 2, 2, 4, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ ST008Q01JA   &lt;dbl&gt; 2, 2, 2, NA, 2, 2, 2, NA, 2, 2, NA, 2, 1, 2, 2, 2, 2, 2, …\n$ ST008Q02JA   &lt;dbl&gt; 2, 2, 2, NA, 2, 1, 2, NA, 2, 2, NA, 1, 1, 1, 2, 2, 1, 2, …\n$ ST008Q03JA   &lt;dbl&gt; 2, 2, 2, NA, 2, 1, 2, 1, 1, 2, NA, 1, 1, 2, 2, 2, 1, 1, 2…\n$ ST008Q04JA   &lt;dbl&gt; 1, 1, 2, NA, 1, 1, 2, 1, 1, 1, NA, 2, 1, 2, 2, 2, 1, 2, 2…\n$ ST008Q05JA   &lt;dbl&gt; 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, NA, 1, 2, 2, 1, 2, 1, 2, 1,…\n$ ST258Q01JA   &lt;dbl&gt; 1, 1, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 2, 1, 1, …\n$ ST259Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST259Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST019AQ01T   &lt;dbl&gt; 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 2, 2, 1, …\n$ ST019BQ01T   &lt;dbl&gt; 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 2, 2, 2, 1, …\n$ ST019CQ01T   &lt;dbl&gt; 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 2, 2, 2, 1, …\n$ ST021Q01TA   &lt;dbl&gt; NA, NA, NA, NA, NA, 1, NA, 1, NA, NA, NA, NA, NA, 7, NA, …\n$ ST022Q01TA   &lt;dbl&gt; 1, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 1, 1, 2, 2, 2, 2, 1, 1, …\n$ ST226Q01JA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST125Q01NA   &lt;dbl&gt; 3, 5, 8, 4, 8, 3, 4, 8, 8, 4, 8, 2, 7, 2, 3, 5, 8, 8, 3, …\n$ ST126Q01TA   &lt;dbl&gt; 4, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 4, 5, 5, 4, 5, …\n$ ST127Q01TA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST127Q02TA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST127Q03TA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST260Q01JA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, …\n$ ST260Q02JA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST260Q03JA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST261Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 2, NA, NA…\n$ ST261Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 2, NA, NA…\n$ ST261Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 2, NA, NA…\n$ ST261Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 2, NA, NA…\n$ ST261Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1, NA, NA…\n$ ST261Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 2, NA, NA…\n$ ST261Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 2, NA, NA…\n$ ST261Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1, NA, NA…\n$ ST261Q09JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 2, NA, NA…\n$ ST261Q10JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 2, NA, NA…\n$ ST261Q11JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 2, NA, NA…\n$ ST062Q01TA   &lt;dbl&gt; 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST062Q02TA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, …\n$ ST062Q03TA   &lt;dbl&gt; 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST267Q01JA   &lt;dbl&gt; NA, 4, 3, 3, NA, 4, 3, NA, 4, NA, NA, 4, 3, 3, 3, NA, 3, …\n$ ST267Q02JA   &lt;dbl&gt; 2, 3, 3, NA, 3, 3, 1, NA, 4, NA, 3, 4, 1, NA, 4, 2, 1, 4,…\n$ ST267Q03JA   &lt;dbl&gt; NA, NA, 3, 2, NA, 3, 1, 3, NA, 3, NA, 4, NA, 3, NA, 3, NA…\n$ ST267Q04JA   &lt;dbl&gt; 2, 2, NA, NA, 3, NA, 1, 3, 2, 2, 2, NA, 3, 1, 4, 1, 2, NA…\n$ ST267Q05JA   &lt;dbl&gt; 3, 3, 3, 3, 3, 3, NA, 3, NA, NA, 3, 4, NA, NA, NA, 4, 3, …\n$ ST267Q06JA   &lt;dbl&gt; NA, NA, NA, 3, 4, 4, NA, 3, 4, 3, NA, NA, 2, 3, 3, 4, 4, …\n$ ST267Q07JA   &lt;dbl&gt; 3, 4, 4, NA, NA, NA, NA, NA, 4, 3, 3, 4, 1, 3, 4, NA, NA,…\n$ ST267Q08JA   &lt;dbl&gt; 2, NA, NA, 1, 1, NA, 1, 3, NA, 2, 2, NA, NA, NA, NA, NA, …\n$ ST034Q01TA   &lt;dbl&gt; 4, 4, 3, 2, 4, 4, 2, 1, 3, 3, 3, 4, 2, 4, 3, 4, 3, 4, 3, …\n$ ST034Q02TA   &lt;dbl&gt; NA, 2, 2, 3, 1, 2, 2, NA, 1, 2, NA, 1, 3, 1, 4, 2, 3, 2, …\n$ ST034Q03TA   &lt;dbl&gt; 2, NA, NA, 3, 1, 2, 3, 3, 2, NA, 2, 1, 3, 1, 2, NA, 2, NA…\n$ ST034Q04TA   &lt;dbl&gt; 4, 3, 2, 2, 4, NA, 3, 2, NA, 3, 3, 4, 2, 4, 2, 4, NA, 4, …\n$ ST034Q05TA   &lt;dbl&gt; 2, 2, 2, 2, NA, 2, NA, 3, 2, 2, 2, NA, 3, 1, NA, 2, 2, 3,…\n$ ST034Q06TA   &lt;dbl&gt; 3, 3, 3, NA, 4, 4, 3, 4, 3, 3, 3, 4, NA, NA, 3, 4, 3, 4, …\n$ ST038Q03NA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 2, 3, 1, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, …\n$ ST038Q04NA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 4, 3, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, …\n$ ST038Q05NA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST038Q06NA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, …\n$ ST038Q07NA   &lt;dbl&gt; 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, …\n$ ST038Q08NA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, …\n$ ST038Q09JA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST038Q10JA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, …\n$ ST038Q11JA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST265Q01JA   &lt;dbl&gt; 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 3, 1, 2, 1, 2, 1, 2, …\n$ ST265Q02JA   &lt;dbl&gt; 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 2, 1, 3, 1, 2, 1, 2, 1, 2, …\n$ ST265Q03JA   &lt;dbl&gt; 2, 1, 2, 2, 1, 2, 3, 1, 1, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, …\n$ ST265Q04JA   &lt;dbl&gt; 2, 1, 2, 2, 1, 2, 3, 1, 1, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, …\n$ ST266Q01JA   &lt;dbl&gt; 2, 1, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 2, 1, 1, 2, 2, 2, 2, …\n$ ST266Q02JA   &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ ST266Q03JA   &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ ST266Q04JA   &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ ST266Q05JA   &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ ST294Q01JA   &lt;dbl&gt; 5, 4, 6, 1, 1, 1, 1, 6, 6, 1, 5, 1, 1, 1, 4, 3, 6, 6, 1, …\n$ ST294Q02JA   &lt;dbl&gt; 1, 5, 2, 1, 4, 6, 1, 6, 1, 1, 1, 1, 6, 1, 5, 1, 1, 1, 6, …\n$ ST294Q03JA   &lt;dbl&gt; 6, 2, 1, 6, 1, 3, 4, 1, 1, 1, 1, 1, 6, 1, 6, 1, 3, 1, 1, …\n$ ST294Q04JA   &lt;dbl&gt; 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST294Q05JA   &lt;dbl&gt; 1, 3, 1, 6, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 3, …\n$ ST295Q01JA   &lt;dbl&gt; 6, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 1, 6, 6, 6, 3, 6, 6, …\n$ ST295Q02JA   &lt;dbl&gt; 5, 4, 3, 6, 5, 6, 1, 6, 6, 4, 6, 6, 6, 1, 5, 6, 6, 6, 6, …\n$ ST295Q03JA   &lt;dbl&gt; 6, 2, 1, 6, 6, 4, 5, 1, 1, 5, 3, 3, 6, 1, 6, 1, 4, 6, 1, …\n$ ST295Q04JA   &lt;dbl&gt; 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST295Q05JA   &lt;dbl&gt; 2, 3, 3, 1, 5, 2, 3, 1, 4, 6, 2, 3, 6, 3, 3, 1, 3, 4, 1, …\n$ ST326Q01JA   &lt;dbl&gt; 3, 2, 3, 2, 6, 3, 3, 8, 4, 3, 4, 4, 3, 7, 5, 3, 7, 6, 2, …\n$ ST326Q02JA   &lt;dbl&gt; 4, 2, 2, 3, 5, 2, 2, 5, 5, 5, 5, 3, 5, 2, 3, 3, 6, 4, 4, …\n$ ST326Q03JA   &lt;dbl&gt; 2, 3, 2, 2, 4, 1, 1, NA, 4, 4, 6, 5, 8, 1, 4, 3, 9, 4, 2,…\n$ ST326Q04JA   &lt;dbl&gt; 1, 1, 2, 1, 5, 1, 2, 3, 1, 2, 1, 2, 1, 2, 3, 2, 3, 2, 1, …\n$ ST326Q05JA   &lt;dbl&gt; NA, 1, 5, 1, 5, 2, 4, 5, 3, 5, 5, 5, 1, 9, 3, 1, 4, 6, 5,…\n$ ST326Q06JA   &lt;dbl&gt; 8, 1, 7, 1, 2, 5, 9, 7, 4, 5, 4, 7, 2, 9, 5, 1, 7, 7, 6, …\n$ ST326Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST326Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST326Q09JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST326Q10JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST326Q11JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST326Q12JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST322Q01JA   &lt;dbl&gt; 5, 6, 3, 5, 4, 6, 5, 5, 4, NA, 4, 5, 5, 1, 2, 5, NA, 4, 5…\n$ ST322Q02JA   &lt;dbl&gt; 5, 1, 3, 5, 5, 5, 5, 5, NA, 4, 2, 5, 5, NA, NA, 5, 4, 3, …\n$ ST322Q03JA   &lt;dbl&gt; NA, 5, 2, 5, NA, 4, 1, 5, 3, 5, NA, 4, 3, 4, 4, NA, 4, 4,…\n$ ST322Q04JA   &lt;dbl&gt; 2, 5, 2, 1, 5, NA, NA, 5, 3, 3, 4, 5, NA, 5, 5, 2, 2, NA,…\n$ ST322Q06JA   &lt;dbl&gt; 1, 1, NA, NA, 4, 6, 1, NA, 6, 4, 1, NA, 1, 1, 4, 1, 3, 4,…\n$ ST322Q07JA   &lt;dbl&gt; 1, NA, 2, 1, 4, 2, 2, 1, 1, 5, 2, 3, 1, 1, 1, 1, 5, 3, 1,…\n$ ST307Q01JA   &lt;dbl&gt; 4, NA, NA, NA, 4, NA, 4, 5, NA, NA, 5, NA, 4, NA, NA, 4, …\n$ ST307Q02JA   &lt;dbl&gt; NA, 5, 4, NA, NA, 3, NA, NA, 5, NA, NA, 5, NA, 4, NA, NA,…\n$ ST307Q03JA   &lt;dbl&gt; 4, 4, 3, 4, 3, NA, NA, 4, 2, 3, 4, 5, NA, NA, 4, 5, NA, N…\n$ ST307Q04JA   &lt;dbl&gt; 2, NA, 3, 2, NA, NA, NA, 3, 4, 4, NA, NA, NA, 1, 4, 2, 3,…\n$ ST307Q05JA   &lt;dbl&gt; NA, 5, NA, NA, NA, NA, NA, NA, NA, NA, 3, 4, NA, 4, 4, NA…\n$ ST307Q06JA   &lt;dbl&gt; 2, NA, 2, 2, 5, 3, 1, 3, NA, NA, NA, NA, 2, NA, 3, NA, NA…\n$ ST307Q07JA   &lt;dbl&gt; 2, NA, NA, 2, NA, NA, 5, NA, 4, 4, 2, 1, 2, 1, NA, 2, NA,…\n$ ST307Q08JA   &lt;dbl&gt; NA, 3, NA, 4, NA, 4, 5, NA, NA, 4, NA, NA, NA, NA, NA, 4,…\n$ ST307Q09JA   &lt;dbl&gt; NA, NA, 3, NA, 4, 5, NA, NA, 4, NA, 4, 4, 4, 5, 3, NA, 3,…\n$ ST307Q10JA   &lt;dbl&gt; NA, 3, NA, NA, 5, 4, 2, 1, NA, 3, NA, NA, 3, NA, NA, NA, …\n$ ST309Q01JA   &lt;dbl&gt; NA, NA, 5, NA, 4, 4, 5, 4, 5, NA, 4, NA, 5, NA, 5, NA, 4,…\n$ ST309Q02JA   &lt;dbl&gt; 3, 5, NA, NA, 5, NA, NA, NA, 4, NA, NA, 2, 5, NA, NA, NA,…\n$ ST309Q03JA   &lt;dbl&gt; NA, 3, 3, NA, NA, NA, 2, 4, 1, 4, 2, 1, 1, 5, 2, NA, NA, …\n$ ST309Q04JA   &lt;dbl&gt; 3, NA, NA, NA, NA, 4, 5, NA, NA, NA, NA, NA, NA, 4, NA, N…\n$ ST309Q05JA   &lt;dbl&gt; NA, 4, 3, NA, 4, NA, NA, NA, 5, 4, NA, NA, NA, NA, 4, 1, …\n$ ST309Q06JA   &lt;dbl&gt; NA, NA, 4, 3, NA, 4, NA, NA, 4, NA, NA, 1, 3, NA, NA, 3, …\n$ ST309Q07JA   &lt;dbl&gt; 3, NA, NA, 4, NA, NA, 2, 1, NA, 4, 3, 1, NA, 4, 4, 3, NA,…\n$ ST309Q08JA   &lt;dbl&gt; 4, 3, NA, 4, 5, NA, NA, 4, NA, 4, 4, NA, NA, 2, 5, NA, 3,…\n$ ST309Q09JA   &lt;dbl&gt; 2, NA, 1, 4, NA, 2, NA, 2, NA, NA, 3, NA, 2, 4, NA, 4, NA…\n$ ST309Q10JA   &lt;dbl&gt; NA, 5, NA, 3, 4, 4, 4, NA, NA, 3, NA, 5, NA, NA, NA, 3, 4…\n$ ST301Q01JA   &lt;dbl&gt; 5, NA, NA, 4, 5, 4, 4, NA, 3, 5, NA, NA, 3, NA, NA, NA, N…\n$ ST301Q02JA   &lt;dbl&gt; NA, NA, NA, NA, 1, NA, NA, NA, NA, 4, 4, 5, NA, NA, NA, 5…\n$ ST301Q03JA   &lt;dbl&gt; NA, NA, 3, NA, 4, 4, 1, NA, 2, NA, 2, 4, NA, NA, 4, 2, 3,…\n$ ST301Q04JA   &lt;dbl&gt; 5, NA, NA, 4, 5, NA, NA, 5, NA, 4, NA, 5, NA, 4, NA, 4, N…\n$ ST301Q05JA   &lt;dbl&gt; NA, 4, NA, NA, 4, 4, NA, NA, 5, NA, 4, 4, 3, NA, NA, 4, 3…\n$ ST301Q06JA   &lt;dbl&gt; 5, 5, 3, 4, NA, NA, NA, NA, 3, 4, 4, 5, 2, 4, 5, NA, 3, 4…\n$ ST301Q07JA   &lt;dbl&gt; 5, NA, 3, 4, NA, 4, NA, 5, NA, NA, NA, NA, 2, 4, NA, 4, N…\n$ ST301Q08JA   &lt;dbl&gt; NA, 2, NA, 3, NA, NA, 4, 2, 3, 2, NA, NA, NA, 2, 2, NA, 3…\n$ ST301Q09JA   &lt;dbl&gt; NA, 3, 4, NA, NA, 4, 5, 5, NA, NA, 5, NA, NA, 4, 5, NA, N…\n$ ST301Q10JA   &lt;dbl&gt; 5, 4, 3, NA, NA, NA, 4, 5, NA, NA, NA, NA, 3, NA, 4, NA, …\n$ ST343Q01JA   &lt;dbl&gt; 4, NA, 3, NA, NA, NA, 5, NA, 5, NA, NA, 5, 4, NA, NA, NA,…\n$ ST343Q02JA   &lt;dbl&gt; NA, 2, NA, 3, NA, 1, NA, 4, NA, 3, NA, 2, NA, 4, NA, NA, …\n$ ST343Q03JA   &lt;dbl&gt; NA, NA, NA, NA, 4, NA, 3, NA, NA, NA, 4, 5, NA, NA, NA, N…\n$ ST343Q04JA   &lt;dbl&gt; 2, NA, NA, 2, NA, 1, 3, 1, NA, 2, NA, NA, 3, NA, 3, 2, 2,…\n$ ST343Q05JA   &lt;dbl&gt; 3, 3, 3, 2, 2, 1, NA, NA, 1, NA, 2, NA, NA, NA, 5, 2, 4, …\n$ ST343Q06JA   &lt;dbl&gt; 4, 4, 3, 4, 5, NA, NA, 1, 5, 5, 4, 5, 4, 5, NA, 5, 3, NA,…\n$ ST343Q07JA   &lt;dbl&gt; 1, 3, NA, 2, NA, NA, NA, 1, NA, 3, 3, NA, NA, 3, 1, 3, NA…\n$ ST343Q08JA   &lt;dbl&gt; NA, 5, NA, NA, 5, 5, NA, 2, 5, NA, NA, NA, NA, 2, 3, NA, …\n$ ST343Q09JA   &lt;dbl&gt; NA, NA, 4, NA, NA, 5, 2, NA, 5, NA, NA, 5, 3, 3, 3, NA, N…\n$ ST343Q10JA   &lt;dbl&gt; NA, NA, 2, NA, 2, NA, 2, NA, NA, 2, 3, NA, 3, NA, NA, 4, …\n$ ST311Q01JA   &lt;dbl&gt; NA, 2, NA, NA, 2, 1, 5, 2, NA, NA, 2, 1, 2, 2, NA, 2, NA,…\n$ ST311Q02JA   &lt;dbl&gt; 5, 4, 4, 4, NA, 1, NA, NA, NA, NA, NA, NA, NA, NA, 4, NA,…\n$ ST311Q03JA   &lt;dbl&gt; 5, NA, NA, NA, NA, NA, NA, 5, 5, 5, NA, NA, NA, NA, 5, NA…\n$ ST311Q04JA   &lt;dbl&gt; 5, 4, 4, 4, NA, 5, NA, NA, NA, NA, 4, 5, 4, NA, 4, NA, 3,…\n$ ST311Q05JA   &lt;dbl&gt; NA, 2, 2, 2, NA, 1, 3, NA, NA, 1, NA, NA, NA, 2, 1, 2, NA…\n$ ST311Q06JA   &lt;dbl&gt; NA, NA, NA, NA, 3, NA, 3, NA, 5, NA, 3, 5, NA, NA, NA, NA…\n$ ST311Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, 1, 5, NA, NA, NA, 3, 3, 2, NA, 1,…\n$ ST311Q08JA   &lt;dbl&gt; NA, NA, 3, NA, 3, NA, 2, 5, 5, 4, 3, 5, 3, 4, 4, 5, NA, N…\n$ ST311Q09JA   &lt;dbl&gt; 3, 4, 3, 4, 4, NA, NA, NA, 5, 4, NA, NA, 4, NA, NA, 4, NA…\n$ ST311Q10JA   &lt;dbl&gt; 5, NA, NA, 3, 3, 3, NA, 5, 5, 4, 3, NA, NA, 4, NA, NA, 3,…\n$ ST315Q01JA   &lt;dbl&gt; NA, NA, NA, 4, NA, NA, 5, 4, NA, NA, 3, 4, 4, NA, NA, NA,…\n$ ST315Q02JA   &lt;dbl&gt; 4, 3, 3, 2, NA, 3, 1, NA, 5, NA, NA, 5, NA, NA, NA, 5, NA…\n$ ST315Q03JA   &lt;dbl&gt; 3, NA, NA, NA, NA, 5, NA, NA, NA, 4, NA, NA, NA, 2, 2, NA…\n$ ST315Q04JA   &lt;dbl&gt; 3, 3, 3, 4, 3, NA, NA, 1, NA, 4, 4, NA, 3, 2, NA, NA, NA,…\n$ ST315Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 1, 2, NA, 1, 2, 3, 4, NA,…\n$ ST315Q06JA   &lt;dbl&gt; 3, 3, 3, NA, NA, 5, 2, 1, NA, NA, 3, NA, NA, NA, NA, 5, N…\n$ ST315Q07JA   &lt;dbl&gt; NA, 2, NA, NA, 2, NA, 4, 5, 1, NA, 3, NA, NA, 3, NA, 1, N…\n$ ST315Q08JA   &lt;dbl&gt; NA, NA, 2, 1, 2, 3, NA, 1, 3, 4, NA, NA, 3, NA, 4, 5, 2, …\n$ ST315Q09JA   &lt;dbl&gt; NA, 3, NA, NA, 5, 5, 1, NA, 4, 4, 3, 5, NA, 3, 3, 5, 4, 3…\n$ ST315Q10JA   &lt;dbl&gt; 4, NA, 4, 4, 5, NA, NA, NA, NA, NA, NA, 5, 4, NA, 3, NA, …\n$ ST303Q01JA   &lt;dbl&gt; 5, NA, NA, NA, NA, NA, 4, NA, 5, 4, 4, 5, NA, NA, NA, NA,…\n$ ST303Q02JA   &lt;dbl&gt; 5, NA, 3, NA, 5, 4, NA, 5, NA, NA, 4, 5, NA, 4, NA, NA, 3…\n$ ST303Q03JA   &lt;dbl&gt; 5, NA, 2, 3, 4, NA, NA, 5, 5, 4, NA, NA, 3, 4, 3, 5, 3, N…\n$ ST303Q04JA   &lt;dbl&gt; NA, 4, 4, 4, 5, 5, NA, 5, 5, NA, 4, 5, 3, 4, 5, 5, NA, NA…\n$ ST303Q05JA   &lt;dbl&gt; NA, 3, NA, 2, NA, 1, 3, NA, NA, 4, NA, 1, NA, 2, 4, 1, NA…\n$ ST303Q06JA   &lt;dbl&gt; 5, 5, 4, 4, 5, NA, 4, 5, 5, 4, 4, NA, 3, NA, 5, 5, 3, 4, …\n$ ST303Q07JA   &lt;dbl&gt; 1, 4, 3, NA, NA, 2, 2, NA, 3, 3, 3, NA, 3, NA, NA, 1, NA,…\n$ ST303Q08JA   &lt;dbl&gt; NA, 4, NA, 3, 5, 5, 4, 5, NA, NA, NA, 5, 3, 4, 4, NA, 3, …\n$ ST305Q01JA   &lt;dbl&gt; NA, 2, NA, NA, 5, NA, 2, NA, 5, NA, NA, 5, NA, 5, 2, 5, 2…\n$ ST305Q02JA   &lt;dbl&gt; NA, NA, 2, NA, NA, NA, 4, 3, NA, 3, 4, 3, 2, NA, NA, 5, N…\n$ ST305Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, 3, 2, 5, 4, 4, NA, 3, NA, 2, NA, …\n$ ST305Q04JA   &lt;dbl&gt; 3, NA, NA, NA, 5, 2, NA, NA, NA, NA, NA, 5, NA, NA, NA, 1…\n$ ST305Q05JA   &lt;dbl&gt; 4, 4, 3, 3, 2, 3, NA, NA, NA, 4, NA, NA, 2, 5, 2, NA, 3, …\n$ ST305Q06JA   &lt;dbl&gt; 3, NA, 3, 3, NA, 4, NA, NA, 5, NA, 4, NA, NA, 4, NA, 5, 3…\n$ ST305Q07JA   &lt;dbl&gt; 4, 4, NA, 3, NA, NA, NA, 4, 3, 4, 3, 1, NA, NA, 4, NA, 3,…\n$ ST305Q08JA   &lt;dbl&gt; 2, NA, NA, 3, NA, 5, 2, NA, NA, NA, NA, NA, 3, 3, NA, NA,…\n$ ST305Q09JA   &lt;dbl&gt; NA, 3, 2, NA, 3, 1, 4, 1, 3, NA, 3, 4, NA, NA, 2, 3, NA, …\n$ ST305Q10JA   &lt;dbl&gt; NA, 4, 3, 3, 3, NA, NA, 1, NA, 4, NA, NA, 3, 4, NA, NA, N…\n$ ST345Q01JA   &lt;dbl&gt; 3, 4, NA, 3, 5, NA, 3, 5, 4, 4, NA, 3, 5, NA, NA, NA, 4, …\n$ ST345Q02JA   &lt;dbl&gt; 5, NA, 4, 3, 3, NA, 5, NA, NA, NA, NA, 3, 2, NA, 4, NA, 3…\n$ ST345Q03JA   &lt;dbl&gt; 2, 4, NA, NA, NA, 4, NA, 4, NA, NA, NA, NA, 5, 2, NA, NA,…\n$ ST345Q04JA   &lt;dbl&gt; 1, NA, 4, NA, NA, 2, 3, NA, 4, 3, 4, 3, NA, NA, NA, 2, 3,…\n$ ST345Q05JA   &lt;dbl&gt; 3, NA, NA, 3, NA, NA, NA, 4, 4, NA, NA, 3, 1, NA, 3, NA, …\n$ ST345Q06JA   &lt;dbl&gt; NA, 2, NA, NA, 3, 5, NA, NA, 5, 4, NA, NA, NA, 5, 3, 4, N…\n$ ST345Q07JA   &lt;dbl&gt; NA, 5, 3, 3, 5, 5, NA, 3, NA, 5, 5, NA, NA, 1, 5, NA, 5, …\n$ ST345Q08JA   &lt;dbl&gt; NA, NA, 3, 3, NA, NA, NA, NA, NA, 3, 4, 5, NA, NA, NA, 5,…\n$ ST345Q09JA   &lt;dbl&gt; NA, NA, 3, NA, 2, NA, 5, NA, 5, NA, 4, NA, 1, 5, 2, 5, NA…\n$ ST345Q10JA   &lt;dbl&gt; NA, 4, NA, NA, NA, 3, 2, 4, NA, NA, 4, NA, NA, 2, NA, 1, …\n$ ST313Q01JA   &lt;dbl&gt; NA, NA, NA, 3, 2, NA, 5, NA, NA, 4, NA, 5, NA, 4, NA, 3, …\n$ ST313Q02JA   &lt;dbl&gt; 2, NA, 2, NA, NA, 1, 2, 1, 4, NA, NA, 1, 3, NA, NA, 3, NA…\n$ ST313Q03JA   &lt;dbl&gt; 2, 3, 3, 3, NA, NA, NA, NA, 3, NA, NA, NA, 2, 2, NA, NA, …\n$ ST313Q04JA   &lt;dbl&gt; 1, 4, NA, 3, 4, NA, 1, 2, 2, NA, 2, NA, NA, NA, 4, 3, 1, …\n$ ST313Q05JA   &lt;dbl&gt; 3, NA, 3, 3, NA, 1, NA, NA, 4, NA, 4, 4, 1, 5, 3, NA, 3, …\n$ ST313Q06JA   &lt;dbl&gt; NA, 4, 2, NA, 4, 1, 2, 2, NA, 2, 4, 2, 5, 2, 5, NA, 1, 2,…\n$ ST313Q07JA   &lt;dbl&gt; NA, NA, NA, NA, 4, 1, NA, 4, 3, 4, NA, NA, NA, NA, 3, NA,…\n$ ST313Q08JA   &lt;dbl&gt; NA, 4, NA, NA, NA, NA, NA, 2, NA, NA, 2, 1, NA, NA, NA, 3…\n$ ST313Q09JA   &lt;dbl&gt; 1, NA, 2, 3, 4, NA, 3, NA, NA, 4, 3, NA, 3, NA, NA, 3, 1,…\n$ ST313Q10JA   &lt;dbl&gt; NA, 4, NA, NA, NA, 1, NA, NA, NA, 3, NA, NA, NA, 2, 4, NA…\n$ ST263Q02JA   &lt;dbl&gt; 1, 1, 2, 2, 1, 4, 2, 3, 3, 3, 3, 2, 4, 3, 2, 2, 2, 2, 3, …\n$ ST263Q04JA   &lt;dbl&gt; 2, 1, 2, 3, 1, 1, 1, 3, 2, 3, 1, 1, 3, 3, 2, 2, 1, 2, 3, …\n$ ST263Q06JA   &lt;dbl&gt; 2, 1, 3, 2, 1, 1, 1, 3, 2, 3, 1, 1, 3, 3, 2, 2, 1, 2, 3, …\n$ ST263Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST016Q01NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST059Q01TA   &lt;dbl&gt; 4, 10, 8, 10, 5, 7, 20, 18, 6, 3, 9, 10, 5, 12, 5, 7, 8, …\n$ ST059Q02JA   &lt;dbl&gt; 55, 45, 56, 11, 30, 26, 75, 75, 28, 15, 51, 63, 40, 75, 3…\n$ ST296Q01JA   &lt;dbl&gt; 1, 3, 2, 3, 4, 1, 1, 2, 1, 3, 3, 4, 3, 1, 2, 1, 3, 3, 2, …\n$ ST296Q02JA   &lt;dbl&gt; 1, 2, 3, 1, 3, 1, 1, 2, 1, 3, 3, 3, 2, 1, 4, 1, 3, 1, 1, …\n$ ST296Q03JA   &lt;dbl&gt; 2, 3, 3, 2, 4, 1, 1, 2, 1, 2, 3, 4, 3, 1, 3, 1, 3, 3, 2, …\n$ ST296Q04JA   &lt;dbl&gt; 3, 5, 5, 2, 4, 2, 2, 4, 4, 6, 4, 6, 6, 1, 5, 3, 4, 4, 3, …\n$ ST272Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST273Q01JA   &lt;dbl&gt; 3, 4, 3, 2, 4, 3, 2, 3, 2, 3, 3, 3, NA, 2, NA, NA, 4, 4, …\n$ ST273Q02JA   &lt;dbl&gt; NA, 3, 3, NA, NA, 3, 2, 2, 2, 3, 3, NA, 4, 2, 4, 4, NA, 3…\n$ ST273Q03JA   &lt;dbl&gt; NA, 4, NA, NA, 4, NA, NA, 3, 3, 4, NA, 4, 4, NA, 4, 1, 4,…\n$ ST273Q04JA   &lt;dbl&gt; 4, 4, 3, 3, 3, 3, 1, 3, NA, NA, 3, 4, 4, 2, 4, 4, 4, NA, …\n$ ST273Q05JA   &lt;dbl&gt; 4, NA, 4, 2, 3, 3, 2, NA, 3, 3, 3, NA, 4, NA, 2, NA, 4, 4…\n$ ST273Q06JA   &lt;dbl&gt; 3, NA, 3, 4, NA, NA, NA, 3, NA, 3, 3, 4, NA, 2, NA, 1, NA…\n$ ST273Q07JA   &lt;dbl&gt; 3, 4, NA, 4, 2, 3, 4, NA, 3, NA, NA, 4, 4, 2, 2, 1, 4, 4,…\n$ ST270Q01JA   &lt;dbl&gt; 3, 2, 3, 2, 1, 3, 2, 2, 1, 2, 2, 1, 3, 3, 1, 1, 2, 1, 4, …\n$ ST270Q02JA   &lt;dbl&gt; 2, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 1, 3, 1, 1, 1, 2, 1, 2, …\n$ ST270Q03JA   &lt;dbl&gt; 2, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 1, 3, 2, 1, 1, 2, 1, 1, …\n$ ST270Q04JA   &lt;dbl&gt; 3, 1, 3, 2, 1, 2, 3, 2, 1, 2, 2, 1, 4, 2, 1, 1, 2, 1, 3, …\n$ ST285Q01JA   &lt;dbl&gt; NA, 1, NA, 3, 3, 1, 1, 1, NA, NA, 2, 1, 1, 1, 4, 5, NA, 2…\n$ ST285Q02JA   &lt;dbl&gt; 3, NA, 2, 3, 4, NA, NA, 5, 5, 4, NA, NA, 3, NA, 4, 5, NA,…\n$ ST285Q03JA   &lt;dbl&gt; NA, NA, 3, 3, 2, 1, NA, NA, 4, 4, 3, NA, NA, 1, 4, 5, NA,…\n$ ST285Q04JA   &lt;dbl&gt; NA, 2, NA, NA, 5, NA, 3, NA, NA, 4, NA, 5, 3, NA, 4, NA, …\n$ ST285Q05JA   &lt;dbl&gt; 2, 2, 1, NA, NA, 5, NA, NA, NA, NA, 2, NA, NA, 2, NA, NA,…\n$ ST285Q06JA   &lt;dbl&gt; 3, 2, NA, 3, 5, NA, 4, 5, 5, NA, 2, 5, NA, NA, NA, NA, 4,…\n$ ST285Q07JA   &lt;dbl&gt; 1, NA, 3, NA, NA, 5, NA, NA, 5, 2, 2, 5, 4, 2, NA, 5, 4, …\n$ ST285Q08JA   &lt;dbl&gt; 4, NA, 2, 3, NA, 5, 4, 5, 5, NA, NA, 5, 5, NA, NA, NA, 4,…\n$ ST285Q09JA   &lt;dbl&gt; NA, 3, NA, NA, NA, NA, 4, 5, NA, 4, NA, NA, NA, 1, 4, 5, …\n$ ST283Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, 4, 1, NA, NA, 3, 2, 5, NA, NA, NA, 4,…\n$ ST283Q02JA   &lt;dbl&gt; NA, 3, NA, 3, NA, NA, 1, NA, 3, NA, 3, 5, NA, NA, NA, NA,…\n$ ST283Q03JA   &lt;dbl&gt; 3, 3, 1, NA, 5, 4, NA, 4, 5, 3, NA, 5, 3, 1, 4, 2, 4, NA,…\n$ ST283Q04JA   &lt;dbl&gt; 3, 3, 3, 3, NA, NA, NA, 5, NA, 3, NA, 5, NA, NA, 4, 5, NA…\n$ ST283Q05JA   &lt;dbl&gt; 3, 3, NA, 3, 5, NA, 3, NA, 5, 3, NA, NA, 4, NA, NA, NA, N…\n$ ST283Q06JA   &lt;dbl&gt; 2, 2, 1, NA, 5, 3, 1, 4, NA, NA, 2, NA, NA, 1, 4, NA, 4, …\n$ ST283Q07JA   &lt;dbl&gt; 4, NA, NA, 3, 5, 4, 3, 5, 5, NA, 3, NA, 3, 1, 4, NA, NA, …\n$ ST283Q08JA   &lt;dbl&gt; NA, NA, 2, 3, NA, NA, NA, 5, NA, NA, NA, 5, 2, 1, NA, 5, …\n$ ST283Q09JA   &lt;dbl&gt; NA, NA, 2, NA, 5, 4, NA, NA, 5, 3, 3, NA, 3, 1, 4, 5, 4, …\n$ ST275Q01WA   &lt;dbl&gt; NA, 3, 2, NA, NA, 2, 3, NA, 3, NA, 3, NA, NA, 4, 2, 1, 3,…\n$ ST275Q02WA   &lt;dbl&gt; 2, 4, NA, 2, 2, 2, NA, NA, NA, 3, NA, 1, NA, NA, NA, 3, 3…\n$ ST275Q03WA   &lt;dbl&gt; 2, 4, NA, 4, 2, 2, NA, NA, 2, 3, 3, NA, 3, 4, 1, 4, NA, N…\n$ ST275Q04WA   &lt;dbl&gt; 2, NA, 2, 2, NA, NA, 1, 1, 2, NA, NA, 1, NA, NA, 3, NA, N…\n$ ST275Q05WA   &lt;dbl&gt; NA, NA, 1, NA, 1, NA, 3, 1, NA, 2, NA, 1, 3, 2, 1, 1, 1, …\n$ ST275Q06WA   &lt;dbl&gt; 1, 4, NA, 3, 2, NA, NA, 1, 1, NA, NA, 1, NA, NA, 2, NA, N…\n$ ST275Q07WA   &lt;dbl&gt; NA, NA, NA, 1, NA, NA, NA, 1, 1, 2, 4, NA, 3, NA, NA, NA,…\n$ ST275Q08WA   &lt;dbl&gt; 4, NA, 2, NA, 2, 2, 2, NA, NA, NA, 2, 1, 4, 2, NA, NA, NA…\n$ ST275Q09WA   &lt;dbl&gt; NA, 2, 1, NA, NA, 1, 4, 1, NA, 2, 4, NA, 3, 2, NA, 1, 1, …\n$ ST276Q01JA   &lt;dbl&gt; NA, NA, NA, 2, NA, NA, NA, NA, NA, 2, 2, 1, 2, 2, 1, NA, …\n$ ST276Q02JA   &lt;dbl&gt; NA, NA, NA, NA, 2, 2, 2, 1, NA, NA, 2, NA, NA, NA, NA, NA…\n$ ST276Q03JA   &lt;dbl&gt; 4, 3, NA, NA, 2, NA, 2, NA, NA, NA, NA, 1, NA, 3, 1, 2, 3…\n$ ST276Q04JA   &lt;dbl&gt; NA, 2, NA, NA, 2, 2, 2, 1, 1, 2, NA, NA, NA, NA, NA, 2, 2…\n$ ST276Q05JA   &lt;dbl&gt; NA, NA, 2, 2, NA, NA, 2, 1, 2, NA, NA, 1, NA, 2, NA, 2, N…\n$ ST276Q06JA   &lt;dbl&gt; NA, 3, 1, NA, 2, NA, NA, NA, 1, 2, 2, NA, 1, NA, 1, NA, 2…\n$ ST276Q07JA   &lt;dbl&gt; 3, 3, NA, 2, NA, 2, NA, NA, NA, NA, NA, NA, NA, NA, 2, 2,…\n$ ST276Q08JA   &lt;dbl&gt; 3, NA, 3, 2, 2, 2, 4, 4, 4, NA, 3, 1, 4, NA, NA, NA, 4, 4…\n$ ST276Q09JA   &lt;dbl&gt; 3, NA, 4, 2, NA, 2, NA, 4, 1, 3, 2, 1, 3, 2, 3, 4, NA, NA…\n$ ST276Q10JA   &lt;dbl&gt; 3, 4, 3, NA, NA, NA, NA, NA, NA, 2, NA, NA, 3, 2, NA, NA,…\n$ ST268Q01JA   &lt;dbl&gt; 2, 4, 3, 2, 3, 3, 4, 4, 2, 3, 3, 4, 3, 4, 1, 3, 2, 3, 2, …\n$ ST268Q02JA   &lt;dbl&gt; 3, 3, 2, 3, 4, 3, 3, 2, 2, 2, 2, 3, 2, 2, 4, 3, 2, 3, 2, …\n$ ST268Q03JA   &lt;dbl&gt; 3, 3, 3, 3, 4, 3, 4, 3, 3, 3, 3, 4, 2, 4, 2, 2, 2, 3, 3, …\n$ ST268Q04JA   &lt;dbl&gt; 2, 3, 3, 2, 1, 3, 4, 4, 2, 3, 4, 4, 3, 4, 1, 3, 2, 2, 2, …\n$ ST268Q05JA   &lt;dbl&gt; 3, 2, 2, 2, 3, 3, 4, 2, 2, 3, 2, 3, 2, 3, 2, 3, 2, 2, 2, …\n$ ST268Q06JA   &lt;dbl&gt; 3, 3, 2, 2, 2, 2, 3, 2, 2, 3, 3, 3, 2, 4, 1, 2, 2, 2, 3, …\n$ ST268Q07JA   &lt;dbl&gt; 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, …\n$ ST268Q08JA   &lt;dbl&gt; 3, 4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, …\n$ ST268Q09JA   &lt;dbl&gt; 3, 4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, …\n$ ST290Q01WA   &lt;dbl&gt; NA, NA, NA, 3, NA, NA, NA, 3, NA, 3, 4, 4, NA, 4, 2, NA, …\n$ ST290Q02WA   &lt;dbl&gt; 3, 3, 3, 3, 2, NA, NA, NA, 4, NA, 4, 4, 3, 4, NA, NA, NA,…\n$ ST290Q03WA   &lt;dbl&gt; NA, NA, NA, NA, NA, 3, 4, 3, NA, NA, NA, 4, NA, 4, 2, NA,…\n$ ST290Q04WA   &lt;dbl&gt; 3, 3, 2, NA, 2, NA, NA, NA, 3, 3, NA, 4, 2, NA, 2, 3, 2, …\n$ ST290Q05WA   &lt;dbl&gt; 4, 3, 4, NA, 2, 4, NA, NA, NA, 3, 4, NA, 4, NA, NA, NA, N…\n$ ST290Q06WA   &lt;dbl&gt; NA, NA, 3, NA, NA, 2, 4, NA, NA, NA, 4, NA, 4, 4, NA, 4, …\n$ ST290Q07WA   &lt;dbl&gt; NA, 3, 4, 2, NA, 4, 4, 4, 4, 3, NA, NA, 4, NA, 2, 4, 4, N…\n$ ST290Q08WA   &lt;dbl&gt; 2, NA, NA, 2, 2, NA, 3, 4, 4, NA, NA, NA, NA, 4, 2, 4, NA…\n$ ST290Q09WA   &lt;dbl&gt; 4, 3, NA, 2, 4, 4, 4, 4, 4, 3, 4, NA, NA, NA, NA, 4, 4, 4…\n$ ST291Q01JA   &lt;dbl&gt; 3, NA, 2, 2, NA, NA, 3, NA, 4, NA, 4, NA, 3, 4, NA, 4, NA…\n$ ST291Q02JA   &lt;dbl&gt; 3, 3, NA, NA, 2, NA, 3, 4, 4, 3, NA, NA, 4, NA, 2, NA, 2,…\n$ ST291Q03JA   &lt;dbl&gt; 3, 3, 2, 2, 2, 2, NA, 4, 4, NA, 4, NA, NA, NA, NA, NA, 2,…\n$ ST291Q04JA   &lt;dbl&gt; NA, NA, 2, NA, NA, 2, NA, NA, NA, NA, NA, 4, NA, NA, NA, …\n$ ST291Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, 4, 4, NA, NA, NA, 4, 2, 3, 2, 3, …\n$ ST291Q06JA   &lt;dbl&gt; NA, 3, 3, NA, 2, 2, 4, 4, 4, 2, 4, NA, NA, 4, 2, 3, NA, N…\n$ ST291Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, 3, NA, 4, 4, 2, NA, NA, 2, 3, 2, NA, …\n$ ST291Q08JA   &lt;dbl&gt; NA, 3, NA, 2, NA, 2, NA, NA, NA, NA, 3, 4, NA, 2, 2, 2, N…\n$ ST291Q09JA   &lt;dbl&gt; 3, 3, 1, 2, 2, NA, 3, NA, NA, 2, 3, 4, NA, NA, NA, NA, NA…\n$ ST291Q10JA   &lt;dbl&gt; 2, NA, NA, 2, 2, NA, NA, NA, NA, 3, NA, 4, 2, NA, NA, NA,…\n$ ST289Q01WA   &lt;dbl&gt; 2, NA, NA, NA, NA, NA, NA, NA, NA, NA, 5, NA, NA, 2, NA, …\n$ ST289Q02JA   &lt;dbl&gt; 5, NA, 5, NA, NA, NA, NA, NA, 5, NA, NA, 5, 3, 5, NA, NA,…\n$ ST289Q03WA   &lt;dbl&gt; NA, NA, NA, NA, 1, NA, NA, NA, NA, 1, NA, 5, 1, NA, 1, NA…\n$ ST289Q04JA   &lt;dbl&gt; NA, 5, 4, 1, NA, 4, 5, 5, 5, NA, NA, 5, NA, NA, NA, NA, N…\n$ ST289Q05WA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, 5, NA, NA, 4, NA, NA, NA, NA, 4, …\n$ ST289Q06JA   &lt;dbl&gt; NA, NA, NA, 4, NA, 5, NA, 5, 5, NA, 5, NA, NA, 5, NA, 5, …\n$ ST289Q07JA   &lt;dbl&gt; 5, NA, 3, NA, 4, 5, 5, 5, NA, NA, NA, 5, NA, NA, 4, NA, N…\n$ ST289Q08WA   &lt;dbl&gt; NA, 5, NA, NA, 3, NA, NA, 5, NA, 2, NA, NA, 1, NA, 4, 2, …\n$ ST289Q09WA   &lt;dbl&gt; 4, 5, NA, NA, NA, 5, 5, NA, NA, NA, 5, NA, 4, NA, NA, 5, …\n$ ST289Q10WA   &lt;dbl&gt; NA, 5, 5, 5, 4, 5, 5, NA, 5, 5, NA, NA, NA, 5, NA, NA, NA…\n$ ST289Q11WA   &lt;dbl&gt; 1, NA, NA, 4, 2, NA, NA, NA, NA, NA, 1, NA, NA, 2, NA, 2,…\n$ ST289Q14JA   &lt;dbl&gt; NA, 5, 2, 4, NA, NA, NA, 5, 5, 5, 4, 5, 1, NA, 2, NA, 3, …\n$ ST293Q01JA   &lt;dbl&gt; NA, NA, NA, 4, 5, 4, 3, NA, 5, 3, 4, 5, 3, NA, NA, NA, NA…\n$ ST293Q02JA   &lt;dbl&gt; NA, 5, 4, 5, 5, NA, NA, 3, NA, NA, NA, 5, 4, 2, 4, 3, 5, …\n$ ST293Q03JA   &lt;dbl&gt; 4, 5, 3, NA, NA, 5, NA, NA, NA, 3, 5, NA, 4, NA, NA, NA, …\n$ ST293Q04JA   &lt;dbl&gt; NA, NA, 3, NA, NA, NA, NA, 1, 1, NA, NA, NA, 2, NA, 4, NA…\n$ ST293Q05JA   &lt;dbl&gt; 3, 5, NA, NA, 5, 5, 1, 1, 5, 4, 5, 5, NA, 1, NA, NA, 3, 2…\n$ ST293Q06JA   &lt;dbl&gt; 2, NA, 4, NA, NA, NA, NA, NA, NA, NA, 5, 5, NA, 1, 5, 5, …\n$ ST293Q07JA   &lt;dbl&gt; 2, NA, NA, 3, NA, NA, 3, 3, 2, NA, NA, 1, NA, 4, NA, 3, N…\n$ ST293Q08JA   &lt;dbl&gt; NA, 4, NA, 5, 5, 5, 1, NA, 5, 4, NA, NA, 2, NA, 4, 5, 2, …\n$ ST293Q09JA   &lt;dbl&gt; 4, 2, 2, 5, 3, 3, 2, 5, NA, 3, 5, NA, NA, 5, 3, 4, NA, NA…\n$ ST292Q01JA   &lt;dbl&gt; 2, 2, 2, 2, 1, 2, 4, 4, 2, 2, 3, 2, 2, 3, 1, NA, 3, 2, 3,…\n$ ST292Q02JA   &lt;dbl&gt; 2, NA, 3, 2, 1, 4, 4, NA, 3, 2, 3, 2, 3, 3, 1, 3, 3, 3, 3…\n$ ST292Q03JA   &lt;dbl&gt; 3, 3, 3, 2, NA, NA, 4, 4, 3, 2, NA, 3, 2, 3, 1, 4, 3, 3, …\n$ ST292Q04JA   &lt;dbl&gt; 2, 3, 3, NA, 1, 4, 4, 4, 3, 2, 3, 3, 2, 3, 1, 4, 3, 3, 3,…\n$ ST292Q05JA   &lt;dbl&gt; NA, 1, 2, 2, 1, 4, 1, 4, NA, NA, 2, 2, 1, 3, NA, 4, NA, N…\n$ ST292Q06JA   &lt;dbl&gt; 3, 1, NA, 2, 1, 2, NA, 4, 3, 1, 3, NA, NA, NA, 1, 2, 1, 2…\n$ ST297Q01JA   &lt;dbl&gt; 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, …\n$ ST297Q03JA   &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, …\n$ ST297Q05JA   &lt;dbl&gt; 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, …\n$ ST297Q06JA   &lt;dbl&gt; 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, …\n$ ST297Q07JA   &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, …\n$ ST297Q09JA   &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, …\n$ ST334Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST334Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST334Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST334Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST334Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST334Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST334Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST334Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST334Q09JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST334Q10JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST335Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST335Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST335Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST335Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST335Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST335Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST336Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST336Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST336Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST336Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST336Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST336Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST337Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST337Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST337Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST337Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST337Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST337Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST337Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST337Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST338Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST338Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST338Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST338Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST338Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST338Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST338Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST338Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST339Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST339Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST340Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST340Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST340Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST340Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST340Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST340Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST340Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST340Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST340Q09JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST340Q10JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST341Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST341Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST341Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST341Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST341Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST342Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST342Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST342Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST342Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST342Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST342Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST342Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST300Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, 5, NA, 5, NA, NA, 3, 5, NA, NA, NA, N…\n$ ST300Q02JA   &lt;dbl&gt; 5, NA, 5, 3, NA, 5, 5, NA, 5, 5, 5, 5, NA, NA, NA, NA, 5,…\n$ ST300Q03JA   &lt;dbl&gt; NA, NA, NA, 3, NA, 5, NA, NA, NA, NA, NA, 5, 1, NA, 4, NA…\n$ ST300Q04JA   &lt;dbl&gt; NA, 2, 2, 3, 5, NA, NA, NA, 1, NA, 1, NA, 1, NA, 3, NA, N…\n$ ST300Q05JA   &lt;dbl&gt; 4, NA, NA, NA, NA, NA, 4, 5, 5, 5, NA, NA, NA, 1, 3, 1, 2…\n$ ST300Q06JA   &lt;dbl&gt; 2, 3, 2, NA, 5, NA, NA, 5, 5, 5, 3, NA, 1, NA, 2, 1, NA, …\n$ ST300Q07JA   &lt;dbl&gt; 4, 3, NA, NA, 5, NA, 3, 4, NA, 5, NA, NA, NA, 4, NA, 5, 4…\n$ ST300Q08JA   &lt;dbl&gt; NA, NA, 3, NA, 5, NA, NA, 5, 5, NA, 3, 5, 1, 4, NA, 1, 2,…\n$ ST300Q09JA   &lt;dbl&gt; NA, 3, NA, 3, 5, 5, 2, NA, NA, NA, NA, NA, 1, 4, NA, 5, N…\n$ ST300Q10JA   &lt;dbl&gt; 4, 5, 1, 3, NA, 5, 1, NA, NA, 5, NA, 5, NA, 4, 2, NA, 1, …\n$ ST327Q01JA   &lt;dbl&gt; 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST327Q02JA   &lt;dbl&gt; 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST327Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST327Q04JA   &lt;dbl&gt; 2, 1, 3, 3, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST327Q05JA   &lt;dbl&gt; 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 3, 3, 1, 1, 1, 2, 1, 1, 1, …\n$ ST327Q06JA   &lt;dbl&gt; 1, 1, 3, 3, 2, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST327Q07JA   &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 1, 1, 3, 3, 3, 1, 1, 1, 1, 1, NA, 3, 3,…\n$ ST327Q08JA   &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 1, 1, 3, 3, 3, 2, 1, 1, 1, 1, NA, 3, 3,…\n$ ST330Q01WA   &lt;dbl&gt; 3, NA, NA, 3, 3, 3, NA, 3, 3, NA, 3, 3, 3, 1, NA, NA, NA,…\n$ ST330Q02WA   &lt;dbl&gt; 2, NA, 1, NA, 3, NA, 3, NA, 3, 3, NA, NA, 1, 1, 2, NA, NA…\n$ ST330Q03WA   &lt;dbl&gt; NA, NA, 3, 3, NA, 3, 3, NA, NA, NA, NA, 1, 3, NA, 3, 1, 3…\n$ ST330Q04WA   &lt;dbl&gt; NA, 1, NA, NA, 3, 3, NA, 2, NA, 1, NA, NA, NA, NA, NA, 3,…\n$ ST330Q05WA   &lt;dbl&gt; NA, 1, NA, NA, NA, NA, 3, NA, NA, NA, NA, 2, NA, 3, 3, NA…\n$ ST330Q06WA   &lt;dbl&gt; NA, 1, NA, NA, 2, NA, 2, NA, 3, 1, 3, 1, NA, NA, 2, 1, NA…\n$ ST330Q07WA   &lt;dbl&gt; 2, NA, 1, 3, NA, NA, 2, 2, 2, 1, NA, NA, NA, NA, NA, NA, …\n$ ST330Q08WA   &lt;dbl&gt; NA, NA, 3, 3, NA, NA, NA, NA, NA, NA, 3, NA, 3, 1, NA, NA…\n$ ST330Q09WA   &lt;dbl&gt; NA, 1, NA, NA, 2, 3, NA, 2, NA, NA, 2, NA, NA, NA, NA, 1,…\n$ ST330Q11WA   &lt;dbl&gt; 2, 1, NA, 3, NA, 3, NA, NA, NA, 1, 2, NA, NA, 3, NA, NA, …\n$ ST330D10WA   &lt;chr&gt; \"7020003\", \"9999999\", \"7020003\", \"9999999\", \"9999999\", \"9…\n$ ST324Q02JA   &lt;dbl&gt; 3, 3, NA, NA, 4, NA, 3, 2, 3, 3, 2, 3, 4, 3, 3, 4, NA, NA…\n$ ST324Q04JA   &lt;dbl&gt; 3, NA, NA, NA, 4, 3, NA, NA, NA, NA, 3, 4, 3, NA, NA, 3, …\n$ ST324Q05JA   &lt;dbl&gt; NA, NA, 2, 2, NA, 4, NA, 2, 3, 1, 2, 4, NA, 3, 4, NA, 3, …\n$ ST324Q07JA   &lt;dbl&gt; 4, NA, 3, NA, 2, NA, 3, NA, NA, 3, NA, 2, NA, NA, 3, 4, 3…\n$ ST324Q10JA   &lt;dbl&gt; NA, 2, 2, 2, NA, 3, NA, 2, 1, 2, NA, NA, NA, 3, 3, NA, 3,…\n$ ST324Q11JA   &lt;dbl&gt; NA, 1, 2, NA, NA, 2, NA, NA, NA, NA, NA, NA, 2, NA, NA, N…\n$ ST324Q12JA   &lt;dbl&gt; 3, NA, 3, 2, NA, NA, 2, NA, 4, NA, NA, 3, NA, 3, NA, NA, …\n$ ST324Q13JA   &lt;dbl&gt; NA, 1, NA, 2, 4, 3, 1, 2, 4, 3, 3, NA, 3, NA, 3, 1, 2, 3,…\n$ ST324Q14JA   &lt;dbl&gt; 2, 3, NA, 2, 2, NA, 2, 3, NA, NA, 3, NA, 2, 3, NA, 2, 2, …\n$ ST347Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST347Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST348Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST348Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST348Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST348Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST348Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST348Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST348Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST348Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST349Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST350Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST351Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST351Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST351Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST351Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST351Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST351Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST351Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST351Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST352Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST352Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST352Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST352Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST352Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST352Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST352Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST352Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST353Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST353Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST353Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST353Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST353Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST353Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST353Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST353Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST354Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST354Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST354Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST354Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST354Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST354Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST354Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST354Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST354Q09JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST354Q10JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST355Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST355Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST355Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST355Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST355Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST355Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST355Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST355Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST356Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST331Q01JA   &lt;dbl&gt; 7, 8, 8, 8, 10, 6, 7, 10, 7, 8, 10, 9, 7, 6, 8, 4, 7, 8, …\n$ ST331Q02JA   &lt;dbl&gt; 8, 10, 9, 10, 10, 10, 8, 10, 10, 10, 10, 10, 9, 7, 10, 10…\n$ ST331Q03JA   &lt;dbl&gt; 8, 8, 7, 10, 10, 7, 8, 10, 9, 8, 9, 10, 10, 6, 10, 10, 8,…\n$ FL150Q01TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL150Q02TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL150Q03TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL164Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL164Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL164Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL164Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL164Q05HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL164Q06HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL164Q07HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL164Q08HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL164Q09HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL164Q10HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL164Q11HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL164Q12HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL164Q13HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL164Q14HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL164Q15HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL164Q16HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL166Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL166Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL166Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL166Q05HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL166Q06HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL166Q07HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL174Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL174Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL174Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL174Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL174Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL174Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL174Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL167Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL167Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL167Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL167Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL167Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL167Q05HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL167Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL170Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL170Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL170Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL170Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL170Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL170Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL170Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL159Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL159Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL159Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL159Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL160Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL160Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL160Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL160Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL161Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL161Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL161Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL162Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL162Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL162Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL162Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL162Q05HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL162Q06HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL163Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL163Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL163Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL163Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL163Q05HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL171Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL171Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL171Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL171Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL171Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL171Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL171Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL171Q09JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL171Q10JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL171Q11JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL171Q12JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL169Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL169Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL169Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL169Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL169Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL169Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL169Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL169Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL169Q10JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL169Q11JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL172Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL172Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL172Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL172Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ IC170Q01JA   &lt;dbl&gt; 4, 4, 1, 2, 5, 3, 5, 3, 4, 4, 5, 2, 1, 5, 4, 4, 4, 1, 3, …\n$ IC170Q02JA   &lt;dbl&gt; 4, 4, 1, 4, 5, 6, 5, 6, 2, 5, 5, 4, 1, 5, 5, 1, 3, 4, 5, …\n$ IC170Q03JA   &lt;dbl&gt; 1, 4, 5, 1, 6, 3, 1, 5, 1, 6, 6, 4, 4, 5, 5, 1, 1, 4, 1, …\n$ IC170Q04JA   &lt;dbl&gt; 4, 4, 5, 1, 5, 3, 5, 5, 1, 4, 5, 4, 5, 5, 5, 4, 1, 4, 5, …\n$ IC170Q05JA   &lt;dbl&gt; 2, 4, 1, 1, 4, 3, 1, 5, 2, 4, 5, 4, 5, 5, 1, 4, 1, 1, 5, …\n$ IC170Q06JA   &lt;dbl&gt; 2, 4, 5, 1, 6, 3, 3, 5, 3, 1, 3, 4, 2, 5, 5, 2, 2, 4, 1, …\n$ IC170Q07JA   &lt;dbl&gt; 4, 4, 5, 1, 5, 3, 3, 5, 5, 4, 5, 4, 4, 5, 5, 4, 5, 3, 3, …\n$ IC171Q01JA   &lt;dbl&gt; 4, 2, 4, 1, 4, 5, 5, 5, 4, 5, 5, 4, 3, 5, 2, 5, 1, 4, 5, …\n$ IC171Q02JA   &lt;dbl&gt; 5, 4, 4, 5, 4, 5, 5, 5, 4, 5, 5, 4, 4, 5, 5, 5, 5, 4, 5, …\n$ IC171Q03JA   &lt;dbl&gt; 1, 1, 4, 1, 1, 5, 1, 5, 4, 1, 6, 4, 4, 5, 5, 6, 1, 4, 1, …\n$ IC171Q04JA   &lt;dbl&gt; 5, 4, 4, 5, 4, 5, 5, 5, 4, 5, 5, 4, 4, 5, 5, 5, 5, 4, 5, …\n$ IC171Q05JA   &lt;dbl&gt; 2, 1, 3, 1, 1, 1, 4, 5, 3, 1, 4, 4, 2, 5, 5, 1, 1, 4, 3, …\n$ IC171Q06JA   &lt;dbl&gt; 4, 1, 5, 1, 1, 5, 5, 5, 4, 5, 4, 4, 1, 5, 2, 5, 1, 4, 3, …\n$ IC172Q01JA   &lt;dbl&gt; 3, 4, 4, 4, 4, 4, 3, 3, 4, 3, 3, 4, 3, 3, 4, 4, 3, 3, 3, …\n$ IC172Q02JA   &lt;dbl&gt; 3, 4, 4, 4, 4, 4, 3, 3, 4, 3, 3, 4, 3, 3, 4, 4, 3, 3, 2, …\n$ IC172Q03JA   &lt;dbl&gt; 3, 4, 3, 4, 3, 4, 1, 3, 3, 2, 4, 2, 3, 3, 4, 4, 3, 3, 2, …\n$ IC172Q04JA   &lt;dbl&gt; 3, 4, 3, 4, 3, 4, 3, 3, 4, 3, 3, 3, 3, 3, 4, 4, 3, 3, 2, …\n$ IC172Q05JA   &lt;dbl&gt; 3, 4, 3, 4, 4, 4, 3, 3, 4, 3, 3, 4, 3, 3, 4, 4, 3, 3, 3, …\n$ IC172Q06JA   &lt;dbl&gt; 3, 4, 4, 4, 4, 4, 2, 3, 4, 3, 3, 4, 4, 3, 4, 4, 3, 3, 2, …\n$ IC172Q07JA   &lt;dbl&gt; 3, 4, 3, 4, 4, 3, 3, 3, 4, 3, 3, 4, 3, 3, 4, 4, 3, 3, 3, …\n$ IC172Q08JA   &lt;dbl&gt; 3, 4, 3, 4, 4, 3, 2, 3, 3, 3, 3, 4, 2, 3, 4, 4, 3, 3, 3, …\n$ IC172Q09JA   &lt;dbl&gt; 3, 4, 4, 4, 4, 3, 4, 3, 4, 3, 3, 4, 3, 3, 4, 4, 3, 3, 2, …\n$ IC173Q01JA   &lt;dbl&gt; 3, 4, 4, 1, 5, 5, 3, 4, 5, 4, 3, 2, 3, 4, 5, 3, 5, 3, 2, …\n$ IC173Q02JA   &lt;dbl&gt; 1, 2, 5, 1, 1, 3, 5, 4, 3, 1, 1, 3, 1, 2, 2, 2, 2, 4, 1, …\n$ IC173Q03JA   &lt;dbl&gt; 2, 4, 4, 1, 5, 3, 3, 4, 2, 3, 3, 2, 3, 2, 2, 1, 2, 2, 1, …\n$ IC173Q04JA   &lt;dbl&gt; 6, 6, 6, 1, 6, 6, 6, 4, 6, 6, 6, 6, 6, 6, 6, 1, 6, 6, 6, …\n$ IC174Q01JA   &lt;dbl&gt; 2, 3, 1, 3, 3, 2, 1, 1, 2, 3, NA, 4, 1, 1, 3, 2, 2, 1, 1,…\n$ IC174Q02JA   &lt;dbl&gt; 4, 3, 4, 3, 5, 4, 3, 4, 4, 4, NA, 5, 4, 3, 1, 4, 4, 4, 4,…\n$ IC174Q03JA   &lt;dbl&gt; 3, 3, 3, 3, 5, 1, 1, 4, 3, 4, 2, 4, 1, 3, 4, 4, 2, 4, 4, …\n$ IC174Q04JA   &lt;dbl&gt; 2, 3, 4, 3, 3, 3, 3, 4, 3, 4, 2, 5, 3, 3, 2, 1, 2, 3, 1, …\n$ IC174Q05JA   &lt;dbl&gt; 1, 3, 1, 3, 3, 2, 1, 4, 3, 2, 2, 3, 1, 2, 1, 1, 2, 1, 1, …\n$ IC174Q06JA   &lt;dbl&gt; 1, 3, 1, 3, 5, 2, 1, 1, 3, 1, 3, 3, 1, 2, 1, 2, 2, 1, 1, …\n$ IC174Q07JA   &lt;dbl&gt; 2, 3, 1, 3, 5, 2, 5, 4, 3, 4, 3, 5, 4, 2, 1, 3, 2, 1, 1, …\n$ IC174Q08JA   &lt;dbl&gt; 2, 3, 3, 3, 3, 3, 4, 4, 3, 3, 3, 5, 3, 2, 5, 3, 3, 1, 1, …\n$ IC174Q09JA   &lt;dbl&gt; 2, 3, 3, 3, 3, 3, 3, 1, 3, 4, 3, 4, 1, 3, 1, 4, 3, 1, 1, …\n$ IC174Q10JA   &lt;dbl&gt; 2, 3, 3, 3, 3, 3, 1, 4, 3, 3, 1, 4, 1, 1, 1, 1, 2, 1, 1, …\n$ IC175Q01JA   &lt;dbl&gt; 2, 4, 4, 1, 5, 4, 3, 5, 4, 4, 3, 4, 3, 3, 4, 5, 1, 3, 3, …\n$ IC175Q02JA   &lt;dbl&gt; 2, 3, 2, 2, 5, 5, 3, 5, 4, 4, 3, 4, 1, 3, 1, 5, 1, 1, 3, …\n$ IC175Q03JA   &lt;dbl&gt; 2, 3, 1, 1, 1, 3, 1, 5, 2, 4, 1, 4, 1, 3, 4, 5, 1, 1, 2, …\n$ IC175Q05JA   &lt;dbl&gt; 1, 3, 1, 2, 2, 3, 1, 5, 3, 4, 1, 5, 3, 3, 4, 3, 1, 1, 3, …\n$ IC176Q01JA   &lt;dbl&gt; 3, 2, 5, 1, 5, 3, 3, 5, 3, 5, 3, 4, 1, 3, 4, 4, 3, 4, 3, …\n$ IC176Q02JA   &lt;dbl&gt; 3, 2, 4, 1, 5, 4, 1, 5, 5, 5, 5, 5, 4, 3, 5, 5, 3, 3, 3, …\n$ IC176Q03JA   &lt;dbl&gt; 4, 2, 3, 1, 5, 4, 1, 5, 4, 5, 4, 5, 1, 3, 5, 5, 3, 4, 3, …\n$ IC176Q04JA   &lt;dbl&gt; 4, 3, 5, 2, 5, 4, 3, 5, 5, 5, 5, 5, 5, 3, 5, 5, 3, 5, 4, …\n$ IC176Q05JA   &lt;dbl&gt; 4, 2, 4, 2, 5, 4, 3, 5, 5, 4, 4, 5, 3, 3, 3, 5, 3, 5, 3, …\n$ IC176Q06JA   &lt;dbl&gt; 3, 1, 2, 1, 5, 4, 4, 5, 5, 2, 3, 5, 1, 3, 3, 3, 4, 5, 1, …\n$ IC176Q07JA   &lt;dbl&gt; 4, 2, 4, 1, 5, 4, 5, 5, 5, 4, 5, 5, 4, 3, 5, 3, 4, 5, 5, …\n$ IC176Q08JA   &lt;dbl&gt; 4, 3, 1, 3, 5, 3, 4, 5, 4, 5, 5, 4, 1, 3, 5, 4, 4, 5, 3, …\n$ IC184Q01JA   &lt;dbl&gt; 3, 3, 1, 5, 1, 4, 1, 1, 3, 4, 4, 5, 1, 5, 1, 4, 3, 5, 1, …\n$ IC184Q02JA   &lt;dbl&gt; 4, 3, 3, 4, 1, 4, 1, 1, 3, 4, 4, 5, 1, 5, 4, 4, 3, 4, 3, …\n$ IC184Q03JA   &lt;dbl&gt; 1, 3, 3, 1, 1, 3, 3, 1, 3, 6, 6, 4, 3, 5, 1, 1, 3, 3, 1, …\n$ IC184Q04JA   &lt;dbl&gt; 1, 3, 1, 1, 1, 3, 6, 1, 2, 6, 6, 2, 6, 1, 1, 1, 3, 6, 1, …\n$ IC177Q01JA   &lt;dbl&gt; 3, 2, 4, 2, 4, 4, 4, 3, 1, 5, 3, 2, 1, 4, 4, 3, 4, 3, 1, …\n$ IC177Q02JA   &lt;dbl&gt; 2, 3, 2, 2, 6, 4, 1, 3, 2, 5, 3, 3, 2, 2, 4, 3, 3, 5, 3, …\n$ IC177Q03JA   &lt;dbl&gt; 3, 3, 3, 2, 4, 4, 3, 3, 2, 5, 3, 3, 4, 4, 5, 3, 2, 4, 3, …\n$ IC177Q04JA   &lt;dbl&gt; 2, 3, 2, 2, 4, 4, 3, 1, 2, 3, 2, 2, 1, 3, 5, 2, 1, 3, 2, …\n$ IC177Q05JA   &lt;dbl&gt; 2, 3, 2, 2, 4, 4, 2, 1, 2, 2, 2, 4, 2, 2, 5, 1, 4, 4, 1, …\n$ IC177Q06JA   &lt;dbl&gt; 2, 3, 2, 2, 4, 1, 2, 1, 2, 4, 2, 2, 1, 3, 5, 3, 1, 3, 2, …\n$ IC177Q07JA   &lt;dbl&gt; 1, 3, 1, 2, 5, 1, 1, 3, 1, 3, 1, 2, 1, 1, 5, 2, 1, 1, 1, …\n$ IC178Q01JA   &lt;dbl&gt; 3, 3, 5, 1, 4, 4, 6, 3, 1, 6, 3, 3, 1, 5, 5, 4, 4, 4, 1, …\n$ IC178Q02JA   &lt;dbl&gt; 2, 3, 2, 1, 6, 4, 1, 3, 2, 6, 3, 4, 3, 2, 5, 3, 4, 5, 3, …\n$ IC178Q03JA   &lt;dbl&gt; 3, 3, 2, 1, 4, 4, 2, 3, 2, 6, 3, 4, 5, 5, 5, 3, 1, 4, 2, …\n$ IC178Q04JA   &lt;dbl&gt; 2, 3, 2, 1, 4, 4, 2, 1, 2, 2, 2, 3, 1, 2, 5, 2, 1, 3, 2, …\n$ IC178Q05JA   &lt;dbl&gt; 2, 3, 1, 1, 4, 4, 1, 1, 2, 5, 1, 4, 2, 2, 5, 2, 1, 4, 1, …\n$ IC178Q06JA   &lt;dbl&gt; 3, 3, 2, 1, 4, 1, 1, 1, 2, 3, 3, 4, 1, 2, 5, 2, 1, 3, 2, …\n$ IC178Q07JA   &lt;dbl&gt; 1, 3, 1, 1, 1, 1, 1, 3, 1, 2, 1, 2, 1, 1, 5, 2, 1, 1, 1, …\n$ IC179Q01JA   &lt;dbl&gt; 1, 3, 2, 1, 2, 4, 2, 2, 3, 1, 2, 1, 1, 1, 2, 1, 2, 3, 3, …\n$ IC179Q02JA   &lt;dbl&gt; 1, 2, 2, 1, 2, 2, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 2, 3, 1, …\n$ IC179Q03JA   &lt;dbl&gt; 3, 3, 3, 4, 4, 4, 4, 3, 3, 4, 3, 4, 3, 4, 3, 4, 2, 2, 3, …\n$ IC179Q04JA   &lt;dbl&gt; 2, 3, 2, 1, 2, 4, 2, 2, 4, 1, 2, 4, 1, 3, 3, 3, 2, 2, 2, …\n$ IC179Q05JA   &lt;dbl&gt; 2, 3, 2, 1, 4, 4, 2, 2, 4, 1, 3, 4, 1, 3, 4, 3, 2, 2, 3, …\n$ IC179Q06JA   &lt;dbl&gt; 1, 3, 3, 2, 3, 4, 3, 3, 4, 1, 3, 1, 1, 1, 2, 1, 2, 2, 3, …\n$ IC180Q01JA   &lt;dbl&gt; 3, 2, 2, 2, 3, 4, 2, 2, 2, 3, 2, 2, 2, 3, 2, 1, 3, 2, 2, …\n$ IC180Q02JA   &lt;dbl&gt; 3, 3, 3, 2, 3, 2, 3, 3, 2, 3, 3, 4, 3, 3, 4, 4, 2, 3, 3, …\n$ IC180Q03JA   &lt;dbl&gt; 3, 3, 3, 2, 3, 4, 4, 3, 3, 3, 3, 4, 3, 3, 4, 4, 2, 3, 3, …\n$ IC180Q04JA   &lt;dbl&gt; 2, 2, 2, 2, 3, 1, 2, 3, 3, 3, 3, 4, 1, 1, 1, 2, 2, 3, 2, …\n$ IC180Q05JA   &lt;dbl&gt; 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 4, 1, 1, NA, 2, 2, 3, 3,…\n$ IC180Q06JA   &lt;dbl&gt; 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 4, 1, 1, 3, 2, 2, 3, 2, …\n$ IC180Q07JA   &lt;dbl&gt; 3, 2, 2, 2, 2, 4, 1, 3, 3, 3, 2, 2, 3, 2, 1, 4, 2, 3, 2, …\n$ IC180Q08JA   &lt;dbl&gt; 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 2, …\n$ IC181Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ IC181Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ IC181Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ IC181Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ IC182Q01JA   &lt;dbl&gt; 3, 3, 2, 4, 2, 2, 3, 2, 2, 3, 3, 4, 3, 3, 3, 4, 2, 3, 2, …\n$ IC182Q02JA   &lt;dbl&gt; 3, 2, 3, 4, 2, 2, 4, 2, 2, 2, 3, 3, 3, 2, 3, 3, 2, 3, 3, …\n$ IC182Q03JA   &lt;dbl&gt; 3, 3, 3, 4, 3, 3, 4, 3, 3, 3, 3, 4, 3, 3, 4, 4, 2, 3, 3, …\n$ IC183Q01JA   &lt;dbl&gt; 4, 4, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, …\n$ IC183Q02JA   &lt;dbl&gt; 4, 4, 5, 3, 4, 4, 3, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 3, 4, …\n$ IC183Q03JA   &lt;dbl&gt; 4, 4, 2, 3, 4, 4, 3, 4, 4, 3, 3, 4, 3, 4, 4, 4, 4, 4, 4, …\n$ IC183Q04JA   &lt;dbl&gt; 4, 4, 4, 3, 4, 4, 2, 4, 4, 4, 4, 4, 3, 4, 2, 4, 4, 3, 4, …\n$ IC183Q05JA   &lt;dbl&gt; 4, 4, 3, 3, 4, 3, 3, 1, 4, 4, 3, 4, 3, 4, 3, 4, 3, 3, 4, …\n$ IC183Q07JA   &lt;dbl&gt; 4, 4, 3, 3, 4, 4, 3, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, …\n$ IC183Q08JA   &lt;dbl&gt; 4, 4, 2, 3, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 2, 4, 4, 3, 4, …\n$ IC183Q09JA   &lt;dbl&gt; 4, 4, 1, 3, 4, 4, 2, 4, 4, 4, 3, 4, 2, 4, 3, 4, 4, 3, 4, …\n$ IC183Q10JA   &lt;dbl&gt; 4, 2, 1, 3, 4, 4, 2, 4, 3, 2, 3, 4, 1, 4, 2, 4, 1, 4, 2, …\n$ IC183Q12JA   &lt;dbl&gt; 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, …\n$ IC183Q13JA   &lt;dbl&gt; 4, 4, 2, 3, 4, 4, 4, 1, 4, 3, 3, 4, 2, 1, 4, 4, 1, 3, 4, …\n$ IC183Q14JA   &lt;dbl&gt; 2, 4, 1, 3, 5, 2, 2, 1, 2, 5, 2, 2, 1, 1, 1, 2, 1, 2, 1, …\n$ IC183Q15JA   &lt;dbl&gt; 2, 4, 1, 3, 5, 3, 1, 4, 2, 5, 3, 3, 1, 1, 1, 4, 1, 2, 1, …\n$ IC183Q16JA   &lt;dbl&gt; 3, 4, 3, 3, 5, 3, 2, 4, 3, 5, 3, 3, 1, 1, 1, 4, 1, 2, 1, …\n$ WB150Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB151Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB152Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB153Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB153Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB153Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB153Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB153Q05HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB154Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB154Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB154Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB154Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB154Q05HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB154Q06HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB154Q07HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB154Q08HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB154Q09HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB155Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB155Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB155Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB155Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB155Q05HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB155Q06HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB155Q07HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB155Q08HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB155Q09HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB155Q10HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB156Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB158Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB160Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB161Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB162Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB162Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB162Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB162Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB162Q05HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB162Q06HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB162Q07HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB162Q08HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB162Q09HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB163Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB163Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB163Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB163Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB163Q05HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB163Q06HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB163Q07HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB163Q08HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB164Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB165Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB166Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB166Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB166Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB166Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB167Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB168Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB168Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB168Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB168Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB171Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB171Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB171Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB171Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB172Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB173Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB173Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB173Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB173Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB176Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB177Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB177Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB177Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB177Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB032Q01NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB032Q02NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB031Q01NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB178Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB178Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB178Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB178Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB178Q05HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB178Q06HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB178Q07HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA001Q01TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA001Q02TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA001Q03TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA003Q01TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA003Q02TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA003Q03TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA003Q05IA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA003Q18WA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA003Q19WA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA003Q20WA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA003Q11JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA003Q12JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA003Q13JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA003Q14JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA003Q15JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA003Q16JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA003Q17JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA196Q01WA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA196Q02WA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA196Q03WA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA196Q04WA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA197Q01WA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA197Q02WA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA197Q03WA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA197Q04WA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA197Q05WA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA008Q01TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA008Q02TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA008Q03TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA008Q04TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA008Q05TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA008Q06NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA008Q07NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA008Q08NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA008Q09NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA008Q10NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA009Q01NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA009Q02NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA009Q03NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA009Q04NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA009Q05NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA009Q06NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA009Q07NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA009Q08NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA009Q09NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA009Q10NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA009Q11NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA007Q01TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA007Q02TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA007Q03TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA007Q04TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA007Q05TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA007Q06TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA007Q07TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA007Q09NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA007Q11NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA007Q12NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA007Q13NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA007Q14NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA007Q15NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA005Q01TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA006Q01TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA006Q02TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA006Q03TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA006Q04TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA006Q05TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA006Q06TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA006Q07TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA006Q08TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA006Q09TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA006Q10TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA006Q11TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA006Q12HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA006Q13HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA006Q14HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA166Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA167Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA167Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA167Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA167Q05HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA183Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA183Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA183Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA183Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA183Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA183Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA183Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA183Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA018Q01NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA018Q02NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA018Q03NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA177Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA177Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA177Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA177Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA177Q05HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA177Q06HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA177Q07HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA177Q08HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA180Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA182Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA175Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA175Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA175Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA175Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA185Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA185Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA185Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA185Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA185Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA185Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA185Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA185Q09JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA185Q10JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA186Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA186Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA186Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA186Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA186Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA186Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA186Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA186Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA187Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA187Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA188Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA188Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA188Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA188Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA188Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA188Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA188Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA188Q09JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA188Q10JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA189Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA189Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA189Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA189Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA189Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA189Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA189Q09JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA189Q10JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA194Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA195Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA041Q01TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA042Q01TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ EFFORT1      &lt;dbl&gt; 7, 8, 7, NA, 10, NA, 7, 10, 7, 7, 10, 9, 7, 6, 8, NA, 6, …\n$ EFFORT2      &lt;dbl&gt; 9, 8, 8, NA, 10, NA, 8, 10, 10, 10, 10, 10, 10, 7, 10, NA…\n$ OCOD1        &lt;chr&gt; \"9701\", \"31\", \"9701\", \"41\", \"23\", \"9701\", \"11\", \"23\", \"1\"…\n$ OCOD2        &lt;chr&gt; \"83\", \"21\", \"9704\", \"9705\", \"83\", \"34\", \"31\", \"21\", \"14\",…\n$ OCOD3        &lt;chr&gt; \"2634\", \"9705\", \"9704\", \"2411\", \"21\", \"22\", \"2512\", \"2310…\n$ PROGN        &lt;chr&gt; \"07020002\", \"07020002\", \"07020002\", \"07020002\", \"07020002…\n$ AGE          &lt;dbl&gt; 15.50, 15.83, 15.75, 16.17, 15.58, 15.58, 16.08, 16.00, 1…\n$ GRADE        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ ISCEDP       &lt;dbl&gt; 344, 344, 344, 344, 344, 344, 344, 344, 344, 344, 344, 34…\n$ IMMIG        &lt;dbl&gt; 1, 1, 1, 1, 1, 3, 1, 3, 1, 1, 1, 1, 1, 3, 1, 2, 3, 3, 1, …\n$ COBN_S       &lt;chr&gt; \"070200\", \"070200\", \"070200\", \"070200\", \"070200\", \"970200…\n$ COBN_M       &lt;chr&gt; \"070200\", \"070200\", \"970200\", \"070200\", \"070200\", \"970200…\n$ COBN_F       &lt;chr&gt; \"070200\", \"070200\", \"070200\", \"070200\", \"070200\", \"970200…\n$ LANGN        &lt;dbl&gt; 998, 998, 998, 998, 998, 998, 998, 998, 998, 998, 998, 99…\n$ REPEAT       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ MISSSC       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, …\n$ SKIPPING     &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, …\n$ TARDYSD      &lt;dbl&gt; 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ EXERPRAC     &lt;dbl&gt; 1, 4, 2, 5, 9, 1, 2, 0, 3, 5, 1, 2, 5, 2, 4, 0, 2, 3, 2, …\n$ STUDYHMW     &lt;dbl&gt; 4, 7, 3, 5, 7, 10, 0, 10, 5, 3, 5, 5, 10, 0, 8, 5, 5, 5, …\n$ WORKPAY      &lt;dbl&gt; 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ WORKHOME     &lt;dbl&gt; 10, 2, 0, 10, 5, 5, 7, 0, 0, 4, 2, 2, 10, 0, 10, 0, 5, 5,…\n$ EXPECEDU     &lt;dbl&gt; 7, 7, 6, NA, 6, 7, 9, 9, 7, 6, 7, 8, 9, 9, 9, 9, 7, 7, 7,…\n$ MATHPREF     &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, …\n$ MATHEASE     &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, …\n$ MATHMOT      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ DURECEC      &lt;dbl&gt; 3, 2, NA, 3, NA, 4, 3, NA, NA, 3, NA, 5, NA, 5, 3, 2, NA,…\n$ BSMJ         &lt;dbl&gt; 85.85, NA, NA, 76.65, 79.49, 76.98, 74.66, 85.41, 24.79, …\n$ SISCO        &lt;dbl&gt; 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ RELATST      &lt;dbl&gt; -0.2606, 1.2437, 0.7190, -0.2194, 1.0726, 1.1296, -1.0841…\n$ BELONG       &lt;dbl&gt; 0.2442, -0.0437, -0.6137, -1.1147, 2.1143, 0.5159, -0.718…\n$ BULLIED      &lt;dbl&gt; -1.2280, -0.2016, -1.2280, -1.2280, -1.2280, -1.2280, 1.0…\n$ FEELSAFE     &lt;dbl&gt; -0.7560, 1.1246, 0.4417, -0.7560, 1.1246, 0.1413, 0.0936,…\n$ SCHRISK      &lt;dbl&gt; -0.6386, 0.1810, -0.6386, -0.6386, -0.6386, 0.1810, 0.181…\n$ PERSEVAGR    &lt;dbl&gt; 0.4369, 0.4540, -0.4017, 0.5617, -0.5954, -0.1554, 0.4375…\n$ CURIOAGR     &lt;dbl&gt; 2.7951, 0.3058, -0.6563, 0.1778, 0.3406, 0.3247, 0.0134, …\n$ COOPAGR      &lt;dbl&gt; -0.0319, 0.1187, -0.6986, 0.0849, 0.8806, 4.8203, -0.7118…\n$ EMPATAGR     &lt;dbl&gt; 1.3979, 0.1290, -0.2087, -0.1344, -0.5172, -0.5881, -1.26…\n$ ASSERAGR     &lt;dbl&gt; -0.2970, -0.1734, -0.4816, -0.1538, 0.2470, -0.8113, -0.1…\n$ STRESAGR     &lt;dbl&gt; 0.9777, -0.7402, -0.2053, 0.2201, -0.9940, 0.2183, 0.4922…\n$ EMOCOAGR     &lt;dbl&gt; 1.2321, -0.5609, 0.5777, -0.0563, -0.8597, NA, 0.8646, 1.…\n$ GROSAGR      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ INFOSEEK     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FAMSUP       &lt;dbl&gt; -0.3780, -0.5969, -1.0537, -0.8521, 1.7459, 1.7327, -0.88…\n$ DISCLIM      &lt;dbl&gt; 0.3884, 1.1687, 0.2002, -0.1219, 0.1166, -0.1006, -0.8709…\n$ TEACHSUP     &lt;dbl&gt; -0.5635, 0.1475, -0.5635, -0.1002, 1.5558, 0.1475, -0.332…\n$ COGACRCO     &lt;dbl&gt; -0.4092, -0.7102, -0.6541, -0.0820, 0.5051, 0.2635, 0.142…\n$ COGACMCO     &lt;dbl&gt; 0.1666, 0.0024, -0.5604, 0.1868, 2.3426, 0.7819, -0.5560,…\n$ EXPOFA       &lt;dbl&gt; 0.3356, -1.2147, 0.6908, -0.2194, 0.5478, 0.5515, -0.1287…\n$ EXPO21ST     &lt;dbl&gt; -0.7746, -0.5240, 0.0949, 0.4959, 0.4121, 0.4936, 0.2869,…\n$ MATHEFF      &lt;dbl&gt; 0.1429, -0.2874, 0.2226, -0.9344, -0.9322, 0.2637, 1.2000…\n$ MATHEF21     &lt;dbl&gt; 0.4317, 0.7644, -0.4779, -0.5515, -0.5730, -0.2489, 1.401…\n$ FAMCON       &lt;dbl&gt; 1.3180, 4.7588, 0.4736, 0.0594, 0.8973, 2.2322, 3.6192, 4…\n$ ANXMAT       &lt;dbl&gt; 0.3729, 0.6647, 0.0510, 0.6387, 2.5026, -0.7358, -0.9755,…\n$ MATHPERS     &lt;dbl&gt; -0.1305, 0.6178, -0.3993, 1.8158, 1.1353, 0.9918, -1.5183…\n$ CREATEFF     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ CREATSCH     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ CREATFAM     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ CREATAS      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ CREATOOS     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ CREATOP      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ OPENART      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ IMAGINE      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ SCHSUST      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ LEARRES      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PROBSELF     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FAMSUPSL     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FEELLAH      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ SDLEFF       &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ MISCED       &lt;dbl&gt; 8, 7, 4, 6, 7, 7, 6, 9, 8, 8, 4, 9, 10, 6, 4, 9, 8, 8, 6,…\n$ FISCED       &lt;dbl&gt; 7, 7, 4, 6, 7, 9, 2, 8, 8, 7, 4, 9, 10, 9, 6, 4, 9, 8, 6,…\n$ HISCED       &lt;dbl&gt; 8, 7, 4, 6, 7, 9, 6, 9, 8, 8, 4, 9, 10, 9, 6, 9, 9, 8, 6,…\n$ PAREDINT     &lt;dbl&gt; 16.0, 14.5, 12.0, 12.0, 14.5, 16.0, 12.0, 16.0, 16.0, 16.…\n$ BMMJ1        &lt;dbl&gt; 17.00, 37.83, 17.00, 43.33, 75.54, 17.00, 70.34, 75.54, 6…\n$ BFMJ2        &lt;dbl&gt; 30.34, 77.10, NA, NA, 30.34, 57.64, 40.54, 80.78, 43.85, …\n$ HISEI        &lt;dbl&gt; 30.34, 77.10, 17.00, 43.33, 75.54, 57.64, 70.34, 80.78, 6…\n$ ICTRES       &lt;dbl&gt; 0.1940, 0.6249, -0.3987, -0.9028, 0.2514, -0.4733, 0.9904…\n$ HOMEPOS      &lt;dbl&gt; 0.7524, 0.7842, 0.0666, -0.9300, -0.8949, -0.5988, 0.0975…\n$ ESCS         &lt;dbl&gt; 0.1836, 0.8261, -1.0357, -0.9606, 0.0856, 0.1268, -0.0154…\n$ FCFMLRTY     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FLSCHOOL     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FLMULTSB     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FLFAMILY     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ACCESSFP     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FLCONFIN     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FLCONICT     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ACCESSFA     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ATTCONFM     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FRINFLFM     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ICTSCH       &lt;dbl&gt; 0.4062, 0.4062, 0.4062, 0.4062, -1.6647, -0.8411, 0.4062,…\n$ ICTAVSCH     &lt;dbl&gt; 7, 7, 7, 7, 5, 6, 7, 6, 7, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, …\n$ ICTHOME      &lt;dbl&gt; 0.3346, 0.3346, 0.3346, 0.3346, 0.3346, 0.3346, 0.3346, 0…\n$ ICTAVHOM     &lt;dbl&gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6, 6, 6, 5, 6, 6, 6, …\n$ ICTQUAL      &lt;dbl&gt; 0.3623, 2.8889, 1.2313, 2.8889, 1.8870, 1.6615, -0.2086, …\n$ ICTSUBJ      &lt;dbl&gt; -0.1315, 0.5048, 0.9454, -2.0101, 0.6775, 0.6012, 0.5918,…\n$ ICTENQ       &lt;dbl&gt; -0.3767, 0.3109, -0.1658, 0.3109, 1.0316, -0.0477, -0.021…\n$ ICTFEED      &lt;dbl&gt; -0.4038, 0.4297, -0.4292, -0.6744, 0.2776, 0.9257, -0.354…\n$ ICTOUT       &lt;dbl&gt; 0.2260, -0.8080, 0.1088, -1.2894, 2.9804, 0.3464, -0.4834…\n$ ICTWKDY      &lt;dbl&gt; -0.4469, 0.4182, -0.3710, -0.5032, 1.3145, 0.4565, -0.250…\n$ ICTWKEND     &lt;dbl&gt; -0.3452, 0.3311, -0.7926, -3.5000, 0.8948, 0.4976, -1.085…\n$ ICTREG       &lt;dbl&gt; -0.1855, 0.9444, 0.2941, -0.6148, 0.8255, 1.9301, 0.3353,…\n$ ICTINFO      &lt;dbl&gt; 0.2929, -0.4797, 0.0811, -0.8360, 0.4191, 0.2147, 0.1939,…\n$ ICTDISTR     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ICTEFFIC     &lt;dbl&gt; 0.5764, 0.7781, -0.8446, -0.5172, 1.0613, 0.3941, -0.5344…\n$ STUBMI       &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ BODYIMA      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ SOCONPA      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ LIFESAT      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PSYCHSYM     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ SOCCON       &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ EXPWB        &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ CURSUPP      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PQMIMP       &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PQMCAR       &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PARINVOL     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PQSCHOOL     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PASCHPOL     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ATTIMMP      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PAREXPT      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ CREATHME     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ CREATACT     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ CREATOPN     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ CREATOR      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ W_FSTUWT     &lt;dbl&gt; 5.35943, 6.74318, 6.77500, 5.43200, 5.82353, 5.69931, 4.9…\n$ W_FSTURWT1   &lt;dbl&gt; 8.20073, 9.88533, 10.16250, 7.98824, 9.00000, 2.68850, 2.…\n$ W_FSTURWT2   &lt;dbl&gt; 2.63139, 10.36607, 10.16250, 2.66275, 8.48571, 8.76685, 2…\n$ W_FSTURWT3   &lt;dbl&gt; 8.03388, 10.33466, 10.16250, 8.31429, 2.82857, 8.34850, 2…\n$ W_FSTURWT4   &lt;dbl&gt; 8.18690, 3.44489, 3.38750, 7.98824, 9.00000, 8.40156, 7.3…\n$ W_FSTURWT5   &lt;dbl&gt; 7.89416, 3.29511, 3.38750, 2.56226, 8.48571, 2.89878, 7.5…\n$ W_FSTURWT6   &lt;dbl&gt; 8.04452, 3.44489, 3.38750, 2.56226, 8.48571, 9.60179, 2.4…\n$ W_FSTURWT7   &lt;dbl&gt; 2.67796, 3.29830, 3.38750, 2.66275, 9.00000, 2.92228, 7.5…\n$ W_FSTURWT8   &lt;dbl&gt; 2.72897, 10.36607, 10.16250, 2.66275, 3.00000, 8.76685, 7…\n$ W_FSTURWT9   &lt;dbl&gt; 7.74437, 3.29511, 3.38750, 7.98824, 2.82857, 3.05511, 2.4…\n$ W_FSTURWT10  &lt;dbl&gt; 2.67796, 9.89489, 10.16250, 2.66275, 8.48571, 2.89878, 2.…\n$ W_FSTURWT11  &lt;dbl&gt; 2.73358, 3.29830, 3.38750, 8.31429, 2.82857, 3.02482, 7.3…\n$ W_FSTURWT12  &lt;dbl&gt; 8.18690, 9.88533, 10.16250, 2.66275, 2.82857, 2.80052, 7.…\n$ W_FSTURWT13  &lt;dbl&gt; 8.20073, 10.33466, 10.16250, 7.68679, 9.00000, 8.02740, 7…\n$ W_FSTURWT14  &lt;dbl&gt; 2.58146, 10.36607, 10.16250, 7.68679, 8.48571, 8.34850, 7…\n$ W_FSTURWT15  &lt;dbl&gt; 2.68151, 9.89489, 10.16250, 7.98824, 3.00000, 2.58510, 2.…\n$ W_FSTURWT16  &lt;dbl&gt; 2.63139, 3.45536, 3.38750, 8.31429, 2.82857, 8.40156, 7.5…\n$ W_FSTURWT17  &lt;dbl&gt; 2.73358, 3.45536, 3.38750, 2.66275, 3.00000, 8.69635, 2.4…\n$ W_FSTURWT18  &lt;dbl&gt; 7.89416, 9.88533, 10.16250, 2.77143, 3.00000, 2.89878, 7.…\n$ W_FSTURWT19  &lt;dbl&gt; 2.72897, 3.29830, 3.38750, 7.68679, 9.00000, 2.78283, 2.5…\n$ W_FSTURWT20  &lt;dbl&gt; 8.03388, 3.44489, 3.38750, 2.77143, 3.00000, 8.34850, 2.5…\n$ W_FSTURWT21  &lt;dbl&gt; 7.89416, 9.88533, 3.38750, 2.66275, 9.00000, 2.92228, 2.4…\n$ W_FSTURWT22  &lt;dbl&gt; 2.63139, 10.36607, 3.38750, 7.98824, 8.48571, 8.06550, 2.…\n$ W_FSTURWT23  &lt;dbl&gt; 8.03388, 10.33466, 3.38750, 2.77143, 2.82857, 7.73009, 2.…\n$ W_FSTURWT24  &lt;dbl&gt; 7.88647, 3.44489, 10.16250, 2.66275, 9.00000, 9.16534, 7.…\n$ W_FSTURWT25  &lt;dbl&gt; 7.89416, 3.29511, 10.16250, 7.68679, 8.48571, 2.89878, 7.…\n$ W_FSTURWT26  &lt;dbl&gt; 8.36312, 3.44489, 10.16250, 7.68679, 8.48571, 8.76685, 2.…\n$ W_FSTURWT27  &lt;dbl&gt; 2.67796, 3.29830, 10.16250, 7.98824, 9.00000, 2.92228, 7.…\n$ W_FSTURWT28  &lt;dbl&gt; 2.62882, 10.36607, 3.38750, 7.98824, 3.00000, 8.76685, 7.…\n$ W_FSTURWT29  &lt;dbl&gt; 8.03388, 3.29511, 10.16250, 2.66275, 2.82857, 2.80052, 2.…\n$ W_FSTURWT30  &lt;dbl&gt; 2.78196, 9.89489, 3.38750, 7.98824, 8.48571, 2.89878, 2.4…\n$ W_FSTURWT31  &lt;dbl&gt; 2.73358, 3.29830, 10.16250, 2.77143, 2.82857, 2.78283, 7.…\n$ W_FSTURWT32  &lt;dbl&gt; 8.18690, 9.88533, 3.38750, 7.98824, 2.82857, 2.80052, 7.5…\n$ W_FSTURWT33  &lt;dbl&gt; 8.20073, 10.33466, 3.38750, 2.56226, 9.00000, 8.02740, 7.…\n$ W_FSTURWT34  &lt;dbl&gt; 2.67796, 10.36607, 3.38750, 2.56226, 8.48571, 8.34850, 7.…\n$ W_FSTURWT35  &lt;dbl&gt; 2.68151, 9.89489, 3.38750, 2.66275, 3.00000, 2.80052, 2.5…\n$ W_FSTURWT36  &lt;dbl&gt; 2.73358, 3.45536, 10.16250, 2.77143, 2.82857, 8.40156, 7.…\n$ W_FSTURWT37  &lt;dbl&gt; 2.63139, 3.45536, 10.16250, 7.98824, 3.00000, 9.48693, 2.…\n$ W_FSTURWT38  &lt;dbl&gt; 7.89416, 9.88533, 3.38750, 8.31429, 3.00000, 2.89878, 7.3…\n$ W_FSTURWT39  &lt;dbl&gt; 2.62882, 3.29830, 10.16250, 2.56226, 9.00000, 2.78283, 2.…\n$ W_FSTURWT40  &lt;dbl&gt; 8.03388, 3.44489, 10.16250, 8.31429, 3.00000, 9.07446, 2.…\n$ W_FSTURWT41  &lt;dbl&gt; 8.20073, 9.88533, 3.38750, 2.66275, 2.82857, 9.07446, 7.5…\n$ W_FSTURWT42  &lt;dbl&gt; 2.63139, 10.36607, 3.38750, 7.98824, 3.00000, 2.78283, 7.…\n$ W_FSTURWT43  &lt;dbl&gt; 8.03388, 10.33466, 3.38750, 2.77143, 9.00000, 2.92228, 7.…\n$ W_FSTURWT44  &lt;dbl&gt; 8.18690, 3.44489, 10.16250, 2.66275, 2.82857, 2.89878, 2.…\n$ W_FSTURWT45  &lt;dbl&gt; 7.89416, 3.29511, 10.16250, 7.68679, 3.00000, 8.40156, 2.…\n$ W_FSTURWT46  &lt;dbl&gt; 8.04452, 3.44489, 10.16250, 7.68679, 3.00000, 2.57670, 7.…\n$ W_FSTURWT47  &lt;dbl&gt; 2.67796, 3.29830, 10.16250, 7.98824, 2.82857, 8.34850, 2.…\n$ W_FSTURWT48  &lt;dbl&gt; 2.72897, 10.36607, 3.38750, 7.98824, 8.48571, 2.78283, 2.…\n$ W_FSTURWT49  &lt;dbl&gt; 7.74437, 3.29511, 10.16250, 2.66275, 9.00000, 8.02740, 7.…\n$ W_FSTURWT50  &lt;dbl&gt; 2.67796, 9.89489, 3.38750, 7.98824, 3.00000, 8.40156, 7.5…\n$ W_FSTURWT51  &lt;dbl&gt; 2.73358, 3.29830, 10.16250, 2.77143, 9.00000, 8.06550, 2.…\n$ W_FSTURWT52  &lt;dbl&gt; 8.18690, 9.88533, 3.38750, 7.98824, 9.00000, 8.69635, 2.4…\n$ W_FSTURWT53  &lt;dbl&gt; 8.20073, 10.33466, 3.38750, 2.56226, 2.82857, 3.05511, 2.…\n$ W_FSTURWT54  &lt;dbl&gt; 2.58146, 10.36607, 3.38750, 2.56226, 3.00000, 2.92228, 2.…\n$ W_FSTURWT55  &lt;dbl&gt; 2.68151, 9.89489, 3.38750, 2.66275, 8.48571, 9.48693, 7.3…\n$ W_FSTURWT56  &lt;dbl&gt; 2.63139, 3.45536, 10.16250, 2.77143, 9.00000, 2.89878, 2.…\n$ W_FSTURWT57  &lt;dbl&gt; 2.73358, 3.45536, 10.16250, 7.98824, 8.48571, 2.80052, 7.…\n$ W_FSTURWT58  &lt;dbl&gt; 7.89416, 9.88533, 3.38750, 8.31429, 8.48571, 8.40156, 2.5…\n$ W_FSTURWT59  &lt;dbl&gt; 2.72897, 3.29830, 10.16250, 2.56226, 2.82857, 8.76685, 7.…\n$ W_FSTURWT60  &lt;dbl&gt; 8.03388, 3.44489, 10.16250, 8.31429, 8.48571, 2.92228, 7.…\n$ W_FSTURWT61  &lt;dbl&gt; 7.89416, 9.88533, 10.16250, 7.98824, 2.82857, 8.34850, 7.…\n$ W_FSTURWT62  &lt;dbl&gt; 2.63139, 10.36607, 10.16250, 2.66275, 3.00000, 3.02482, 7…\n$ W_FSTURWT63  &lt;dbl&gt; 8.03388, 10.33466, 10.16250, 8.31429, 9.00000, 3.20060, 7…\n$ W_FSTURWT64  &lt;dbl&gt; 7.88647, 3.44489, 3.38750, 7.98824, 2.82857, 2.67580, 2.5…\n$ W_FSTURWT65  &lt;dbl&gt; 7.89416, 3.29511, 3.38750, 2.56226, 3.00000, 8.40156, 2.4…\n$ W_FSTURWT66  &lt;dbl&gt; 8.36312, 3.44489, 3.38750, 2.56226, 3.00000, 2.78283, 7.5…\n$ W_FSTURWT67  &lt;dbl&gt; 2.67796, 3.29830, 3.38750, 2.66275, 2.82857, 8.34850, 2.4…\n$ W_FSTURWT68  &lt;dbl&gt; 2.62882, 10.36607, 10.16250, 2.66275, 8.48571, 2.78283, 2…\n$ W_FSTURWT69  &lt;dbl&gt; 8.03388, 3.29511, 3.38750, 7.98824, 9.00000, 8.69635, 7.5…\n$ W_FSTURWT70  &lt;dbl&gt; 2.78196, 9.89489, 10.16250, 2.66275, 3.00000, 8.40156, 7.…\n$ W_FSTURWT71  &lt;dbl&gt; 2.73358, 3.29830, 3.38750, 8.31429, 9.00000, 8.76685, 2.5…\n$ W_FSTURWT72  &lt;dbl&gt; 8.18690, 9.88533, 10.16250, 2.66275, 9.00000, 8.69635, 2.…\n$ W_FSTURWT73  &lt;dbl&gt; 8.20073, 10.33466, 10.16250, 7.68679, 2.82857, 3.05511, 2…\n$ W_FSTURWT74  &lt;dbl&gt; 2.67796, 10.36607, 10.16250, 7.68679, 3.00000, 2.92228, 2…\n$ W_FSTURWT75  &lt;dbl&gt; 2.68151, 9.89489, 10.16250, 7.98824, 8.48571, 8.69635, 7.…\n$ W_FSTURWT76  &lt;dbl&gt; 2.73358, 3.45536, 3.38750, 8.31429, 9.00000, 2.89878, 2.4…\n$ W_FSTURWT77  &lt;dbl&gt; 2.63139, 3.45536, 3.38750, 2.66275, 8.48571, 2.58510, 7.5…\n$ W_FSTURWT78  &lt;dbl&gt; 7.89416, 9.88533, 10.16250, 2.77143, 8.48571, 8.40156, 2.…\n$ W_FSTURWT79  &lt;dbl&gt; 2.62882, 3.29830, 3.38750, 7.68679, 2.82857, 8.76685, 7.3…\n$ W_FSTURWT80  &lt;dbl&gt; 8.03388, 3.44489, 3.38750, 2.77143, 8.48571, 2.68850, 7.3…\n$ UNIT         &lt;dbl&gt; 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 2, 2, 2, 1, 1, 1, 2, 2, …\n$ WVARSTRR     &lt;dbl&gt; 70, 53, 2, 27, 35, 50, 65, 1, 63, 4, 13, 65, 42, 68, 15, …\n$ PV1MATH      &lt;dbl&gt; 639.004, 697.191, 693.710, 427.317, 436.462, 569.982, 771…\n$ PV2MATH      &lt;dbl&gt; 601.251, 754.277, 654.450, 410.376, 453.450, 539.609, 672…\n$ PV3MATH      &lt;dbl&gt; 621.480, 671.940, 696.938, 423.586, 392.315, 531.648, 653…\n$ PV4MATH      &lt;dbl&gt; 631.596, 657.300, 646.187, 388.935, 439.986, 534.368, 734…\n$ PV5MATH      &lt;dbl&gt; 579.276, 621.126, 678.119, 330.962, 443.125, 465.815, 727…\n$ PV6MATH      &lt;dbl&gt; 591.791, 655.729, 644.019, 379.988, 452.648, 528.509, 729…\n$ PV7MATH      &lt;dbl&gt; 600.709, 747.934, 720.531, 398.535, 396.970, 514.326, 597…\n$ PV8MATH      &lt;dbl&gt; 587.322, 694.365, 671.425, 422.127, 459.945, 521.029, 772…\n$ PV9MATH      &lt;dbl&gt; 618.131, 742.732, 694.085, 375.354, 438.166, 472.382, 694…\n$ PV10MATH     &lt;dbl&gt; 581.973, 656.934, 668.304, 453.348, 448.084, 503.387, 725…\n$ PV1READ      &lt;dbl&gt; 676.298, 625.585, 620.116, 381.495, 448.199, 469.441, 744…\n$ PV2READ      &lt;dbl&gt; 692.247, 686.716, 559.078, 400.815, 560.636, 500.350, 679…\n$ PV3READ      &lt;dbl&gt; 690.981, 663.147, 554.767, 374.911, 365.478, 375.703, 635…\n$ PV4READ      &lt;dbl&gt; 643.067, 567.435, 587.026, 367.484, 469.970, 377.452, 725…\n$ PV5READ      &lt;dbl&gt; 627.908, 614.500, 591.806, 336.009, 503.664, 470.781, 731…\n$ PV6READ      &lt;dbl&gt; 684.676, 604.745, 570.547, 324.630, 481.215, 415.448, 684…\n$ PV7READ      &lt;dbl&gt; 661.380, 669.375, 599.078, 396.242, 436.800, 448.547, 646…\n$ PV8READ      &lt;dbl&gt; 674.070, 623.735, 545.610, 374.723, 531.226, 434.381, 756…\n$ PV9READ      &lt;dbl&gt; 666.282, 649.579, 610.466, 314.704, 480.997, 411.703, 653…\n$ PV10READ     &lt;dbl&gt; 657.387, 571.261, 590.758, 342.956, 478.578, 410.846, 784…\n$ PV1SCIE      &lt;dbl&gt; 710.634, 670.646, 666.095, 340.308, 456.333, 475.158, 693…\n$ PV2SCIE      &lt;dbl&gt; 618.739, 748.839, 604.771, 329.889, 453.400, 470.030, 626…\n$ PV3SCIE      &lt;dbl&gt; 591.623, 635.443, 704.217, 411.353, 498.937, 461.218, 627…\n$ PV4SCIE      &lt;dbl&gt; 659.770, 639.735, 687.659, 327.974, 532.324, 504.199, 676…\n$ PV5SCIE      &lt;dbl&gt; 635.892, 608.385, 690.974, 292.183, 508.231, 486.930, 661…\n$ PV6SCIE      &lt;dbl&gt; 646.901, 670.662, 617.175, 355.423, 504.461, 493.011, 618…\n$ PV7SCIE      &lt;dbl&gt; 603.569, 734.807, 692.886, 400.182, 404.572, 469.950, 602…\n$ PV8SCIE      &lt;dbl&gt; 621.352, 639.748, 630.900, 317.518, 549.457, 464.012, 653…\n$ PV9SCIE      &lt;dbl&gt; 659.674, 716.768, 656.620, 298.893, 411.062, 440.113, 645…\n$ PV10SCIE     &lt;dbl&gt; 649.719, 655.670, 649.087, 362.702, 473.613, 495.410, 662…\n$ PV1MCCR      &lt;dbl&gt; 649.392, 636.431, 645.218, 437.613, 474.516, 471.365, 718…\n$ PV2MCCR      &lt;dbl&gt; 575.372, 674.370, 680.260, 498.958, 521.987, 546.743, 708…\n$ PV3MCCR      &lt;dbl&gt; 603.792, 716.787, 711.792, 402.737, 434.209, 581.279, 707…\n$ PV4MCCR      &lt;dbl&gt; 656.162, 616.569, 633.124, 407.151, 432.725, 567.093, 720…\n$ PV5MCCR      &lt;dbl&gt; 625.136, 701.626, 711.941, 450.975, 392.901, 498.145, 736…\n$ PV6MCCR      &lt;dbl&gt; 582.174, 686.793, 749.576, 487.698, 444.589, 519.416, 670…\n$ PV7MCCR      &lt;dbl&gt; 665.104, 665.775, 692.915, 391.175, 382.733, 521.639, 749…\n$ PV8MCCR      &lt;dbl&gt; 599.694, 729.194, 648.755, 426.641, 497.778, 513.726, 729…\n$ PV9MCCR      &lt;dbl&gt; 616.165, 715.697, 721.097, 366.883, 439.754, 519.125, 696…\n$ PV10MCCR     &lt;dbl&gt; 628.181, 688.830, 657.165, 435.455, 486.126, 544.408, 779…\n$ PV1MCQN      &lt;dbl&gt; 615.137, 661.706, 569.292, 403.429, 432.794, 526.659, 703…\n$ PV2MCQN      &lt;dbl&gt; 538.617, 762.401, 647.507, 419.847, 494.698, 580.938, 707…\n$ PV3MCQN      &lt;dbl&gt; 587.580, 712.182, 660.431, 418.996, 429.147, 577.745, 703…\n$ PV4MCQN      &lt;dbl&gt; 686.280, 685.954, 569.983, 375.158, 464.070, 622.873, 631…\n$ PV5MCQN      &lt;dbl&gt; 589.078, 722.402, 637.599, 433.093, 446.069, 557.871, 673…\n$ PV6MCQN      &lt;dbl&gt; 554.608, 677.709, 652.780, 453.028, 441.193, 488.815, 645…\n$ PV7MCQN      &lt;dbl&gt; 682.886, 691.586, 636.887, 435.661, 417.452, 602.295, 740…\n$ PV8MCQN      &lt;dbl&gt; 574.889, 688.120, 611.227, 441.660, 474.220, 578.260, 757…\n$ PV9MCQN      &lt;dbl&gt; 601.470, 704.436, 685.701, 430.417, 443.916, 541.486, 721…\n$ PV10MCQN     &lt;dbl&gt; 627.403, 692.040, 604.016, 382.344, 476.528, 572.958, 715…\n$ PV1MCSS      &lt;dbl&gt; 686.193, 664.374, 655.070, 397.737, 429.063, 466.035, 637…\n$ PV2MCSS      &lt;dbl&gt; 656.043, 660.937, 612.146, 336.048, 437.136, 498.416, 712…\n$ PV3MCSS      &lt;dbl&gt; 650.782, 692.654, 646.904, 438.551, 451.101, 502.442, 702…\n$ PV4MCSS      &lt;dbl&gt; 643.126, 654.111, 590.767, 347.416, 464.708, 517.999, 702…\n$ PV5MCSS      &lt;dbl&gt; 675.389, 644.941, 727.077, 420.278, 338.774, 452.092, 663…\n$ PV6MCSS      &lt;dbl&gt; 591.001, 699.918, 648.958, 401.609, 453.538, 494.653, 632…\n$ PV7MCSS      &lt;dbl&gt; 684.203, 653.602, 623.166, 413.761, 366.873, 463.116, 727…\n$ PV8MCSS      &lt;dbl&gt; 617.908, 629.891, 677.449, 299.027, 469.776, 498.289, 719…\n$ PV9MCSS      &lt;dbl&gt; 601.076, 696.030, 654.643, 373.784, 438.499, 441.962, 635…\n$ PV10MCSS     &lt;dbl&gt; 645.941, 663.634, 649.460, 363.817, 484.175, 411.098, 774…\n$ PV1MCUD      &lt;dbl&gt; 597.328, 655.345, 658.932, 393.019, 429.756, 491.327, 650…\n$ PV2MCUD      &lt;dbl&gt; 564.341, 732.821, 648.412, 393.366, 491.067, 522.747, 721…\n$ PV3MCUD      &lt;dbl&gt; 655.238, 737.658, 672.524, 384.864, 445.708, 486.597, 663…\n$ PV4MCUD      &lt;dbl&gt; 638.884, 651.654, 614.591, 379.824, 510.568, 524.480, 680…\n$ PV5MCUD      &lt;dbl&gt; 604.706, 690.450, 664.442, 501.245, 403.908, 483.108, 753…\n$ PV6MCUD      &lt;dbl&gt; 576.996, 666.422, 687.078, 453.339, 412.346, 505.925, 667…\n$ PV7MCUD      &lt;dbl&gt; 672.527, 673.451, 708.829, 425.168, 419.173, 501.566, 746…\n$ PV8MCUD      &lt;dbl&gt; 599.424, 728.294, 640.191, 407.016, 576.384, 506.734, 801…\n$ PV9MCUD      &lt;dbl&gt; 604.423, 701.038, 732.526, 421.738, 392.309, 499.849, 638…\n$ PV10MCUD     &lt;dbl&gt; 664.795, 650.797, 619.894, 421.415, 461.595, 442.838, 734…\n$ PV1MPEM      &lt;dbl&gt; 604.382, 705.040, 676.642, 401.548, 437.563, 528.852, 692…\n$ PV2MPEM      &lt;dbl&gt; 575.460, 710.217, 705.385, 389.686, 474.378, 604.877, 678…\n$ PV3MPEM      &lt;dbl&gt; 534.443, 713.023, 585.184, 390.502, 433.958, 498.130, 711…\n$ PV4MPEM      &lt;dbl&gt; 571.301, 679.747, 670.486, 434.343, 442.276, 511.479, 711…\n$ PV5MPEM      &lt;dbl&gt; 675.638, 661.754, 645.880, 366.385, 522.369, 555.593, 706…\n$ PV6MPEM      &lt;dbl&gt; 566.880, 718.268, 760.958, 401.969, 417.482, 555.839, 749…\n$ PV7MPEM      &lt;dbl&gt; 582.805, 613.478, 731.917, 496.875, 444.665, 553.932, 690…\n$ PV8MPEM      &lt;dbl&gt; 558.696, 643.541, 676.092, 353.233, 464.823, 571.562, 708…\n$ PV9MPEM      &lt;dbl&gt; 662.795, 640.522, 684.182, 476.154, 434.014, 520.155, 719…\n$ PV10MPEM     &lt;dbl&gt; 640.998, 655.268, 702.866, 342.948, 458.216, 609.277, 671…\n$ PV1MPFS      &lt;dbl&gt; 518.732, 763.661, 690.547, 421.798, 454.383, 493.759, 702…\n$ PV2MPFS      &lt;dbl&gt; 557.279, 729.497, 728.787, 467.856, 401.375, 538.097, 662…\n$ PV3MPFS      &lt;dbl&gt; 497.254, 714.971, 633.737, 414.444, 453.331, 485.088, 670…\n$ PV4MPFS      &lt;dbl&gt; 615.386, 753.899, 703.501, 445.029, 414.743, 465.568, 658…\n$ PV5MPFS      &lt;dbl&gt; 615.007, 719.492, 654.486, 389.460, 443.411, 493.251, 653…\n$ PV6MPFS      &lt;dbl&gt; 591.702, 715.191, 734.709, 414.555, 391.559, 545.850, 699…\n$ PV7MPFS      &lt;dbl&gt; 595.836, 702.035, 737.481, 489.032, 386.903, 504.961, 697…\n$ PV8MPFS      &lt;dbl&gt; 540.481, 704.257, 685.489, 363.830, 452.824, 532.093, 692…\n$ PV9MPFS      &lt;dbl&gt; 600.664, 664.705, 665.867, 419.104, 478.373, 568.115, 656…\n$ PV10MPFS     &lt;dbl&gt; 613.118, 705.987, 727.280, 394.856, 407.059, 485.863, 698…\n$ PV1MPIN      &lt;dbl&gt; 602.757, 733.566, 682.130, 407.066, 414.746, 459.876, 691…\n$ PV2MPIN      &lt;dbl&gt; 571.184, 744.273, 692.729, 381.339, 399.365, 490.634, 671…\n$ PV3MPIN      &lt;dbl&gt; 646.605, 758.913, 647.770, 364.773, 447.814, 386.584, 737…\n$ PV4MPIN      &lt;dbl&gt; 679.914, 695.003, 629.600, 406.470, 454.758, 499.082, 731…\n$ PV5MPIN      &lt;dbl&gt; 685.582, 714.181, 693.276, 433.901, 442.892, 441.024, 737…\n$ PV6MPIN      &lt;dbl&gt; 637.760, 716.221, 660.979, 406.423, 463.269, 544.965, 763…\n$ PV7MPIN      &lt;dbl&gt; 645.213, 663.813, 684.474, 489.700, 423.967, 475.647, 782…\n$ PV8MPIN      &lt;dbl&gt; 577.579, 662.428, 656.617, 432.277, 464.304, 504.819, 769…\n$ PV9MPIN      &lt;dbl&gt; 661.673, 640.743, 687.070, 512.069, 435.375, 459.819, 686…\n$ PV10MPIN     &lt;dbl&gt; 670.254, 768.695, 648.410, 374.502, 495.469, 481.600, 708…\n$ PV1MPRE      &lt;dbl&gt; 537.068, 706.337, 630.753, 378.730, 364.784, 523.219, 756…\n$ PV2MPRE      &lt;dbl&gt; 614.320, 672.767, 694.543, 400.807, 399.972, 536.264, 700…\n$ PV3MPRE      &lt;dbl&gt; 583.272, 651.949, 604.546, 407.607, 452.831, 454.319, 687…\n$ PV4MPRE      &lt;dbl&gt; 620.093, 620.759, 614.087, 336.451, 394.357, 501.514, 701…\n$ PV5MPRE      &lt;dbl&gt; 634.054, 645.072, 603.798, 317.742, 409.755, 498.824, 724…\n$ PV6MPRE      &lt;dbl&gt; 602.552, 677.174, 644.046, 349.040, 428.787, 523.705, 680…\n$ PV7MPRE      &lt;dbl&gt; 595.217, 634.813, 710.851, 450.198, 375.015, 497.360, 697…\n$ PV8MPRE      &lt;dbl&gt; 603.353, 648.907, 656.938, 392.060, 414.975, 508.547, 736…\n$ PV9MPRE      &lt;dbl&gt; 611.942, 641.203, 690.323, 447.422, 441.178, 499.691, 734…\n$ PV10MPRE     &lt;dbl&gt; 663.352, 644.001, 664.134, 382.088, 421.531, 580.387, 727…\n$ SENWT        &lt;dbl&gt; 0.63867, 0.80357, 0.80736, 0.64732, 0.69397, 0.67917, 0.5…\n$ VER_DAT      &lt;chr&gt; \"01MAY23:14:19:45\", \"01MAY23:14:19:44\", \"01MAY23:14:19:45…\n$ i            &lt;dbl&gt; 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 21…",
    "crumbs": [
      "In-class Exercises",
      "In-class Exercise 1: Now You See it!"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07b.html",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07b.html",
    "title": "Hands-on Exercise 7b: Visualising Geospatial Point Data",
    "section": "",
    "text": "In this exercise, we will learn how to: - convert an aspatial data file into simple point feature data frame, and assign it an appropriate projection reference to the newly created simple point feature data frame. - plot interactive proportional symbol maps.\nWe will be creating a proportional symbol map showing the number of Group1 and Group 2 wins by Singapore Pools’ outlets using an tmap package.\n\n\n\n\n\n\nWhat is proportional symbol map?\n\n\n\nProportional symbol maps (also known as graduate symbol maps) are a class of maps that use the visual variable of size to represent differences in the magnitude of a discrete, abruptly changing phenomenon, e.g. counts of people. Like choropleth maps, you can create classed or unclassed versions of these maps. The classed ones are known as range-graded or graduated symbols, and the unclassed are called proportional symbols, where the area of the symbols are proportional to the values of the attribute being mapped.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 7b: Visualising Geospatial Point Data"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07b.html#installing-and-loading-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07b.html#installing-and-loading-r-packages",
    "title": "Hands-on Exercise 7b: Visualising Geospatial Point Data",
    "section": "2.1 Installing and Loading R Packages",
    "text": "2.1 Installing and Loading R Packages\nFor this exercise, other than tmap, we will use the following packages:\n\ntidyverse for tidying and wrangling data\nsf for handling geospatial data\n\nThe code chunk below uses p_load() of pacman package to check if the abovementioned packages are installed in the computer. If they are, they will be launched in R. Otherwise, pacman will install the relevant packages before launching them.\n\n\nShow the code\npacman::p_load(tidyverse, tmap, sf)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 7b: Visualising Geospatial Point Data"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07b.html#importing-the-data",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07b.html#importing-the-data",
    "title": "Hands-on Exercise 7b: Visualising Geospatial Point Data",
    "section": "2.2 Importing the Data",
    "text": "2.2 Importing the Data\nWe will be using the data set SGPools_svy21 for this exercise. The data is in csv file format.\nThe code chunk below uses read_csv() function of readr package to import SGPools_svy21.csv into R as a tibble data frame called sgpools.\n\n\nShow the code\nsgpools &lt;- read_csv(\"data/aspatial/SGPools_svy21.csv\")\nhead(sgpools)\n\n\n# A tibble: 6 × 7\n  NAME            ADDRESS POSTCODE XCOORD YCOORD `OUTLET TYPE` `Gp1Gp2 Winnings`\n  &lt;chr&gt;           &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;\n1 Livewire (Mari… 2 Bayf…    18972 30842. 29599. Branch                        5\n2 Livewire (Reso… 26 Sen…    98138 26704. 26526. Branch                       11\n3 SportsBuzz (Kr… Lotus …   738078 20118. 44888. Branch                        0\n4 SportsBuzz (Po… 1 Sele…   188306 29777. 31382. Branch                       44\n5 Prime Serangoo… Blk 54…   552542 32239. 39519. Branch                        0\n6 Singapore Pool… 1A Woo…   731001 21012. 46987. Branch                        3\n\n\nFrom the above output, we see that the sgpools dataset consists of seven columns. The XCOORD and YCOORD columns are the x-coordinates and y-coordinates of SingPools outlets and branches. They are in Singapore SVY21 Projected Coordinates System.\n\n\n\n\n\n\nObservations\n\n\n\n\nNotice that the sgpools data in tibble data frame and not the common R data frame.\nwe can also use list() instead of glimpse().",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 7b: Visualising Geospatial Point Data"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07b.html#creating-an-sf-dataframe-from-an-aspatial-data-frame",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07b.html#creating-an-sf-dataframe-from-an-aspatial-data-frame",
    "title": "Hands-on Exercise 7b: Visualising Geospatial Point Data",
    "section": "3.1 Creating an sf dataframe from an aspatial data frame",
    "text": "3.1 Creating an sf dataframe from an aspatial data frame\nWe will convert sgpools data frame into a simple feature data frame using st_as_sf() of sf package.\n\n\nShow the code\nsgpools_sf &lt;- st_as_sf(sgpools, \n         coords = c(\"XCOORD\", \"YCOORD\"),\n         crs = 3414)\nsgpools_sf \n\n\nSimple feature collection with 306 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 7844.194 ymin: 26525.7 xmax: 45176.57 ymax: 47987.13\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 306 × 6\n   NAME                         ADDRESS POSTCODE `OUTLET TYPE` `Gp1Gp2 Winnings`\n * &lt;chr&gt;                        &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;\n 1 Livewire (Marina Bay Sands)  2 Bayf…    18972 Branch                        5\n 2 Livewire (Resorts World Sen… 26 Sen…    98138 Branch                       11\n 3 SportsBuzz (Kranji)          Lotus …   738078 Branch                        0\n 4 SportsBuzz (PoMo)            1 Sele…   188306 Branch                       44\n 5 Prime Serangoon North        Blk 54…   552542 Branch                        0\n 6 Singapore Pools Woodlands C… 1A Woo…   731001 Branch                        3\n 7 Singapore Pools 64 Circuit … Blk 64…   370064 Branch                       17\n 8 Singapore Pools 88 Circuit … Blk 88…   370088 Branch                       16\n 9 Singapore Pools Anchorvale … Blk 30…   540308 Branch                       21\n10 Singapore Pools Ang Mo Kio … Blk 20…   560202 Branch                       25\n# ℹ 296 more rows\n# ℹ 1 more variable: geometry &lt;POINT [m]&gt;\n\n\nFrom the above output, we noticed that: - a new column called geometry has been added into the data frame. - sgppols_sf is in point feature class. - its epsg ID is 3414.\n\n\n\n\n\n\nAbout the above code chunk\n\n\n\n\nthe coords = argument requires us to provide the column name of the x-coordinates first, followed by the column name of the y-coordinates.\nthe crs = argument requires us to provide the coordinates system in epsg format. From epsg.io, we know that EPSG: 3414 is Singapore SVY21 Projected Coordinate System.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 7b: Visualising Geospatial Point Data"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07b.html#interactive-point-symbol-map",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07b.html#interactive-point-symbol-map",
    "title": "Hands-on Exercise 7b: Visualising Geospatial Point Data",
    "section": "4.1 Interactive point symbol map",
    "text": "4.1 Interactive point symbol map\n\ntmap_mode(\"view\")\ntm_basemap(\"CartoDB.Positron\") +\n  tm_shape(sgpools_sf) +\n  tm_bubbles(col = \"red\",\n           size = 1,\n           border.col = \"black\",\n           border.lwd = 1) +\n  tm_layout(title = \"Locations of SGPools Branches and Outlets\",\n    title.size = 1)\n\n\n\n\n\nTo draw a proportional symbol map, we need to assign a numerical variable to the size visual attribute. The code chunks below show that the variable Gp1Gp2 Winnings is assigned to size visual attribute.\n\ntm_basemap(\"CartoDB.Positron\") +\n  tm_shape(sgpools_sf) +\n  tm_bubbles(col = \"red\",\n           size = \"Gp1Gp2 Winnings\",\n           border.col = \"black\",\n           border.lwd = 1) +\n  tm_layout(title = \"Number of Gp1Gp2 Winnings\",\n    title.size = 1)\n\n\n\n\n\nThe proportional symbol map can be further improved using the colour visual attribute. In the code chunk below, OUTLET TYPE variable is used as the colour attribute variable, as such, Singapore Pools Branches and Outlets have different colour representation on the map.\n\ntm_basemap(\"CartoDB.Positron\") +\n  tm_shape(sgpools_sf) +\n  tm_bubbles(col = \"OUTLET TYPE\",\n           size = \"Gp1Gp2 Winnings\",\n           border.col = \"black\",\n           border.lwd = 1) +\n  tm_layout(title = \"Number of Gp1Gp2 Winnings by Outlet Type\",\n    title.size = 1)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 7b: Visualising Geospatial Point Data"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html",
    "title": "Take-home Exercise 6 - Visualing and Analysing Time-Oriented Data",
    "section": "",
    "text": "In this exercise, we will be learning how to create the following visualisations:\n\na calendar heatmap using ggplot2 functions\na cycle plot using ggplot2 function\na slopegraph",
    "crumbs": [
      "Hands-on Exercises",
      "Take-home Exercise 6 - Visualing and Analysing Time-Oriented Data"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#installing-and-loading-the-packages",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#installing-and-loading-the-packages",
    "title": "Take-home Exercise 6 - Visualing and Analysing Time-Oriented Data",
    "section": "2.1 Installing and Loading the Packages",
    "text": "2.1 Installing and Loading the Packages\nFor this exercise, other than tidyverse, we will use the following packages:\n\nscales\nviridis\nlubridate\nggthemes\ngridExtra\nreadxl\nknitr\ndata.table\nCGPfunctions\n\n\n\nShow the code\npacman::p_load(tidyverse, scales, viridis, lubridate, ggthemes, gridExtra, readxl,knitr, data.table, CGPfunctions)",
    "crumbs": [
      "Hands-on Exercises",
      "Take-home Exercise 6 - Visualing and Analysing Time-Oriented Data"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#importing-the-data",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#importing-the-data",
    "title": "Take-home Exercise 6 - Visualing and Analysing Time-Oriented Data",
    "section": "3.1 Importing the Data",
    "text": "3.1 Importing the Data\nFor the purpose of this hands-on exercise, eventlog.csv file will be used. This data file consists of 199,999 rows of time-series cyber attack records by country.\nFirst, we will use the code chunk below to import eventlog.csv file into R environment and called the data frame as attacks.\n\n\nShow the code\nattacks &lt;- read_csv(\"data/eventlog.csv\")",
    "crumbs": [
      "Hands-on Exercises",
      "Take-home Exercise 6 - Visualing and Analysing Time-Oriented Data"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#examining-the-data-structure",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#examining-the-data-structure",
    "title": "Take-home Exercise 6 - Visualing and Analysing Time-Oriented Data",
    "section": "3.2 Examining the Data Structure",
    "text": "3.2 Examining the Data Structure\nWe will use kable() to review the structure of the imported data frame.\n\n\nShow the code\nkable(head(attacks))\n\n\n\n\n\ntimestamp\nsource_country\ntz\n\n\n\n\n2015-03-12 15:59:16\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:00:48\nFR\nEurope/Paris\n\n\n2015-03-12 16:02:26\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:02:38\nUS\nAmerica/Chicago\n\n\n2015-03-12 16:03:22\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:03:45\nCN\nAsia/Shanghai\n\n\n\n\n\nFrom the above output, we see there there are three columns in the attack dataset: timestamp, source_country and tz.\n\ntimestamp: stores datetime vales in POSIXct format\nsource_country: stores the source of the attack. It is in ISO 3166-1 alpha-2 country code\ntz: stores time zone of the source IP address.",
    "crumbs": [
      "Hands-on Exercises",
      "Take-home Exercise 6 - Visualing and Analysing Time-Oriented Data"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#data-preparation",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#data-preparation",
    "title": "Take-home Exercise 6 - Visualing and Analysing Time-Oriented Data",
    "section": "3.3 Data Preparation",
    "text": "3.3 Data Preparation\n\n3.3.1 Step 1\nBefore we can plot the calendar heatmap, we need to derive two new fields: wkday and hour using a function. To get the hour function, we will make use of hour() from lubridate package. To get the wkday field, we will make use of weekdays(), which is a base R function.\n\nMethod 1: Using lubridate and base R functionMethod 2: Using lubridate package\n\n\n\nmake_hr_wkday &lt;- function(ts,sc,tz){\n  real_times &lt;- ymd_hms(ts,\n                        tz = tz[1],\n                        quiet = TRUE)\n  \n  dt &lt;- data.table(source_country = sc,\n                   wkday = weekdays(real_times),\n                   hour = hour(real_times))\n  return(dt)\n}\n\n\n\n\n\n\n\nAbout the code chunk\n\n\n\n\nymd_hms() and hour() are from lubridate package.\n\nNote! before using ymd_hms(), we should examine the date/time format on our laptops. The date/time format on our laptops should be yyyy-mm-dd hh:mm:ss before using ymd_hms(). If the print date/time format is different, then we should either (1) use different function or (2) change the date/time format of our computer.\n\nweekdays() is a base R function.\n\n\n\n\n\n\nattacks1 &lt;- attacks %&gt;%\n  mutate(wkday = lubridate:: wday(timestamp,\n                       label = TRUE,\n                      abbr = FALSE, # to display the day of the week \n                       week_start = 7), #Sun is the first level \n         hr = hour(timestamp))\n\n\n\n\n\n\n\nAbout the code chunk\n\n\n\n\nhour() and wday() are from lubridate package.\n\nNote: these two functions are relatively more efficient than Base R function.\n\n\n\n\n\n\n\n\n\n3.3.2 Step 2\nWe will now derive the attacks tibble data frame using the following code chunk.\n\nMethod 1Method 2\n\n\nAfter creating the function in earlier section, we will now use it to derive the wkday and hour columns. After that we will also use mutate() to convert wkday and hour fields into factor so that they will be ordered when plotting.\n\nwkday_levels &lt;- c('Saturday', 'Friday', \n                  'Thursday', 'Wednesday', \n                  'Tuesday', 'Monday', \n                  'Sunday')\n\nattacks2 &lt;- attacks %&gt;%\n  group_by(tz) %&gt;%\n  do(make_hr_wkday(.$timestamp,\n                   .$source_country,\n                   .$tz)) %&gt;%\n  ungroup() %&gt;%\n  mutate(wkday = factor(\n    wkday, levels = wkday_levels),\n    hour = factor(hour, levels = 0:23))\n\n\n\nUsing the attacks1 dataframe from previous section’s method 2, we will just select the columns that we want and convert the wkday and hr columns into factors using as.factor().\n\nattacks3 &lt;- attacks1 %&gt;%\n  select(tz, source_country, wkday, hr)\n\nattacks3$wkday &lt;- as.factor(attacks3$wkday)\n\nattacks3$hr &lt;- as.factor(attacks3$hr)\n\n\n\n\n\n\n3.3.3 Step 3 - Building Calendar Heatmaps\n\ngrouped &lt;- attacks2 %&gt;%\n  count(wkday, hour) %&gt;%\n  ungroup %&gt;%\n  na.omit()\n\nggplot(grouped,\n       aes(hour, \n           wkday, \n           fill = n)) +\n  geom_tile(color = \"white\", \n            size = 0.1) +\n  \n  theme_tufte(base_family = \"sans serif\") + \n  coord_equal() +\n  scale_fill_gradient(name = \"# of attacks\",\n                    low = \"lavender\", \n                    high = \"maroon4\") +\n  labs(x = NULL, \n       y = NULL, \n       title = \"Attacks by Weekday and Time of Day\") +\n  \n  theme(axis.ticks = element_blank(),\n      plot.title = element_text(hjust = 0.5),\n      legend.title = element_text(size = 8),\n      legend.text = element_text(size = 6) )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbout the code chunk\n\n\n\n\nA tibble data table called grouped is derived by aggregating the attack by wkday and hour fields\nA new field called n is derived by using group_by() and count() functions.\nna.omit() is used to exclude missing value.\ngeom_tile() is used to plot tiles(grids) at each x and y positions. color and size arguments are used to specify the border color and line size of the tiles. In this case, the border color is white and the line size is 0.1.\ntheme_tufte() of ggthemes package is used to remove unnecessary chart junk.\ncoord_equal() is used to ensure the plot will have an aspect ratio of 1:1\nscale_fill_gradient() function is used to create a two color gradient (low-high)",
    "crumbs": [
      "Hands-on Exercises",
      "Take-home Exercise 6 - Visualing and Analysing Time-Oriented Data"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-multiple-calendar-heatmaps",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-multiple-calendar-heatmaps",
    "title": "Take-home Exercise 6 - Visualing and Analysing Time-Oriented Data",
    "section": "3.4 Plotting Multiple Calendar Heatmaps",
    "text": "3.4 Plotting Multiple Calendar Heatmaps\nWe can also plot multiple calendar heatmaps for the top four countries with the highest number of attacks.\n\n3.4.1 Step 1: Deriving Attack by Country Object\nFirst we need to identify the top 4 countries with the highest number of attacks: - count the number of attacks by country - calculate the percent of attacks by country, and - save the results in a tibble data frame\n\nattacks_by_cty &lt;- count(\n  attacks, source_country) %&gt;%\n  mutate(percent = percent(n/sum(n))) %&gt;%\n  arrange(desc(n))\n\n\n\n3.4.2 Step 2: Preparing the tidy data frame\nNow we will extract the attack records of the top 4 countries from attacks2data frame. Then save the data in a new tibble data frame (i.e. top4_attacks).\n\ntop4 &lt;- attacks_by_cty$source_country[1:4]\n\ntop4_attacks &lt;- attacks2 %&gt;%\n  filter(source_country %in% top4) %&gt;%\n  count(source_country, wkday, hour) %&gt;%\n  ungroup() %&gt;%\n  mutate(source_country = factor(\n    source_country, levels = top4)) %&gt;%\n  na.omit()\n\n\n\n3.4.3 Step 3: Plotting the Multiple Calendar Heatmap using ggplot2 package\nNow, we are ready to plot multiple calendar heatmap!\n\n# New facet label names\ncty.labs &lt;- c(\"CHINA\", \"UNITED STATES\", \"KOREA\", \"NETHERLANDS\")\nnames(cty.labs) &lt;- c(\"CN\", \"US\", \"KR\", \"NL\")\n\n\n#baseplot \nggplot(top4_attacks,\n       aes(hour,\n           wkday,\n           fill = n)) +\n  geom_tile(color = \"white\",\n            size = 0.1) +\n  theme_tufte(base_family = \"sans serif\") + \n  coord_equal() +\n  scale_fill_gradient(name = \"# of attacks\",\n                    low = \"grey80\", \n                    high = \"maroon4\") +\n  #plot for each country using facet_wrap \n  facet_wrap(~source_country, ncol = 2, \n             labeller = labeller(source_country = cty.labs)) + \n  labs(x = NULL, y = NULL,\n       title = \"Attacks on Top 4 Countries by Weekday and Time of Day\") + \n   theme(axis.ticks = element_blank(),\n        axis.text.x = element_text(size = 7),\n        plot.title = element_text(hjust = 0.5),\n        legend.title = element_text(size = 8),\n        legend.text = element_text(size = 6))",
    "crumbs": [
      "Hands-on Exercises",
      "Take-home Exercise 6 - Visualing and Analysing Time-Oriented Data"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#importing-the-data-1",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#importing-the-data-1",
    "title": "Take-home Exercise 6 - Visualing and Analysing Time-Oriented Data",
    "section": "4.1 Importing the Data",
    "text": "4.1 Importing the Data\nFor the purpose of this hands-on exercise, arrivals_by_air.xlsx will be used.\nFirst, we will use the code chunk below to import arrivals_by_air.xlsx file into R environment and call the tibble data frame as air.\n\n\nShow the code\nair &lt;- read_excel(\"data/arrivals_by_air.xlsx\")\nkable(head(air))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMonth-Year\nRepublic of South Africa\nCanada\nUSA\nBangladesh\nBrunei\nChina\nHong Kong SAR (China)\nIndia\nIndonesia\nJapan\nSouth Korea\nKuwait\nMalaysia\nMyanmar\nPakistan\nPhilippines\nSaudi Arabia\nSri Lanka\nTaiwan\nThailand\nUnited Arab Emirates\nVietnam\nBelgium & Luxembourg\nCIS\nFinland\nFrance\nGermany\nIreland\nItaly\nNetherlands\nSpain\nSwitzerland\nUnited Kingdom\nAustralia\nNew Zealand\n\n\n\n\n2000-01-01\n3291\n5545\n25906\n2883\n3749\n33895\n13692\n19235\n65151\n59288\n21457\n507\n27472\n1177\n2150\n8404\n1312\n3922\n15766\n12048\n1318\n1527\n1434\n2703\n1634\n4752\n12739\n1292\n3544\n4962\n925\n3731\n28986\n34616\n5034\n\n\n2000-02-01\n2357\n6120\n28262\n2469\n3236\n34344\n19870\n18975\n37105\n58188\n19634\n199\n29084\n1161\n2496\n9128\n623\n3988\n24861\n12745\n899\n2269\n1596\n1182\n1297\n6391\n13093\n1200\n2897\n5054\n747\n3980\n35148\n26030\n3938\n\n\n2000-03-01\n4036\n6255\n30439\n2904\n3342\n27053\n17086\n21049\n44205\n74426\n20719\n386\n30504\n1355\n2429\n11691\n1578\n4259\n18767\n16971\n1474\n2034\n1548\n1088\n1220\n5528\n13645\n1368\n2717\n4950\n935\n3576\n36117\n31119\n4668\n\n\n2000-04-01\n4241\n4521\n25378\n2843\n5117\n30464\n22346\n26160\n45480\n49985\n17489\n221\n34478\n1593\n2711\n14141\n705\n6579\n22735\n20397\n1284\n2420\n1592\n1012\n1208\n5544\n13366\n1345\n2512\n4149\n941\n3850\n33792\n34824\n6890\n\n\n2000-05-01\n2841\n3914\n26163\n2793\n4152\n30775\n16357\n35869\n38350\n48937\n19398\n164\n34795\n1397\n2594\n13305\n679\n4625\n18399\n15769\n1042\n1833\n1167\n660\n743\n4225\n10878\n1067\n2205\n3643\n764\n3025\n23377\n33139\n7006\n\n\n2000-06-01\n2776\n3487\n28179\n3146\n5018\n26720\n18133\n31314\n47982\n53798\n17522\n440\n34660\n1715\n2924\n10555\n2749\n4740\n21042\n17217\n1545\n2480\n1170\n712\n982\n4047\n9054\n1363\n2196\n3544\n855\n2580\n21769\n35731\n7634",
    "crumbs": [
      "Hands-on Exercises",
      "Take-home Exercise 6 - Visualing and Analysing Time-Oriented Data"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#deriving-month-and-year-fields",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#deriving-month-and-year-fields",
    "title": "Take-home Exercise 6 - Visualing and Analysing Time-Oriented Data",
    "section": "4.2 Deriving month and year fields",
    "text": "4.2 Deriving month and year fields\nWe will now derive two new fields month and year from the Month-Year field.\n\n\nShow the code\nair$month &lt;- factor(month(air$`Month-Year`),\n                    levels = 1:12,\n                    labels = month.abb,\n                    ordered = TRUE)\n\nair$year &lt;- year(ymd(air$`Month-Year`))\n\nkable(head(air))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMonth-Year\nRepublic of South Africa\nCanada\nUSA\nBangladesh\nBrunei\nChina\nHong Kong SAR (China)\nIndia\nIndonesia\nJapan\nSouth Korea\nKuwait\nMalaysia\nMyanmar\nPakistan\nPhilippines\nSaudi Arabia\nSri Lanka\nTaiwan\nThailand\nUnited Arab Emirates\nVietnam\nBelgium & Luxembourg\nCIS\nFinland\nFrance\nGermany\nIreland\nItaly\nNetherlands\nSpain\nSwitzerland\nUnited Kingdom\nAustralia\nNew Zealand\nmonth\nyear\n\n\n\n\n2000-01-01\n3291\n5545\n25906\n2883\n3749\n33895\n13692\n19235\n65151\n59288\n21457\n507\n27472\n1177\n2150\n8404\n1312\n3922\n15766\n12048\n1318\n1527\n1434\n2703\n1634\n4752\n12739\n1292\n3544\n4962\n925\n3731\n28986\n34616\n5034\nJan\n2000\n\n\n2000-02-01\n2357\n6120\n28262\n2469\n3236\n34344\n19870\n18975\n37105\n58188\n19634\n199\n29084\n1161\n2496\n9128\n623\n3988\n24861\n12745\n899\n2269\n1596\n1182\n1297\n6391\n13093\n1200\n2897\n5054\n747\n3980\n35148\n26030\n3938\nFeb\n2000\n\n\n2000-03-01\n4036\n6255\n30439\n2904\n3342\n27053\n17086\n21049\n44205\n74426\n20719\n386\n30504\n1355\n2429\n11691\n1578\n4259\n18767\n16971\n1474\n2034\n1548\n1088\n1220\n5528\n13645\n1368\n2717\n4950\n935\n3576\n36117\n31119\n4668\nMar\n2000\n\n\n2000-04-01\n4241\n4521\n25378\n2843\n5117\n30464\n22346\n26160\n45480\n49985\n17489\n221\n34478\n1593\n2711\n14141\n705\n6579\n22735\n20397\n1284\n2420\n1592\n1012\n1208\n5544\n13366\n1345\n2512\n4149\n941\n3850\n33792\n34824\n6890\nApr\n2000\n\n\n2000-05-01\n2841\n3914\n26163\n2793\n4152\n30775\n16357\n35869\n38350\n48937\n19398\n164\n34795\n1397\n2594\n13305\n679\n4625\n18399\n15769\n1042\n1833\n1167\n660\n743\n4225\n10878\n1067\n2205\n3643\n764\n3025\n23377\n33139\n7006\nMay\n2000\n\n\n2000-06-01\n2776\n3487\n28179\n3146\n5018\n26720\n18133\n31314\n47982\n53798\n17522\n440\n34660\n1715\n2924\n10555\n2749\n4740\n21042\n17217\n1545\n2480\n1170\n712\n982\n4047\n9054\n1363\n2196\n3544\n855\n2580\n21769\n35731\n7634\nJun\n2000",
    "crumbs": [
      "Hands-on Exercises",
      "Take-home Exercise 6 - Visualing and Analysing Time-Oriented Data"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#extracting-the-target-country",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#extracting-the-target-country",
    "title": "Take-home Exercise 6 - Visualing and Analysing Time-Oriented Data",
    "section": "4.3 Extracting the target country",
    "text": "4.3 Extracting the target country\nNext, we will extract the data for the target country (i.e. Vietnam) and data beyond year 2010 using the following code chunk.\n\nviet &lt;- air %&gt;% \n  select(Vietnam, month, year) %&gt;%\n  filter(year &gt;= 2010)",
    "crumbs": [
      "Hands-on Exercises",
      "Take-home Exercise 6 - Visualing and Analysing Time-Oriented Data"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#computing-year-average-arrivals-by-month",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#computing-year-average-arrivals-by-month",
    "title": "Take-home Exercise 6 - Visualing and Analysing Time-Oriented Data",
    "section": "4.4 Computing year average arrivals by month",
    "text": "4.4 Computing year average arrivals by month\nNow we will use group_by() and summarise() of dplyr to compute average arrivals by month across all years.\n\nhline.data &lt;- viet %&gt;%\n  group_by(month) %&gt;%\n  summarise(avgvalue = mean(Vietnam))",
    "crumbs": [
      "Hands-on Exercises",
      "Take-home Exercise 6 - Visualing and Analysing Time-Oriented Data"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-the-cycle-plot",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-the-cycle-plot",
    "title": "Take-home Exercise 6 - Visualing and Analysing Time-Oriented Data",
    "section": "4.5 Plotting the cycle plot",
    "text": "4.5 Plotting the cycle plot\nTo plot a cycle plot for each month, we will make use of geom_line() to plot the number of visitors in each year for every month. Then, we will use geom_hline()to plot out the “average value” for each month. To have a chart for every month, we will make use of facet_grid().\n\nggplot() +\n  geom_line(data = viet,\n            aes(x = year,\n                y = `Vietnam`,\n                group = month),\n            color = \"black\") +\n  geom_hline(aes(yintercept =avgvalue),\n             data = hline.data,\n             linetype = 6,\n             color = \"red\",\n             size = 0.5) +\n  facet_grid(~month) + \n  labs(axis.text.x = element_blank(),\n       title = \"Visitor Arrivals from Vietnam by Air, Jan 2010 - Dec 2019\") +\n  xlab(\"\") +\n  ylab(\"No. of Visitors\") +\n  theme_minimal() +\n  scale_x_binned(guide = guide_axis(angle = 45)) +\n  theme(axis.text.x = element_text(size = rel(0.75)))",
    "crumbs": [
      "Hands-on Exercises",
      "Take-home Exercise 6 - Visualing and Analysing Time-Oriented Data"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#importing-the-data-2",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#importing-the-data-2",
    "title": "Take-home Exercise 6 - Visualing and Analysing Time-Oriented Data",
    "section": "5.1 Importing the Data",
    "text": "5.1 Importing the Data\nFor the purpose of this hands-on exercise, `rice.csv`` will be used.\nFirst, we will use the code chunk below to import rice.csv file into R environment and call the tibble data frame as `rice``.\n\n\nShow the code\nrice &lt;- read_csv(\"data/rice.csv\")\nkable(head(rice))\n\n\n\n\n\nCountry\nYear\nYield\nProduction\n\n\n\n\nChina\n1961\n20787\n56217601\n\n\nChina\n1962\n23700\n65675288\n\n\nChina\n1963\n26833\n76439280\n\n\nChina\n1964\n28289\n85853780\n\n\nChina\n1965\n29667\n90705630\n\n\nChina\n1966\n31445\n98403990",
    "crumbs": [
      "Hands-on Exercises",
      "Take-home Exercise 6 - Visualing and Analysing Time-Oriented Data"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-the-slopegraph",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-the-slopegraph",
    "title": "Take-home Exercise 6 - Visualing and Analysing Time-Oriented Data",
    "section": "5.2 Plotting the slopegraph",
    "text": "5.2 Plotting the slopegraph\nLet us convert the Year to a factor so that the years will be arranged in levels. Then we filter the rows where the Years are 1961 and 1980.\n\n\nShow the code\nrice &lt;- rice %&gt;%\n  mutate(Year = factor(Year)) %&gt;%\n  filter(Year %in% c(1961, 1980)) \n\nglimpse(rice)\n\n\nRows: 22\nColumns: 4\n$ Country    &lt;chr&gt; \"China\", \"China\", \"India\", \"India\", \"Indonesia\", \"Indonesia…\n$ Year       &lt;fct&gt; 1961, 1980, 1961, 1980, 1961, 1980, 1961, 1980, 1961, 1980,…\n$ Yield      &lt;dbl&gt; 20787, 41435, 15419, 20002, 17623, 32928, 48793, 51279, 414…\n$ Production &lt;dbl&gt; 56217601, 142876520, 53494500, 80312000, 12084000, 29651900…\n\n\nUsing the filtered data, we will plot the slopegraph using newggslopegraph(). We will indicate the dataframe argument as rice, which is the tibble dataframe that we have prepared in the earlier step. The Times argument refers to the column inside the dataframe that will be plotted on the x axis. Traditionally this Times argument is some measure of time. However, newggslopegraph() accepts a column of class ordered, factor or character. The Measurement argument refers to the column inside the dataframe that will be plotted on the y axis. Traditionally this Measurement argument is some measure such as a percentage. Currently, the function accepts a column of type integer or number. The Grouping argument refers to a column inside the dataframe that will be used to group and distinguish measurements.\n\n\n\n\n\n\nAbout the code chunk\n\n\n\nIn the following code chunk we also: - Added a title using the argument Title. Note that Title = \"\" will provide an empty title but retain the spacing. - Added a subtitle using the argument SubTitle. Note that SubTitle = \"\" will provide and empty title but retain the spacing. - Added a caption using the argument Caption. Note that Caption = \"\" will provide and empty title but retain the spacing. - tried to space out the data labels by adjusting the DataLabelPadding argument. - changed the theme to wsj using ThemeChoice argument.Note that by default ThemeChoice is set to “bw” and the other choices are “ipsum”, “econ”, “wsj”, “gdocs”, and “tufte”.\n\n\n\nnewggslopegraph(dataframe = rice,\n                Times = Year, \n                Measurement = Yield, \n                Grouping = Country,\n                Title = \"Rice Yield of Top 11 Asian Countries\",\n                SubTitle = \"1961 - 1980\",\n                Caption = \"Prepared by: Goh Si Hui\",\n                DataLabelPadding = 0.07,\n                ThemeChoice = \"wsj\"\n                )",
    "crumbs": [
      "Hands-on Exercises",
      "Take-home Exercise 6 - Visualing and Analysing Time-Oriented Data"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html",
    "title": "Hands-on Exercise 5d - Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "",
    "text": "In this exercise, we will learn how to:\n\nplotting static parallel coordinates plots by using ggparcoord() of GGally package, and\nplotting interactive parallel coordinates plots by using parallelPlot package.\n\n\n\n\n\n\n\nWhat is Parallel Coordinates Plot?\n\n\n\nParallel coordinates plot is a data visualisation specially designed for visualising and analysing multivariate, numerical data. It is ideal for comparing multiple variables together and seeing the relationships between them. For example, the variables contribute to Happiness Index. Parallel coordinates was invented by Alfred Inselberg in the 1970s as a way to visualize high-dimensional data. This data visualisation technique is more often found in academic and scientific communities than in business and consumer data visualizations. As pointed out by Stephen Few(2006), “This certainly isn’t a chart that you would present to the board of directors or place on your Web site for the general public. In fact, the strength of parallel coordinates isn’t in their ability to communicate some truth in the data to others, but rather in their ability to bring meaningful multivariate patterns and comparisons to light when used interactively for analysis.” For example, parallel coordinates plot can be used to characterise clusters detected during customer segmentation.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5d - Visual Multivariate Analysis with Parallel Coordinates Plot"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html#installing-and-loading-the-packages",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html#installing-and-loading-the-packages",
    "title": "Hands-on Exercise 5d - Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "2.1 Installing and Loading the Packages",
    "text": "2.1 Installing and Loading the Packages\nFor this exercise, the GGally, parcoords, parallelPlot and tidyverse packages will be used.\nThe code chunk below uses p_load() of pacman package to check if the abovementioned packages are installed in the computer. If they are, they will be launched in R. Otherwise, pacman will install the relevant packages before launching them.\n\n\nShow the code\npacman::p_load(tidyverse, DT, GGally, parallelPlot)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5d - Visual Multivariate Analysis with Parallel Coordinates Plot"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html#importing-the-data",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html#importing-the-data",
    "title": "Hands-on Exercise 5d - Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "2.2 Importing the Data",
    "text": "2.2 Importing the Data\nFor this exercise, we will be using the data from World Happiness 2018 report. The original data set is in Microsoft Excel format. It has been extracted and saved in csv file called WHData-2018.csv.\nIn the code chunk below, read_csv() of readr is used to import WHData-2018.csv into R and parsed it into tibble R data frame format.\n\nhappy &lt;- read_csv(\"data/WHData-2018.csv\")",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5d - Visual Multivariate Analysis with Parallel Coordinates Plot"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html#plotting-a-simple-parallel-coordinae-plot",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html#plotting-a-simple-parallel-coordinae-plot",
    "title": "Hands-on Exercise 5d - Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "3.1 Plotting a Simple Parallel Coordinae Plot",
    "text": "3.1 Plotting a Simple Parallel Coordinae Plot\n\nggparcoord(data = happy, \n           columns = c(7:12)) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice that only two argument namely data and columns is used. Data argument is used to map the data object (i.e. wh) and columns is used to select the columns for preparing the parallel coordinates plot.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5d - Visual Multivariate Analysis with Parallel Coordinates Plot"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html#plotting-a-parallel-coordinates-plot-with-boxplots",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html#plotting-a-parallel-coordinates-plot-with-boxplots",
    "title": "Hands-on Exercise 5d - Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "3.2 Plotting a parallel coordinates plot with boxplots",
    "text": "3.2 Plotting a parallel coordinates plot with boxplots\nThe earlier chart does not provide us with useful understanding of the World Happiness measures. As such, we will make over the plot using a collection of arguments provided by ggparcoord().\n\nggparcoord(data = happy, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Parallel Coordinates Plot of World Happines Variables\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\ngroupColumn argument is used to group the observations (i.e. parallel lines) by using a single variable (i.e. Region) and colour the parallel coordinates lines by region name.\nscale argument is used to scale the variables in the parallel coordinate plot by using uniminmax method. The method univariately scale each variable so the minimum of the variable is zero and the maximum is one.\nalphaLines argument is used to reduce the intensity of the line colour to 0.2. The permissible value range is between 0 to 1.\nboxplot argument is used to turn on the boxplot by using logical TRUE. The default is FALSE.\ntitle argument is used to provide the parallel coordinates plot a title.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5d - Visual Multivariate Analysis with Parallel Coordinates Plot"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html#parallel-coordinates-with-facet",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html#parallel-coordinates-with-facet",
    "title": "Hands-on Exercise 5d - Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "3.3 Parallel coordinates with facet",
    "text": "3.3 Parallel coordinates with facet\nSince ggparcoord() is developed by extending ggplot2 package, we can combination use some of the ggplot2 function when plotting a parallel coordinates plot.\nIn the code chunk below, facet_wrap() of ggplot2 is used to plot 10 small multiple parallel coordinates plots. Each plot represent one geographical region such as East Asia.\n\nggparcoord(data = happy, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nSome of the variable names overlap on x-axis!",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5d - Visual Multivariate Analysis with Parallel Coordinates Plot"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html#rotaing-x-axis-text-label",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html#rotaing-x-axis-text-label",
    "title": "Hands-on Exercise 5d - Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "3.4 Rotaing x-axis text label",
    "text": "3.4 Rotaing x-axis text label\nTo make the x-axis text label easy to read, let us rotate the labels by 30 degrees. We can rotate axis text labels using axis.text.x as argument to the theme() function. And we specify element_text(angle = 30) to rotate the x-axis text by an angle 30 degree.\n\nggparcoord(data = happy, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30))",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5d - Visual Multivariate Analysis with Parallel Coordinates Plot"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html#adjusting-the-rotated-x-axis-text-label",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html#adjusting-the-rotated-x-axis-text-label",
    "title": "Hands-on Exercise 5d - Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "3.5 Adjusting the Rotated x-axis text label",
    "text": "3.5 Adjusting the Rotated x-axis text label\nRotating x-axis text labels to 30 degrees makes the label overlap with the plot and we can avoid this by adjusting the text location using hjust argument to theme’s text element with element_text(). We use axis.text.x as we want to change the look of x-axis text.\n\nggparcoord(data = happy, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30, hjust=1))",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5d - Visual Multivariate Analysis with Parallel Coordinates Plot"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html#the-basic-plot",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html#the-basic-plot",
    "title": "Hands-on Exercise 5d - Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "4.1 The basic plot",
    "text": "4.1 The basic plot\nFirst we select the columns that we want using the following code chunk.\n\nhappy &lt;- happy %&gt;%\n  select(\"Happiness score\", c(7:12))\n\nThen we plot the interactive parallel coordinates plot using parallelPlot() function.\n\nparallelPlot(happy,\n             width = 320,\n             height = 500)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5d - Visual Multivariate Analysis with Parallel Coordinates Plot"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html#rotate-axis-label",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html#rotate-axis-label",
    "title": "Hands-on Exercise 5d - Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "4.2 Rotate Axis Label",
    "text": "4.2 Rotate Axis Label\nWe will use the following code chunk to rotate the axis label to avoid them from overlapping.\n\nparallelPlot(happy,\n             rotateTitle = TRUE)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5d - Visual Multivariate Analysis with Parallel Coordinates Plot"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html#changing-the-colour-scheme",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html#changing-the-colour-scheme",
    "title": "Hands-on Exercise 5d - Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "4.3 Changing the colour scheme",
    "text": "4.3 Changing the colour scheme\nWe can change the default blue colour scheme by using continousCS argument as shown in the code chunk below.\n\nparallelPlot(happy,\n             continuousCS = \"Reds\",\n             rotateTitle = TRUE)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5d - Visual Multivariate Analysis with Parallel Coordinates Plot"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html#parallel-coordinates-plot-with-histogram",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05d.html#parallel-coordinates-plot-with-histogram",
    "title": "Hands-on Exercise 5d - Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "4.4 Parallel coordinates plot with histogram",
    "text": "4.4 Parallel coordinates plot with histogram\nIn the code chunk below, histoVisibility argument is used to plot histogram along the axis of each variables.\n\nhistoVisibility &lt;- rep(TRUE, ncol(happy))\nparallelPlot(happy,\n             rotateTitle = TRUE,\n             histoVisibility = histoVisibility)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5d - Visual Multivariate Analysis with Parallel Coordinates Plot"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html",
    "title": "Hands-on Exercise 5b - Visual Correlation Analysis",
    "section": "",
    "text": "In this exercise, we will learn how to visualise correlation matrix using R. There are 3 main sections of this exercise. First, we will learn how to create a correlation matrix using pairs() of R Graphics. Then we will learn how to plot corrgram using corrplot package of R. Lastly, we will create an interactive correlation matrix using plotly R.\n\n\n\n\n\n\n\nWhy use correlation matrix?\n\n\n\nCorrelation coefficient measures the type and strength of the relationship between 2 variables. The values of a correlation coefficient ranges between -1.0 and 1.0. A correlation coefficient of 1 shows a perfect linear relationship between the two variables, while a -1.0 shows a perfect inverse relationship between the two variables. A correlation coefficient of 0.0 shows no linear relationship between the two variables.\nWhen we have multivariate data, the correlation coefficients are pairwise comparisons displayed in a table form, also known as correlation matrix.\nThere are three main reasons for computing a correlation matrix:\n\nTo reveal the relationship between high-dimensional variables pair-wisely\nTo input into other analyses. For example, correlation matrices can be inputs for exploratory factor analysis, confirmatory factor analysis and linear regression when excluding missing values pairwise.\nAs a diagnostic when checking other analyses. For example, in linear regression, a high amount of correlation suggests that the linear regression’s estimates would be unreliable.\n\nWhen the data is large, both in terms of the number of observations and the number of variables, Corrgram tend to be used to visually explore and analyse the structure and the patterns of relations among variables. It is designed based on two main schemes:\n\nRendering the value of a correlation to depict its sign and magnitude, and\nReordering the variables in a correlation matrix so that “similar” variables are positioned adjacently, facilitating perception.\n\nWe will see more of corrgram later in this exercise!",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5b - Visual Correlation Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#installing-and-loading-the-packages",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#installing-and-loading-the-packages",
    "title": "Hands-on Exercise 5b - Visual Correlation Analysis",
    "section": "2.1 Installing and Loading the Packages",
    "text": "2.1 Installing and Loading the Packages\nFor this exercise, other than tidyverse , we will use the following packages:\n\ncorrplot: provides a visual exploratory tool on correlation matrix that supports automatic variable reordering to help detect hidden patterns among variables.\nggstatsplot: to create correlation matrix\nplotly: makes interactive, publication-quality graphs.\n\nThe code chunk below uses p_load() of pacman package to check if the abovementioned packages are installed in the computer. If they are, they will be launched in R. Otherwise, pacman will install the relevant packages before launching them.\n\n\nShow the code\npacman::p_load(tidyverse, plotly, corrplot, ggpubr, DT, ggstatsplot)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5b - Visual Correlation Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#importing-the-data",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#importing-the-data",
    "title": "Hands-on Exercise 5b - Visual Correlation Analysis",
    "section": "2.2 Importing the Data",
    "text": "2.2 Importing the Data\nFor this exercise, we will be using the Wine Quality Data Set of UCI Machine Learning Repository. The data set consists of 13 variables and 6497 observations. For the purpose of this exercise, we have combined the red wine and white wine data into one data file. It is called wine_quality and is in csv file format.\nThe following code chunk uses read_csv() function of readr package to import the data into R.\n\n\nShow the code\nwine &lt;- read_csv(\"data/wine_quality.csv\")\n\n\n\nThe DataChecking the Data\n\n\n\ndatatable(wine)\n\n\n\n\n\n\n\n\nglimpse(wine)\n\nRows: 6,497\nColumns: 13\n$ `fixed acidity`        &lt;dbl&gt; 7.4, 7.8, 7.8, 11.2, 7.4, 7.4, 7.9, 7.3, 7.8, 7…\n$ `volatile acidity`     &lt;dbl&gt; 0.700, 0.880, 0.760, 0.280, 0.700, 0.660, 0.600…\n$ `citric acid`          &lt;dbl&gt; 0.00, 0.00, 0.04, 0.56, 0.00, 0.00, 0.06, 0.00,…\n$ `residual sugar`       &lt;dbl&gt; 1.9, 2.6, 2.3, 1.9, 1.9, 1.8, 1.6, 1.2, 2.0, 6.…\n$ chlorides              &lt;dbl&gt; 0.076, 0.098, 0.092, 0.075, 0.076, 0.075, 0.069…\n$ `free sulfur dioxide`  &lt;dbl&gt; 11, 25, 15, 17, 11, 13, 15, 15, 9, 17, 15, 17, …\n$ `total sulfur dioxide` &lt;dbl&gt; 34, 67, 54, 60, 34, 40, 59, 21, 18, 102, 65, 10…\n$ density                &lt;dbl&gt; 0.9978, 0.9968, 0.9970, 0.9980, 0.9978, 0.9978,…\n$ pH                     &lt;dbl&gt; 3.51, 3.20, 3.26, 3.16, 3.51, 3.51, 3.30, 3.39,…\n$ sulphates              &lt;dbl&gt; 0.56, 0.68, 0.65, 0.58, 0.56, 0.56, 0.46, 0.47,…\n$ alcohol                &lt;dbl&gt; 9.4, 9.8, 9.8, 9.8, 9.4, 9.4, 9.4, 10.0, 9.5, 1…\n$ quality                &lt;dbl&gt; 5, 5, 5, 6, 5, 5, 5, 7, 7, 5, 5, 5, 5, 5, 5, 5,…\n$ type                   &lt;chr&gt; \"red\", \"red\", \"red\", \"red\", \"red\", \"red\", \"red\"…",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5b - Visual Correlation Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#changing-data-type",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#changing-data-type",
    "title": "Hands-on Exercise 5b - Visual Correlation Analysis",
    "section": "2.3 Changing Data Type",
    "text": "2.3 Changing Data Type\nNotice that quality should be considered a factor, rather than a numerical value, since the number represents the “level” of wine quality.\n\nwine$quality &lt;- as.factor(wine$quality)\nglimpse(wine)\n\nRows: 6,497\nColumns: 13\n$ `fixed acidity`        &lt;dbl&gt; 7.4, 7.8, 7.8, 11.2, 7.4, 7.4, 7.9, 7.3, 7.8, 7…\n$ `volatile acidity`     &lt;dbl&gt; 0.700, 0.880, 0.760, 0.280, 0.700, 0.660, 0.600…\n$ `citric acid`          &lt;dbl&gt; 0.00, 0.00, 0.04, 0.56, 0.00, 0.00, 0.06, 0.00,…\n$ `residual sugar`       &lt;dbl&gt; 1.9, 2.6, 2.3, 1.9, 1.9, 1.8, 1.6, 1.2, 2.0, 6.…\n$ chlorides              &lt;dbl&gt; 0.076, 0.098, 0.092, 0.075, 0.076, 0.075, 0.069…\n$ `free sulfur dioxide`  &lt;dbl&gt; 11, 25, 15, 17, 11, 13, 15, 15, 9, 17, 15, 17, …\n$ `total sulfur dioxide` &lt;dbl&gt; 34, 67, 54, 60, 34, 40, 59, 21, 18, 102, 65, 10…\n$ density                &lt;dbl&gt; 0.9978, 0.9968, 0.9970, 0.9980, 0.9978, 0.9978,…\n$ pH                     &lt;dbl&gt; 3.51, 3.20, 3.26, 3.16, 3.51, 3.51, 3.30, 3.39,…\n$ sulphates              &lt;dbl&gt; 0.56, 0.68, 0.65, 0.58, 0.56, 0.56, 0.46, 0.47,…\n$ alcohol                &lt;dbl&gt; 9.4, 9.8, 9.8, 9.8, 9.4, 9.4, 9.4, 10.0, 9.5, 1…\n$ quality                &lt;fct&gt; 5, 5, 5, 6, 5, 5, 5, 7, 7, 5, 5, 5, 5, 5, 5, 5,…\n$ type                   &lt;chr&gt; \"red\", \"red\", \"red\", \"red\", \"red\", \"red\", \"red\"…\n\n\nSince correlation matrices are only for numerical variables, we can also create another dataframe by dropping the non-numerical variables (i.e. quality and type).\n\nwine2 &lt;- wine %&gt;% \n  select(-c(quality, type))\n\nglimpse(wine2)\n\nRows: 6,497\nColumns: 11\n$ `fixed acidity`        &lt;dbl&gt; 7.4, 7.8, 7.8, 11.2, 7.4, 7.4, 7.9, 7.3, 7.8, 7…\n$ `volatile acidity`     &lt;dbl&gt; 0.700, 0.880, 0.760, 0.280, 0.700, 0.660, 0.600…\n$ `citric acid`          &lt;dbl&gt; 0.00, 0.00, 0.04, 0.56, 0.00, 0.00, 0.06, 0.00,…\n$ `residual sugar`       &lt;dbl&gt; 1.9, 2.6, 2.3, 1.9, 1.9, 1.8, 1.6, 1.2, 2.0, 6.…\n$ chlorides              &lt;dbl&gt; 0.076, 0.098, 0.092, 0.075, 0.076, 0.075, 0.069…\n$ `free sulfur dioxide`  &lt;dbl&gt; 11, 25, 15, 17, 11, 13, 15, 15, 9, 17, 15, 17, …\n$ `total sulfur dioxide` &lt;dbl&gt; 34, 67, 54, 60, 34, 40, 59, 21, 18, 102, 65, 10…\n$ density                &lt;dbl&gt; 0.9978, 0.9968, 0.9970, 0.9980, 0.9978, 0.9978,…\n$ pH                     &lt;dbl&gt; 3.51, 3.20, 3.26, 3.16, 3.51, 3.51, 3.30, 3.39,…\n$ sulphates              &lt;dbl&gt; 0.56, 0.68, 0.65, 0.58, 0.56, 0.56, 0.46, 0.47,…\n$ alcohol                &lt;dbl&gt; 9.4, 9.8, 9.8, 9.8, 9.4, 9.4, 9.4, 10.0, 9.5, 1…\n\n\nNow we are ready to plot the correlation matrix!",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5b - Visual Correlation Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#drawing-half-of-the-matrix",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#drawing-half-of-the-matrix",
    "title": "Hands-on Exercise 5b - Visual Correlation Analysis",
    "section": "3.1 Drawing Half of the Matrix",
    "text": "3.1 Drawing Half of the Matrix\nAs a correlation matrix is symmetric, we can customise the pairs() function to show the upper or lower half of the matrix.\n\nLower Half of the MatrixUpper Half of the Matrix\n\n\n\npairs(wine2, upper.panel = NULL)\n\n\n\n\n\n\n\n\n\n\n\npairs(wine2, lower.panel = NULL)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5b - Visual Correlation Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#including-correlation-coefficients",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#including-correlation-coefficients",
    "title": "Hands-on Exercise 5b - Visual Correlation Analysis",
    "section": "3.2 Including Correlation Coefficients",
    "text": "3.2 Including Correlation Coefficients\nFor easy interpretation, we can also show the correlation coefficient of each pair of variables rather than a scatter plot by creating a panel.cor function. Higher correlations are shown in a larger font.\n\npanel.cor &lt;- function (x,y, digits = 2, prefix = \"\", cex.cor, ...){\n  usr &lt;- par(\"usr\")\n  on.exit(par(usr))\n  par(usr = c(0, 1, 0, 1))\n  r &lt;- abs(cor(x, y, use=\"complete.obs\"))\n  txt &lt;- format(c(r, 0.123456789), digits=digits)[1]\n  txt &lt;- paste(prefix, txt, sep=\"\")\n  if(missing(cex.cor)) cex.cor &lt;- 0.8/strwidth(txt)\n  text(0.5, 0.5, txt, cex = cex.cor * (1 + r) / 2)\n}\n\n\npairs(wine2, upper.panel = panel.cor)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5b - Visual Correlation Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#the-basic-plot",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#the-basic-plot",
    "title": "Hands-on Exercise 5b - Visual Correlation Analysis",
    "section": "4.1 The Basic Plot",
    "text": "4.1 The Basic Plot\nOn of the advantage of using ggcorrmat() over many other methods to visualise a correlation matrix is it’s ability to provide a comprehensive and yet professional statistical report as shown below.\n\nggcorrmat(wine, \n          cor.vars = 1:11)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5b - Visual Correlation Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#further-customising-the-plot",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#further-customising-the-plot",
    "title": "Hands-on Exercise 5b - Visual Correlation Analysis",
    "section": "4.2 Further Customising the plot",
    "text": "4.2 Further Customising the plot\nWe can further customise the correlation matrix by adding additional arguments and also include title and subtitle!\n\nggcorrmat(\n  data = wine, \n  cor.vars = 1:11,\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  title    = \"Correlogram for wine dataset\",\n  subtitle = \"Four pairs are no significant at p &lt; 0.05\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\ncor.vars: List of variables for which the correlation matrix is to be computed and visualized. If NULL (default), all numeric variables from data will be used.\ngcorrplot.args: A list of additional (mostly aesthetic) arguments that will be passed to ggcorrplot::ggcorrplot() function. The list should avoid any of the following arguments since they are already internally being used: corr, method, p.mat, sig.level, ggtheme, colors, lab, pch, legend.title, digits.\n\n\nTo control specific components of the plot such as the font size of x-axis, y-axis and the statistical report, we can add the following code:\n\nggcorrmat(\n  data = wine, \n  cor.vars = 1:11,\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  title    = \"Correlogram for wine dataset\",\n  subtitle = \"Four pairs are no significant at p &lt; 0.05\", \n  ggplot.component = list(\n    theme(text=element_text(size=7),\n      axis.text.x = element_text(size = 8),\n      axis.text.y = element_text(size = 8)))) \n\n\n\n\n\n\n\n\nNotice that the font size of the axes are smaller now.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5b - Visual Correlation Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#multiple-plots",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#multiple-plots",
    "title": "Hands-on Exercise 5b - Visual Correlation Analysis",
    "section": "4.3 Multiple Plots",
    "text": "4.3 Multiple Plots\nTo build facetted correlation matrix, we have to use grouped_ggcorrmat() of gstatsplot instead.\n\ngrouped_ggcorrmat(\n  data = wine,\n  cor.vars = 1:11,\n  grouping.var = type, # to have a plot for red wines and another plot for white wines\n  type = \"robust\",\n  p.adjust.method = \"holm\",\n  plotgrid.args = list(ncol = 2),\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  annotation.args = list(\n    tag_levels = \"a\",\n    title = \"Correlogram for wine dataset\",\n    subtitle = \"The measures are: alcohol, sulphates, fixed acidity, citric acid, chlorides, residual sugar, density, free sulfur dioxide and volatile acidity\",\n    caption = \"Dataset: UCI Machine Learning Repository\"\n  )\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\nTo build a facet plot, the only argument needed is grouping.var.\nBehind group_ggcorrmat(), patchwork package is used to create the multiplot. plotgrid.args argument provides a list of additional arguments passed to patchwork::wrap_plots, except for guides argument which is already separately specified earlier.\nLikewise, annotation.args argument is calling plot annotation arguments of patchwork package.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5b - Visual Correlation Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#observations",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#observations",
    "title": "Hands-on Exercise 5b - Visual Correlation Analysis",
    "section": "5.1 Observations",
    "text": "5.1 Observations\n\nthe default visual object to plot the corrgram is circle\nthe default layout of the corrgram is a symmetric matrix\nthe default colour scheme is diverging blue-red. Blue colours are used to represent pair variables with positive correlation coefficients and red colours are used to represent pair variables with negative correlation coefficients.. The intensity of the colour or also know as saturation is used to represent the strength of the correlation coefficient. Darker colours indicate relatively stronger linear relationship between the paired variables. On the other hand, lighter colours indicates relatively weaker linear relationship.\n\n:::",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5b - Visual Correlation Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#working-with-visual-geometrics",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#working-with-visual-geometrics",
    "title": "Hands-on Exercise 5b - Visual Correlation Analysis",
    "section": "5.2 Working with visual geometrics",
    "text": "5.2 Working with visual geometrics\nIn corrplot package, there are seven visual geometrics (parameter method) can be used to encode the attribute values. They are: circle, square, ellipse, number, shade, color and pie. The default is circle. However, this default setting can be changed by using the method argument as shown in the code chunk below.\n\nellipseshadesquarenumbercolourpie\n\n\n\ncorrplot(wine.cor, \n         method = \"ellipse\") \n\n\n\n\n\n\n\n\n\n\n\ncorrplot(wine.cor, \n         method = \"shade\") \n\n\n\n\n\n\n\n\n\n\n\ncorrplot(wine.cor, \n         method = \"square\") \n\n\n\n\n\n\n\n\n\n\n\ncorrplot(wine.cor, \n         method = \"number\") \n\n\n\n\n\n\n\n\n\n\n\ncorrplot(wine.cor, \n         method = \"color\") \n\n\n\n\n\n\n\n\n\n\n\ncorrplot(wine.cor, \n         method = \"pie\")",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5b - Visual Correlation Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#working-with-layout",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#working-with-layout",
    "title": "Hands-on Exercise 5b - Visual Correlation Analysis",
    "section": "5.3 Working with layout",
    "text": "5.3 Working with layout\ncorrplor() supports three layout types, namely: “full”, “upper” or “lower”. The default is “full” which display full matrix. The default setting can be changed by using the type argument of corrplot().\n\nupperlower\n\n\n\ncorrplot(wine.cor, \n         method = \"number\", \n         type=\"upper\")\n\n\n\n\n\n\n\n\n\n\n\ncorrplot(wine.cor, \n         method = \"number\", \n         type=\"lower\")\n\n\n\n\n\n\n\n\n\n\n\nThe default layout of the corrgram can be further customised. For example, arguments diag and tl.col are used to turn off the diagonal cells and to change the axis text label colour to black colour respectively as shown in the code chunk and figure below.\n\ncorrplot(wine.cor, \n         method = \"square\", \n         type=\"lower\",\n         diag = FALSE,\n         tl.col = \"black\")",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5b - Visual Correlation Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#working-with-a-mixed-layout",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#working-with-a-mixed-layout",
    "title": "Hands-on Exercise 5b - Visual Correlation Analysis",
    "section": "5.4 Working with a mixed layout",
    "text": "5.4 Working with a mixed layout\nIt is possible to design corrgram with mixed visual matrix of one half and numerical matrix on the other half. In order to create a coorgram with mixed layout, the corrplot.mixed(), a wrapped function for mixed visualisation style will be used.\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotice that argument lower and upper are used to define the visualisation method used. In this case ellipse is used to map the lower half of the corrgram and numerical matrix (i.e. number) is used to map the upper half of the corrgram. The argument tl.pos, on the other, is used to specify the placement of the axis label. Lastly, the diag argument is used to specify the glyph on the principal diagonal of the corrgram.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5b - Visual Correlation Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#combining-corrgram-with-significance-test",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#combining-corrgram-with-significance-test",
    "title": "Hands-on Exercise 5b - Visual Correlation Analysis",
    "section": "5.5 Combining corrgram with significance test",
    "text": "5.5 Combining corrgram with significance test\nIn statistical analysis, we are also interested to know which pair of variables their correlation coefficients are statistically significant.\nWith corrplot package, we can combine corrgram with significance test. First, we use the cor.mtest() to compute the p-values and confidence interval for each pair of variables.\n\nwine.sig &lt;- cor.mtest(wine.cor, conf.level=0.95)\n\nWe then use the p.mat argument of corrplot function as shown below.\n\ncorrplot(wine.cor,\n         method = \"number\",\n         type = \"lower\",\n         diag = FALSE,\n         tl.col = \"black\",\n         tl.srt = 45,\n         p.mat = wine.sig$p,\n         sig.level = .05)\n\n\n\n\n\n\n\n\nThe corrgram reveals that not all correlation pairs are statistically significant. For example the correlation between total sulfur dioxide and free surfur dioxide is statistically significant at significant level of 0.05 but not the pair between total sulfur dioxide and citric acid.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5b - Visual Correlation Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#reorder-a-corrgram",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#reorder-a-corrgram",
    "title": "Hands-on Exercise 5b - Visual Correlation Analysis",
    "section": "5.6 Reorder a corrgram",
    "text": "5.6 Reorder a corrgram\nMatrix reorder is important for mining the hiden structure and pattern in a corrgram. By default, the order of attributes of a corrgram is sorted according to the correlation matrix (i.e. “original”). The default setting can be over-write by using the order argument of corrplot(). Currently, corrplot package support four sorting methods, they are:\n\n“AOE” is for the angular order of the eigenvectors. See Michael Friendly (2002) for details.\n“FPC” for the first principal component order.\n“hclust” for hierarchical clustering order, and “hclust.method” for the agglomeration method to be used. “hclust.method” should be one of “ward”, “single”, “complete”, “average”, “mcquitty”, “median” or “centroid”.\n“alphabet” for alphabetical order.\n\n“AOE”, “FPC”, “hclust”, “alphabet”. More algorithms can be found in seriation package.\n\nUsing AOE orderUsing FPC orderUsing hclust orderUsing alphabet order\n\n\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               order=\"AOE\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\n\n\n\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               order=\"FPC\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\n\n\n\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               order=\"hclust\",\n               hclust.method= \"ward.D\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\n\n\n\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               order=\"alphabet\",\n               tl.col = \"black\")",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5b - Visual Correlation Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04d.html",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04d.html",
    "title": "Hands-on Exercise 4d - Funnel Plots for Fair Comparisons",
    "section": "",
    "text": "In this exercise, we will learn how to plot the following:\n\nfunnel plots using funnelPlotR package,\nstatic funnel plot using ggplot2 package, and\ninteractive funnel plot using both plotly R and ggplot2 packages.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4d - Funnel Plots for Fair Comparisons"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04d.html#installing-and-loading-the-packages",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04d.html#installing-and-loading-the-packages",
    "title": "Hands-on Exercise 4d - Funnel Plots for Fair Comparisons",
    "section": "3.1 Installing and Loading the Packages",
    "text": "3.1 Installing and Loading the Packages\nFor this exercise, other than tidyverse, we will use the following packages:\n\nreadr for importing csv into R.\nFunnelPlotR for creating funnel plot.\nggplot2 for creating funnel plot manually.\nknitr for building static html table.\nplotly for creating interactive funnel plot.\n\n\npacman::p_load(tidyverse, FunnelPlotR, plotly, knitr)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4d - Funnel Plots for Fair Comparisons"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04d.html#importing-data",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04d.html#importing-data",
    "title": "Hands-on Exercise 4d - Funnel Plots for Fair Comparisons",
    "section": "3.2 Importing Data",
    "text": "3.2 Importing Data\nIn this exercise, COVID-19_DKI_Jakarta will be used. The data was downloaded from Open Data Covid-19 Provinsi DKI Jakarta portal. \nWe will use this data to compare the cumulative COVID-19 cases and deaths by sub-districts as at 31 Jul 2021, DKI Jakarta.\nThe following code chunk imports the data into R and save it as a tibble dataframe object called covid19.\n\ncovid19 &lt;- read_csv(\"data/COVID-19_DKI_Jakarta.csv\") %&gt;%\n  mutate_if(is.character, as.factor)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4d - Funnel Plots for Fair Comparisons"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04d.html#about-funnelplotr",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04d.html#about-funnelplotr",
    "title": "Hands-on Exercise 4d - Funnel Plots for Fair Comparisons",
    "section": "4.1 About FunnelPlotR",
    "text": "4.1 About FunnelPlotR\nFunnelPlotR package uses ggplot to generate funnel plots. It requires a numerator (events of interest), denominator (population to be considered) and group. The key arguments selected for customisation are:\n\nlimit: plot limits (95 or 99).\nlabel_outliers: to label outliers (true or false).\nPoisson_limits: to add Poisson limits to the plot.\nOD_adjust: to add overdispersed limits to the plot.\nxrange and yrange: to specify the range to display for axes, acts like a zoom function.\nOther aesthetic components such as graph title, axis labels etc",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4d - Funnel Plots for Fair Comparisons"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04d.html#basic-plot",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04d.html#basic-plot",
    "title": "Hands-on Exercise 4d - Funnel Plots for Fair Comparisons",
    "section": "4.2 Basic Plot",
    "text": "4.2 Basic Plot\n\nfunnel_plot(\n  numerator = covid19$Positive,\n  denominator = covid19$Death,\n  group = covid19$`Sub-district`,\n  title = \"COVID-19 Cases and Deaths \\nby Sub-Districts in Jakarta (31 Jul 2021)\"\n)\n\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 0 are outliers. \nPlot is adjusted for overdispersion. \n\n\n\n\n\n\n\n\nNote\n\n\n\ngroup in this function is dfferent from the scatterplot. Here, it defines the level of the points to be plotted i.e. Sub-district, District or City. If Cityc is chosen, there are only six data points.\nBy default, data_type argument is “SR”.\nlimit: Plot limits, accepted values are: 95 or 99, corresponding to 95% or 99.8% quantiles of the distribution.\n\n\nBut the above chart is not easy to read because all the dots are close to each other.\nSo we will change the data_type from “SR” to “PR” (proportions) and add xrange and yrange to set the range of x-axis and y-axis.\n\nfunnel_plot(\n  numerator = covid19$Death,\n  denominator = covid19$Positive,\n  group = covid19$`Sub-district`,\n  data_type = \"PR\",     \n  xrange = c(0, 6500),  \n  yrange = c(0, 0.05)   \n)\n\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\nWe can further ehnance the chart by adding title and axis labels, and removing the point labels to avoid overly cluttering the chart.\n\nfunnel_plot(\n  numerator = covid19$Death,\n  denominator = covid19$Positive,\n  group = covid19$`Sub-district`,\n  data_type = \"PR\",   \n  xrange = c(0, 6500),  \n  yrange = c(0, 0.05),\n  label = NA,\n  title = \"Cumulative COVID-19 Fatality Rate by \\nCumulative Total Number of COVID-19 Positive Cases\", #&lt;&lt;           \n  x_label = \"Cumulative COVID-19 Positive Cases\", #&lt;&lt;\n  y_label = \"Cumulative Fatality Rate\"  #&lt;&lt;\n)\n\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4d - Funnel Plots for Fair Comparisons"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04d.html#computing-basic-derived-fields",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04d.html#computing-basic-derived-fields",
    "title": "Hands-on Exercise 4d - Funnel Plots for Fair Comparisons",
    "section": "5.1 Computing Basic Derived Fields",
    "text": "5.1 Computing Basic Derived Fields\nTo plot the funnel plot from scratch, we need to calculate the death rate and the standard error of cumulative death rate.\n\ndf &lt;- covid19 %&gt;%\n  mutate(rate = Death / Positive) %&gt;%\n  mutate(rate.se = sqrt((rate*(1-rate)) / (Positive))) %&gt;%\n  filter(rate &gt; 0)\n\nThen we compute the fit.mean using the following code chunk.\n\nfit.mean &lt;- weighted.mean(df$rate, 1/df$rate.se^2)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4d - Funnel Plots for Fair Comparisons"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04d.html#computing-upper-and-lower-limits-for-95-and-99-confiedence-intervals",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04d.html#computing-upper-and-lower-limits-for-95-and-99-confiedence-intervals",
    "title": "Hands-on Exercise 4d - Funnel Plots for Fair Comparisons",
    "section": "5.2 Computing Upper and Lower Limits for 95% and 99% Confiedence Intervals",
    "text": "5.2 Computing Upper and Lower Limits for 95% and 99% Confiedence Intervals\nThe following code chun computes the upper and lowr limits for 95% and 99% Confidence Intervals.\n\nnumber.seq &lt;- seq(1, max(df$Positive), 1)\nnumber.ll95 &lt;- fit.mean - 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul95 &lt;- fit.mean + 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ll999 &lt;- fit.mean - 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul999 &lt;- fit.mean + 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \ndfCI &lt;- data.frame(number.ll95, number.ul95, number.ll999, \n                   number.ul999, number.seq, fit.mean)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4d - Funnel Plots for Fair Comparisons"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04d.html#static-funnel-plot",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04d.html#static-funnel-plot",
    "title": "Hands-on Exercise 4d - Funnel Plots for Fair Comparisons",
    "section": "5.3 Static Funnel Plot",
    "text": "5.3 Static Funnel Plot\nNow we are ready to start plotting!\nWe can create a static funnel plot using a combination of geom_point and geom_line functions, as seen in the following code chunk.\n\np &lt;- ggplot(df, aes(x = Positive, y = rate)) +\n  geom_point(aes(label=`Sub-district`), \n             alpha=0.4) +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_hline(data = dfCI, \n             aes(yintercept = fit.mean), \n             size = 0.4, \n             colour = \"grey40\") +\n  coord_cartesian(ylim=c(0,0.05)) +\n  annotate(\"text\", x = 1, y = -0.13, label = \"95%\", size = 3, colour = \"grey40\") + \n  annotate(\"text\", x = 4.5, y = -0.18, label = \"99%\", size = 3, colour = \"grey40\") + \n  ggtitle(\"Cumulative Fatality Rate by Cumulative Number of COVID-19 Cases\") +\n  xlab(\"Cumulative Number of COVID-19 Cases\") + \n  ylab(\"Cumulative Fatality Rate\") +\n  theme_light() +\n  theme(plot.title = element_text(size=12),\n        legend.position = c(0.91,0.85), \n        legend.title = element_text(size=7),\n        legend.text = element_text(size=7),\n        legend.background = element_rect(colour = \"grey60\", linetype = \"dotted\"),\n        legend.key.height = unit(0.3, \"cm\"))\n\np",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4d - Funnel Plots for Fair Comparisons"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04d.html#interactive-funnel-plot",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04d.html#interactive-funnel-plot",
    "title": "Hands-on Exercise 4d - Funnel Plots for Fair Comparisons",
    "section": "5.4 Interactive Funnel Plot",
    "text": "5.4 Interactive Funnel Plot\nWe can make the funnel plot interactive using ggplotly() of plotly r package.\n\nfp_ggplotly &lt;- ggplotly(p,\n  tooltip = c(\"label\", \n              \"x\", \n              \"y\"))\nfp_ggplotly",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4d - Funnel Plots for Fair Comparisons"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html",
    "title": "Hands-on Exercise 4b - Visual Statistical Analysis",
    "section": "",
    "text": "In this hands-on exercise, we will learn how to:\n\ncreate visual graphics with rich statistical information using ggstatsplot package\nvisualise model diagnostics using performance package\nvisualise model parameters using parameters package.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4b - Visual Statistical Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#installing-and-loading-the-packages",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#installing-and-loading-the-packages",
    "title": "Hands-on Exercise 4b - Visual Statistical Analysis",
    "section": "2.1 Installing and Loading the Packages",
    "text": "2.1 Installing and Loading the Packages\nFor this exercise, other than tidyverse, we will use the following packages:\n\nggstatplot: an extension of ggplot2 package for creating graphics with details from statistical tests included in the information-rich plots themselves.\nperformance: to provide utilities for computing indices of model quality and goodness of fit.\nparameters: to provide utilities for processing the parameters of various statistical models\ntidyverse: a family of R packages for data science processing\nreadxl: to import excel files into R\n\nThe code chunk below uses p_load() of pacman package to check if the abovementioned packages are installed in the computer. If they are, they will be launched in R. Otherwise, pacman will install the relevant packages before launching them.\n\n\nShow the code\npacman::p_load(ggstatsplot, performance, parameters, tidyverse, readxl, see)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4b - Visual Statistical Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#importing-the-data",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#importing-the-data",
    "title": "Hands-on Exercise 4b - Visual Statistical Analysis",
    "section": "2.2 Importing the Data",
    "text": "2.2 Importing the Data\nFor this exercise, we will be using the Exam_data.csv provided by the course instructor and we have used it in Hands-on Exercises 1 and 2. It consists of year end examination grades of a cohort of primary 3 students from a local school. It is in csv file format.\nWe use read_csv() function of readr to import the Exam_data.csv file into R and save it as a tibble data frame called exam_data. Then we will use datatable() of DT to have an overview of the imported data.\n\n\nShow the code\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")\nglimpse(exam_data)\n\n\nRows: 322\nColumns: 7\n$ ID      &lt;chr&gt; \"Student321\", \"Student305\", \"Student289\", \"Student227\", \"Stude…\n$ CLASS   &lt;chr&gt; \"3I\", \"3I\", \"3H\", \"3F\", \"3I\", \"3I\", \"3I\", \"3I\", \"3I\", \"3H\", \"3…\n$ GENDER  &lt;chr&gt; \"Male\", \"Female\", \"Male\", \"Male\", \"Male\", \"Female\", \"Male\", \"M…\n$ RACE    &lt;chr&gt; \"Malay\", \"Malay\", \"Chinese\", \"Chinese\", \"Malay\", \"Malay\", \"Chi…\n$ ENGLISH &lt;dbl&gt; 21, 24, 26, 27, 27, 31, 31, 31, 33, 34, 34, 36, 36, 36, 37, 38…\n$ MATHS   &lt;dbl&gt; 9, 22, 16, 77, 11, 16, 21, 18, 19, 49, 39, 35, 23, 36, 49, 30,…\n$ SCIENCE &lt;dbl&gt; 15, 16, 16, 31, 25, 16, 25, 27, 15, 37, 42, 22, 32, 36, 35, 45…\n\n\nIn addition, we will be using the ToyotaCorolla.xls for Visualising Models and Parameters. We use read_xls() of readxl package to import the data worksheet of ToyotaCorolla.xls workbook into R.\n\n\nShow the code\ncar_resale &lt;- read_xls(\"data/ToyotaCorolla.xls\", \n                       \"data\")\nglimpse(car_resale)\n\n\nRows: 1,436\nColumns: 38\n$ Id               &lt;dbl&gt; 81, 1, 2, 3, 4, 5, 6, 7, 8, 44, 45, 46, 47, 49, 51, 6…\n$ Model            &lt;chr&gt; \"TOYOTA Corolla 1.6 5drs 1 4/5-Doors\", \"TOYOTA Coroll…\n$ Price            &lt;dbl&gt; 18950, 13500, 13750, 13950, 14950, 13750, 12950, 1690…\n$ Age_08_04        &lt;dbl&gt; 25, 23, 23, 24, 26, 30, 32, 27, 30, 27, 22, 23, 27, 2…\n$ Mfg_Month        &lt;dbl&gt; 8, 10, 10, 9, 7, 3, 1, 6, 3, 6, 11, 10, 6, 11, 11, 11…\n$ Mfg_Year         &lt;dbl&gt; 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002,…\n$ KM               &lt;dbl&gt; 20019, 46986, 72937, 41711, 48000, 38500, 61000, 9461…\n$ Quarterly_Tax    &lt;dbl&gt; 100, 210, 210, 210, 210, 210, 210, 210, 210, 234, 234…\n$ Weight           &lt;dbl&gt; 1180, 1165, 1165, 1165, 1165, 1170, 1170, 1245, 1245,…\n$ Guarantee_Period &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,…\n$ HP_Bin           &lt;chr&gt; \"100-120\", \"&lt; 100\", \"&lt; 100\", \"&lt; 100\", \"&lt; 100\", \"&lt; 100…\n$ CC_bin           &lt;chr&gt; \"1600\", \"&gt;1600\", \"&gt;1600\", \"&gt;1600\", \"&gt;1600\", \"&gt;1600\", …\n$ Doors            &lt;dbl&gt; 5, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 3, 3,…\n$ Gears            &lt;dbl&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,…\n$ Cylinders        &lt;dbl&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,…\n$ Fuel_Type        &lt;chr&gt; \"Petrol\", \"Diesel\", \"Diesel\", \"Diesel\", \"Diesel\", \"Di…\n$ Color            &lt;chr&gt; \"Blue\", \"Blue\", \"Silver\", \"Blue\", \"Black\", \"Black\", \"…\n$ Met_Color        &lt;dbl&gt; 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,…\n$ Automatic        &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Mfr_Guarantee    &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,…\n$ BOVAG_Guarantee  &lt;dbl&gt; 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ ABS              &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Airbag_1         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Airbag_2         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Airco            &lt;dbl&gt; 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Automatic_airco  &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,…\n$ Boardcomputer    &lt;dbl&gt; 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ CD_Player        &lt;dbl&gt; 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,…\n$ Central_Lock     &lt;dbl&gt; 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Powered_Windows  &lt;dbl&gt; 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Power_Steering   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Radio            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Mistlamps        &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Sport_Model      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,…\n$ Backseat_Divider &lt;dbl&gt; 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Metallic_Rim     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Radio_cassette   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Tow_Bar          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4b - Visual Statistical Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#one-sample-test-gghistostats-method",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#one-sample-test-gghistostats-method",
    "title": "Hands-on Exercise 4b - Visual Statistical Analysis",
    "section": "3.1 One-sample test: gghistostats() method",
    "text": "3.1 One-sample test: gghistostats() method\nWe can use gghistostats() to build an visual of one-sample test on English Scores.\n\nset.seed(2024)\n\ngghistostats(data = exam_data, x = ENGLISH, \n             type = \"bayes\",\n             test.value = 60, \n             xlab= \"English Scores\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe default information presented are:\n\nStatistical details\nBayes Factor\nSample Sizes\nDistribution Summary\n\n\n\n\n3.1.1 Unpacking the Bayes Factor\n\nA Bayes factor is the ratio of the likelihood of one particular hypothesis to the likelihood of another. It can be interpreted as a measure of the strength of evidence in favor of one theory among two competing theories.\nBayes factor gives us a way to evaluate the data in favor of a null hypothesis, and to use external information to do so. It tells us what the weight of the evidence is in favor of a given hypothesis.\nWhen we are comparing two hypotheses, H1 (the alternate hypothesis) and H0 (the null hypothesis), the Bayes Factor is often written as B10.\nThe Schwarz criterion is one of the easiest ways to calculate rough approximation of the Bayes Factor.\nA Bayes Factor can be any positive number (i.e., 0 and ∞).\n\n1 indicates the data do not favor either theory more than the other;\nvalues greater than 1 indicate increasing evidence for one theory over the other (e.g., the alternative over a null hypothesis) ; and\nvalues less than 1 the converse (e.g., increasing evidence for the null over the alternative hypothesis).\n\nThus, Bayes factors allow three different types of conclusions:\n\nThere is strong evidence for the alternative (B much greater than 1);\nthere is strong evidence for the null (B close to 0); and\nthe evidence is insensitive (B close to 1).",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4b - Visual Statistical Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#two-sample-mean-test-ggbetweenstats",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#two-sample-mean-test-ggbetweenstats",
    "title": "Hands-on Exercise 4b - Visual Statistical Analysis",
    "section": "3.2 Two-sample mean test: ggbetweenstats()",
    "text": "3.2 Two-sample mean test: ggbetweenstats()\nWe can use ggbetweenstats() to build an visual of two sample means test of math scores by gender.\n\nggbetweenstats(data = exam_data, x = GENDER, \n               y = MATHS, type = \"np\", \n               messages = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe default information presented are:\n\nStatistical details\nBayes Factor\nSample Sizes\nDistribution Summary",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4b - Visual Statistical Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#one-way-anova-test-ggbetweenstats-method",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#one-way-anova-test-ggbetweenstats-method",
    "title": "Hands-on Exercise 4b - Visual Statistical Analysis",
    "section": "3.3 One Way ANOVA Test: ggbetweenstats() method",
    "text": "3.3 One Way ANOVA Test: ggbetweenstats() method\nWe can use ggbetweenstats() to build a visual for one-way ANOVA test on English Score by race.\n\nggbetweenstats(\n  data = exam_data, x = RACE, y = ENGLISH, \n  type = \"p\", mean.ci = TRUE, \n  pairwise_comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\", \n  messages = FALSE)\n\n\n\n\n\n\n\n\nTry: explore the aov object!",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4b - Visual Statistical Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#significant-test-of-correlation-ggscatterstats",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#significant-test-of-correlation-ggscatterstats",
    "title": "Hands-on Exercise 4b - Visual Statistical Analysis",
    "section": "3.4 Significant Test of Correlation: ggscatterstats()",
    "text": "3.4 Significant Test of Correlation: ggscatterstats()\nWe can use ggscatterstats() to build a visual for significant test of correlation between Math Scores and English Scores.\n\nggscatterstats(data = exam_data, \n               x = MATHS,\n               y = ENGLISH, \n               marginal = FALSE)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4b - Visual Statistical Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#significant-test-of-association-dependence-ggbarstats",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#significant-test-of-association-dependence-ggbarstats",
    "title": "Hands-on Exercise 4b - Visual Statistical Analysis",
    "section": "3.5 Significant Test of Association (Dependence): ggbarstats()",
    "text": "3.5 Significant Test of Association (Dependence): ggbarstats()\nWe can use ggbarstats() to build a visual for significant test of association.\nFor our data, we will first bin the Math scores into a 4-class variable using cut()\n\nexam1 &lt;- exam_data %&gt;%\n  mutate(MATHS_bins = cut(MATHS, breaks = c(0, 60,75,85,100)))\n\nThen we will build the visual using ggbarstats().\n\nggbarstats(exam1, x = MATHS_bins, \n           y = GENDER)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4b - Visual Statistical Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#multiple-regression-model-using-lm",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#multiple-regression-model-using-lm",
    "title": "Hands-on Exercise 4b - Visual Statistical Analysis",
    "section": "4.1 Multiple Regression Model using lm()",
    "text": "4.1 Multiple Regression Model using lm()\nThe following code chunk is used to calibrate a multiple linear regression model using lm() of Base Stats of R.\n\nmodel &lt;- lm(Price ~ Age_08_04 + Mfg_Year + KM + \n              Weight + Guarantee_Period, data = car_resale)\nmodel\n\n\nCall:\nlm(formula = Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period, \n    data = car_resale)\n\nCoefficients:\n     (Intercept)         Age_08_04          Mfg_Year                KM  \n      -2.637e+06        -1.409e+01         1.315e+03        -2.323e-02  \n          Weight  Guarantee_Period  \n       1.903e+01         2.770e+01",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4b - Visual Statistical Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#model-diagnostic-checks-for-multicolinearity",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#model-diagnostic-checks-for-multicolinearity",
    "title": "Hands-on Exercise 4b - Visual Statistical Analysis",
    "section": "4.2 Model Diagnostic: Checks for Multicolinearity",
    "text": "4.2 Model Diagnostic: Checks for Multicolinearity\nWe check for multicolinearity using check_collinearity() function of performance package.\n\ncheck_collinearity(model)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n             Term  VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n               KM 1.46 [ 1.37,  1.57]         1.21      0.68     [0.64, 0.73]\n           Weight 1.41 [ 1.32,  1.51]         1.19      0.71     [0.66, 0.76]\n Guarantee_Period 1.04 [ 1.01,  1.17]         1.02      0.97     [0.86, 0.99]\n\nHigh Correlation\n\n      Term   VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n Age_08_04 31.07 [28.08, 34.38]         5.57      0.03     [0.03, 0.04]\n  Mfg_Year 31.16 [28.16, 34.48]         5.58      0.03     [0.03, 0.04]\n\n\n\ncheck_c &lt;- check_collinearity(model)\nplot(check_c)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4b - Visual Statistical Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#model-diagnostic-check-for-normality",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#model-diagnostic-check-for-normality",
    "title": "Hands-on Exercise 4b - Visual Statistical Analysis",
    "section": "4.3 Model Diagnostic: Check for Normality",
    "text": "4.3 Model Diagnostic: Check for Normality\nWe use check_normality() of performance package to check if the model follows the normality assumption.\n\nmodel1 &lt;- lm(Price ~ Age_08_04 + KM + \n              Weight + Guarantee_Period, data = car_resale)\n\ncheck_n &lt;- check_normality(model1)\n\nplot(check_n)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4b - Visual Statistical Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#model-diagnostic-check-for-homogeneity-of-variances",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#model-diagnostic-check-for-homogeneity-of-variances",
    "title": "Hands-on Exercise 4b - Visual Statistical Analysis",
    "section": "4.4 Model Diagnostic: Check for Homogeneity of Variances",
    "text": "4.4 Model Diagnostic: Check for Homogeneity of Variances\nWe check for homogeneity of Variances using check_heterscedasticity of performance package.\n\ncheck_h &lt;- check_heteroscedasticity(model1)\nplot(check_h)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4b - Visual Statistical Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#model-diagnostic-complete-check",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#model-diagnostic-complete-check",
    "title": "Hands-on Exercise 4b - Visual Statistical Analysis",
    "section": "4.5 Model Diagnostic: Complete Check",
    "text": "4.5 Model Diagnostic: Complete Check\nWe can also perform the complete by using check_model().\n\ncheck_model(model1)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4b - Visual Statistical Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#visualising-regression-parameters",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04b.html#visualising-regression-parameters",
    "title": "Hands-on Exercise 4b - Visual Statistical Analysis",
    "section": "4.6 Visualising Regression Parameters",
    "text": "4.6 Visualising Regression Parameters\nWe can use two methods to visualise regression parameters: (i) see method and (ii) ggcoefstats() method.\n\nsee methodggcoefstats() method\n\n\n\nplot(parameters(model1))\n\n\n\n\n\n\n\n\n\n\n\nggcoefstats(model1, \n            output = \"plot\")",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4b - Visual Statistical Analysis"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03b.html",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03b.html",
    "title": "Hands-On Exercise 3b: Programming Animated Statistical Graphics with R",
    "section": "",
    "text": "In this exercise, we will learn how to create animated data visualisation by using gganimate and plotly r packages. At the same time, we will also learn how to (i) reshape data by using tidyr package, and (ii) process, wrangle and transform data by using dplyr package.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-On Exercise 3b: Programming Animated Statistical Graphics with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03b.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03b.html#installing-and-launching-r-packages",
    "title": "Hands-On Exercise 3b: Programming Animated Statistical Graphics with R",
    "section": "3.1 Installing and Launching R packages",
    "text": "3.1 Installing and Launching R packages\nFor this exercise, other than tidyverse, we will use the following packages:\n\nplotly: an R library for plotting interactive statistical graphs\ngganimate: an ggplot extension for creating animated statistical graphs\ngifski: converts video frames to GIF animations using pngquant’s fancy features for efficient crossframe palettes and temporal dithering. It produces animated GIFs that use thousands of colours per frame.\ngapminder: an excerpt of the data available at Gapminder.org. We want to use its country_colors scheme for this exercise.\n\nThe code chunk below uses p_load() of pacman package to check if the abovementioned packages are installed in the computer. If they are, they will be launched in R. Otherwise, pacman will install the relevant packages before launching them.\n\n\nShow the code\npacman::p_load(readxl, gifski, gapminder, plotly, gganimate, tidyverse)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-On Exercise 3b: Programming Animated Statistical Graphics with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03b.html#importing-the-data",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03b.html#importing-the-data",
    "title": "Hands-On Exercise 3b: Programming Animated Statistical Graphics with R",
    "section": "3.2 Importing the Data",
    "text": "3.2 Importing the Data\nIn this hands-on exercise, the Data worksheet from GlobalPopulation Excel workbook will be used.We will first use read_xls() of readxl package to import the excel sheet, then use mutate_each_() of dplyr package to convert all character data type into factor, then we will use mutate() to convert data values of the Year field into integer.\n\nUsing mutate_each_()Using mutate_at()Using across()\n\n\n\ncol &lt;- c(\"Country\", \"Continent\")\n\nglobalpop &lt;- read_xls(\"data/GlobalPopulation.xls\",\n                      sheet = \"Data\") %&gt;%\n  mutate_each_(funs(factor(.)), col) %&gt;%\n  mutate(Year = as.integer(Year))\n\n\n\nUnfortunately, mutate_each_() was deprecated in dplyr 0.7.0. and funs()was deprecated in dplyr 0.8.0. In view of this, we will re-write the code by using mutate_at() as shown in the code chunk below.\n\ncol &lt;- c(\"Country\", \"Continent\")\n\nglobalpop &lt;- read_xls(\"data/GlobalPopulation.xls\",\n                      sheet = \"Data\") %&gt;%\n  mutate_at(col, as.factor) %&gt;%\n  mutate(Year = as.integer(Year))\n\n\n\nWe can also use across() instead of mutate_at() to derive the same output.\n\ncol &lt;- c(\"Country\", \"Continent\")\n\nglobalpop &lt;- read_xls(\"data/GlobalPopulation.xls\",\n                      sheet = \"Data\") %&gt;%\n  mutate(across(col, as.factor)) %&gt;%\n  mutate(Year = as.integer(Year))",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-On Exercise 3b: Programming Animated Statistical Graphics with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html",
    "title": "Hands-On Exercise 2: Beyond ggplot2 Fundamentals",
    "section": "",
    "text": "In this exercise, we will be introduced to the following ggplot2 extensions to create more elegant and effective statistical graphics:\n\nggrepel: allows us to control the placement of annotation on a graph\nggthemes and hrbrthemes: allows us to create professional publication quality figures\npatchwork: allow us to plot composite figure by combininig ggplot2 graphs",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-On Exercise 2: Beyond ggplot2 Fundamentals"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#installing-and-launching-r-packages",
    "title": "Hands-On Exercise 2: Beyond ggplot2 Fundamentals",
    "section": "2.1 Installing and Launching R packages",
    "text": "2.1 Installing and Launching R packages\nFor this exercise, other than tidyverse, we will use the following packages:\n\nggrepel: an R package that provides geoms for ggplot2 to repel overlapping text labels\nggthemes: an R package that provides extra themes, geoms and scales for ggplot2.\nhrbrthemes: an R package that providestypography-centric themes and theme components for ggplot2\npatchwork: an R package for preparing composite figures created using ggplot2.\n\nThe code chunk below uses p_load() of pacman package to check if the abovementioned packages are installed in the computer. If they are, they will be launched in R. Otherwise, pacman will install the relevant packages before launching them.\n\n\nShow the code\npacman::p_load(tidyverse, ggrepel, patchwork, ggthemes, hrbrthemes)\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that pacman package has already been installed before using the above code chunk. If you have not yet installed pacman please install it via Rstudios’ “Tools” &gt; “Install Packages” before using the above code chunk.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-On Exercise 2: Beyond ggplot2 Fundamentals"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#importing-the-data",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#importing-the-data",
    "title": "Hands-On Exercise 2: Beyond ggplot2 Fundamentals",
    "section": "2.2 Importing the Data",
    "text": "2.2 Importing the Data\nFor this exercise, we will be using the Exam_data provided by the course instructor.It consists of year end examination grades of a cohort of primary 3 students from a local school. It is in csv file format.\nWe use read_csv() function of readr to import the Exam_data csv file into R then we will use glimpse() of dplyr to learn about the associated attribute information in the dataframe.\n\nCodeData\n\n\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")\n\n\n\n\nglimpse(exam_data)\n\nRows: 322\nColumns: 7\n$ ID      &lt;chr&gt; \"Student321\", \"Student305\", \"Student289\", \"Student227\", \"Stude…\n$ CLASS   &lt;chr&gt; \"3I\", \"3I\", \"3H\", \"3F\", \"3I\", \"3I\", \"3I\", \"3I\", \"3I\", \"3H\", \"3…\n$ GENDER  &lt;chr&gt; \"Male\", \"Female\", \"Male\", \"Male\", \"Male\", \"Female\", \"Male\", \"M…\n$ RACE    &lt;chr&gt; \"Malay\", \"Malay\", \"Chinese\", \"Chinese\", \"Malay\", \"Malay\", \"Chi…\n$ ENGLISH &lt;dbl&gt; 21, 24, 26, 27, 27, 31, 31, 31, 33, 34, 34, 36, 36, 36, 37, 38…\n$ MATHS   &lt;dbl&gt; 9, 22, 16, 77, 11, 16, 21, 18, 19, 49, 39, 35, 23, 36, 49, 30,…\n$ SCIENCE &lt;dbl&gt; 15, 16, 16, 31, 25, 16, 25, 27, 15, 37, 42, 22, 32, 36, 35, 45…\n\n\n\n\n\nFrom the output from glimpse(), we note that:\n\nThere are a total of seven attributes in the exam_data tibble data frame.\nFour of these attributes are categorical data: ID, CLASS, GENDER and RACE .\nThree of these attributes are continuous data: MATHS, ENGLISH and SCIENCE.\n\nWe will use the summarize()to get the summary statistics of the continuous data: MATHS, ENGLISH and SCIENCE.\n\nSummary Statistics for MATHS ScoresSummary Statistics for ENGLISH ScoresSummary Statistics for SCIENCE Scores\n\n\n\nexam_data %&gt;% \n  summarize(min = min(MATHS),\n            q1 = quantile(MATHS, 0.25),\n            median = median(MATHS),\n            mean = mean(MATHS),\n            q3 = quantile(MATHS, 0.75),\n            max = max(MATHS))\n\n# A tibble: 1 × 6\n    min    q1 median  mean    q3   max\n  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     9    58     74  69.3    85    99\n\n\n\n\n\nexam_data %&gt;% \n  summarize(min = min(ENGLISH),\n            q1 = quantile(ENGLISH, 0.25),\n            median = median(ENGLISH),\n            mean = mean(ENGLISH),\n            q3 = quantile(ENGLISH, 0.75),\n            max = max(ENGLISH))\n\n# A tibble: 1 × 6\n    min    q1 median  mean    q3   max\n  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    21    59     70  67.2    78    96\n\n\n\n\n\nexam_data %&gt;% \n  summarize(min = min(SCIENCE),\n            q1 = quantile(SCIENCE, 0.25),\n            median = median(SCIENCE),\n            mean = mean(SCIENCE),\n            q3 = quantile(SCIENCE, 0.75),\n            max = max(SCIENCE))\n\n# A tibble: 1 × 6\n    min    q1 median  mean    q3   max\n  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    15  49.2     65  61.2  74.8    96\n\n\n\n\n\n\nIn Hands-on Exercise 1, I used the summary() function of Basic R package to calculate the summary statistics for the continous attribute in the exam_data dataset. For this exercise, I used the summarize() function of dplyr package. While the dplyr package requires more codes to calculate the summary statistics as compared to the summary() function, it offers more flexibility and also returns a tibble dataframe. Hence, whether to use summary() or summarize() depends on user preference and what we want to do with the output.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-On Exercise 2: Beyond ggplot2 Fundamentals"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#using-ggrepel",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#using-ggrepel",
    "title": "Hands-On Exercise 2: Beyond ggplot2 Fundamentals",
    "section": "3.1 Using ggrepel",
    "text": "3.1 Using ggrepel\nggrepel is an extension of ggplot2 package. It provides geoms of ggplot2 to repel overlapping text by replacing geom_text() with geom_text_repel() and replacing geom_label() with geom_label_repel().\nLet us try it for the above chart!\n\n\nShow the code\nggplot(data = exam_data, \n       aes(x= MATHS, y= ENGLISH)) +\n  geom_point() + \n  geom_smooth(method = lm, \n              size=0.5)+\n  coord_cartesian(xlim = c(0,100), \n                  ylim = c(0,100))+\n  geom_label_repel(aes(label = ID), \n             fontface = \"bold\") +\n  ggtitle(\"English Scores versus Maths Scores for Primary 3 Students (with ggrepel)\") \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn the above output, we end up with only 5 labelled points, even though we did not limit or specify the number of labels. This is because ggrepel will discard some text labels if they overlap too many other things (default limit is 10). So if a text label overlaps 10 other text labels or data points, then it will be discarded.\nWe can expect to see a warning if some data points could not be labeled due to too many overlaps.\nSet max.overlaps = Inf to override this behavior and always show all labels, regardless of whether or not a text label overlaps too many other things.\nUse options(ggrepel.max.overlaps = Inf) to set this globally for your entire session. The global option can be overridden by providing the max.overlaps argument to geom_text_repel().",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-On Exercise 2: Beyond ggplot2 Fundamentals"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#other-examples-of-ggrepel",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#other-examples-of-ggrepel",
    "title": "Hands-On Exercise 2: Beyond ggplot2 Fundamentals",
    "section": "3.2 Other examples of ggrepel",
    "text": "3.2 Other examples of ggrepel\nThere are other exciting customisations that we can do for ggreply. Here are some examples that I think will be useful for my personal work.\n\n3.2.1 Hide Some of the labels (or show only certain labels)\nt For example, we only want to label and highlight those students who score less than 30 for both English and Maths amongst those students who score less than 50 for both English and Maths (assuming 50 is the passing score), we will: 1. Subset the tibble dataframe to contain only those who did not meet the passing score of English and Maths. 2. Create a new column to set the IDs of those students who score more than 30 for English and Maths to an empty string “” to hide them, and those who score less than 30 for both English and Maths would show their ID. 3. Then we plot the scatterplot using geom_point and also indicate to color those points with students who score less than 30 for both English and Maths in red.\n\n\nShow the code\nexam_data2 &lt;- subset(exam_data, MATHS &lt; 50 & ENGLISH &lt; 50)\n\nexam_data2$ID_select &lt;- ifelse(exam_data2$MATHS &lt;30 & exam_data2$ENGLISH &lt;30, exam_data2$ID, \"\")\n\nggplot(exam_data2, \n       aes(MATHS, ENGLISH, label = ID_select)) +\n  geom_text_repel() + \n  geom_point(color = ifelse(exam_data2$MATHS &lt; 30 & exam_data2$ENGLISH &lt; 30, \"red\", \"grey50\"))+\n  coord_cartesian(xlim = c(0,50), \n                  ylim = c(0,50))\n\n\n\n\n\n\n\n\n\n\n\n3.2.2 Using ggrepel with stat_summary()\nWe can use stat_summary() with geom = “text_repel”. The position_nudge_repel() function nudges the text label’s position, but it also remembers the original position of the data point.\n\n\nShow the code\nggplot(exam_data, aes(factor(CLASS), MATHS)) + \n  stat_summary(\n    fill = \"darkseagreen\",\n    color = \"black\", \n    fun = \"mean\", \n    geom = \"col\") + stat_summary(\n      aes(label = round(stat(y))),\n      fun = \"mean\",\n      geom = \"text_repel\",\n      min.segment.length = 0, \n      position = position_nudge_repel(y = -2)) +\n  labs(title = \"Mean Maths Scores Across Classes\")\n\n\n\n\n\n\n\n\n\nYou can learn more about ggrepel here!",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-On Exercise 2: Beyond ggplot2 Fundamentals"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#ggthemes",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#ggthemes",
    "title": "Hands-On Exercise 2: Beyond ggplot2 Fundamentals",
    "section": "4.1 ggthemes",
    "text": "4.1 ggthemes\nggthemes provides ggplots2 with themes that replicate the look of plots by Edward Tufte, Stephen Few, Fivethirtyeight, The Economist, ‘Stata’, ‘Excel’, and The Wall Street Journal, among others.\n\ntheme_economist()theme_solarized()\n\n\n\n\nShow the code\nggplot(exam_data, \n       aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color = \"black\",\n                 fill = \"darkslategrey\") + \n  geom_vline(aes(xintercept=mean(MATHS)),\n            color=\"deepskyblue\", linetype=\"dashed\", size=0.7)+\n  annotate(\"text\",\n           x = mean(exam_data$MATHS), \n           y = 40, \n           label = paste(\"Mean=\", round(mean(exam_data$MATHS))),\n           col = \"black\") + \n  theme_economist() + \n  ggtitle(\"Distribution of Maths Scores\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nggplot(exam_data, \n       aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color = \"black\",\n                 fill = \"darkslategrey\") + \n  geom_vline(aes(xintercept=mean(MATHS)),\n            color=\"deepskyblue\", linetype=\"dashed\", size=0.7)+\n  annotate(\"text\",\n           x = mean(exam_data$MATHS), \n           y = 40, \n           label = paste(\"Mean=\", round(mean(exam_data$MATHS))),\n           col = \"black\") + \n  theme_solarized() + \n  ggtitle(\"Distribution of Maths Scores\")",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-On Exercise 2: Beyond ggplot2 Fundamentals"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#hrbrthemes",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#hrbrthemes",
    "title": "Hands-On Exercise 2: Beyond ggplot2 Fundamentals",
    "section": "4.2 hrbrthemes",
    "text": "4.2 hrbrthemes\nhrbrthemes package provides a base theme that focuses on typographic elements, including where various labels are placed as well as the fonts that are used.\n\ntheme_ipsum_rc()theme_ft_rc()\n\n\n\n\nShow the code\nggplot(exam_data, \n       aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color = \"black\",\n                 fill = \"darkslategrey\") + \n  geom_vline(aes(xintercept=mean(MATHS)),\n            color=\"deepskyblue\", linetype=\"dashed\", linewidth=0.7)+\n  annotate(\"text\",\n           x = mean(exam_data$MATHS), \n           y = 40, \n           label = paste(\"Mean=\", round(mean(exam_data$MATHS))),\n           col = \"black\") + \n  ggtitle(\"Distribution of Maths Scores\") + \n  theme_ipsum_rc()\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nggplot(exam_data, aes(MATHS, ENGLISH)) +\n  geom_point(color = ft_cols$yellow, show.legend = FALSE) + \n  labs(title=\"Maths Scores Versus English Scores for Primary 3 Students\",\n       subtitle = \"Passing Score for Maths and English is 50 marks\",\n       caption = \"VAA Hands-on Exercise 2\")+ \n  geom_vline(aes(xintercept=50, color=ft_cols$peach), show.legend = FALSE) + \n  geom_hline(aes(yintercept=50, color=ft_cols$peach), show.legend = FALSE)+\n  coord_cartesian(xlim = c(0,100),\n                  ylim = c(0,100)) + \n  theme_ft_rc(font_rc)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHit a Speed Bump Here! \n\n\n\nWhen trying out this section I encountered error messages stating that I do not have the font family in Windows font database.\nSo here’s what I have tried:\n\nInstall the fonts from hrbrthemes folder (found in “R” folder’s “library” folder) but that did not work.\nSo I found some websites stating that I should import_roboto_condensed() first and also install the fonts on my system before trying to use this theme. So I imported all the fonts needed for hrbrtheme and this method works for me!\n\nReferences used are here, here and here.\n\n\nThe second goal centers around productivity for a production workflow. In fact, this “production workflow” is the context for where the elements of hrbrthemes should be used.\nA “production workflow” is when you intend for the output of your work to be put into a publication of some kind, whether it be a blog post, academic paper, presentation, internal report or industry publication. When you’re cranking through an analysis, the visual elements don’t need to be perfect. They are there to validate/support your work and are more of a starting point for the finished product than anything else. The level of attention to detail on the final graphical products can be a great motivator for your audience to either dive deep into your analysis text or relegate it to the TLDR pile.\nSounds like hrbrtheme package has its own ideas and views on what makes an effective and visually appealing chart! Read more about why they chose certain fonts here!\nLet us try to explore more about hrbrthemes using the following code chunk.\n\n\nShow the code\nggplot(exam_data, aes(MATHS, ENGLISH)) +\n  geom_point(color = ft_cols$yellow, show.legend = FALSE) + \n  labs(title=\"Maths Scores Versus English Scores for Primary 3 Students\",\n       subtitle = \"Passing Score for Maths and English is 50 marks\",\n       caption = \"VAA Hands-on Exercise 2\")+ \n  geom_vline(aes(xintercept=50, color=ft_cols$peach), show.legend = FALSE) + \n  geom_hline(aes(yintercept=50, color=ft_cols$peach), show.legend = FALSE)+\n  coord_cartesian(xlim = c(0,100),\n                  ylim = c(0,100)) + \n  theme_ft_rc(plot_title_size = 15, \n              subtitle_size = 10, \n              axis_title_size = 10, \n              caption_size = 10,\n              base_size = 10,\n              plot_margin = margin(10,10,10,10),\n              grid = \"Y\") \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat the arguments in the above code chunk’s theme_ft_rc means\n\n\n\n\nplot_title_size argument decreased the font size of the chart title from 18 (default) to 15\nsubtitle_size argument decreased the font size of the subtitle from 13 (default) to 10\naxis_title_size argument increased the font size of the axis title from 9 (default) to 10\ncaption_size argument increased the font size from 9 (default) to 10\nbase_size argument decreased the default axis label from 11.5 (default) to 10\nplot_margin argument narrowed the margins of all 4 sides from 30 (default) to 10.\ngrid argument removed the x-axis grid lines\n\nFor more details on the arguments of the theme, take a look here!",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-On Exercise 2: Beyond ggplot2 Fundamentals"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#combining-two-ggplot2-graphs",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#combining-two-ggplot2-graphs",
    "title": "Hands-On Exercise 2: Beyond ggplot2 Fundamentals",
    "section": "5.1 Combining two ggplot2 graphs",
    "text": "5.1 Combining two ggplot2 graphs\nThe figure in the tabset below shows a composite of two histograms created using patchwork. Note how simple the syntax used to create the plot!\n\n\nShow the code\np1 + p2",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-On Exercise 2: Beyond ggplot2 Fundamentals"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#combining-three-ggplot2-graphs",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#combining-three-ggplot2-graphs",
    "title": "Hands-On Exercise 2: Beyond ggplot2 Fundamentals",
    "section": "5.2 Combining three ggplot2 graphs",
    "text": "5.2 Combining three ggplot2 graphs\nIn the code chunk we below, we use tThe pipe sign | will place subplots group (i.e. p1 and p2 next to another plot (i.e. p3) while the division sign / will place p1 and p2 on top of each other.\n\n\nShow the code\n(p1 / p2) | p3",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-On Exercise 2: Beyond ggplot2 Fundamentals"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#creating-a-composite-figure-with-a-tag",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#creating-a-composite-figure-with-a-tag",
    "title": "Hands-On Exercise 2: Beyond ggplot2 Fundamentals",
    "section": "5.3 Creating a composite figure with a tag",
    "text": "5.3 Creating a composite figure with a tag\nIn order to identify subplots in text, patchwork also provides auto-tagging capabilities as shown below.\n\n\nShow the code\n((p1 / p2) | p3) + plot_annotation(tag_levels = \"A\")",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-On Exercise 2: Beyond ggplot2 Fundamentals"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#creating-figure-with-inset_element",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#creating-figure-with-inset_element",
    "title": "Hands-On Exercise 2: Beyond ggplot2 Fundamentals",
    "section": "5.4 Creating Figure with inset_element()",
    "text": "5.4 Creating Figure with inset_element()\nBesides providing functions to place plots next to each other based on the provided layout, with inset_element() of patchwork, we can place one or several plots or graphic elements freely on top or below another plot.\n\n\nShow the code\np3 + inset_element(p2, \n                   left = 0,\n                   bottom = 0.6,\n                   right = 0.4,\n                   top =1, align_to = 'panel')",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-On Exercise 2: Beyond ggplot2 Fundamentals"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#creating-a-composite-figure-using-patchwork-and-ggtheme",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#creating-a-composite-figure-using-patchwork-and-ggtheme",
    "title": "Hands-On Exercise 2: Beyond ggplot2 Fundamentals",
    "section": "5.5 Creating a composite figure using patchwork and ggtheme",
    "text": "5.5 Creating a composite figure using patchwork and ggtheme\nWe can also add a theme from ggtheme to the output created from patchwork, as shown in the code chunk below.\n\npatchwork &lt;- (p1 / p2) | p3\npatchwork & theme_calc()",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-On Exercise 2: Beyond ggplot2 Fundamentals"
    ]
  },
  {
    "objectID": "Extra/Post-Lesson_02/Post-Lesson_02.html",
    "href": "Extra/Post-Lesson_02/Post-Lesson_02.html",
    "title": "Post-Lesson Thoughts 2: More uses of Plotly!",
    "section": "",
    "text": "1 About Plotly R\nIn Hands-on Exercises and Take Home Exercise 3, we explored some of the Plotly Functions.\nToday, I will be sharing more about some of the interesting plotly functions that I have explored when working on Take Home Exercise 3.\n\n\n2 Creating Dropdown list\n\n\n3 Creating Heatmaps\n\n\n4 Creating Mixed Subplots\n\n\n5"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Extra/Post-Lesson_01/Post-Lesson_01.html",
    "href": "Extra/Post-Lesson_01/Post-Lesson_01.html",
    "title": "Post-Lesson Thoughts 1: Annotations",
    "section": "",
    "text": "[Updated with references on 16 Jan 2024]",
    "crumbs": [
      "Extra Stuff",
      "Post-Lesson Thoughts 1: Annotations"
    ]
  },
  {
    "objectID": "Extra/Post-Lesson_01/Post-Lesson_01.html#import-packages",
    "href": "Extra/Post-Lesson_01/Post-Lesson_01.html#import-packages",
    "title": "Post-Lesson Thoughts 1: Annotations",
    "section": "2.1 Import Packages",
    "text": "2.1 Import Packages\n\npacman::p_load(tidyverse)",
    "crumbs": [
      "Extra Stuff",
      "Post-Lesson Thoughts 1: Annotations"
    ]
  },
  {
    "objectID": "Extra/Post-Lesson_01/Post-Lesson_01.html#import-data",
    "href": "Extra/Post-Lesson_01/Post-Lesson_01.html#import-data",
    "title": "Post-Lesson Thoughts 1: Annotations",
    "section": "2.2 Import Data",
    "text": "2.2 Import Data\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")",
    "crumbs": [
      "Extra Stuff",
      "Post-Lesson Thoughts 1: Annotations"
    ]
  },
  {
    "objectID": "Extra/Post-Lesson_01/Post-Lesson_01.html#using-geom_text",
    "href": "Extra/Post-Lesson_01/Post-Lesson_01.html#using-geom_text",
    "title": "Post-Lesson Thoughts 1: Annotations",
    "section": "3.1 Using geom_text()",
    "text": "3.1 Using geom_text()\nFor example, we want to find out how the students did overall for math as compared to the mean Math score. So we will first calculate the mean Math score.\n\nmean_MATHS &lt;- round(mean(exam_data$MATHS),1)\n\n\nggplot(exam_data, aes(x = MATHS)) +\n  geom_histogram() + \n  geom_text(x = mean_MATHS, y = 20, \n            label = paste(\"mean\\n\", mean_MATHS),\n            color = \"blue\")\n\n\n\n\n\n\n\n\nNow we realised that the number is just floating around, so let us draw a line using geom_segment.\n\nggplot(exam_data, aes(x = MATHS)) +\n  geom_histogram() + \n  geom_text(x = mean_MATHS, y = 20, \n            label = paste(\"mean\\n\", mean_MATHS),\n            color = \"blue\") + \n  geom_segment(x = mean_MATHS, xend = mean_MATHS,\n               y = 0, yend = 18, color= \"blue\")",
    "crumbs": [
      "Extra Stuff",
      "Post-Lesson Thoughts 1: Annotations"
    ]
  },
  {
    "objectID": "Extra/Post-Lesson_01/Post-Lesson_01.html#using-geom_label",
    "href": "Extra/Post-Lesson_01/Post-Lesson_01.html#using-geom_label",
    "title": "Post-Lesson Thoughts 1: Annotations",
    "section": "3.2 Using geom_label()",
    "text": "3.2 Using geom_label()\n\nggplot(exam_data, aes(x = MATHS)) +\n  geom_histogram() + \n  geom_label(x = mean_MATHS, y = 20, \n            label = paste(\"mean=\", mean_MATHS),\n            color = \"blue\", fill = \"lightblue\") + \n  geom_segment(x = mean_MATHS, xend = mean_MATHS,\n               y = 0, yend = 18, color= \"blue\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nCurrently geom_label() does not support the check_overlap argument or the angle aesthetic. Also, it is considerably slower than geom_text(). The fill aesthetic controls the background colour of the label.",
    "crumbs": [
      "Extra Stuff",
      "Post-Lesson Thoughts 1: Annotations"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html",
    "title": "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods",
    "section": "",
    "text": "[Updated charts with annotations and references on 16 Jan 2024]",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#installing-and-launching-r-packages",
    "title": "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods",
    "section": "2.1 Installing and Launching R packages",
    "text": "2.1 Installing and Launching R packages\nThe code chunk below uses p_load() of pacman package to check if tidyverse packages are installed in the computer. If they are, they will be launched in R. Otherwise, pacman will install the relevant packages before launching them.\n\n\nShow the code\npacman::p_load(tidyverse)\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that pacman package has already been installed before using the above code chunk. If you have not yet installed pacman please install it via Rstudios’ “Tools” &gt; “Install Packages” before using the above code chunk.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#importing-the-data-into-r",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#importing-the-data-into-r",
    "title": "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods",
    "section": "2.2 Importing the data into R",
    "text": "2.2 Importing the data into R\nWe use read_csv() function of readr to import the data, glimpse() of dplyr to learn about the associated attribute information in the dataframe, and summary() of base R to get the summary statistics of the data.\n\nCodeDataSummary of Data\n\n\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")\n\n\n\n\nglimpse(exam_data)\n\nRows: 322\nColumns: 7\n$ ID      &lt;chr&gt; \"Student321\", \"Student305\", \"Student289\", \"Student227\", \"Stude…\n$ CLASS   &lt;chr&gt; \"3I\", \"3I\", \"3H\", \"3F\", \"3I\", \"3I\", \"3I\", \"3I\", \"3I\", \"3H\", \"3…\n$ GENDER  &lt;chr&gt; \"Male\", \"Female\", \"Male\", \"Male\", \"Male\", \"Female\", \"Male\", \"M…\n$ RACE    &lt;chr&gt; \"Malay\", \"Malay\", \"Chinese\", \"Chinese\", \"Malay\", \"Malay\", \"Chi…\n$ ENGLISH &lt;dbl&gt; 21, 24, 26, 27, 27, 31, 31, 31, 33, 34, 34, 36, 36, 36, 37, 38…\n$ MATHS   &lt;dbl&gt; 9, 22, 16, 77, 11, 16, 21, 18, 19, 49, 39, 35, 23, 36, 49, 30,…\n$ SCIENCE &lt;dbl&gt; 15, 16, 16, 31, 25, 16, 25, 27, 15, 37, 42, 22, 32, 36, 35, 45…\n\n\n\n\n\nsummary(exam_data)\n\n      ID               CLASS              GENDER              RACE          \n Length:322         Length:322         Length:322         Length:322        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n    ENGLISH          MATHS          SCIENCE     \n Min.   :21.00   Min.   : 9.00   Min.   :15.00  \n 1st Qu.:59.00   1st Qu.:58.00   1st Qu.:49.25  \n Median :70.00   Median :74.00   Median :65.00  \n Mean   :67.18   Mean   :69.33   Mean   :61.16  \n 3rd Qu.:78.00   3rd Qu.:85.00   3rd Qu.:74.75  \n Max.   :96.00   Max.   :99.00   Max.   :96.00  \n\n\n\n\n\nFrom the above output, we note that exam_data has 7 columns:\n\nID is a unique identifier for students.\nCLASS represents the class that the student is in.\nGENDER tells us the student’s gender.\nRACE tells us the student’s race.\nENGLISH is the score that the student got for English subject.\nMATHS is the score that the student got for Mathematics subject.\nSCIENCE is the score that the student got for Science subject.\n\nThere are 322 rows in total. For English, the minimum score (i.e., the lowest score gotten by a student) is 21, median is 70, and the maximum (or rather the highest score gotten by a student) is 96. For Mathematics, the lowest score gotten by a student is 9, median is 74 and the highest score gotten by a student is 99. For Science, the lowest score gotten by a student is 15, median is 65 and the highest score gotten by a student is 96.\n\n2.2.1 Check for Missing Data\nIt is good to check if there are any missing data in our imported dataset so that we are aware if there are any missing data at the onset and also decide how to manage the missing data subsequently.\n\n\nShow the code\nexam_data %&gt;% \n  map(is.na) %&gt;%\n  map(sum)\n\n\n$ID\n[1] 0\n\n$CLASS\n[1] 0\n\n$GENDER\n[1] 0\n\n$RACE\n[1] 0\n\n$ENGLISH\n[1] 0\n\n$MATHS\n[1] 0\n\n$SCIENCE\n[1] 0\n\n\nFrom the above result, we see that there are no missing data in exam_data.\nNow that we have some idea of the data imported in, let us move to the next section!",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#principles-of-grammar-of-graphics",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#principles-of-grammar-of-graphics",
    "title": "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods",
    "section": "3.1 Principles of Grammar of Graphics",
    "text": "3.1 Principles of Grammar of Graphics\nThere are two principles in Grammar of Graphics, they are:\n\nGraphics = distinct layers of grammatical elements; and\nMeaningful plots through aesthetic mapping\n\nA good grammar of graphics will allow us to gain insight into the composition of complicated graphics, and reveal unexpected connections between seemingly different graphics (Cox, 1978). It also provides a strong foundation for understanding a diverse range of graphics. Furthermore, it may also help guide us on what a well-formed or correct graphic looks like, but there will still be many grammatically correct but nonsensical graphics.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#a-layered-grammar-of-graphics",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#a-layered-grammar-of-graphics",
    "title": "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods",
    "section": "4.1 A Layered Grammar of Graphics",
    "text": "4.1 A Layered Grammar of Graphics\nThe R package, ggplot2 is an implementation of Leland Wilkinson’s Grammar of Graphics, and is developed by Hadley Wickham. Hadley’s layered grammar of graphics uses several layered components to describe any graphic or visualization. The figure below shows the seven grammars of ggplot2.\n\nHere is what each layer means:\n\nData refers to the dataset being plotted.\nAesthetics take the attributes of the data and use them to influence visual characteristics, such as position, colours, size, shape, or transparency.\nGeometrics refers to the visual elements used for our data, such as point, bar or line.\nFacets split the data into subsets to create multiple variations of the same graph (paneling, multiple plots).\nStatistics refers to statistical transformations that summarise data (e.g. mean, confidence intervals).\nCoordinate systems define the plane on which data are mapped on the graphic.\nThemes modify all non-data components of a plot, such as main title, sub-title, y-aixs title, or legend background.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#comparison-between-r-graphics-and-ggplot2",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#comparison-between-r-graphics-and-ggplot2",
    "title": "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods",
    "section": "4.2 Comparison between R Graphics and ggplot2",
    "text": "4.2 Comparison between R Graphics and ggplot2\n\nR Graphicsggplot2\n\n\n\nhist(exam_data$MATHS)\n\n\n\n\n\n\n\n\n\n\n\nggplot(data= exam_data, aes(x = MATHS)) + \n  geom_histogram(bins = 10, \n                 boundary = 100, \n                 color = \"black\", \n                 fill = \"grey\") + \n  ggtitle(\"Distribution of Maths Scores\")\n\n\n\n\n\n\n\n\n\n\n\nYou may ask, why should we use ggplot2 rather than R Graphics, especially when the code chunk for R Graphics is relatively simple. As pointed out by Hadley Wickham, the creator of ggplot2:\n\n\n\n\n\n\nImportant\n\n\n\nThe transferable skills from ggplot2 are not the idiosyncrasies of plotting syntax, but a powerful way of thinking about visualisation, as a way of mapping between variables and the visual properties of geometric objects that you can perceive.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#data-1",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#data-1",
    "title": "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods",
    "section": "5.1 Data",
    "text": "5.1 Data\nAs seen from the figure in Section 4.1, the first layer or element of a plot begins with data. Let us call the ggplot() function using the following code chunk.\n\nggplot(data = exam_data)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nNote that a blank canvas appears.\nggplot() initialises a ggplot object.\nThe data argument defines the dataset to be used for plotting. In our case, it is exam_data.\nIf the dataset is not already a dataframe, it will be converted to one by fortify().",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#aesthetic-mappings",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#aesthetic-mappings",
    "title": "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods",
    "section": "5.2 Aesthetic Mappings",
    "text": "5.2 Aesthetic Mappings\nThe aesthetic mappings take attributes of the data and and use them to influence visual characteristics, such as position, colour, size, shape, or transparency. Each visual characteristic can thus encode an aspect of the data and be used to convey information.\nAll aesthetics of a plot are specified in the aes() function call.\n\n\n\n\n\n\nNote\n\n\n\nIn the later part of this lesson, you will see that each geom layer can have its own aes specification.\n\n\nThe code chunk below adds the aesthetics element into the plot.\n\nggplot(data = exam_data, \n       aes(x = MATHS))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nggplot includes the x-axis and the axis’ label.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geoms",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geoms",
    "title": "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods",
    "section": "5.3 Geoms",
    "text": "5.3 Geoms\nGeoms (Geometric objects) are the actual marks we put on a plot. Examples include:\n\ngeom_point for drawing individual points (e.g., scatter plots)\ngeom_line for drawing lines (e.g., for line charts)\ngeom_smooth for drawing smoothed lines (e.g., for simple trends or approximations)\ngeom_bar for drawing bars (e.g., for bar charts)\ngeom_histogram for drawing binned values (e.g., for histograms)\ngeom_polygon for drawing arbitrary shapes\ngeom_map for drawing polygons in the shape of a map! (You can access the data to use for these maps by using the map_data() function).\n\n\n\n\n\n\nA plot must have at least one geom; there is no upper limit. You can add a geom to a plot using the + operator.\n\n5.3.1 Geometric Objects: geom_bar()\nThe code chunk below plots a bar chart using geom_bar().\n\n\nShow the code\nexam_data$RACE &lt;- as.factor(exam_data$RACE)\n\nRACE_count &lt;- exam_data %&gt;% \n  count(exam_data$RACE, sort=TRUE)\n\nggplot(data = exam_data, \n       aes(x = RACE)) + \n  geom_bar() +\n  labs(title=\"Race Distribution of Primary 3 Students\", caption = \"Hands-on Exercise 1\") + \n  xlab(\"Race\") +\n  ylab(\"Number of Students\")+\n  geom_text(stat = \"count\", aes(label = after_stat(count)), vjust= -0.5)\n\n\n\n\n\n\n\n\n\n\n\n5.3.2 Geometric Objects: geom_dotplot()\nIn a dot plot, the width of a dot corresponds to the bin width (or maximum width, depending on the binning algorithm). The dots are stacked, with each dot representing one observation. Note that there are two basic methods for dotplot: dot-density and histodot. The default method is “dotdensity”. When the method is “dotdensity”, the bin width argument specfies maximum bin width. When the method is “histodot”, the binwidth argument specifics bin width.\nIn the code chunk below, geom_dotplot() is used to plot a dot plot. We also adjusted the size of the dots using the dotsize= argument.\n\nmethod=“dotdensity” (default)method=“histodot”\n\n\n\nggplot(data = exam_data, \n       aes(x = MATHS)) + \n  geom_dotplot(method = \"dotdensity\", dotsize = 0.5)+ \n  labs(title=\"Distribution of Maths Scores for Primary 3 Students\", caption = \"Hands-on Exercise 1\") + \n  xlab(\"Maths Score\") \n\n\n\n\n\n\n\n\n\n\n\nggplot(data = exam_data, \n       aes(x = MATHS)) + \n  geom_dotplot(method = \"histodot\", dotsize = 0.5)+ \n  labs(title=\"Distribution of Maths Scores for Primary 3 Students\", caption = \"Hands-on Exercise 1\") + \n  xlab(\"Maths Score\") \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe y axis scale is not very useful because it is in numbers less than 1. It can be very misleading!\n\n\nAs such, we will use: 1. scale_y_continuous() to turn off the misleading y-axis, 2. binwidth= argument to change the binwidth to 2.5. 3. fill= argument to color the dots based on the student’s gender.\n\nggplot(data = exam_data, \n       aes(x = MATHS, fill=GENDER)) + \n  geom_dotplot(binwidth = 2.5,\n               dotsize = 0.5) + \n  scale_y_continuous(NULL, breaks = NULL)+\n  labs(title=\"Distribution of Maths Scores for Primary 3 Students\", caption = \"Hands-on Exercise 1\") + \n  xlab(\"Maths Score\") \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: Why do we use dotplot over bar charts?\n\n\n\nThere is a big difference between dot plot and bar chart. A value in a bar chart is visualised by the length of the bar but in dot plot, the value is visualised by its poistion on an axis.\nAnother difference between a bar chart and a dot plot is that, since a dot plot uses a simple dot on a numerical axis, it is far easier to add more series (more values per category) without needing to stack these series on top of each other and make them rather unreadable, like in a stacked bar chart. This results in a chart that packs a lot of information in a small space. A multi-series dot plot lets you compare values within a category as easily as between categories.\nSource\n\n\n\n\n5.3.3 Geometric Objects: geom_histogram()\nThe following code chunk uses geom_histogram() to create a simple histogram using values in MATHS fields of exam_data.\n\nggplot(data = exam_data, \n       aes(x = MATHS)) +\n  geom_histogram() \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe default bin is 30\n\n\n\n\n5.3.4 Modifying a geometric object by changing geom()\nIn the code chunk below,\n\nbins arugment is used to change the number of bins to 20.\nfill argument is used to shade the histogram with pink color, and\ncolor argument is used to change the outline of the bars to black.\n\n\nmean_MATHS &lt;- round(mean(exam_data$MATHS),1)\n\n\nggplot(data = exam_data,\n       aes(x = MATHS)) + \n  geom_histogram(bins = 25, \n                 color = \"black\",\n                 fill = \"pink\")+ \n  geom_text(x = mean_MATHS, y = 28, \n            label = paste(\"mean Maths\\n Score =\", mean_MATHS),\n            color = \"blue\") + \n  geom_segment(x = mean_MATHS, xend = mean_MATHS,\n               y = 0, yend = 25, color= \"blue\")\n\n\n\n\n\n\n\n\n\n\n5.3.5 Modifying a geometric object by changing aes()\nThe code chunk below changes the fill colour of the histogram based on the subgroup of aes().\n\nggplot(data = exam_data,\n       aes(x = MATHS,\n           fill = GENDER)) + \n  geom_histogram(bins = 20, \n                 color = \"grey30\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis approach can be used to colour, fill and alpha of the geometric object.\n\n\n\n\n5.3.6 Geometric Objects: geom_density()\ngeom_density computes and plots kernel density estimate, which is a smoothed version of the histogram. It is an useful alternative to histrogram for continuous data that comes from an underlying smooth distribution.\nThe code below plots the distribution of Maths scores in a kernel density estimate plot.\n\nggplot(data = exam_data, \n       aes(x = MATHS)) + \n  geom_density()\n\n\n\n\n\n\n\n\nThe code chunks below plot two kernel density lines by using color and fill arguments of aes().\n\nUsing color argument of aes()Using fill argument of aes()\n\n\n\nggplot(data = exam_data, \n       aes(x = MATHS, \n           color = GENDER)) + \n  geom_density()\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = exam_data, \n       aes(x = MATHS, \n           fill = GENDER)) + \n  geom_density()\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.3.7 Geometric Objects: geom_boxplot()\ngeom_boxplot displays continuous value list. It visualises 5 summary statistics: median, two hinges (first quartile and third quartile), and two whiskers (minimum and maximum)), and all “outlying” points individually.\nThe following code chunk plots boxplots using geom_boxplot().\n\nggplot(data = exam_data,\n       aes(y = MATHS,\n           x = GENDER)) + \n  geom_boxplot()\n\n\n\n\n\n\n\n\nNotches are used in box plots to help visually assess whether the medians of distributions differ. If the notches do not overlap, this is evidence that the medians are different.\nThe code chunk below plots the distribution of Maths scores by gender in notched plot instead of boxplot.\n\nggplot(data = exam_data,\n       aes(y = MATHS,\n           x = GENDER)) + \n  geom_boxplot(notch=TRUE)\n\n\n\n\n\n\n\n\n\n\n5.3.8 Geometric Objects: geom_violin()\ngeom_violin is designed for creating violin plots. Violin plots are a way of comparing multiple data distributions. It is difficult to compare more than few distributions with ordingary density curves because the lines visually interfere with each other. With a violin plot, it’s easier to compare several distributions since they’re placed side by side.\nThe code below plots the distribution of Maths score by gender using violin plot.\n\nggplot(data = exam_data, \n       aes(y = MATHS, \n           x = GENDER)) + \n  geom_violin()\n\n\n\n\n\n\n\n\n\n\n5.3.9 Geometric Objects: geom_point()\ngeom_point() is useful for creating scatterplots.\nThe code chunk below plots a scatterplot showing the Maths and English Scores of students using geom_point().\n\nggplot(data = exam_data,\n       aes(x = MATHS,\n           y = ENGLISH)) +\n  \n  geom_point()\n\n\n\n\n\n\n\n\n\n\n5.3.10 Geometric objects can be combined\nAs mentioned earlier, we need to specify at least 1 geom object and can have more than 1 geom object by combining other geom objects to create a plot. The code chunk below plots the data points on the boxplots using both geom_boxplot() and geom_point().\n\nggplot(data = exam_data, \n       aes(y = MATHS, \n           x = GENDER)) + \n  geom_boxplot() + \n  geom_point(position = \"jitter\", \n             size = 0.5)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#stat",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#stat",
    "title": "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods",
    "section": "5.4 Stat",
    "text": "5.4 Stat\nThe Statistics functions statistically transform data, usually as some form of summary. For example:\n\nfrequency of values of a variable (e.g. for a bar graph)\n\nmean\nconfidence limit\n\nThere are two ways to use these functions:\n\nadd a stat_() function and override the default geom, or\nadd a geom_() function and override the default stat.\n\n\n\n5.4.1 Working with stat()\nThe boxplots below are incomplete because the positions of the means were not shown.\n\nggplot(data = exam_data, \n       aes(y = MATHS, x = GENDER)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n5.4.2 Working with stat() - the stat_summary() method\nTo add the positions of the means, we use stat_summary() function and override the default geom.\n\nggplot(data = exam_data, \n       aes(y = MATHS, x = GENDER)) + \n  geom_boxplot() + \n  stat_summary(geom = \"point\", \n                fun.y = \"mean\",\n                colour = \"red\", \n                size = 4)\n\n\n\n\n\n\n\n\n\n\n5.4.3 Working with stat() - the geom() method\nWe now try adding the mean values and position using geom_() function and override the default stat.\n\nggplot(data = exam_data, \n       aes(y = MATHS, x = GENDER)) + \n  geom_boxplot() + \n  geom_point(stat = \"summary\", \n             fun.y = \"mean\",\n             colour = \"red\",\n             size = 4)\n\n\n\n\n\n\n\n\n\n\n5.4.4 Adding a best fit cure on a scatterplot\nThe scatterplot below shows the relationship of students’ Maths and English scores. We can imporve the interpretability of the graph by adding a best fit curve.\n\nWithout best fit curveWith best fit curve\n\n\n\nggplot(data = exam_data,\n       aes(x = MATHS, y = ENGLISH)) + \n  geom_point() + \n  geom_hline(yintercept = 50, color=\"orange\", size = 1) +\n  geom_vline(xintercept = 50, color=\"orange\", size = 1)\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = exam_data,\n       aes(x = MATHS, y = ENGLISH)) + \n  geom_point() + \n  geom_smooth(size=0.5) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe default method used by geom_smooth is loess.\nFor info here.\n\n\nThe default smoothing method can be overridden by specifying the method parameter of geom_smooth().\n\nggplot(data = exam_data,\n       aes(x = MATHS, y = ENGLISH)) + \n  geom_point() +\n  geom_smooth(method = lm, size = 0.5)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#facets",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#facets",
    "title": "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods",
    "section": "5.5 Facets",
    "text": "5.5 Facets\nFacetting generates small multiples of plots (sometimes also called trellis plot), each displaying a different subset of the data. They are an alternative to aesthetics for displaying additional discrete variables. ggplot2 supports two types of facets, namely: facet_grid() and facet_wrap().\n\n5.5.1 Working with facet_wrap()\nfacet_wrap wraps a 1d sequence of panels into 2d. This is generally a bette use of screen space than facet_grid because most displays are roughly rectangular.\nThe following code chunk plots a trellis plot using facet_wrap().\n\nggplot(data = exam_data,\n       aes(x = MATHS)) + \n  geom_histogram(bins = 20) + \n  facet_wrap(~ CLASS)\n\n\n\n\n\n\n\n\n\n\n5.5.2 Working with facet_grid()\nfacet_grid forms a matrix of panels defined by row and coloumn facetting variables. It is most useful when we have two discrete variables, and all combinations of varialbes exist in the data.\nThe following code chunk plots a trellis plot using facet_grid().\n\nggplot(data = exam_data, \n       aes(x = MATHS)) + \n  geom_histogram(bins = 20) + \n  facet_grid(~ CLASS)\n\n\n\n\n\n\n\n\nWhile it is not better use of screen space, using facet_grid() allows us to compare all the charts side by size with the same y-axis.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#coordinates",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#coordinates",
    "title": "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods",
    "section": "5.6 Coordinates",
    "text": "5.6 Coordinates\nThe Coordinates function map the position of objects onto the plane of the plot. There are several different possible coordinate systems to use:\n\ncoord_cartesian(): the default cartesian coordinate systems, where you specify x and y values (e.g. allows you to zoom in or out).\ncoord_flip(): a cartesian system with the x and y flipped.\ncoord_fixed(): a cartesian system with a “fixed” aspect ratio (e.g. 1.78 for a “widescreen” plot).\ncoord_quickmap(): a coordinate system that approximates a good aspect ratio for maps.\n\n\n5.6.1 Working with Coordinates\nBy default, the bar charts of ggplot2 is in vertical form. We can flip it into a horizontal bar chart using coord_flip().\n\nVertical Bar ChartHorizontal Bar Chart\n\n\n\nggplot(data = exam_data, \n       aes(x = RACE)) +\n  geom_bar() \n\n\n\n\n\n\n\n\n\n\n\nggplot(data = exam_data, \n       aes(x = RACE)) +\n  geom_bar() + \n  coord_flip()\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.6.2 Changing the ranges for x- and y- axis\nWe can fix the x- and y-axis ranges for the following scatterplot so that both axes have the same range (0,100) for better interpretability.\n\nBefore Changing the RangesAfter Changing the Ranges\n\n\n\nggplot(data = exam_data, \n       aes(x = MATHS, y = ENGLISH)) +\n  geom_point() + \n  geom_smooth(method = lm, size = 0.5)\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = exam_data, \n       aes(x = MATHS, y = ENGLISH)) +\n  geom_point() + \n  geom_smooth(method = lm, size = 0.5) + \n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#themes",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#themes",
    "title": "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods",
    "section": "5.7 Themes",
    "text": "5.7 Themes\nThemes control elements of the graph not related to the data. For example:\n\nbackground colour\nfont size\ngridlines\ncolour of labels\n\nBuilt-in themes include: theme_gray() (default theme), theme_bw(), theme_classic().\nA list of theme can be found here. Each theme element can be conceived of as either a line (e.g. x-axis), a rectangle (e.g. graph background), or text (e.g. axis title). You can also edit the themes’ indivudal settings using theme().\nThe following code chunks illustrates the different themes that we can use.\n\ntheme_gray (default theme)theme_classictheme_minimal\n\n\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_gray()\n\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_minimal()",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 1: A Layered Grammar of Graphics: ggplot2 methods"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html",
    "title": "Hands-On Exercise 3a: Programming Interactive Data Visualisation with R",
    "section": "",
    "text": "[Updated with in-class notes on 3 Feb 2024]",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-On Exercise 3a: Programming Interactive Data Visualisation with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#installing-and-launching-r-packages",
    "title": "Hands-On Exercise 3a: Programming Interactive Data Visualisation with R",
    "section": "2.1 Installing and Launching R packages",
    "text": "2.1 Installing and Launching R packages\nFor this exercise, other than tidyverse, we will use the following packages:\n\nggiraph: to make interactive ggplot2 plots\nplotly: to plot interactive statistical graphs\nDT: to create interactive tables using the JavaScript library DataTables\npatchwork: to combine multiple ggplot2 graphs into one figure [Note: We have introduced patchwork in Hands-on Exercise 2!]\ncrosstalk: to implement cross-widget interactions.\n\nThe code chunk below uses p_load() of pacman package to check if the abovementioned packages are installed in the computer. If they are, they will be launched in R. Otherwise, pacman will install the relevant packages before launching them.\n\n\nShow the code\npacman::p_load(tidyverse, ggiraph, plotly, DT, patchwork, crosstalk)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-On Exercise 3a: Programming Interactive Data Visualisation with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#importing-the-data",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#importing-the-data",
    "title": "Hands-On Exercise 3a: Programming Interactive Data Visualisation with R",
    "section": "2.2 Importing the Data",
    "text": "2.2 Importing the Data\nFor this exercise, we will be using the Exam_data.csv provided by the course instructor and we have used it in Hands-on Exercises 1 and 2. It consists of year end examination grades of a cohort of primary 3 students from a local school. It is in csv file format.\nWe use read_csv() function of readr to import the Exam_data csv file into R and save it as a tibble data frame called exam_data. Then we will use datatable() of DT to have an overview of the imported data.\n\n\nShow the code\n#import the data into R\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")\n\n#to display the imported data\ndatatable(exam_data, caption = \"Table 1: Exam Data of Primary 3 Students\")\n\n\n\n\n\n\n:::\nFrom the above output, we note that:\n\nThere are a total of seven attributes in the exam_data tibble data frame.\nFour of these attributes are categorical data: ID, CLASS, GENDER and RACE.\nThree of these attributes are continuous data: MATHS, ENGLISH and SCIENCE.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-On Exercise 3a: Programming Interactive Data Visualisation with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#making-use-of-tooltips",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#making-use-of-tooltips",
    "title": "Hands-On Exercise 3a: Programming Interactive Data Visualisation with R",
    "section": "3.1 Making use of Tooltips",
    "text": "3.1 Making use of Tooltips\nLet us introduce ggiraph by creating a graph with tooltip effect. The following code chunk consists two parts: (i) a ggplot object will be created using geom_dotplot_interactive(), (ii) girafe() of ggiraph will be used to create an interactive svg object.\n\n\nShow the code\np1 &lt;- ggplot(data = exam_data, aes(x = MATHS)) + \n  geom_dotplot_interactive(aes(tooltip = ID),\n                           stackgroups = TRUE, \n                           binwidth = 1,\n                           method = \"histodot\") +\n  scale_y_continuous(NULL, breaks = NULL)\n\ngirafe(ggobj = p1, \n       width_svg = 6,\n       height_svg = 6 * 0.618)\n\n\n\n\n\n\n\n\n\n\n\n\nAbout the arguments used\n\n\n\n\ntooltip: this is the interactive parameter provided by ggiraph. if this argument is used, a tooltip is shown when the element is hovered. There are other interactive parameters such as onclick, hover_css and selected_css that we will explore later on! In the above code chunk, we want the tooltip to show the student ID, so tooltip = ID is used.\nstackgroups: this is a geom_dotplot argument from ggplot2. if TRUE, dots would be stacked across groups.\nbinwidth: this is a geom_dotplot argument from ggplot2. When method is “dotdensity”, this specifies maximum bin width. When method is “histodot”, this specifies bin width.\nmethod: this is a geom_dotplot argument from ggplot2. “dotdensity” (default) for dot-density binning, or “histodot” for fixed bin widths (like stat_bin).\nwidth_svg and height_svg: this argument specify the width and height of the graphics region in inches. The default values are 6 and 5 inches. This will define the aspect ratio of the graphic as it will be used to define viewbox attribute of the SVG result.\n\n\n\n\n3.1.1 Displaying Multiple Information on Tooltip\nIn the above example, we only displayed student ID using tooltip. We can make use of tooltip to display more than 1 piece of information.\nThe following code chunk is an example. First, we will create a new column in exam_data to contain the information we want the tooltip to display. Then we will plot the graph using ggplot and make it interactive using ggiraph (as seen in the earlier code chunk).\n\n\nShow the code\nexam_data$tooltip &lt;- c(paste0(\"Name = \", exam_data$ID, \n                              \"\\n Class = \", exam_data$CLASS))\n\np2 &lt;- ggplot(data = exam_data, \n            aes(x = MATHS)) + \n  geom_dotplot_interactive(\n    aes(tooltip = exam_data$tooltip),\n    stackgroups = TRUE, \n    binwidth = 1.2, \n    method = \"histodot\") + \n  scale_y_continuous(NULL, breaks = NULL) \n\ngirafe(ggobj = p2, \n       width_svg = 8, \n       height_svg = 8 * 0.618)\n\n\n\n\n\n\nBy hovering the mouse pointer on an data point of interest, the student’s ID and Class will be displayed.\n\n\n3.1.2 Customising Tooltip Style\nWe can also customise the tooltip (e.g. fonts, the color of the tooltip) using opts_tooltip() of ggiraph. This function customises tooltip rendering by adding css declarations.\n\n\nShow the code\ntooltip_css &lt;- \"background-color:white; #&lt;&lt; font-style:bold; color:black;\"#&lt;&lt;\n\nexam_data$tooltip &lt;- c(paste0(\"Name = \", exam_data$ID, \n                              \"\\n Class = \", exam_data$CLASS))\n\np2 &lt;- ggplot(data = exam_data, \n            aes(x = MATHS)) + \n  geom_dotplot_interactive(\n    aes(tooltip = exam_data$tooltip),\n    stackgroups = TRUE, \n    binwidth = 1.2, \n    method = \"histodot\") + \n  scale_y_continuous(NULL, breaks = NULL) + \n  theme_minimal(base_family = \"Open Sans\")\n\ngirafe(ggobj = p2, \n       width_svg = 8, \n       height_svg = 8 * 0.618, \n       options = list(  #&lt;&lt;\n         opts_tooltip(  #&lt;&lt;\n           css = tooltip_css)) #&lt;&lt; \n       )\n\n\n\n\n\n\n\n\n3.1.3 Displaying Statistics on Tooltip\nWe can also use tooltip to display statistics. In the following code chunk, a function is used to compute 90% confidence interval of the mean. The derived statistics is then displayed on the tooltip.\n\n\nShow the code\ntooltip_stat &lt;- function(y, ymax, accuracy = .01) {\n  mean &lt;- scales::number(y, accuracy = accuracy)\n  sem &lt;- scales:: number(ymax-y, accuracy = accuracy)\n  paste(\"Mean Maths Scores: \", mean, \"+/-\", sem)\n}\n\np3 &lt;- ggplot(data = exam_data,\n             aes(x = RACE),) + \n               stat_summary(aes(y = MATHS, tooltip = after_stat(tooltip_stat(y, ymax))),\n                                           fun.data = \"mean_se\",\n                                           geom = GeomInteractiveCol,\n                                           fill = \"pink\") + \n               stat_summary(aes(y = MATHS),\n                            fun.data = mean_se, \n                            geom = \"errorbar\", width = 0.2, linewidth = 0.2)\n\ngirafe( ggobj = p3,\n        width_svg = 8,\n        height_svg = 8 * 0.618)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-On Exercise 3a: Programming Interactive Data Visualisation with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#making-use-of-hover-effect",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#making-use-of-hover-effect",
    "title": "Hands-On Exercise 3a: Programming Interactive Data Visualisation with R",
    "section": "3.2 Making Use of Hover Effect",
    "text": "3.2 Making Use of Hover Effect\nThe code chunk below shows the second interactive feature of ggiraph: data_id. By making use the data_id parameter, we can specify the effect when we mouse over the various graph components.For example, in the following chart, when we mouse over dots, we see other dots (i.e. students) from the same class being highlighted.\n\n\nShow the code\np4 &lt;- ggplot(data=exam_data, \n             aes(x = MATHS)) + \n  geom_dotplot_interactive(\n    aes(data_id = CLASS, \n        tooltip = exam_data$tooltip), \n    stackgroups = TRUE, \n    binwidth = 1, \n    method = \"histodot\") + \n  scale_y_continuous(NULL, breaks = NULL)+ theme_minimal()\n\ngirafe(ggobj = p4,\n       width_svg = 6,\n       height_svg = 6*0.618)\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe default value of the hover css is hover_css = “fill:orange;”\nI also added the tooltip function in the above chart, just to show that we can have both tooltip and hover effect together!\n\n\n\n\n3.2.1 Styling Hover Effect\nWe can also style the hover effect. For example, in the following code chunk, css codes are used to change the highlighting effect by specifying the options argument in girafe().\n\n\nShow the code\np4 &lt;- ggplot(data=exam_data, \n             aes(x = MATHS)) + \n  geom_dotplot_interactive(\n    aes(data_id = CLASS, \n        tooltip = exam_data$tooltip), \n    stackgroups = TRUE, \n    binwidth = 1, \n    method = \"histodot\") + \n  scale_y_continuous(NULL, breaks = NULL) +\n  theme_minimal() \n\ngirafe(ggobj = p4,\n       width_svg = 6,\n       height_svg = 6*0.618,\n       options = list(\n         opts_hover(css = \"fill:#FF33A2;\"),\n         opts_hover_inv(css = \"opacity:0.2;\")\n       ))\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDifferent from the tooltip customisation, in the above example, the css customisation requests are encoded directly.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-On Exercise 3a: Programming Interactive Data Visualisation with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#making-use-of-click-effect",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#making-use-of-click-effect",
    "title": "Hands-On Exercise 3a: Programming Interactive Data Visualisation with R",
    "section": "3.3 Making Use of Click Effect",
    "text": "3.3 Making Use of Click Effect\nThe onclick argument of ggiraph provides hotlink interactivity on the web. The following code chunk is an example of onclick. We would add a column onclick in the exam_data dataset first, then plot the chart and onclick = was assigned to the exam_data$onlick.\n\n\nShow the code\nexam_data$onclick &lt;- sprintf(\"window.open(\\\"%s%s\\\")\",\n                             \"https://www.moe.gov.sg/schoolfinder?journey=Primary%20school\", \n                             as.character(exam_data$ID))\n\np5 &lt;- ggplot(data = exam_data, aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(onclick = onclick),\n    stackgroups = TRUE,\n    binwidth = 1, \n    method = \"histodot\") + \n  scale_y_continuous(NULL, breaks = NULL)\n\ngirafe(ggobj = p5, \n       width_svg = 6,\n       height_svg = 6 * 0.618)\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe click actions must be a string column in the dataset containing valid javascript instructions in order for the onclick function to work.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-On Exercise 3a: Programming Interactive Data Visualisation with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#coordinated-multiple-views-using-ggiraph",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#coordinated-multiple-views-using-ggiraph",
    "title": "Hands-On Exercise 3a: Programming Interactive Data Visualisation with R",
    "section": "3.4 Coordinated Multiple Views using ggiraph",
    "text": "3.4 Coordinated Multiple Views using ggiraph\nWe can also use ggiraph to coordinate the views when we have multiple plots. This means that when a datapoint of one of a dotplot is selected, the corresponding data point on the second data visualisation will be highlighted too.\nTo build a coordinated multiple views, we will: 1. Use the appropriate interactive functions of ggiraph to create the multiple views 2. Use patchwork inside girafe() function to create interactive coordinate multiple views.\n\n\nShow the code\np6 &lt;- p4 + coord_cartesian(xlim = c(0,100)) + theme(panel.grid.major.x = element_blank(),\n  panel.grid.minor.x = element_blank())\n\np7 &lt;- ggplot(data=exam_data, \n             aes(x = ENGLISH)) + \n  geom_dotplot_interactive(\n    aes(data_id = CLASS, \n        tooltip = exam_data$tooltip), \n    stackgroups = TRUE, \n    binwidth = 1, \n    method = \"histodot\") + \n  scale_y_continuous(NULL, breaks = NULL)+ \n  theme_minimal() + theme(panel.grid.major.x = element_blank(),\n  panel.grid.minor.x = element_blank())\n\ngirafe(code = print(p6 + p7),\n       width_svg = 6,\n       height_svg = 3,\n       options = list(\n          opts_hover(css = \"fill:#FF33A2;\"),\n         opts_hover_inv(css = \"opacity:0.2;\")\n       ))\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe data_id aesthetic is critical to link observations between plots and the tooltip aesthetic is optional and good to have when mouse over a point.\nNote that when using patchwork within girafe(), we need to have “code = print()”. Otherwise, the combined plot won’t appear.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-On Exercise 3a: Programming Interactive Data Visualisation with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#coordinated-multiple-views-using-plotly",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#coordinated-multiple-views-using-plotly",
    "title": "Hands-On Exercise 3a: Programming Interactive Data Visualisation with R",
    "section": "4.1 Coordinated Multiple Views using plotly",
    "text": "4.1 Coordinated Multiple Views using plotly\nWe can also create coordinated linked plots using plotly with the following steps:\n\nhighlight_key() of plotly package is used as shared data. It creates an object class crosstalk shared data frame\ncreate two scatterplots using ggplot2 functions\nsubplot() of plotly package is used to place the two scatterplots side by side\n\n\n\nShow the code\nd &lt;- highlight_key(exam_data)\n\np9 &lt;- ggplot(data = d, \n             aes(x = MATHS, \n                 y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim = c(0,100),\n                  ylim = c(0,100))\n\np10 &lt;- ggplot(data=d,\n              aes(x = MATHS,\n                  y = SCIENCE)) +\n    geom_point(size=1) +\n  coord_cartesian(xlim = c(0,100),\n                  ylim = c(0,100))\n\nsubplot(ggplotly(p9),\n        ggplotly(p10))\n\n\n\n\n\n\n\n\n\n\n\n\nTry it!\n\n\n\nClick on a data point of one of the scatterplot and see how the corresponding point on the other scatterplot is selected.\n\n\n\n\n\n\n\n\nNote\n\n\n\nAs ggplotly cannot use crosstalk, we use subplot() of plotly r to combine the two plots together.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-On Exercise 3a: Programming Interactive Data Visualisation with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#interactive-data-table-dt-package",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#interactive-data-table-dt-package",
    "title": "Hands-On Exercise 3a: Programming Interactive Data Visualisation with R",
    "section": "5.1 Interactive Data Table: DT package",
    "text": "5.1 Interactive Data Table: DT package\nDT package is a wrapper of the Javascript Library DataTables. Data objects in R can be rendered as HTML tables and DataTables provides filtering, pagination, sorting and many other features in the tables.\nThe following is an example of displaying exam_data using DT package.\n\n\nShow the code\ndatatable(exam_data, class=\"compact\")",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-On Exercise 3a: Programming Interactive Data Visualisation with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#linked-brushing-crosstalk-method",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#linked-brushing-crosstalk-method",
    "title": "Hands-On Exercise 3a: Programming Interactive Data Visualisation with R",
    "section": "5.2 Linked brushing: crosstalk method",
    "text": "5.2 Linked brushing: crosstalk method\nAfter we have created the interactive data table, we can make use of plotly and DT packages to implement coordinated brushing using the following code chunk. We will get an interactive chart and data table side by side, and when certain data points are selected on the interactive chart, the data table will be filtered accordingly.\n\n\n\n\n\n\nTry it!\n\n\n\nClick on a row in the data table below and see how the corresponding point on the other scatterplot is selected.\n\n\n\n\nShow the code\nd &lt;- highlight_key(exam_data)\np &lt;- ggplot(d, aes(x=ENGLISH, y=MATHS)) + \n  geom_point(size = 1) +\n  coord_cartesian(xlim = c(0,100), \n                  ylim = c(0,100))\n\ng &lt;- highlight(ggplotly(p),\n               \"plotly_selected\")\n\nbscols(g, datatable(d), widths = 5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbout the arguments used!\n\n\n\nhighlight() is a function of plotly package. It sets a variety of options for brushing (i.e. highlighting) multiple plots. These options are primarily designed for linking multiple plotly graphs, and may not behave as expected when linking plotly to another htmlwidget package via crosstalk. In some cases, other htmlwidgets will respect these options, such as persistent selection in leaflet.\nbscols() is a helper function of crosstalk package. It makes it easy to put HTML elements side by side. It can be called directly from the console but is especially designed to work in an R Markdown document.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-On Exercise 3a: Programming Interactive Data Visualisation with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html",
    "title": "Hands-on Exercise 4a - Visualising Distribution",
    "section": "",
    "text": "[Updated with In-class notes on 3 Feb 2024 and an interactive raincloud plot!]",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4a - Visualising Distribution"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#installing-and-loading-the-packages",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#installing-and-loading-the-packages",
    "title": "Hands-on Exercise 4a - Visualising Distribution",
    "section": "2.1 Installing and Loading the Packages",
    "text": "2.1 Installing and Loading the Packages\nFor this exercise, other than tidyverse, we will use the following packages:\n\nggridges: a ggplot2 extension specially designed for plotting ridgeline plots\nggdist: a package to visualise distribution and uncertainty.\nDT: to create interactive tables using the JavaScript library DataTables\n\nThe code chunk below uses p_load() of pacman package to check if the abovementioned packages are installed in the computer. If they are, they will be launched in R. Otherwise, pacman will install the relevant packages before launching them.\n\n\nShow the code\npacman::p_load(tidyverse, ggridges, ggdist, DT, colorspace, ggthemes, ggiraph, plotly, patchwork, crosstalk)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4a - Visualising Distribution"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#importing-the-data",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#importing-the-data",
    "title": "Hands-on Exercise 4a - Visualising Distribution",
    "section": "2.2 Importing the Data",
    "text": "2.2 Importing the Data\nFor this exercise, we will be using the Exam_data.csv provided by the course instructor and we have used it in Hands-on Exercises 1 and 2. It consists of year end examination grades of a cohort of primary 3 students from a local school. It is in csv file format.\nWe use read_csv() function of readr to import the Exam_data csv file into R and save it as a tibble data frame called exam_data. Then we will use datatable() of DT to have an overview of the imported data.\n\n\nShow the code\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")\n\ndatatable(exam_data, caption= \"Table 1: Exam Data of Primary 3 Students\")\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nggridge and ggdist are slightly different.\nggridge allow us to examine the distribution of the data while ggdist allow us to examine both distribution and uncertainty (hence the box plot).\nggridge also does not require time-series data. It is akin to a histogram.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4a - Visualising Distribution"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#varying-fill-colours-along-the-x-axis",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#varying-fill-colours-along-the-x-axis",
    "title": "Hands-on Exercise 4a - Visualising Distribution",
    "section": "3.1 Varying fill colours along the x axis",
    "text": "3.1 Varying fill colours along the x axis\nSometimes we would like to have the area under a ridgeline filled with colors that vary in some form along the x axis. This effect can be achieved by using either geom_ridgeline_gradient() or geom_density_ridges_gradient(). Both geoms work just like geom_ridgeline() and geom_density_ridges(), except that they allow for varying fill colors. However, they do not allow for alpha transparency in the fill. For technical reasons, we can have changing fill colors or transparency but not both.\n\n\nShow the code\nggplot(exam_data, aes(x = ENGLISH, y = CLASS, fill = stat(x))) +\n  geom_density_ridges_gradient(\n    scale = 3, \n    rel_min_height = 0.01) +\n  scale_fill_viridis_c(name = \"Scores\", option = \"C\") +\n  scale_x_continuous(name = \"English Scores\", \n                     expand = c(0,0)) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) + \ntheme_ridges()\n\n\n\n\n\n\n\n\n\nBy varying the fill colours, we can visualise how the students’ scores are being distributed. For example, we see that the ridges of class 3A is filled with a lot of yellow, meaning that most of the students had relatively higher scores. In comparison, the ridge for class 3I is filled with a lot of purple, meaning that most of the students had relatively lower scores.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4a - Visualising Distribution"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#mapping-the-probabilities-directly-onto-color",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#mapping-the-probabilities-directly-onto-color",
    "title": "Hands-on Exercise 4a - Visualising Distribution",
    "section": "3.2 Mapping the probabilities directly onto color",
    "text": "3.2 Mapping the probabilities directly onto color\nBesides providing additional geom objects to support the need to plot ridgeline plot, ggridges package also provides a stat function called stat_density_ridges() that replaces stat_density() of ggplot2.\nWe can map the probabilities calculated using stat(ecdf) which represents the empirical cumulative density function for the distribution of English Scores.\n\nggplot(exam_data,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = 0.5 - abs(0.5-stat(ecdf)))) +\n  stat_density_ridges(geom = \"density_ridges_gradient\", \n                      calc_ecdf = TRUE) +\n  scale_fill_viridis_c(name = \"Tail probability\",\n                       direction = -1) +\n  theme_ridges()\n\n\n\n\n\n\n\n\n\nIt is important include the argument calc_ecdf = TRUE in stat_density_ridges().",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4a - Visualising Distribution"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#ridgeline-plot-with-quantile-lines",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#ridgeline-plot-with-quantile-lines",
    "title": "Hands-on Exercise 4a - Visualising Distribution",
    "section": "3.3 Ridgeline plot with quantile lines",
    "text": "3.3 Ridgeline plot with quantile lines\nUsing geom_density_ridges_gradient, we can colour the ridgeline plot by quantile via the calculated stat(quantile) aesthetic as shown in the following code chunk.\n\nggplot(exam_data, aes(x = ENGLISH, y = CLASS, fill = factor(stat(quantile)))) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE,\n    quantiles = 4,\n    quantile_lines = TRUE) + \n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n\n\n\n\n\nInstead of using number to define the quantiles, we can also specify quantiles by cut points such as 2.5% and 97.5% tails to colour the ridgeline plot as shown in following code chunk.\n\nggplot(exam_data,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = c(0.025, 0.975)\n    ) +\n  scale_fill_manual(\n    name = \"Probability\",\n    values = c(\"#FF0000A0\", \"#A0A0A0A0\", \"#0000FFA0\"),\n    labels = c(\"(0, 0.025]\", \"(0.025, 0.975]\", \"(0.975, 1]\")\n  ) +\n  theme_ridges()",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4a - Visualising Distribution"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#plotting-a-half-eye-graph",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#plotting-a-half-eye-graph",
    "title": "Hands-on Exercise 4a - Visualising Distribution",
    "section": "4.1 Plotting a Half Eye graph",
    "text": "4.1 Plotting a Half Eye graph\nTo plot a raincloud plot, first we will plot a half-eye graph using stat_halfeye() of ggdist package. This produces a Half Eye visualization, which is contains a half-density and a slab-interval.\n:::{.panel-tabset}",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4a - Visualising Distribution"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#without-slab-interval",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#without-slab-interval",
    "title": "Hands-on Exercise 4a - Visualising Distribution",
    "section": "4.2 Without slab-interval",
    "text": "4.2 Without slab-interval\n\nggplot(exam_data, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA)\n\n\n\n\n\n\n\n\nNote: We remove the slab interval by setting .width = 0and point_colour = NA.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4a - Visualising Distribution"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#with-slab-interval",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#with-slab-interval",
    "title": "Hands-on Exercise 4a - Visualising Distribution",
    "section": "4.3 With Slab Interval",
    "text": "4.3 With Slab Interval\n\nggplot(exam_data, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0.5,\n               point_colour = \"blue\")",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4a - Visualising Distribution"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#adding-the-boxplot",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#adding-the-boxplot",
    "title": "Hands-on Exercise 4a - Visualising Distribution",
    "section": "4.4 Adding the boxplot",
    "text": "4.4 Adding the boxplot\nNext, we will add the second geometry layer using geom_boxplot() of ggplot2. This produces a narrow boxplot. We reduce the width and adjust the opacity.\n\nggplot(exam_data, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .10,\n               outlier.shape = NA,\n               fill = \"skyblue\", alpha = 0.4)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4a - Visualising Distribution"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#adding-dot-plots",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#adding-dot-plots",
    "title": "Hands-on Exercise 4a - Visualising Distribution",
    "section": "4.5 Adding dot plots",
    "text": "4.5 Adding dot plots\nNext, we will add the third geometry layer using stat_dots() of ggdist package. This produces a half-dotplot, which is similar to a histogram that indicates the number of samples (number of dots) in each bin. We select side = “left” to indicate we want it on the left-hand side.\n\nggplot(exam_data, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 2)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4a - Visualising Distribution"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#finishing-touch",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#finishing-touch",
    "title": "Hands-on Exercise 4a - Visualising Distribution",
    "section": "4.6 Finishing Touch",
    "text": "4.6 Finishing Touch\nLastly, we will flip the raincloud chart horizontally using coord_flip() to give it a raincloud appearance. We will also add theme_economist() of ggthemes package to give the raincloud chart a professional publishing standard look.\n\nggplot(exam_data, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 2)+\n  coord_flip() + theme_economist()",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4a - Visualising Distribution"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#adding-interactivity---using-ggirafe",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#adding-interactivity---using-ggirafe",
    "title": "Hands-on Exercise 4a - Visualising Distribution",
    "section": "4.7 Adding Interactivity - Using ggirafe",
    "text": "4.7 Adding Interactivity - Using ggirafe\n\nexam_data$tooltip &lt;- c(paste0(\"Name = \", exam_data$ID, \n                              \"\\n Class = \", exam_data$CLASS))\n\nplot1 &lt;- ggplot(exam_data, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.4,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               position= position_nudge(x = 0.2),\n               outlier.shape = NA) +\n  geom_dotplot_interactive(aes(data_id = CLASS,\n                               tooltip = exam_data$tooltip),\n                           binaxis = \"y\",\n                           stackdir = \"down\",\n                           stackgroups = TRUE, \n                           binwidth = 0.5,\n                           method = \"histodot\"\n                           )+\n  coord_flip() + \n  theme_economist()\n\ngirafe(ggobj = plot1, \n       width_svg = 6,\n       height_svg = 6*0.618,\n       options = list(\n         opts_hover(css = \"fill:#FF33A2;\"),\n         opts_hover_inv(css = \"opacity:0.2\")\n       ))",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4a - Visualising Distribution"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#adding-interactivity---using-ggplotly",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04a.html#adding-interactivity---using-ggplotly",
    "title": "Hands-on Exercise 4a - Visualising Distribution",
    "section": "4.8 Adding Interactivity - Using ggplotly",
    "text": "4.8 Adding Interactivity - Using ggplotly\nWork in progress!",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4a - Visualising Distribution"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04c.html",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04c.html",
    "title": "Hands-on Exercise 4C - Visualising Uncertainty",
    "section": "",
    "text": "In this hands-on exercise, we will learn how to:\n\nplot static error bars using ggplot2\nplot interactive error bars using ggplot2, plotly and DT\ncreate hypothetical outcome plots (HOPs) by using ungeviz package.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4C - Visualising Uncertainty"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04c.html#installing-and-loading-the-packages",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04c.html#installing-and-loading-the-packages",
    "title": "Hands-on Exercise 4C - Visualising Uncertainty",
    "section": "2.1 Installing and Loading the Packages",
    "text": "2.1 Installing and Loading the Packages\nFor this exercise, other than tidyverse, we will use the following packages:\n\ntidyverse: a family of R packages for data science process,\nplotly: to create interactive plot,\ngganimate: to create animation plot,\nDT: to display interactive html table,\ncrosstalk: to implement cross-widget interactions (currently, linked brushing and filtering), and\nggdist: to visualise distribution and uncertainty.\n\nThe code chunk below uses p_load() of pacman package to check if the abovementioned packages are installed in the computer. If they are, they will be launched in R. Otherwise, pacman will install the relevant packages before launching them.\n\n\nShow the code\ndevtools::install_github(\"wilkelab/ungeviz\") #you only need to do this step once \n\n\n\n\nShow the code\npacman::p_load(ungeviz, plotly, crosstalk,\n               DT, ggdist, ggridges,\n               colorspace, gganimate, tidyverse)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4C - Visualising Uncertainty"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04c.html#importing-data-into-r",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04c.html#importing-data-into-r",
    "title": "Hands-on Exercise 4C - Visualising Uncertainty",
    "section": "2.2 Importing Data into R",
    "text": "2.2 Importing Data into R\nWe will use Exam_data.csv for this exercise.\n\n\nShow the code\nexam &lt;- read_csv(\"data/Exam_data.csv\")",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4C - Visualising Uncertainty"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04c.html#standard-error-bars-of-point-estimates",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04c.html#standard-error-bars-of-point-estimates",
    "title": "Hands-on Exercise 4C - Visualising Uncertainty",
    "section": "4.1 Standard Error Bars of Point Estimates",
    "text": "4.1 Standard Error Bars of Point Estimates\nWe can visualise the standard error bars of mean maths score by race using the following code chunk.\nNote that the error bars are computed by using the formula mean+/-se. :::{.callout-important} For geom_point(), it is important to indicate stat=“identity”. :::\n\n\nShow the code\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=RACE, \n        ymin=mean-se, \n        ymax=mean+se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  ggtitle(\"Standard error of mean maths score by race\")",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4C - Visualising Uncertainty"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04c.html#confidence-interval-of-point-estimates",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04c.html#confidence-interval-of-point-estimates",
    "title": "Hands-on Exercise 4C - Visualising Uncertainty",
    "section": "4.2 Confidence Interval of Point Estimates",
    "text": "4.2 Confidence Interval of Point Estimates\nInstead of plotting the standard error bar of point estimates, we can also plot the confidence intervals of mean maths score by race.\n\n\nShow the code\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=reorder(RACE, -mean), \n        ymin=mean-1.96*se, \n        ymax=mean+1.96*se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  labs(x = \"Maths score\",\n       title = \"95% confidence interval of mean maths score by race\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe confidence intervals are computed by using the formula mean+/-1.96*se. The error bars are sorted using the average maths scores. labs() argument of ggplot2 is used to change the x-axis label.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4C - Visualising Uncertainty"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04c.html#interactive-error-bars",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04c.html#interactive-error-bars",
    "title": "Hands-on Exercise 4C - Visualising Uncertainty",
    "section": "4.3 Interactive error bars",
    "text": "4.3 Interactive error bars\nWe can also plot interactive error bars for the 99% confidence interval of mean maths scores by race using the following code chunk.\n\n\nShow the code\nshared_df = SharedData$new(my_sum)\n\nbscols(widths = c(4,8),\n       ggplotly((ggplot(shared_df) +\n                   geom_errorbar(aes(\n                     x=reorder(RACE, -mean),\n                     ymin=mean-2.58*se, \n                     ymax=mean+2.58*se), \n                     width=0.2, \n                     colour=\"black\", \n                     alpha=0.9, \n                     size=0.5) +\n                   geom_point(aes(\n                     x=RACE, \n                     y=mean, \n                     text = paste(\"Race:\", `RACE`, \n                                  \"&lt;br&gt;N:\", `n`,\n                                  \"&lt;br&gt;Avg. Scores:\", round(mean, digits = 2),\n                                  \"&lt;br&gt;95% CI:[\", \n                                  round((mean-2.58*se), digits = 2), \",\",\n                                  round((mean+2.58*se), digits = 2),\"]\")),\n                     stat=\"identity\", \n                     color=\"red\", \n                     size = 1.5, \n                     alpha=1) + \n                   xlab(\"Race\") + \n                   ylab(\"Average Scores\") + \n                   theme_minimal() + \n                   theme(axis.text.x = element_text(\n                     angle = 45, vjust = 0.5, hjust=1)) +\n                   ggtitle(\"99% Confidence interval of average /&lt;br&gt;maths scores by race\")), \n                tooltip = \"text\"), \n       DT::datatable(shared_df, \n                     rownames = FALSE, \n                     class=\"compact\", \n                     width=\"100%\", \n                     options = list(pageLength = 10,\n                                    scrollX=T), \n                     colnames = c(\"No. of pupils\", \n                                  \"Avg Scores\",\n                                  \"Std Dev\",\n                                  \"Std Error\")) %&gt;%\n         formatRound(columns=c('mean', 'sd', 'se'),\n                     digits=2))",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4C - Visualising Uncertainty"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04c.html#about-ggdist",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04c.html#about-ggdist",
    "title": "Hands-on Exercise 4C - Visualising Uncertainty",
    "section": "5.1 About ggdist",
    "text": "5.1 About ggdist\nggdist is an R package that provides a flexible set of ggplot2 geoms and stats designed especially for visualising distributions and uncertainty.\nIt is designed for both frequentist and Bayesian uncertainty visualization, taking the view that uncertainty visualization can be unified through the perspective of distribution visualization:\n\nfor frequentist models, one visualises confidence distributions or bootstrap distributions (see vignette(“freq-uncertainty-vis”));\nfor Bayesian models, one visualises probability distributions (see the tidybayes package, which builds on top of ggdist).",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4C - Visualising Uncertainty"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04c.html#confidence-intervals-of-mean-maths-scores",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04c.html#confidence-intervals-of-mean-maths-scores",
    "title": "Hands-on Exercise 4C - Visualising Uncertainty",
    "section": "5.2 Confidence Intervals of Mean Maths Scores",
    "text": "5.2 Confidence Intervals of Mean Maths Scores\nWe can use stat_pointinterval() or stat_gradientinterval() to build a visual for displaying distribution of maths scores by race\n\nstat_pointinterval()stat_gradientinterval()\n\n\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_pointinterval() +\n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\n\n\n\n\n\n\n\n\n\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_gradientinterval(   \n    fill = \"skyblue\",      \n    show.legend = TRUE     \n  ) +                        \n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Gradient + interval plot\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThese two functions come with many arguments, please refer to the syntax reference for more detail.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 4C - Visualising Uncertainty"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html",
    "title": "Hands-on Exercise 5a - Creating Ternary Plot with R",
    "section": "",
    "text": "In this exercise, we will learn how to build ternary plot programmatically using R for visualising and analysing population structure of Singapore.\n\n\n\n\n\n\nWhat is ternary plot?\n\n\n\nTernary plots are a way of displaying the distribution and variability of three-part compositional data. For example, in this exercise, we will have proportions of population: (1) aged, (2) economy active and (3) young. The plot is displayed in a triangle with sides scaled from 0 to 1. Each side represents one of the three components. A point is plotted so that a line drawn perpendicular from the point to each leg of the triangle intersect at the component values of the point.\nWe will see more of it later!",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5a - Creating Ternary Plot with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#installing-and-loading-the-packages",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#installing-and-loading-the-packages",
    "title": "Hands-on Exercise 5a - Creating Ternary Plot with R",
    "section": "2.1 Installing and Loading the Packages",
    "text": "2.1 Installing and Loading the Packages\nFor this exercise, other than tidyverse (in particular readr, dplyr and tidyr), we will use the following packages:\n\nggtern: a ggplot extension specially designed to plot ternary diagrams. We will use this to plot static ternary plots.\nplotly R: to create interactive web-based graphs based on plotly’s JavaScript graphing library, plotly.js. We will make use of plotly R library’s ggplotly() function to convert ggplot2 figures into a plotly object.\n\nThe code chunk below uses p_load() of pacman package to check if the abovementioned packages are installed in the computer. If they are, they will be launched in R. Otherwise, pacman will install the relevant packages before launching them.\n\n\nShow the code\npacman::p_load(tidyverse, plotly, ggtern, DT)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5a - Creating Ternary Plot with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#importing-the-data",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#importing-the-data",
    "title": "Hands-on Exercise 5a - Creating Ternary Plot with R",
    "section": "2.2 Importing the Data",
    "text": "2.2 Importing the Data\nFor this exercise, we will be using the Singapore Residents by Planning Area Subzone, Age Group, Sex and Type of Dwelling (June 2000-2018) data from Singstats. The course instructor has provided the downloaded data respopagsex2000to2018_tidy.csv in csv file format.\nThe following code chunk uses read_csv() function of readr package to import the data into R.\n\n\nShow the code\npopdata &lt;- read_csv(\"data/respopagsex2000to2018_tidy.csv\")\n\n\n\nThe DataChecking the Data\n\n\n\ndatatable(popdata)\n\n\n\n\n\n\n\n\nglimpse(popdata)\n\nRows: 108,126\nColumns: 5\n$ PA         &lt;chr&gt; \"Ang Mo Kio\", \"Ang Mo Kio\", \"Ang Mo Kio\", \"Ang Mo Kio\", \"An…\n$ SZ         &lt;chr&gt; \"Ang Mo Kio Town Centre\", \"Ang Mo Kio Town Centre\", \"Ang Mo…\n$ AG         &lt;chr&gt; \"AGE0-4\", \"AGE0-4\", \"AGE0-4\", \"AGE0-4\", \"AGE0-4\", \"AGE0-4\",…\n$ Year       &lt;dbl&gt; 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2011, 2012,…\n$ Population &lt;dbl&gt; 290, 270, 260, 250, 260, 250, 200, 180, 290, 290, 270, 300,…\n\n\n\n\n\nNote that the data has information from 2000 to 2018. In addition, each row tells us the number of residents for a particular population age group in a certain planning area subzone for a particular year. This current format is not useful for the ternary plot that we are going to make.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5a - Creating Ternary Plot with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#preparing-the-data",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#preparing-the-data",
    "title": "Hands-on Exercise 5a - Creating Ternary Plot with R",
    "section": "2.3 Preparing the Data",
    "text": "2.3 Preparing the Data\nAs such, we use the mutate() function of dplyr package to:\n\nchange the year from numerical to character\nderive three new measures: young, active, and old using spread()\nfilter only those data from year 2018 and with values more than 0.\n\n\n\nShow the code\nagpop_mutated &lt;- popdata %&gt;%\n  mutate(`Year` = as.character(Year))%&gt;%\n  spread(AG, Population) %&gt;%\n  mutate(YOUNG = rowSums(.[4:8]))%&gt;%\n  mutate(ACTIVE = rowSums(.[9:16]))  %&gt;%\n  mutate(OLD = rowSums(.[17:21])) %&gt;%\n  mutate(TOTAL = rowSums(.[22:24])) %&gt;%\n  filter(Year == 2018)%&gt;%\n  filter(TOTAL &gt; 0)\n\n\n\nThe DataChecking the Data\n\n\n\ndatatable(agpop_mutated)\n\n\n\n\n\n\n\n\nglimpse(agpop_mutated)\n\nRows: 234\nColumns: 25\n$ PA         &lt;chr&gt; \"Ang Mo Kio\", \"Ang Mo Kio\", \"Ang Mo Kio\", \"Ang Mo Kio\", \"An…\n$ SZ         &lt;chr&gt; \"Ang Mo Kio Town Centre\", \"Cheng San\", \"Chong Boon\", \"Kebun…\n$ Year       &lt;chr&gt; \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"20…\n$ `AGE0-4`   &lt;dbl&gt; 180, 1060, 900, 720, 220, 550, 260, 830, 160, 810, 350, 282…\n$ `AGE05-9`  &lt;dbl&gt; 270, 1080, 900, 850, 310, 630, 340, 930, 160, 1070, 460, 32…\n$ `AGE10-14` &lt;dbl&gt; 320, 1080, 1030, 1010, 380, 670, 430, 930, 220, 1300, 490, …\n$ `AGE15-19` &lt;dbl&gt; 300, 1260, 1220, 1120, 500, 780, 500, 860, 260, 1450, 400, …\n$ `AGE20-24` &lt;dbl&gt; 260, 1400, 1380, 1230, 550, 950, 640, 1020, 350, 1500, 330,…\n$ `AGE25-29` &lt;dbl&gt; 300, 1880, 1760, 1460, 500, 1080, 690, 1400, 340, 1590, 310…\n$ `AGE30-34` &lt;dbl&gt; 270, 1940, 1830, 1330, 300, 990, 440, 1350, 230, 1390, 310,…\n$ `AGE35-39` &lt;dbl&gt; 330, 2300, 1920, 1540, 290, 1100, 400, 1700, 250, 1770, 630…\n$ `AGE40-44` &lt;dbl&gt; 430, 2090, 1900, 1700, 420, 1140, 490, 1720, 260, 1860, 810…\n$ `AGE45-49` &lt;dbl&gt; 470, 2180, 1910, 1830, 550, 1230, 580, 1530, 320, 2000, 830…\n$ `AGE50-54` &lt;dbl&gt; 360, 2160, 2070, 1880, 550, 1350, 640, 1480, 300, 1980, 620…\n$ `AGE55-59` &lt;dbl&gt; 310, 2150, 2140, 1810, 560, 1420, 730, 1720, 360, 2010, 460…\n$ `AGE60-64` &lt;dbl&gt; 300, 2270, 2170, 1750, 450, 1290, 680, 1680, 350, 1980, 390…\n$ `AGE65-69` &lt;dbl&gt; 270, 2130, 2050, 1700, 410, 1150, 500, 1610, 250, 1790, 340…\n$ `AGE70-74` &lt;dbl&gt; 190, 1370, 1570, 1240, 290, 830, 280, 1190, 160, 1090, 220,…\n$ `AGE75-79` &lt;dbl&gt; 150, 980, 1170, 870, 220, 680, 210, 980, 100, 690, 110, 257…\n$ `AGE80-84` &lt;dbl&gt; 60, 560, 640, 540, 140, 360, 180, 560, 70, 390, 80, 1520, 2…\n$ AGE85over  &lt;dbl&gt; 60, 440, 530, 430, 140, 340, 130, 460, 60, 310, 100, 1350, …\n$ YOUNG      &lt;dbl&gt; 1330, 5880, 5430, 4930, 1960, 3580, 2170, 4570, 1150, 6130,…\n$ ACTIVE     &lt;dbl&gt; 2770, 16970, 15700, 13300, 3620, 9600, 4650, 12580, 2410, 1…\n$ OLD        &lt;dbl&gt; 730, 5480, 5960, 4780, 1200, 3360, 1300, 4800, 640, 4270, 8…\n$ TOTAL      &lt;dbl&gt; 4830, 28330, 27090, 23010, 6780, 16540, 8120, 21950, 4200, …",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5a - Creating Ternary Plot with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#static-ternary-diagram",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#static-ternary-diagram",
    "title": "Hands-on Exercise 5a - Creating Ternary Plot with R",
    "section": "3.1 Static Ternary Diagram",
    "text": "3.1 Static Ternary Diagram\nWe can use ggtern() function of ggtern package to create a simple static ternary plot.\n\n\nShow the code\nggtern(data = agpop_mutated, \n       aes(x = YOUNG, y = ACTIVE, z = OLD)) + \n  geom_point()\n\n\n\n\n\n\n\n\n\nWe can further customise the ternary chart by adding titles using labs() from ggplot2 and themes from ggtern package.\nFor the list of themes provided by ggtern, please refer to here.\n\n\nShow the code\nggtern(agpop_mutated,\n       aes(x=YOUNG, y=ACTIVE, z=OLD)) +\n  geom_point() +\n  labs(title = \"Population Struction 2018\") +\n  theme_rgbg()",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5a - Creating Ternary Plot with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#interactive-ternary-diagram",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#interactive-ternary-diagram",
    "title": "Hands-on Exercise 5a - Creating Ternary Plot with R",
    "section": "3.2 Interactive Ternary Diagram",
    "text": "3.2 Interactive Ternary Diagram\nTo create an interactive ternary plot, we will be using plot_ly() function of Plotly R. It consists several steps: 1. we will first create a function for creating annotation object. 2. Then we will create a function for axis formating 3. Then we will create a plotly visualisation!\nFirst, we will create a function for the label. In this label, we specify the font size, font color, label background color and border width of the label.\n\n\nShow the code\nlabel &lt;- function(txt){\n  list(\n    text = txt,\n    x = 0.1, \n    y = 0.1,\n    ax = 0,\n    ay = 0,\n    xref=\"paper\",\n    yref = \"paper\",\n    align = \"center\",\n    font = list(family = \"serif\", size = 15, color = \"white\"),\n    bgcolor = \"#760241\", bordercolor = \"black\", borderwidth = 2)\n}\n\n\nThen we will create a function for the axis formatting.\n\n\nShow the code\naxis &lt;- function(txt){\n  list(title = txt, tickformat = \".0%\", tickfont = list(size=10))\n}\n\n\nUsing the axis function created, we will create labels for the axes on the ternary plot.\n\n\nShow the code\nternaryaxes &lt;- list(\n  aaxis = axis(\"Young\"),\n  baxis = axis(\"Active\"), \n  caxis = axis(\"Old\")\n)\n\n\nNow we can plot the ternary chart using the scatterternary chart type in plot_ly.\n\n\nShow the code\nplot_ly(\n  agpop_mutated,\n  a = ~YOUNG,\n  b = ~ACTIVE,\n  c = ~OLD,\n  color = I(\"black\"),\n  type = \"scatterternary\"\n) %&gt;%\n  layout(annotations = label(\"Ternary Markers\"),\n         ternary = ternaryaxes)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5a - Creating Ternary Plot with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html",
    "title": "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data",
    "section": "",
    "text": "In this exercise, we will learn how to use R to plot static and interactive heatmap for visualising and analysing multivariate data.\n\n\n\n\n\n\nWhy heatmaps?\n\n\n\nHeatmaps visualise data through variations in colouring. When applied to a tabular format, heatmaps are useful for cross-examining multivariate data, through placing variables in the columns and observation (or records) in rowa and colouring the cells within the table. Heatmaps are good for showing variance across multiple variables, revealing any patterns, displaying whether any variables are similar to each other, and for detecting if any correlations exist in-between them.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html#installing-and-loading-the-packages",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html#installing-and-loading-the-packages",
    "title": "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data",
    "section": "2.1 Installing and Loading the Packages",
    "text": "2.1 Installing and Loading the Packages\nThe code chunk below uses p_load() of pacman package to check if the abovementioned packages are installed in the computer. If they are, they will be launched in R. Otherwise, pacman will install the relevant packages before launching them.\n\n\nShow the code\npacman::p_load(tidyverse, DT, seriation, dendextend, heatmaply)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html#importing-the-data",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html#importing-the-data",
    "title": "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data",
    "section": "2.2 Importing the Data",
    "text": "2.2 Importing the Data\nFor this exercise, we will be using the data from World Happiness 2018 report. The original data set is in Microsoft Excel format. It has been extracted and saved in csv file called WHData-2018.csv.\nIn the code chunk below, read_csv() of readr is used to import WHData-2018.csv into R and parsed it into tibble R data frame format.\n\nhappy &lt;- read_csv(\"data/WHData-2018.csv\")\n\n\nThe DataChecking the Data\n\n\n\ndatatable(happy)\n\n\n\n\n\n\n\n\nglimpse(happy)\n\nRows: 156\nColumns: 12\n$ Country                        &lt;chr&gt; \"Albania\", \"Bosnia and Herzegovina\", \"B…\n$ Region                         &lt;chr&gt; \"Central and Eastern Europe\", \"Central …\n$ `Happiness score`              &lt;dbl&gt; 4.586, 5.129, 4.933, 5.321, 6.711, 5.73…\n$ `Whisker-high`                 &lt;dbl&gt; 4.695, 5.224, 5.022, 5.398, 6.783, 5.81…\n$ `Whisker-low`                  &lt;dbl&gt; 4.477, 5.035, 4.844, 5.244, 6.639, 5.66…\n$ Dystopia                       &lt;dbl&gt; 1.462, 1.883, 1.219, 1.769, 2.494, 1.45…\n$ `GDP per capita`               &lt;dbl&gt; 0.916, 0.915, 1.054, 1.115, 1.233, 1.20…\n$ `Social support`               &lt;dbl&gt; 0.817, 1.078, 1.515, 1.161, 1.489, 1.53…\n$ `Healthy life expectancy`      &lt;dbl&gt; 0.790, 0.758, 0.712, 0.737, 0.854, 0.73…\n$ `Freedom to make life choices` &lt;dbl&gt; 0.419, 0.280, 0.359, 0.380, 0.543, 0.55…\n$ Generosity                     &lt;dbl&gt; 0.149, 0.216, 0.064, 0.120, 0.064, 0.08…\n$ `Perceptions of corruption`    &lt;dbl&gt; 0.032, 0.000, 0.009, 0.039, 0.034, 0.17…",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html#preparing-the-data",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html#preparing-the-data",
    "title": "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data",
    "section": "2.3 Preparing the Data",
    "text": "2.3 Preparing the Data\nFor the purpose of this exercise, we need to change the rows by country name instead of row number.\n\nrow.names(happy) &lt;- happy$Country\ndatatable(happy)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html#transforming-the-data-frame-into-a-matrix",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html#transforming-the-data-frame-into-a-matrix",
    "title": "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data",
    "section": "2.4 Transforming the data frame into a matrix",
    "text": "2.4 Transforming the data frame into a matrix\nThe data was loaded into a data frame, but it has to be a data matrix to make your heatmap.\nThe code chunk below will be used to transform happy data frame into a data matrix.\n\nhappy_matrix &lt;- data.matrix(happy)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html#heatmap-of-r-stats",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html#heatmap-of-r-stats",
    "title": "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data",
    "section": "3.1 heatmap() of R stats",
    "text": "3.1 heatmap() of R stats\nIn this section, we will learn how to plot static heatmaps by using heatmap() of R Stats package.By default, heatmap() plots a cluster heatmap. The arguments Rowv=NA and Colv=NA are used to switch off the option of plotting the row and column dendrograms.\n\nWith dendrogramsWithout dendrograms\n\n\n\nhappy_heatmap &lt;- heatmap(happy_matrix)\n\n\n\n\n\n\n\n\n\n\n\nhappy_heatmap &lt;- heatmap(happy_matrix,Rowv=NA, Colv=NA )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe order of both rows and columns is different compare to the native happy_matrix. This is because heatmap do a reordering using clusterisation: it calculates the distance between each pair of rows and columns and try to order them by similarity. In addition, the corresponding dendrograms are provided beside the heatmap.\n\n\nBy default, red cells denotes small values. As the Happiness Score variable have relatively higher values, this makes other variables with small values all look the same.Thus, we need to normalize this matrix. This is done using the scale argument. It can be applied to rows or to columns following your needs.\n\nNormalise Matrix column-wiseNormalise Matrix row-wise\n\n\n\nhappy_heatmap_c &lt;- heatmap(happy_matrix,\n                      scale=\"column\",\n                      cexRow = 0.6, \n                      cexCol = 0.8,\n                      margins = c(10, 4))\n\n\n\n\n\n\n\n\n\n\n\nhappy_heatmap_r &lt;- heatmap(happy_matrix,\n                      scale=\"row\",\n                      cexRow = 0.6, \n                      cexCol = 0.8,\n                      margins = c(10, 4))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice that the values are scaled now. Also note that margins argument is used to ensure that the entire x-axis labels are displayed completely and, cexRow and cexCol arguments are used to define the font size used for y-axis and x-axis labels respectively.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html#basic-plot",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html#basic-plot",
    "title": "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data",
    "section": "4.1 Basic Plot",
    "text": "4.1 Basic Plot\nUsing the function heatmaply(), we can easily create an interactive heat map by putting the datamatrix into it. For this example, we will put happy_matrix into it and also tell the function to not use the columns “Country”, “Region”, ‘whisker-high’ and ‘whisker-low’(i.e., columns 1, 2, 4 and 5)\n\nheatmaply(happy_matrix[, -c(1,2,4,5)])\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDifferent from heatmap(), for heatmaply() the default horizontal dendrogram is placed on the left hand side of the heatmap. The text label of each raw, on the other hand, is placed on the right hand side of the heat map. When the x-axis marker labels are too long, they will be rotated by 135 degree from the north.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html#data-transformation",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html#data-transformation",
    "title": "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data",
    "section": "4.2 Data Transformation",
    "text": "4.2 Data Transformation\nWhen analysing multivariate data set, it is very common that the variables in the data sets includes values that reflect different types of measurement. In general, these variables’ values have their own range. In order to ensure that all the variables have comparable values, data transformation are commonly used before clustering.\nThree main data transformation methods are supported by heatmaply(), namely: scale, normalise and percentilse.\n\nScaling MethodNormalising MethodPercentilsing Method\n\n\n\nWhen all variables are came from or assumed to come from some normal distribution, then scaling (i.e.: subtract the mean and divide by the standard deviation) would bring them all close to the standard normal distribution.\nIn such a case, each value would reflect the distance from the mean in units of standard deviation.\nThe scale argument in heatmaply() supports column and row scaling.\n\nThe following code chunk scales the variable values columnwise.\n\nheatmaply(happy_matrix[, -c(1,2,4,5)], \n          scale = \"column\", \n          fontsize_row = 4,\n          fontsize_col = 8)\n\n\n\n\n\n\n\n\nWhen variables in the data comes from possibly different (and non-normal) distributions, the normalize function can be used to bring data to the 0 to 1 scale by subtracting the minimum and dividing by the maximum of all observations.\nThis preserves the shape of each variable’s distribution while making them easily comparable on the same “scale”.\nDifferent from Scaling, the normalise method is performed on the input data set i.e. happy_matrix as shown in the code chunk below.\n\n\nheatmaply(normalize(happy_matrix[, -c(1, 2, 4, 5)]), \n          fontsize_row = 4,\n          fontsize_col = 8)\n\n\n\n\n\n\n\n\nThis is similar to ranking the variables, but instead of keeping the rank values, divide them by the maximal rank.\nThis is done by using the ecdf of the variables on their own values, bringing each value to its empirical percentile.\nThe benefit of the percentize function is that each value has a relatively clear interpretation, it is the percent of observations that got that value or below it.\nSimilar to Normalize method, the Percentize method is also performed on the happy_matrix (i.e, input dataset) as shown in the code chunk below.\n\n\nheatmaply(percentize(happy_matrix[, -c(1, 2, 4, 5)]), \n          fontsize_row = 4,\n          fontsize_col = 8)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html#clustering-algorithm",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html#clustering-algorithm",
    "title": "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data",
    "section": "4.3 Clustering Algorithm",
    "text": "4.3 Clustering Algorithm\nheatmaply supports a variety of hierarchical clustering algorithm. The main arguments provided are:\n\ndistfun: function used to compute the distance (dissimilarity) between both rows and columns. Defaults to dist. The options “pearson”, “spearman” and “kendall” can be used to use correlation-based clustering, which uses as.dist(1 - cor(t(x))) as the distance metric (using the specified correlation method).\nhclustfun: function used to compute the hierarchical clustering when Rowv or Colv are not dendrograms. Defaults to hclust.\ndist_method default is NULL, which results in “euclidean” to be used. It can accept alternative character strings indicating the method to be passed to distfun. By default distfun is “dist”” hence this can be one of “euclidean”, “maximum”, “manhattan”, “canberra”, “binary” or “minkowski”.\nhclust_method default is NULL, which results in “complete” method to be used. It can accept alternative character strings indicating the method to be passed to hclustfun. By default hclustfun is hclust hence this can be one of “ward.D”, “ward.D2”, “single”, “complete”, “average” (= UPGMA), “mcquitty” (= WPGMA), “median” (= WPGMC) or “centroid” (= UPGMC).\n\nIn general, a clustering model can be calibrated either manually or statistically.\n\n4.3.1 Manual Approach\nThe following heatmap is plotted using hierarchical clustering algorithm with Euclidean distance and ward.D method.\n\nheatmaply(normalize(happy_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"ward.D\", \n          fontsize_row = 4,\n          fontsize_col = 8)\n\n\n\n\n\n\n\n4.3.2 Statistical Approach\nIn order to determine the best clustering method and number of clusters the dend_expend() and find_k() functions of dendextend package will be used.\nFirst, the dend_expend() will be used to determine the recommended clustering method to be used.\n\n#normalise the data and compute Eucliean distance \nhappy_d &lt;- dist(normalize(happy_matrix[, -c(1, 2, 4, 5)]), method = \"euclidean\")\n\ndend_expend(happy_d)[[3]]\n\n  dist_methods hclust_methods     optim\n1      unknown         ward.D 0.6137851\n2      unknown        ward.D2 0.6289186\n3      unknown         single 0.4774362\n4      unknown       complete 0.6434009\n5      unknown        average 0.6701688\n6      unknown       mcquitty 0.5020102\n7      unknown         median 0.5901833\n8      unknown       centroid 0.6338734\n\n\nThe output table shows that “average” method should be used because it gave the highest optimum value of 0.6701688.\nNext, find_k() is used to determine the optimal number of cluster.\n\nhappy_clust &lt;- hclust(happy_d, method = \"average\")\nnum_k &lt;- find_k(happy_clust)\nplot(num_k)\n\n\n\n\n\n\n\n\nFigure above shows that k=3 would be good since it has the highest average silhouette width.\nWith reference to the statistical analysis results, we can prepare the code chunk as shown below.\n\nheatmaply(normalize(happy_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"average\",\n          k_row = 3, \n          fontsize_row = 4,\n          fontsize_col = 8)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html#seriation",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html#seriation",
    "title": "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data",
    "section": "4.4 Seriation",
    "text": "4.4 Seriation\nOne of the problems with hierarchical clustering is that it doesn’t actually place the rows in a definite order, it merely constrains the space of possible orderings. Take three items A, B and C. If you ignore reflections, there are three possible orderings: ABC, ACB, BAC. If clustering them gives you ((A+B)+C) as a tree, you know that C can’t end up between A and B, but it doesn’t tell you which way to flip the A+B cluster. It doesn’t tell you if the ABC ordering will lead to a clearer-looking heatmap than the BAC ordering.\nheatmaply uses the seriation package to find an optimal ordering of rows and columns. Optimal means to optimize the Hamiltonian path length that is restricted by the dendrogram structure. This, in other words, means to rotate the branches so that the sum of distances between each adjacent leaf (label) will be minimized. This is related to a restricted version of the travelling salesman problem.\nThe first seriation algorithm that we are going to learn is Optimal Leaf Ordering (OLO). This algorithm starts with the output of an agglomerative clustering algorithm and produces a unique ordering, one that flips the various branches of the dendrogram around so as to minimize the sum of dissimilarities between adjacent leaves. Here is the result of applying Optimal Leaf Ordering (i.e. the default option) to the same clustering result as the heatmap above.\n\nheatmaply(normalize(happy_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"OLO\", \n          fontsize_row = 4,\n          fontsize_col = 8)\n\n\n\n\n\nOptimal leaf ordering optimizes the above criterion (in O(n^4)).\nAnother option is “GW” (Gruvaeus and Wainer) which aims for the same goal but uses a potentially faster heuristic.\n\nheatmaply(normalize(happy_matrix[, -c(1,2,4,5)]),\n          seriate = \"GW\", \n          fontsize_row = 4,\n          fontsize_col = 8)\n\n\n\n\n\nThe third option “mean” gives the output we would get by default from heatmap functions in other packages such as gplots::heatmap.2.\n\nheatmaply(normalize(happy_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"mean\", \n          fontsize_row = 4,\n          fontsize_col = 8)\n\n\n\n\n\nThe option “none” gives us the dendrograms without any rotation that is based on the data matrix.\n\nheatmaply(normalize(happy_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\", \n          fontsize_row = 4,\n          fontsize_col = 8)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html#working-with-color-palettes",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html#working-with-color-palettes",
    "title": "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data",
    "section": "4.5 Working with color palettes",
    "text": "4.5 Working with color palettes\nThe default colour palette uses by heatmaply is viridis. heatmaply users, however, can use other colour palettes in order to improve the aestheticness and visual friendliness of the heatmap.\nIn the following code chunk, we use the RdPu colour palette of rcolorbrewer.\n\nheatmaply(normalize(happy_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\",\n          colors = RdPu, \n          fontsize_row = 4,\n          fontsize_col = 8)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html#finishing-touch",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05c.html#finishing-touch",
    "title": "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data",
    "section": "4.6 Finishing Touch",
    "text": "4.6 Finishing Touch\nheatmaply also provides many plotting features to ensure cartographic quality heatmap can be produced.\nIn the code chunk below the following arguments are used:\n\nk_row is used to produce 5 groups.\nmargins is used to change the top margin to 60 and row margin to 200.\nfontsizw_row and fontsize_col are used to change the font size for row and column labels to 4.\nmain is used to write the main title of the plot.\nxlab and ylab are used to write the x-axis and y-axis labels respectively.\n\n\nheatmaply(normalize(happy_matrix[, -c(1, 2, 4, 5)]),\n          Colv=NA,\n          seriate = \"none\",\n          colors = RdPu,\n          k_row = 5,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 5,\n          fontsize_col = 5,\n          main=\"World Happiness Score and Variables by Country, 2018 \\nDataTransformation using Normalise Method\",\n          xlab = \"World Happiness Indicators\",\n          ylab = \"World Countries\"\n          )",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5c: Heatmap for Visualising and Analysing Multivariate Data"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html",
    "title": "Hands-on Exercise 5e - Treemap Visualisation with R",
    "section": "",
    "text": "In this exercise, we will learn how to design treemap using appropriate R packages. The hands-on exercise consists of three main section. First, you will learn how to manipulate transaction data into a treemap strcuture by using selected functions provided in dplyr package. Then, you will learn how to plot static treemap by using treemap package. In the third section, you will learn how to design interactive treemap by using d3treeR package.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5e - Treemap Visualisation with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#installing-and-loading-the-packages",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#installing-and-loading-the-packages",
    "title": "Hands-on Exercise 5e - Treemap Visualisation with R",
    "section": "2.1 Installing and Loading the Packages",
    "text": "2.1 Installing and Loading the Packages\nFor this exercise, the treemap, treemapify and tidyverse packages will be used.\nThe code chunk below uses p_load() of pacman package to check if the abovementioned packages are installed in the computer. If they are, they will be launched in R. Otherwise, pacman will install the relevant packages before launching them.\n\n\nShow the code\npacman::p_load(tidyverse, DT, treemap, treemapify)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5e - Treemap Visualisation with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#importing-the-data",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#importing-the-data",
    "title": "Hands-on Exercise 5e - Treemap Visualisation with R",
    "section": "2.2 Importing the Data",
    "text": "2.2 Importing the Data\nFor this exercise, REALIS2018.csv data will be used. This dataset provides information of private property transaction records in 2018. The dataset is extracted from REALIS portal of Urban Redevelopment Authority (URA).\nIn the code chunk below, read_csv() of readr is used to import WHData-2018.csv into R and parsed it into tibble R data frame format.\n\nrealis2018 &lt;- read_csv(\"data/realis2018.csv\")\n\n\nThe DataChecking the data\n\n\n\ndatatable(realis2018)\n\n\n\n\n\n\n\n\nglimpse(realis2018)\n\nRows: 23,205\nColumns: 20\n$ `Project Name`                &lt;chr&gt; \"ADANA @ THOMSON\", \"ALANA\", \"ALANA\", \"AL…\n$ Address                       &lt;chr&gt; \"8 Old Upper Thomson Road  #05-03\", \"156…\n$ `No. of Units`                &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ `Area (sqm)`                  &lt;dbl&gt; 52, 284, 256, 256, 277, 285, 234, 155, 1…\n$ `Type of Area`                &lt;chr&gt; \"Strata\", \"Strata\", \"Strata\", \"Strata\", …\n$ `Transacted Price ($)`        &lt;dbl&gt; 888888, 2530000, 2390863, 2450000, 19800…\n$ `Nett Price($)`               &lt;chr&gt; \"-\", \"-\", \"2382517\", \"2441654\", \"-\", \"-\"…\n$ `Unit Price ($ psm)`          &lt;dbl&gt; 17094, 8908, 9307, 9538, 7148, 6947, 147…\n$ `Unit Price ($ psf)`          &lt;dbl&gt; 1588, 828, 865, 886, 664, 645, 1371, 149…\n$ `Sale Date`                   &lt;chr&gt; \"4-Jul-18\", \"5-Oct-18\", \"9-Jun-18\", \"14-…\n$ `Property Type`               &lt;chr&gt; \"Apartment\", \"Terrace House\", \"Terrace H…\n$ Tenure                        &lt;chr&gt; \"Freehold\", \"103 Yrs From 12/08/2013\", \"…\n$ `Completion Date`             &lt;chr&gt; \"2018\", \"2018\", \"2018\", \"2018\", \"2008\", …\n$ `Type of Sale`                &lt;chr&gt; \"New Sale\", \"Sub Sale\", \"New Sale\", \"New…\n$ `Purchaser Address Indicator` &lt;chr&gt; \"Private\", \"Private\", \"HDB\", \"N.A\", \"Pri…\n$ `Postal District`             &lt;dbl&gt; 20, 28, 28, 28, 26, 26, 26, 26, 26, 26, …\n$ `Postal Sector`               &lt;dbl&gt; 57, 80, 80, 80, 78, 78, 78, 78, 78, 78, …\n$ `Postal Code`                 &lt;dbl&gt; 573868, 804555, 804529, 804540, 786300, …\n$ `Planning Region`             &lt;chr&gt; \"North East Region\", \"North East Region\"…\n$ `Planning Area`               &lt;chr&gt; \"Ang Mo Kio\", \"Ang Mo Kio\", \"Ang Mo Kio\"…",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5e - Treemap Visualisation with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#data-wrangling",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#data-wrangling",
    "title": "Hands-on Exercise 5e - Treemap Visualisation with R",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\nAs seen from the above,realis2018 is in trasaction record form, which is highly disaggregated and not appropriate to be used to plot a treemap. In this section, we will perform the following steps to manipulate and prepare a data.frtame that is appropriate for treemap visualisation:\n\ngroup transaction reocrds by Project Name, Planning Region, Planning Area, Property Type and Type of Sale; and\ncompute Total Units Sold, Total Area, Median Unit Price and Median Transacted Price by applying appropriate summary statistics on number of units, area(sqm), unit price (\\(psm) and transacted price (\\)) respectively.\n\nTwo key verbs of dplyr package, namely: group_by() and summarize() will be used to perform these steps.\ngroup_by() breaks down a data.frame into specified groups of rows. When you then apply the verbs above on the resulting object they’ll be automatically applied “by group”.\nGrouping affects the verbs as follows:\n\ngrouped select() is the same as ungrouped select(), except that grouping variables are always retained.\ngrouped arrange() is the same as ungrouped; unless you set .by_group = TRUE, in which case it orders first by the grouping variables.\nmutate() and filter() are most useful in conjunction with window functions (like rank(), or min(x) == x).\nsample_n() and sample_frac() sample the specified number/fraction of rows in each group.\nsummarise() computes the summary for each group.\n\nIn our case, group_by() will used together with summarise() to derive the summarised data.frame.\n\nGrouped summaries without the PipeGrouped summaries with the Pipe\n\n\nThe code chunk below shows a typical two-lines code approach to perform the steps.\n\nrealis2018_grouped &lt;- group_by(realis2018, `Project Name`,\n                               `Planning Region`, `Planning Area`, \n                               `Property Type`, `Type of Sale`)\n\nrealis2018_summarised &lt;- summarise(realis2018_grouped, \n                          `Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE),\n                          `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n                          `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE), \n                          `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))\n\nThe code chunk above is not very efficient because we have to give each intermediate data.frame a name, even though we don’t have to care about it.\n\n\nThe code chunk below shows a more efficient way to tackle the same processes by using the pipe operator, %&gt;%:\n\nrealis2018_summarised &lt;- realis2018 %&gt;% \n  group_by(`Project Name`,`Planning Region`, \n           `Planning Area`, `Property Type`, \n           `Type of Sale`) %&gt;%\n  summarise(`Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE), \n            `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n            `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE),\n            `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5e - Treemap Visualisation with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#designing-a-static-treemap",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#designing-a-static-treemap",
    "title": "Hands-on Exercise 5e - Treemap Visualisation with R",
    "section": "3.1 Designing a static treemap",
    "text": "3.1 Designing a static treemap\nIn this section, treemap() of Treemap package is used to plot a treemap showing the distribution of median unit prices and total unit sold of resale condominium by geographic hierarchy in 2017.\nFirst, we will select records belongs to resale condominium property type from realis2018_selected data frame.\n\nrealis2018_selected &lt;- realis2018_summarised %&gt;%\n  filter(`Property Type` == \"Condominium\", `Type of Sale` == \"Resale\")",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5e - Treemap Visualisation with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#using-the-basic-arguments",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#using-the-basic-arguments",
    "title": "Hands-on Exercise 5e - Treemap Visualisation with R",
    "section": "3.2 Using the basic arguments",
    "text": "3.2 Using the basic arguments\nThe code chunk below designed a treemap by using three core arguments of treemap(), namely: index, vSize and vColor.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThings to learn from the three arguments used:\n\nindex\n\nThe index vector must consist of at least two column names or else no hierarchy treemap will be plotted.\nIf multiple column names are provided, such as the code chunk above, the first name is the highest aggregation level, the second name the second highest aggregation level, and so on.\n\nvSize\n\nThe column must not contain negative values. This is because it’s vaues will be used to map the sizes of the rectangles of the treemaps.\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe treemap above was wrongly coloured. For a correctly designed treemap, the colours of the rectagles should be in different intensity showing, in our case, median unit prices.\nFor treemap(), vColor is used in combination with the argument type to determines the colours of the rectangles. Without defining type, like the code chunk above, treemap() assumes type = index, in our case, the hierarchy of planning areas.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5e - Treemap Visualisation with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#working-with-vcolor-and-type-arguments",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#working-with-vcolor-and-type-arguments",
    "title": "Hands-on Exercise 5e - Treemap Visualisation with R",
    "section": "3.3 Working with vColor and type arguments",
    "text": "3.3 Working with vColor and type arguments\nIn the code chunk below, type argument is define as value.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type = \"value\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe rectangles are coloured with different intensity of green, reflecting their respective median unit prices.\nThe legend reveals that the values are binned into ten bins, i.e. 0-5000, 5000-10000, etc. with an equal interval of 5000.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5e - Treemap Visualisation with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#colours-with-treemap-package",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#colours-with-treemap-package",
    "title": "Hands-on Exercise 5e - Treemap Visualisation with R",
    "section": "3.4 Colours with treemap package",
    "text": "3.4 Colours with treemap package\nThere are two arguments that determine the mapping to color palettes: mapping and palette. The only difference between “value” and “manual” is the default value for mapping. The “value” treemap considers palette to be a diverging color palette (say ColorBrewer’s “RdYlBu”), and maps it in such a way that 0 corresponds to the middle color (typically white or yellow), -max(abs(values)) to the left-end color, and max(abs(values)), to the right-end color. The “manual” treemap simply maps min(values) to the left-end color, max(values) to the right-end color, and mean(range(values)) to the middle color.\n\n3.4.1 The “value” type treemap\nThe\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\nThing to learn from the code chunk above:\n\nalthough the colour palette used is RdYlBu but there are no red rectangles in the treemap above. This is because all the median unit prices are positive.\nThe reason why we see only 5000 to 45000 in the legend is because the range argument is by default c(min(values, max(values)) with some pretty rounding.\n\n\n\n3.4.2 The “manual” type treemap\nThe “manual” type does not interpret the values as the “value” type does. Instead, the value range is mapped linearly to the colour palette.\nThe code chunk below shows a manual type treemap.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\nThe colour scheme used is very confusing. This is because mapping = (min(values), mean(range(values)), max(values)). It is not wise to use diverging colour palette such as RdYlBu if the values are all positive or negative.\n\nTo overcome this problem, a single colour palette such as Blues/Greens should be used.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Greens\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5e - Treemap Visualisation with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#treemap-layout",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#treemap-layout",
    "title": "Hands-on Exercise 5e - Treemap Visualisation with R",
    "section": "3.5 Treemap Layout",
    "text": "3.5 Treemap Layout\ntreemap() supports two popular treemap layouts, namely: “squarified” and “pivotSize”. The default is “pivotSize”.\nThe squarified treemap algorithm (Bruls et al., 2000) produces good aspect ratios, but ignores the sorting order of the rectangles (sortID). The ordered treemap, pivot-by-size, algorithm (Bederson et al., 2002) takes the sorting order (sortID) into account while aspect ratios are still acceptable.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5e - Treemap Visualisation with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#working-with-algorithm-argument",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#working-with-algorithm-argument",
    "title": "Hands-on Exercise 5e - Treemap Visualisation with R",
    "section": "3.6 Working with algorithm argument",
    "text": "3.6 Working with algorithm argument\nThe code chunk below plots a squarified treemap by changing the algorithm argument.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"squarified\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5e - Treemap Visualisation with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#using-sortid",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#using-sortid",
    "title": "Hands-on Exercise 5e - Treemap Visualisation with R",
    "section": "3.7 Using sortID",
    "text": "3.7 Using sortID\nWhen “pivotSize” algorithm is used, sortID argument can be used to dertemine the order in which the rectangles are placed from top left to bottom right.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"pivotSize\",\n        sortID = \"Median Transacted Price\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5e - Treemap Visualisation with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#designing-a-basic-treemap",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#designing-a-basic-treemap",
    "title": "Hands-on Exercise 5e - Treemap Visualisation with R",
    "section": "4.1 Designing a basic treemap",
    "text": "4.1 Designing a basic treemap\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`),\n       layout = \"scol\",\n       start = \"bottomleft\") + \n  geom_treemap() +\n  scale_fill_gradient(low = \"light blue\", high = \"blue\")",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5e - Treemap Visualisation with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#defining-hierarchy",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#defining-hierarchy",
    "title": "Hands-on Exercise 5e - Treemap Visualisation with R",
    "section": "4.2 Defining hierarchy",
    "text": "4.2 Defining hierarchy\nGroup by Planning Region\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`),\n       start = \"topleft\") + \n  geom_treemap()\n\n\n\n\n\n\n\n\nGroup by Planning Area\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap()\n\n\n\n\n\n\n\n\nAdding boundary line\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap() +\n  geom_treemap_subgroup2_border(colour = \"gray40\",\n                                size = 2) +\n  geom_treemap_subgroup_border(colour = \"gray20\")",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5e - Treemap Visualisation with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#installing-d3treer-package",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#installing-d3treer-package",
    "title": "Hands-on Exercise 5e - Treemap Visualisation with R",
    "section": "5.1 Installing d3treeR package",
    "text": "5.1 Installing d3treeR package\nWe will first load the devtools library and install the package found in github using the codes below.\n\nlibrary(devtools)\ninstall_github(\"timelyportfolio/d3treeR\")\n\nThen we will launch d3treeR package.\n\nlibrary(d3treeR)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5e - Treemap Visualisation with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#designing-an-interactive-treemap",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05e.html#designing-an-interactive-treemap",
    "title": "Hands-on Exercise 5e - Treemap Visualisation with R",
    "section": "5.2 Designing an Interactive Treemap",
    "text": "5.2 Designing an Interactive Treemap\nThe following codes perform two processes:\n\ntreemap() is used to build a treemap by using selected variables in condominium data.frame. The treemap created is save as object called tm.\n\n\ntm &lt;- treemap(realis2018_summarised,\n        index=c(\"Planning Region\", \"Planning Area\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        title=\"Private Residential Property Sold, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\nThen d3tree() is used to build an interactive treemap.\n\n\nd3tree(tm,rootname = \"Singapore\")",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 5e - Treemap Visualisation with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07a.html",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07a.html",
    "title": "Take-home Exercise 7a: Choropleth Mapping with R",
    "section": "",
    "text": "In this exercise, we will learn how to plot functional and truthful choropleth maps using tmap package.\n\n\n\n\n\n\nWhat is choropleth mapping?\n\n\n\nChoropleth mapping involves the symbolisation of enumeration units, such as countries, provinces, states, counties or census units, using area patterns or graduated colors. For example, a social scientist may need to use a choropleth map to portray the spatial distribution of aged population of Singapore by Master Plan 2014 Subzone Boundary.",
    "crumbs": [
      "Hands-on Exercises",
      "Take-home Exercise 7a: Choropleth Mapping with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07a.html#installing-and-loading-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07a.html#installing-and-loading-r-packages",
    "title": "Take-home Exercise 7a: Choropleth Mapping with R",
    "section": "2.1 Installing and Loading R Packages",
    "text": "2.1 Installing and Loading R Packages\nFor this exercise, other than tmap, we will use the following packages:\n\nreadr for importing delimited text file\ntidyr for tidying data\ndplyr for wrangling data\nsf for handling geospatial data\n\n\n\n\n\n\n\nNote\n\n\n\nAmong the four packages, readr, tidyr and dplyr are part of tidyverse package.So, we only need to install tidyverse instead of readr, tidyr and dplyr individually.\n\n\n\n\nShow the code\npacman::p_load(tidyverse, sf, tmap)",
    "crumbs": [
      "Hands-on Exercises",
      "Take-home Exercise 7a: Choropleth Mapping with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07a.html#importing-the-data-into-r",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07a.html#importing-the-data-into-r",
    "title": "Take-home Exercise 7a: Choropleth Mapping with R",
    "section": "2.2 Importing the Data into R",
    "text": "2.2 Importing the Data into R\nFor this exercise, we will use two datasets to create the choropleth map:\n\nGeospatial Data: Master Plan 2014 Subzone Boundary (Web) in ESRI shapefile format (MP14_SUBZONE_WEB_PL). It can be downloaded at data.gov.sg.This dataset contains the geographical boundary of Singapore at the planning subzone level. The data is based on URA Master Plan 2014.\nAspatial Data: Singapore Residents by Planning Area/Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 in csv format (respopagesextod2011to2020.csv). It can be downloaded from Department of Statistics, Singapore. Although it does not contain any coordinates values, but it’s PA and SZ fields can be used as unique identifiers to geocode to MP14_SUBZONE_WEB_PL shapefile.\n\n\nGeospatial DataAspatial Data\n\n\nThe code chunk below uses the st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a simple feature data frame called mpsz.\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\",\n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\sihuihui\\VAA\\Hands-on_Ex\\Hands-on_Ex07\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\nmpsz\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\nThe message above also tells us that mpsz’s geometry type is multipolygon, there are a total of 323 multipolygon features and 15 fields in mpsz and it is in svy21 projected coordinates systems.\n\n\nWe will import respopagesextod2011to2020.csv file using read_csv() function of readr package.\n\npopdata &lt;- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\nhead(popdata)\n\n# A tibble: 6 × 7\n  PA         SZ                     AG     Sex   TOD                   Pop  Time\n  &lt;chr&gt;      &lt;chr&gt;                  &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt;\n1 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Males HDB 1- and 2-Room …     0  2011\n2 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Males HDB 3-Room Flats       10  2011\n3 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Males HDB 4-Room Flats       30  2011\n4 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Males HDB 5-Room and Exe…    50  2011\n5 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Males HUDC Flats (exclud…     0  2011\n6 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Males Landed Properties       0  2011\n\n\nFrom the above output, we see that there are 7 columns in the datatable, namely: Planning area (PA), Subzone (SZ), Age Group (AG), Sex, Type of Dwelling (TOD), Population (Pop), Year (Time).",
    "crumbs": [
      "Hands-on Exercises",
      "Take-home Exercise 7a: Choropleth Mapping with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07a.html#data-preparation",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07a.html#data-preparation",
    "title": "Take-home Exercise 7a: Choropleth Mapping with R",
    "section": "2.3 Data Preparation",
    "text": "2.3 Data Preparation\nBefore a thematic map can be prepared, we need to prepare a data table with year 2020 values. The data table should include the variables: PA, SZ, YOUNG(AG 0 to 4 until AG 20 to 24), ECONOMY ACTIVE (AG 25 to 29 until AG 60 - 64), AGED (AG 65 and above), TOTAL (All AG), DEPENDENCY (ratio between young and aged against economy active group).\n\n2.3.1 Data Wrangling\n\nTransforming the DataData Table\n\n\n\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time == 2020) %&gt;%\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(Pop = sum(Pop)) %&gt;%\n  ungroup() %&gt;%\n  pivot_wider(names_from = AG, \n               values_from = Pop) \n\nglimpse(popdata2020)\n\nRows: 332\nColumns: 21\n$ PA            &lt;chr&gt; \"Ang Mo Kio\", \"Ang Mo Kio\", \"Ang Mo Kio\", \"Ang Mo Kio\", …\n$ SZ            &lt;chr&gt; \"Ang Mo Kio Town Centre\", \"Cheng San\", \"Chong Boon\", \"Ke…\n$ `0_to_4`      &lt;dbl&gt; 170, 1060, 850, 680, 210, 560, 200, 670, 0, 160, 0, 740,…\n$ `10_to_14`    &lt;dbl&gt; 280, 1040, 1020, 960, 400, 640, 390, 930, 0, 210, 0, 119…\n$ `15_to_19`    &lt;dbl&gt; 340, 1160, 1070, 1010, 450, 700, 460, 830, 0, 260, 0, 13…\n$ `20_to_24`    &lt;dbl&gt; 270, 1330, 1310, 1170, 500, 860, 590, 890, 0, 300, 0, 14…\n$ `25_to_29`    &lt;dbl&gt; 260, 1720, 1610, 1410, 500, 970, 680, 1310, 0, 320, 0, 1…\n$ `30_to_34`    &lt;dbl&gt; 310, 2020, 1890, 1420, 340, 1030, 500, 1410, 0, 240, 0, …\n$ `35_to_39`    &lt;dbl&gt; 330, 2150, 1720, 1440, 300, 980, 330, 1420, 0, 250, 0, 1…\n$ `40_to_44`    &lt;dbl&gt; 400, 2080, 1810, 1630, 370, 1010, 430, 1640, 0, 260, 0, …\n$ `45_to_49`    &lt;dbl&gt; 480, 2200, 1820, 1810, 550, 1190, 580, 1580, 0, 320, 0, …\n$ `50_to_54`    &lt;dbl&gt; 380, 2050, 1900, 1720, 540, 1200, 580, 1370, 0, 290, 0, …\n$ `55_to_59`    &lt;dbl&gt; 310, 2130, 2100, 1800, 550, 1390, 660, 1570, 0, 340, 0, …\n$ `5_to_9`      &lt;dbl&gt; 230, 1050, 850, 800, 320, 570, 300, 870, 0, 180, 0, 950,…\n$ `60_to_64`    &lt;dbl&gt; 290, 2110, 2150, 1780, 480, 1280, 720, 1650, 0, 390, 0, …\n$ `65_to_69`    &lt;dbl&gt; 250, 2180, 2100, 1710, 410, 1200, 560, 1530, 0, 270, 0, …\n$ `70_to_74`    &lt;dbl&gt; 240, 1750, 1800, 1450, 360, 970, 390, 1430, 0, 200, 0, 1…\n$ `75_to_79`    &lt;dbl&gt; 130, 960, 1120, 830, 230, 630, 210, 890, 0, 120, 0, 650,…\n$ `80_to_84`    &lt;dbl&gt; 100, 650, 800, 630, 150, 430, 190, 700, 0, 80, 0, 480, 9…\n$ `85_to_89`    &lt;dbl&gt; 30, 340, 430, 350, 100, 250, 110, 360, 0, 50, 0, 250, 50…\n$ `90_and_over` &lt;dbl&gt; 10, 170, 220, 150, 60, 130, 70, 190, 0, 30, 0, 100, 50, …\n\n\n\n\n\nhead(popdata2020, 10)\n\n# A tibble: 10 × 21\n   PA      SZ    `0_to_4` `10_to_14` `15_to_19` `20_to_24` `25_to_29` `30_to_34`\n   &lt;chr&gt;   &lt;chr&gt;    &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1 Ang Mo… Ang …      170        280        340        270        260        310\n 2 Ang Mo… Chen…     1060       1040       1160       1330       1720       2020\n 3 Ang Mo… Chon…      850       1020       1070       1310       1610       1890\n 4 Ang Mo… Kebu…      680        960       1010       1170       1410       1420\n 5 Ang Mo… Semb…      210        400        450        500        500        340\n 6 Ang Mo… Shan…      560        640        700        860        970       1030\n 7 Ang Mo… Tago…      200        390        460        590        680        500\n 8 Ang Mo… Town…      670        930        830        890       1310       1410\n 9 Ang Mo… Yio …        0          0          0          0          0          0\n10 Ang Mo… Yio …      160        210        260        300        320        240\n# ℹ 13 more variables: `35_to_39` &lt;dbl&gt;, `40_to_44` &lt;dbl&gt;, `45_to_49` &lt;dbl&gt;,\n#   `50_to_54` &lt;dbl&gt;, `55_to_59` &lt;dbl&gt;, `5_to_9` &lt;dbl&gt;, `60_to_64` &lt;dbl&gt;,\n#   `65_to_69` &lt;dbl&gt;, `70_to_74` &lt;dbl&gt;, `75_to_79` &lt;dbl&gt;, `80_to_84` &lt;dbl&gt;,\n#   `85_to_89` &lt;dbl&gt;, `90_and_over` &lt;dbl&gt;\n\n\n\n\n\nThen we will group the various AG to create YOUNG, ECONOMY ACTIVE, AGED, TOTAL and also calculate the ratio DEPENDENCY using mutate() of dplyr package.\n\nTransforming the DataData Table\n\n\n\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate(YOUNG = rowSums(.[3:6]) + rowSums(.[14])) %&gt;%\n  mutate(ECONOMYACTIVE = rowSums(.[7:13]) + rowSums(.[15])) %&gt;%\n  mutate(AGED = rowSums(.[16:21])) %&gt;%\n  mutate(TOTAL = rowSums(.[3:21])) %&gt;%\n  mutate(DEPENDENCY = (YOUNG + AGED) / ECONOMYACTIVE) %&gt;%\n  select(PA, SZ, YOUNG, ECONOMYACTIVE, AGED, TOTAL, DEPENDENCY)\n\n\n\n\nhead(popdata2020, 10)\n\n# A tibble: 10 × 7\n   PA         SZ                     YOUNG ECONOMYACTIVE  AGED TOTAL DEPENDENCY\n   &lt;chr&gt;      &lt;chr&gt;                  &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;\n 1 Ang Mo Kio Ang Mo Kio Town Centre  1290          2760   760  4810      0.743\n 2 Ang Mo Kio Cheng San               5640         16460  6050 28150      0.710\n 3 Ang Mo Kio Chong Boon              5100         15000  6470 26570      0.771\n 4 Ang Mo Kio Kebun Bahru             4620         13010  5120 22750      0.749\n 5 Ang Mo Kio Sembawang Hills         1880          3630  1310  6820      0.879\n 6 Ang Mo Kio Shangri-La              3330          9050  3610 15990      0.767\n 7 Ang Mo Kio Tagore                  1940          4480  1530  7950      0.775\n 8 Ang Mo Kio Townsville              4190         11950  5100 21240      0.777\n 9 Ang Mo Kio Yio Chu Kang               0             0     0     0    NaN    \n10 Ang Mo Kio Yio Chu Kang East       1110          2410   750  4270      0.772\n\n\n\n\n\n\n\n2.3.2 Joining the aspatial data and geospatial data\nBefore we can perform the georelational join, we need to convert the values in PA and SZ fields to uppercase. This is because the values of PA and SZ fields are made up of upper- and lowercase. On the other, hand the SUBZONE_N and PLN_AREA_N are in uppercase. We will also retain those rows where ECONOMYACTIVE is more than 0 using filter().\n\n\nShow the code\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = funs(toupper)) %&gt;%\n  filter(ECONOMYACTIVE &gt; 0)\n\n\nThen we will use left_join() of dplyr to join the geospatial and aspatial data using planning subzone name (i.e. SUBZONE_N and SZ) as the common identifier.\n\n\nShow the code\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote! The mpsz simple feature data frame is the left data table in the left_join. This is to ensure that the output will be a simple features data frame.\n\n\nThen we will write mpsz_pop2020 into an rds file for easy retrieval in future.\n\n\nShow the code\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")",
    "crumbs": [
      "Hands-on Exercises",
      "Take-home Exercise 7a: Choropleth Mapping with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07a.html#plotting-a-choropleth-map-quickly-using-qtm",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07a.html#plotting-a-choropleth-map-quickly-using-qtm",
    "title": "Take-home Exercise 7a: Choropleth Mapping with R",
    "section": "3.1 Plotting a choropleth map quickly using qtm()",
    "text": "3.1 Plotting a choropleth map quickly using qtm()\n\nStatic MapInteractive Map\n\n\n\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\n\n\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"view\")\nqtm(mpsz_pop2020, fill = \"DEPENDENCY\")\n\n\n\n\n\n\n\n\nThings to learn from the code chunks above:\ntmap_mode() with “plot” option is used to produce a static map. For interactive mode, “view” option should be used. fill argument is used to map the attribute (i.e. DEPENDENCY)",
    "crumbs": [
      "Hands-on Exercises",
      "Take-home Exercise 7a: Choropleth Mapping with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07a.html#creating-a-choropleth-map-using-tmaps-elements",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07a.html#creating-a-choropleth-map-using-tmaps-elements",
    "title": "Take-home Exercise 7a: Choropleth Mapping with R",
    "section": "3.2 Creating a choropleth map using tmap’s elements",
    "text": "3.2 Creating a choropleth map using tmap’s elements\n\n3.2.1 Drawing a base map\nThe basic building block of tmap is tm_shape() followed by one or more layer elemments such as tm_fill() and tm_polygons().\nIn the code chunk below, tm_shape() is used to define the input data (i.e mpsz_pop2020) and tm_polygons() is used to draw the planning subzone polygons.\n\n\nShow the code\ntmap_mode(\"plot\")\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\n\n\n\n\n\n\n3.2.2 Drawing a choropleth map using tm_polygons()\nTo draw a choropleth map showing the geographical distribution of a selected variable by planning subzone, we need to assign the target variable such as Dependency to tm_polygons().\n\n\nShow the code\ntm_shape(mpsz_pop2020) +\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe default interval binning used to draw the choropleth map is called “pretty”. A detailed discussion of the data classification methods supported by tmap will be provided in the following section.\nThe default colour scheme used is YlOrRd of ColorBrewer. We will learn how to change the colour scheme in the subsequent sections.\nBy default, missing values will be shaded in grey.\n\n\n\n\n\n3.2.3 Drawing a choropleth map using tm_fill() and tm_border()\ntm_polygons() is a wraper of tm_fill() and tm_border(). tm_fill() shades the polygons by using the default colour scheme and tm_borders() adds the borders of the shapefile onto the choropleth map.\nThe following code chunk draws a choropleth map using tm_fill() only.\n\n\nShow the code\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice from the above output, there are no borders when only tm_fill() is used.\n\n\nTo add the boundary of the planning subzones, tm_borders() will be used.\n\n\nShow the code\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1, alpha = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe added light grey border lines on the choropleth map using the above code chunk.\nThe alpha argument is used to define transparency number between 0 (totally transparent) and 1 (not transparent). By default, the alpha value of the colour used is 1.\nThere are three other arguments for tm_borders(): - col: border color - lwd: border line width. The default is 1. - lty: border line type. The default is “solid”.\n\n\n\n\n3.2.4 Data Classification Methods of tmap\nMost choropleth maps employ some methods of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\ntmap provides a total ten data classification methods, namely: fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, and jenks.\nWe can define a data classification method using the style argument of tm_fill() or tm_polygons().\n\n3.2.4.1 Plotting choropleth maps with built-in classification methods\n\nJenksEqualquantile\n\n\nThe following code chunk uses jenks data classification. We specify the number of classes to be broken into using n = 5 to group the observations into 5 classes.\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          n = 5, \n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\nThe following code chunk uses equal data classification. We specify the number of classes to be broken into using n = 5 to group the observations into 5 classes.\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          n = 5, \n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\nThe following code chunk uses quantile data classification. We specify the number of classes to be broken into using n = 5 to group the observations into 5 classes.\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          n = 5, \n          style = \"quantile\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n:::{.callout-note} From the above charts, we note that the distribution of different data classification methods lead to different visualisation output. For example, using the equal data classification method, it seemed that most areas had a low ratio except for one area due to it having a very dark shade of orange. In comparison, when the same data uses quantile or jenks data classification methods, the distribution seems more even.\n\n\n3.2.4.2 Plotting choropleth map with custom breaks\nFor all the built-in styles, the category breaks are computed internally. In order to override these defaults, the breakpoints can be set explicitly by means of the breaks argument to the tm_fill(). It is important to note that, in tmap the breaks include a minimum and maximum. As a result, in order to end up with n categories, n+1 elements must be specified in the breaks option (the values must be in increasing order).\nBefore we get started, it is always a good practice to get some descriptive statistics on the variable before setting the break points. Code chunk below will be used to compute and display the descriptive statistics of DEPENDENCY field.\n\n\nShow the code\nsummary(mpsz_pop2020$DEPENDENCY)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.0000  0.6519  0.7025  0.7742  0.7645 19.0000      92 \n\n\nBased on the above results, we set break point at 0.60, 0.70, 0.80, and 0.90. In addition, we also need to include a minimum and maximum, which we set at 0 and 100. Our breaks vector is thus c(0, 0.60, 0.70, 0.80, 0.90, 1.00).\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n3.2.5 Color Scheme\ntmap supports colour ramps either defined by the user or a set of predefined colour ramps from the RColorBrewer package.\n\n3.2.5.1 Using ColourBrewer Palette\nTo change the colour, we assign the preferred colour to palette argument of tm_fill() as shown in the code chunks below.\n\nUsing BluesUsing Reds\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\nTo reverse the colour shading, add a “-” prefix.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Reds\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.2.6 Map Layouts\nMap layout refers to the combination of all map elements into a cohensive map. Map elements include among others the objects to be mapped, the title, the scale bar, the compass, margins and aspects ratios. Colour settings and data classification methods covered in the previous section relate to the palette and break-points are used to affect how the map looks.\n\n3.2.6.1 Map Legend\nIn tmap, several legend options are provided to change the placement, format and appearance of the legend.In the following code chunk, we added a legend histogram using legend.hist = TRUE. adjusted the length height and width using legend.height and legend.width. We also specified the length position using legend.position.\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Greens\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n3.2.6.2 Map Style\ntmap allows a wide variety of layout settings to be changed. They can be called by using tmap_style().\nThe following map uses the classic style.\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Quantile classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.4, \n            legend.width = 0.35,)\n\n\n\n\n\n\n\n\n\n\n\n3.2.6.3 Cartographic Furniture\nBeside map style, tmap also also provides arguments to draw other map furniture such as compass, scale bar and grid lines.\nIn the code chunk below, tm_compass(), tm_scale_bar() and tm_grid() are used to add compass, scale bar and grid lines onto the choropleth map.\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.4, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\n\nTo reset to the default style, we changed the style to white.\n\n\nShow the code\ntmap_style(\"white\")\n\n\n\n\n\n3.2.7 Drawing Multiple Small Choropleth Maps\nSmall multiple maps, also referred to as facet maps, are composed of many maps arrange side-by-side, and sometimes stacked vertically. Small multiple maps enable the visualisation of how spatial relationships change with respect to another variable, such as time.\nIn tmap, small multiple maps can be plotted in three ways: - by assigning multiple values to at least one of the asthetic arguments, - by defining a group-by variable in tm_facets(), and - by creating multiple stand-alone maps with tmap_arrange().\n\n3.2.7.1 By assigning multiple values to at least one of the aesthetic arguments\nIn the following code chunk, small multiple choropleth maps are created by defining ncols in tm_fill().\n\n\nShow the code\ntm_shape(mpsz_pop2020) + \n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\",\n          palette = \"Blues\") +  \n  tm_borders(alpha = 0.5) + \n  tm_layout(main.title = \"Distribution of Young and Old\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.4, \n            legend.width = 0.35,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = TRUE)\n\n\n\n\n\n\n\n\n\nIn the following example, multiple choropleth maps are created by assigning multiple values to at least one of the aesthetic arguments. We can also define different data classification and different colour schemes by assigning multiple values.\n\n\nShow the code\ntm_shape(mpsz_pop2020) +\n  tm_polygons(c(\"DEPENDENCY\", \"AGED\"),\n              style = c(\"equal\", \"quantile\"),\n              palette = list(\"Blues\", \"Greens\")) +\n   tm_layout(main.title = \"Distribution of Young and Old\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.4, \n            legend.width = 0.35,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n3.2.7.2 By defining a group-by variable in tm_facets()\nIn the following example, multiple choropleth maps are created using tm_facets().\n\n\nShow the code\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Greens\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=FALSE) +\n  tm_layout(main.title = \"Dependency Ratio by Regions\",\n            main.title.size = 1,\n            legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\n\n\n\n3.2.7.3 By creating multiple stand-alone maps with tmap_arrange()\nIn the following code chunk. multiple choropleth maps are created by first creating multiple stand-alone maps then arranged them together with tmap_arrange(). This is akin to how we use patchwork to arrange several ggplots together.\n\n\nShow the code\nyoungmap &lt;- tm_shape(mpsz_pop2020) +\n  tm_polygons(\"YOUNG\",\n              style = \"quantile\",\n              palette = \"Reds\") +\n   tm_layout(legend.height = 0.4, \n            legend.width = 0.35,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = TRUE)\n  \n\nagedmap &lt;- tm_shape(mpsz_pop2020) +\n  tm_polygons(\"AGED\",\n              style = \"quantile\",\n              palette = \"Reds\") +\n   tm_layout(legend.height = 0.4, \n            legend.width = 0.35,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = TRUE)\n\ntmap_arrange(youngmap, agedmap, asp = 1, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n3.2.8 Mapping Spatial Object Meeting a Selection Criterion\nInstead of creating small multiple choropleth map, you can also use selection function to map spatial objects meeting the selection criterion.\nIn the following code chunk, we define the selection using tm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ]) to only plot out the Central Region and its dependency ratio.\n\n\nShow the code\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)",
    "crumbs": [
      "Hands-on Exercises",
      "Take-home Exercise 7a: Choropleth Mapping with R"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07c.html",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07c.html",
    "title": "Hands-on Exercise 7c: Analytical Mapping",
    "section": "",
    "text": "In this exercise, we will learn how to plot analytical map such as rate map, percentile map and boxmap.",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 7c: Analytical Mapping"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07c.html#installing-and-loading-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07c.html#installing-and-loading-r-packages",
    "title": "Hands-on Exercise 7c: Analytical Mapping",
    "section": "2.1 Installing and Loading R Packages",
    "text": "2.1 Installing and Loading R Packages\nFor this exercise, other than tmap, we will use the following packages:\n\ntidyverse for tidying and wrangling data\nsf for handling geospatial data\n\nThe code chunk below uses p_load() of pacman package to check if the abovementioned packages are installed in the computer. If they are, they will be launched in R. Otherwise, pacman will install the relevant packages before launching them.\n\n\nShow the code\npacman::p_load(tidyverse, tmap, sf)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 7c: Analytical Mapping"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07c.html#importing-the-data",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07c.html#importing-the-data",
    "title": "Hands-on Exercise 7c: Analytical Mapping",
    "section": "2.2 Importing the Data",
    "text": "2.2 Importing the Data\nFor the purpose of this hands-on exercise, a prepared data set called NGA_wp.rds will be used. The data set is a polygon feature data.frame providing information on water point of Nigeria at the LGA level.\nWe will use read_rds() function to import the data into R.\n\n\nShow the code\nnga &lt;- read_rds(\"data/rds/NGA_wp.rds\")\nglimpse(nga)\n\n\nRows: 774\nColumns: 9\n$ ADM2_EN          &lt;chr&gt; \"Aba North\", \"Aba South\", \"Abadam\", \"Abaji\", \"Abak\", …\n$ ADM2_PCODE       &lt;chr&gt; \"NG001001\", \"NG001002\", \"NG008001\", \"NG015001\", \"NG00…\n$ ADM1_EN          &lt;chr&gt; \"Abia\", \"Abia\", \"Borno\", \"Federal Capital Territory\",…\n$ ADM1_PCODE       &lt;chr&gt; \"NG001\", \"NG001\", \"NG008\", \"NG015\", \"NG003\", \"NG011\",…\n$ geometry         &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((548795.5 11..., MULTIPOL…\n$ total_wp         &lt;int&gt; 17, 71, 0, 57, 48, 233, 34, 119, 152, 66, 39, 135, 63…\n$ wp_functional    &lt;int&gt; 7, 29, 0, 23, 23, 82, 16, 72, 79, 18, 25, 54, 28, 55,…\n$ wp_nonfunctional &lt;int&gt; 9, 35, 0, 34, 25, 42, 15, 33, 62, 26, 13, 73, 35, 36,…\n$ wp_unknown       &lt;int&gt; 1, 7, 0, 0, 0, 109, 3, 14, 11, 22, 1, 8, 0, 37, 88, 1…",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 7c: Analytical Mapping"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07c.html#deriving-proportion-of-functional-water-points-and-non-functional-water-points",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07c.html#deriving-proportion-of-functional-water-points-and-non-functional-water-points",
    "title": "Hands-on Exercise 7c: Analytical Mapping",
    "section": "4.1 Deriving Proportion of Functional Water Points and Non-Functional Water Points",
    "text": "4.1 Deriving Proportion of Functional Water Points and Non-Functional Water Points\nWe will calculate the proportion of functional and non-functional water points in each LGA. In the following code chunk, mutate() from dplyr package is used to derive two fields, namely pct_functional and pct_nonfunctional. We also used round() to round the proportions into 2 decimal places.\n\nnga &lt;- nga %&gt;%\n  mutate(pct_functional = round(wp_functional/total_wp,2)) %&gt;%\n  mutate(pct_nonfunctional = round(wp_nonfunctional/total_wp,2))\n\nglimpse(nga)\n\nRows: 774\nColumns: 11\n$ ADM2_EN           &lt;chr&gt; \"Aba North\", \"Aba South\", \"Abadam\", \"Abaji\", \"Abak\",…\n$ ADM2_PCODE        &lt;chr&gt; \"NG001001\", \"NG001002\", \"NG008001\", \"NG015001\", \"NG0…\n$ ADM1_EN           &lt;chr&gt; \"Abia\", \"Abia\", \"Borno\", \"Federal Capital Territory\"…\n$ ADM1_PCODE        &lt;chr&gt; \"NG001\", \"NG001\", \"NG008\", \"NG015\", \"NG003\", \"NG011\"…\n$ geometry          &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((548795.5 11..., MULTIPO…\n$ total_wp          &lt;int&gt; 17, 71, 0, 57, 48, 233, 34, 119, 152, 66, 39, 135, 6…\n$ wp_functional     &lt;int&gt; 7, 29, 0, 23, 23, 82, 16, 72, 79, 18, 25, 54, 28, 55…\n$ wp_nonfunctional  &lt;int&gt; 9, 35, 0, 34, 25, 42, 15, 33, 62, 26, 13, 73, 35, 36…\n$ wp_unknown        &lt;int&gt; 1, 7, 0, 0, 0, 109, 3, 14, 11, 22, 1, 8, 0, 37, 88, …\n$ pct_functional    &lt;dbl&gt; 0.41, 0.41, NaN, 0.40, 0.48, 0.35, 0.47, 0.61, 0.52,…\n$ pct_nonfunctional &lt;dbl&gt; 0.53, 0.49, NaN, 0.60, 0.52, 0.18, 0.44, 0.28, 0.41,…",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 7c: Analytical Mapping"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07c.html#map-of-rate",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07c.html#map-of-rate",
    "title": "Hands-on Exercise 7c: Analytical Mapping",
    "section": "4.2 Map of Rate",
    "text": "4.2 Map of Rate\nNow, we will plot a rate map based on the functional water points using the pct_functional values.\n\nfunctional_rate &lt;- tm_shape(nga) +\n  tm_polygons(\"pct_functional\",\n              n = 10,\n              palette = \"Blues\",\n              style = \"equal\",\n              lwd = 0.1,\n              alpha = 1,\n              legend.hist = TRUE) +\n  tm_layout(main.title = \"Rate Map of Functional Water Points by LGAs\",\n            main.title.size = 1,\n            legend.outside = TRUE)\n\nfunctional_rate",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 7c: Analytical Mapping"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07c.html#percentile-map",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07c.html#percentile-map",
    "title": "Hands-on Exercise 7c: Analytical Mapping",
    "section": "5.1 Percentile Map",
    "text": "5.1 Percentile Map\nThe percentile map is a special type of quantile map with six specific categories: - 0-1%,1-10%, - 10-50%, - 50-90%, - 90-99%, and - 99-100%.\nThe corresponding breakpoints can be derived by means of the base R quantile command, passing an explicit vector of cumulative probabilities as c(0,.01,.1,.5,.9,.99,1). Note that the begin and endpoint need to be included.\n\n5.1.1 Data Preparation\n\n5.1.1.1 Step 1: Exclude records with NA using drop_na().\n\nnga &lt;- nga %&gt;%\n  drop_na()\n\n\n\n5.1.1.2 Step 2: Creating customised classification and extracting the values\n\npercent &lt;- c(0, 0.01, 0.1, 0.5, 0.9, 0.99, 1)\nvar &lt;- nga[\"pct_functional\"] %&gt;%\n  st_set_geometry(NULL)\n\nquantile(var[,1], percent)\n\n  0%   1%  10%  50%  90%  99% 100% \n0.00 0.00 0.22 0.48 0.86 1.00 1.00 \n\n\n\n\n\n\n\n\nNote\n\n\n\nWhen variables are extracted from an sf data.frame, the geometry is extracted as well. For mapping and spatial manipulation, this is the expected behavior, but many base R functions cannot deal with the geometry. Specifically, the quantile() would give an error. As a result st_set_geometry(NULL) is used to drop geometry field.\n\n\n\n\n5.1.1.3 Step 3: Creating get.var function\nWe will now write an R function as shown below to extract a variable (i.e. wp_nonfunctional) as a vector out of an sf data.frame.\nThe arguments are: - vname: variable name (as character, in quotes) - df: name of sf data frame\nThe return is: - v: vector with values (without a column name)\n\nget.var &lt;- function(vname, df) {\n  v &lt;- df[vname] %&gt;%\n    st_set_geometry(NULL)\n  v &lt;- unname(v[,1])\n  return(v)\n}\n\n\n\n\n\n\n\nWhy write functions?\n\n\n\nWriting a function has three big advantages over using copy-and-paste:\n\nYou can give a function an evocative name that makes your code easier to understand.\nAs requirements change, you only need to update code in one place, instead of many.\nYou eliminate the chance of making incidental mistakes when you copy and paste (i.e. updating a variable name in one place, but not in another).\n\nSource: Chapter 19: Functions of R for Data Science.\n\n\n\n\n5.1.1.4 Step 4: Percentile Mapping Function\nNext, we will write a percentile mapping function by using the following code chunk.\n\npercentmap &lt;- function(vnam, df, legtitle=NA, mtitle=\"Percentile Map\"){\n  percent &lt;- c(0,.01,.1,.5,.9,.99,1)\n  var &lt;- get.var(vnam, df)\n  bperc &lt;- quantile(var, percent)\n  tm_shape(df) +\n  tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,\n             title=legtitle,\n             breaks=bperc,\n             palette=\"Blues\",\n          labels=c(\"&lt; 1%\", \"1% - 10%\", \"10% - 50%\", \"50% - 90%\", \"90% - 99%\", \"&gt; 99%\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"right\",\"bottom\"))\n}\n\n\n\n5.1.1.5 Step 5: Testing the Function\n\npercentmap(\"total_wp\", nga)",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 7c: Analytical Mapping"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07c.html#box-map",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07c.html#box-map",
    "title": "Hands-on Exercise 7c: Analytical Mapping",
    "section": "5.2 Box Map",
    "text": "5.2 Box Map\nA box map is an augmented quartile map, with an additional lower and upper category. When there are lower outliers, then the starting point for the breaks is the minimum value, and the second break is the lower fence. In contrast, when there are no lower outliers, then the starting point for the breaks will be the lower fence, and the second break is the minimum value (there will be no observations that fall in the interval between the lower fence and the minimum value).\n\nggplot(data = nga,\n       aes(x = \"\",\n           y = wp_nonfunctional)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\nDisplaying summary statistics on a choropleth map by using the basic principles of boxplot.\nTo create a box map, a custom breaks specification will be used. However, there is a complication. The break points for the box map vary depending on whether lower or upper outliers are present.\n\n\n5.2.1 Creating the boxbreaks function\nThe code chunk below is an R function that creating break points for a box map.\nThe arguments are: - v: vector with observations - mult: multiplier for IQR (default 1.5)\nThe returns is: - bb: vector with 7 break points compute quartile and fences\n\nboxbreaks &lt;- function(v,mult=1.5) {\n  qv &lt;- unname(quantile(v))\n  iqr &lt;- qv[4] - qv[2]\n  upfence &lt;- qv[4] + mult * iqr\n  lofence &lt;- qv[2] - mult * iqr\n  # initialize break points vector\n  bb &lt;- vector(mode=\"numeric\",length=7)\n  # logic for lower and upper fences\n  if (lofence &lt; qv[1]) {  # no lower outliers\n    bb[1] &lt;- lofence\n    bb[2] &lt;- floor(qv[1])\n  } else {\n    bb[2] &lt;- lofence\n    bb[1] &lt;- qv[1]\n  }\n  if (upfence &gt; qv[5]) { # no upper outliers\n    bb[7] &lt;- upfence\n    bb[6] &lt;- ceiling(qv[5])\n  } else {\n    bb[6] &lt;- upfence\n    bb[7] &lt;- qv[5]\n  }\n  bb[3:5] &lt;- qv[2:4]\n  return(bb)\n}\n\n\n\n5.2.2 Creating the get.var function\n\nget.var &lt;- function(vname,df) {\n  v &lt;- df[vname] %&gt;% st_set_geometry(NULL)\n  v &lt;- unname(v[,1])\n  return(v)\n}\n\nLet’s test the function we have created.\n\nvar &lt;- get.var(\"wp_nonfunctional\", nga) \nboxbreaks(var)\n\n[1] -56.5   0.0  14.0  34.0  61.0 131.5 278.0\n\n\n\n\n5.2.3 Boxmap Function\nThe code chunk below is an R function to create a box map. - arguments: - vnam: variable name (as character, in quotes) - df: simple features polygon layer - legtitle: legend title - mtitle: map title - mult: multiplier for IQR - returns: - a tmap-element (plots a map)\n\nboxmap &lt;- function(vnam, df, \n                   legtitle=NA,\n                   mtitle=\"Box Map\",\n                   mult=1.5){\n  var &lt;- get.var(vnam,df)\n  bb &lt;- boxbreaks(var)\n  tm_shape(df) +\n    tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,title=legtitle,\n             breaks=bb,\n             palette=\"Blues\",\n          labels = c(\"lower outlier\", \n                     \"&lt; 25%\", \n                     \"25% - 50%\", \n                     \"50% - 75%\",\n                     \"&gt; 75%\", \n                     \"upper outlier\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"left\",\n                               \"top\"))\n}\n\nLet us now plot the boxmap using the function created.\n\ntmap_mode(\"view\")\nboxmap(\"wp_nonfunctional\", nga)\n\n\n\n\n\n\ntmap_mode(\"plot\")",
    "crumbs": [
      "Hands-on Exercises",
      "Hands-on Exercise 7c: Analytical Mapping"
    ]
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html",
    "title": "In-class Exercise 2: Horizon Plot",
    "section": "",
    "text": "First, let us ensure that the required R packages have been installed and import the relevant data for this hands-on exercise.\n\n\nFor this in-class exercise, we will be using the following packages:\n\ntidyverse\nggHoriPlot\nggthemes\n\nThe code chunk below uses p_load() of pacman package to check if the packages are installed in the computer. If they are, they will be launched in R. Otherwise, pacman will install the relevant packages before launching them.\n\npacman::p_load(tidyverse, ggHoriPlot, ggthemes)\n\n\n\n\nWe will use read_csv() to import the csv file into R and use glimpse() to check the imported data.\n\naverp &lt;- read_csv(\"data/AVERP.csv\")\nglimpse(averp)\n\nRows: 7,452\nColumns: 3\n$ Date             &lt;chr&gt; \"1/1/2014\", \"1/2/2014\", \"1/3/2014\", \"1/4/2014\", \"1/5/…\n$ `Consumer Items` &lt;chr&gt; \"Wholemeal Bread (Per 400 Gram)\", \"Wholemeal Bread (P…\n$ Values           &lt;dbl&gt; 2.05, 2.05, 2.04, 2.04, 2.05, 2.05, 2.05, 2.05, 2.04,…\n\n\nFrom the above output, we noted that the Date field was read as character data type. We will use Lubriate package to change the Date field to Date data type.\n\naverp &lt;- averp %&gt;%\n  mutate(Date = dmy(Date)) %&gt;%\n  rename(ConsumerItem = `Consumer Items`)\nglimpse(averp)\n\nRows: 7,452\nColumns: 3\n$ Date         &lt;date&gt; 2014-01-01, 2014-02-01, 2014-03-01, 2014-04-01, 2014-05-…\n$ ConsumerItem &lt;chr&gt; \"Wholemeal Bread (Per 400 Gram)\", \"Wholemeal Bread (Per 4…\n$ Values       &lt;dbl&gt; 2.05, 2.05, 2.04, 2.04, 2.05, 2.05, 2.05, 2.05, 2.04, 2.0…",
    "crumbs": [
      "In-class Exercises",
      "In-class Exercise 2: Horizon Plot"
    ]
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#installing-the-r-packages",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#installing-the-r-packages",
    "title": "In-class Exercise 2: Horizon Plot",
    "section": "",
    "text": "For this in-class exercise, we will be using the following packages:\n\ntidyverse\nggHoriPlot\nggthemes\n\nThe code chunk below uses p_load() of pacman package to check if the packages are installed in the computer. If they are, they will be launched in R. Otherwise, pacman will install the relevant packages before launching them.\n\npacman::p_load(tidyverse, ggHoriPlot, ggthemes)",
    "crumbs": [
      "In-class Exercises",
      "In-class Exercise 2: Horizon Plot"
    ]
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#importing-the-data",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#importing-the-data",
    "title": "In-class Exercise 2: Horizon Plot",
    "section": "",
    "text": "We will use read_csv() to import the csv file into R and use glimpse() to check the imported data.\n\naverp &lt;- read_csv(\"data/AVERP.csv\")\nglimpse(averp)\n\nRows: 7,452\nColumns: 3\n$ Date             &lt;chr&gt; \"1/1/2014\", \"1/2/2014\", \"1/3/2014\", \"1/4/2014\", \"1/5/…\n$ `Consumer Items` &lt;chr&gt; \"Wholemeal Bread (Per 400 Gram)\", \"Wholemeal Bread (P…\n$ Values           &lt;dbl&gt; 2.05, 2.05, 2.04, 2.04, 2.05, 2.05, 2.05, 2.05, 2.04,…\n\n\nFrom the above output, we noted that the Date field was read as character data type. We will use Lubriate package to change the Date field to Date data type.\n\naverp &lt;- averp %&gt;%\n  mutate(Date = dmy(Date)) %&gt;%\n  rename(ConsumerItem = `Consumer Items`)\nglimpse(averp)\n\nRows: 7,452\nColumns: 3\n$ Date         &lt;date&gt; 2014-01-01, 2014-02-01, 2014-03-01, 2014-04-01, 2014-05-…\n$ ConsumerItem &lt;chr&gt; \"Wholemeal Bread (Per 400 Gram)\", \"Wholemeal Bread (Per 4…\n$ Values       &lt;dbl&gt; 2.05, 2.05, 2.04, 2.04, 2.05, 2.05, 2.05, 2.05, 2.04, 2.0…",
    "crumbs": [
      "In-class Exercises",
      "In-class Exercise 2: Horizon Plot"
    ]
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html",
    "title": "Take-home Exercise 1",
    "section": "",
    "text": "Programme for International Student Assessment (PISA) is a study conducted by the Organisation for Economic Co-operation and Development (OECD) that measures 15-year-olds’ ability to use their reading, mathematics and science knowledge and skills to meet real life challenges. It was first performed in 2000 and then repeated every three years. The results of the 2022 PISA were released in December 2023.\nIn this take-home exercise, I will be using appropriate Exploratory Data Analysis (EDA) methods and ggplot2 functions to reveal:\n\nthe distribution of Singapore students’ performance in mathematics, reading and science, and\nthe relationship between Singapore students’ performance with students’ schools, gender and socioeconomic status.\n\n\n\nWe can download the PISA 2022 dataset with the full set of responses from individual students, school principals, teachers and parents from PISA 2022 Database.\nThe main data files for 2022 PISA are:\n\nstudent-questionnaire data file (which also includes estimates of student performance and parent-questionnaire data);\nschool-questionnaire data file;\nteacher-questionnaire data file;\ncognitive item data file; and\nquestionnaire timing data.\n\nThese data files are in SAS and SPSS formats. For the purpose of this exercise, I will be using thestudent-questionnaire data file only.",
    "crumbs": [
      "Take-home Exercises",
      "Take-home Exercise 1"
    ]
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#about-pisa-data-files",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#about-pisa-data-files",
    "title": "Take-home Exercise 1",
    "section": "",
    "text": "We can download the PISA 2022 dataset with the full set of responses from individual students, school principals, teachers and parents from PISA 2022 Database.\nThe main data files for 2022 PISA are:\n\nstudent-questionnaire data file (which also includes estimates of student performance and parent-questionnaire data);\nschool-questionnaire data file;\nteacher-questionnaire data file;\ncognitive item data file; and\nquestionnaire timing data.\n\nThese data files are in SAS and SPSS formats. For the purpose of this exercise, I will be using thestudent-questionnaire data file only.",
    "crumbs": [
      "Take-home Exercises",
      "Take-home Exercise 1"
    ]
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#loading-r-packages",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#loading-r-packages",
    "title": "Take-home Exercise 1",
    "section": "2.1 Loading R packages",
    "text": "2.1 Loading R packages\nFor this exercise, we will be using the following packages:\n\ntidyverse : to load the core tidyverse packages, which includes ggplot2 and dplyr.\nhaven : to read and write various data formats used by other statistical packages by wrapping the ReadStat C library. It is part of the tidyverse family too! haven currently supports SAS, SPSS and Stata. We will need haven to import the PISA 2022’s student questionnaire data file because it is in SAS file type.\npatchwork: to create composition of ggplot2 plots using arithmetic operators.\nggrepel: to repel overlapping text labels away from each other.\nggdist: provides stats and geoms for visualising distributions and uncertainty.\nggridges: provides geoms to plot ridgeline plots, which are partially overlapping line plots that create the impression of a mountain range.\nknitr: provides a general-purpose tool for dynamic report generation in R. We will use this to mainly help us generate simple tables.\n\nThe code chunk below uses p_load() of pacman package to check if the abovementioned packages are installed in the computer. If they are, they will be launched in R. Otherwise, pacman will install the relevant packages before launching them.\n\n\nShow the code\npacman::p_load(tidyverse, haven, patchwork, ggrepel, ggdist, ggridges, knitr)",
    "crumbs": [
      "Take-home Exercises",
      "Take-home Exercise 1"
    ]
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#importing-the-data-into-r",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#importing-the-data-into-r",
    "title": "Take-home Exercise 1",
    "section": "2.2 Importing the Data into R",
    "text": "2.2 Importing the Data into R\nFor this exercise, we are using PISA 2022 database’s student questionnaire data file. As the data file is in SAS file format, we will use haven’s read_sas() function to import the data into R environment. Then we will filter the data to those data from Singapore since this exercise is only focusing on responses from Singapore students.\n\nImport the Data (All Countries)Filter the Imported Data (Singapore only)\n\n\n\nstu_qqq &lt;- read_sas(\"data/cy08msp_stu_qqq.sas7bdat\")\n\n\n\n\nstu_qqq_SG &lt;- stu_qqq %&gt;% \n  filter(CNT == \"SGP\")",
    "crumbs": [
      "Take-home Exercises",
      "Take-home Exercise 1"
    ]
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#saving-stu_qqq_sg-into-rds-format",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#saving-stu_qqq_sg-into-rds-format",
    "title": "Take-home Exercise 1",
    "section": "2.3 Saving stu_qqq_SG into RDS format",
    "text": "2.3 Saving stu_qqq_SG into RDS format\nLet us save the filtered data into an R data format (RDS) so that we can easily retrieve in future without importing the stu_qqq dataset again because the entire stu_qqq file is very big (more than 3GB)!\n\n\nShow the code\nwrite_rds(stu_qqq_SG, \"data/stu_qqq_SG.rds\")\n\n\nWe will read the stu_qqq_SG.rdsusing the following code chunk.\n\n\nShow the code\nstu_qqq_SG &lt;- read_rds(\"data/stu_qqq_SG.rds\")",
    "crumbs": [
      "Take-home Exercises",
      "Take-home Exercise 1"
    ]
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#filtering-the-columns-for-this-exercise",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#filtering-the-columns-for-this-exercise",
    "title": "Take-home Exercise 1",
    "section": "3.1 Filtering the columns for this exercise",
    "text": "3.1 Filtering the columns for this exercise\nThere are a lot of columns in stu_qqq_SG so let us filter out the columns in stu_qqq_SG that we want to explore for this exercise. We selected columns that were related to the students demographics (such as school, gender, age), indices related to socio-economic status and the columns containing PV values for Maths, Reading and Science.\n\n\nShow the code\nstu2 &lt;- stu_qqq_SG %&gt;% \n  select(CNTSTUID,CNTSCHID, ST004D01T, PROGN, AGE, ST003D02T, ST003D03T, ESCS, PAREDINT, HISEI, HOMEPOS,MISCED,FISCED,HISCED,LEARRES, SCHSUST,FAMSUP, WORKHOME,WORKPAY, STUDYHMW, EXERPRAC, LANGN, PQSCHOOL, ICTOUT, ICTAVHOM,  ICTHOME, ICTAVSCH, ICTSCH, ICTRES, COBN_S,COBN_M, COBN_F, OCOD1, OCOD2, PA042Q01TA, PA194Q01JA, PA195Q01JA, PA041Q01TA,  ends_with(\"MATH\"), ends_with(\"READ\"), ends_with(\"SCIE\"), )\n\nglimpse(stu2)\n\n\nRows: 6,606\nColumns: 68\n$ CNTSTUID   &lt;dbl&gt; 70200001, 70200002, 70200003, 70200004, 70200005, 70200006,…\n$ CNTSCHID   &lt;dbl&gt; 70200052, 70200134, 70200112, 70200004, 70200152, 70200043,…\n$ ST004D01T  &lt;dbl&gt; 1, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 1, 1, 2, 1, 2, 1, 2, 1, 1,…\n$ PROGN      &lt;chr&gt; \"07020002\", \"07020002\", \"07020002\", \"07020002\", \"07020002\",…\n$ AGE        &lt;dbl&gt; 15.50, 15.83, 15.75, 16.17, 15.58, 15.58, 16.08, 16.00, 15.…\n$ ST003D02T  &lt;dbl&gt; 10, 6, 7, 2, 9, 9, 3, 4, 8, 6, 10, 7, 9, 11, 5, 10, 11, 4, …\n$ ST003D03T  &lt;dbl&gt; 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006,…\n$ ESCS       &lt;dbl&gt; 0.1836, 0.8261, -1.0357, -0.9606, 0.0856, 0.1268, -0.0154, …\n$ PAREDINT   &lt;dbl&gt; 16.0, 14.5, 12.0, 12.0, 14.5, 16.0, 12.0, 16.0, 16.0, 16.0,…\n$ HISEI      &lt;dbl&gt; 30.34, 77.10, 17.00, 43.33, 75.54, 57.64, 70.34, 80.78, 65.…\n$ HOMEPOS    &lt;dbl&gt; 0.7524, 0.7842, 0.0666, -0.9300, -0.8949, -0.5988, 0.0975, …\n$ MISCED     &lt;dbl&gt; 8, 7, 4, 6, 7, 7, 6, 9, 8, 8, 4, 9, 10, 6, 4, 9, 8, 8, 6, 6…\n$ FISCED     &lt;dbl&gt; 7, 7, 4, 6, 7, 9, 2, 8, 8, 7, 4, 9, 10, 9, 6, 4, 9, 8, 6, 7…\n$ HISCED     &lt;dbl&gt; 8, 7, 4, 6, 7, 9, 6, 9, 8, 8, 4, 9, 10, 9, 6, 9, 9, 8, 6, 7…\n$ LEARRES    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ SCHSUST    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ FAMSUP     &lt;dbl&gt; -0.3780, -0.5969, -1.0537, -0.8521, 1.7459, 1.7327, -0.8815…\n$ WORKHOME   &lt;dbl&gt; 10, 2, 0, 10, 5, 5, 7, 0, 0, 4, 2, 2, 10, 0, 10, 0, 5, 5, 0…\n$ WORKPAY    &lt;dbl&gt; 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ STUDYHMW   &lt;dbl&gt; 4, 7, 3, 5, 7, 10, 0, 10, 5, 3, 5, 5, 10, 0, 8, 5, 5, 5, 10…\n$ EXERPRAC   &lt;dbl&gt; 1, 4, 2, 5, 9, 1, 2, 0, 3, 5, 1, 2, 5, 2, 4, 0, 2, 3, 2, 2,…\n$ LANGN      &lt;dbl&gt; 998, 998, 998, 998, 998, 998, 998, 998, 998, 998, 998, 998,…\n$ PQSCHOOL   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ ICTOUT     &lt;dbl&gt; 0.2260, -0.8080, 0.1088, -1.2894, 2.9804, 0.3464, -0.4834, …\n$ ICTAVHOM   &lt;dbl&gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6, 6, 6, 5, 6, 6, 6, 6,…\n$ ICTHOME    &lt;dbl&gt; 0.3346, 0.3346, 0.3346, 0.3346, 0.3346, 0.3346, 0.3346, 0.3…\n$ ICTAVSCH   &lt;dbl&gt; 7, 7, 7, 7, 5, 6, 7, 6, 7, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,…\n$ ICTSCH     &lt;dbl&gt; 0.4062, 0.4062, 0.4062, 0.4062, -1.6647, -0.8411, 0.4062, -…\n$ ICTRES     &lt;dbl&gt; 0.1940, 0.6249, -0.3987, -0.9028, 0.2514, -0.4733, 0.9904, …\n$ COBN_S     &lt;chr&gt; \"070200\", \"070200\", \"070200\", \"070200\", \"070200\", \"970200\",…\n$ COBN_M     &lt;chr&gt; \"070200\", \"070200\", \"970200\", \"070200\", \"070200\", \"970200\",…\n$ COBN_F     &lt;chr&gt; \"070200\", \"070200\", \"070200\", \"070200\", \"070200\", \"970200\",…\n$ OCOD1      &lt;chr&gt; \"9701\", \"31\", \"9701\", \"41\", \"23\", \"9701\", \"11\", \"23\", \"1\", …\n$ OCOD2      &lt;chr&gt; \"83\", \"21\", \"9704\", \"9705\", \"83\", \"34\", \"31\", \"21\", \"14\", \"…\n$ PA042Q01TA &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ PA194Q01JA &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ PA195Q01JA &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ PA041Q01TA &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ PV1MATH    &lt;dbl&gt; 639.004, 697.191, 693.710, 427.317, 436.462, 569.982, 771.6…\n$ PV2MATH    &lt;dbl&gt; 601.251, 754.277, 654.450, 410.376, 453.450, 539.609, 672.8…\n$ PV3MATH    &lt;dbl&gt; 621.480, 671.940, 696.938, 423.586, 392.315, 531.648, 653.7…\n$ PV4MATH    &lt;dbl&gt; 631.596, 657.300, 646.187, 388.935, 439.986, 534.368, 734.8…\n$ PV5MATH    &lt;dbl&gt; 579.276, 621.126, 678.119, 330.962, 443.125, 465.815, 727.5…\n$ PV6MATH    &lt;dbl&gt; 591.791, 655.729, 644.019, 379.988, 452.648, 528.509, 729.8…\n$ PV7MATH    &lt;dbl&gt; 600.709, 747.934, 720.531, 398.535, 396.970, 514.326, 597.2…\n$ PV8MATH    &lt;dbl&gt; 587.322, 694.365, 671.425, 422.127, 459.945, 521.029, 772.2…\n$ PV9MATH    &lt;dbl&gt; 618.131, 742.732, 694.085, 375.354, 438.166, 472.382, 694.3…\n$ PV10MATH   &lt;dbl&gt; 581.973, 656.934, 668.304, 453.348, 448.084, 503.387, 725.2…\n$ PV1READ    &lt;dbl&gt; 676.298, 625.585, 620.116, 381.495, 448.199, 469.441, 744.5…\n$ PV2READ    &lt;dbl&gt; 692.247, 686.716, 559.078, 400.815, 560.636, 500.350, 679.8…\n$ PV3READ    &lt;dbl&gt; 690.981, 663.147, 554.767, 374.911, 365.478, 375.703, 635.1…\n$ PV4READ    &lt;dbl&gt; 643.067, 567.435, 587.026, 367.484, 469.970, 377.452, 725.5…\n$ PV5READ    &lt;dbl&gt; 627.908, 614.500, 591.806, 336.009, 503.664, 470.781, 731.1…\n$ PV6READ    &lt;dbl&gt; 684.676, 604.745, 570.547, 324.630, 481.215, 415.448, 684.6…\n$ PV7READ    &lt;dbl&gt; 661.380, 669.375, 599.078, 396.242, 436.800, 448.547, 646.0…\n$ PV8READ    &lt;dbl&gt; 674.070, 623.735, 545.610, 374.723, 531.226, 434.381, 756.8…\n$ PV9READ    &lt;dbl&gt; 666.282, 649.579, 610.466, 314.704, 480.997, 411.703, 653.7…\n$ PV10READ   &lt;dbl&gt; 657.387, 571.261, 590.758, 342.956, 478.578, 410.846, 784.7…\n$ PV1SCIE    &lt;dbl&gt; 710.634, 670.646, 666.095, 340.308, 456.333, 475.158, 693.8…\n$ PV2SCIE    &lt;dbl&gt; 618.739, 748.839, 604.771, 329.889, 453.400, 470.030, 626.9…\n$ PV3SCIE    &lt;dbl&gt; 591.623, 635.443, 704.217, 411.353, 498.937, 461.218, 627.3…\n$ PV4SCIE    &lt;dbl&gt; 659.770, 639.735, 687.659, 327.974, 532.324, 504.199, 676.7…\n$ PV5SCIE    &lt;dbl&gt; 635.892, 608.385, 690.974, 292.183, 508.231, 486.930, 661.8…\n$ PV6SCIE    &lt;dbl&gt; 646.901, 670.662, 617.175, 355.423, 504.461, 493.011, 618.3…\n$ PV7SCIE    &lt;dbl&gt; 603.569, 734.807, 692.886, 400.182, 404.572, 469.950, 602.0…\n$ PV8SCIE    &lt;dbl&gt; 621.352, 639.748, 630.900, 317.518, 549.457, 464.012, 653.9…\n$ PV9SCIE    &lt;dbl&gt; 659.674, 716.768, 656.620, 298.893, 411.062, 440.113, 645.4…\n$ PV10SCIE   &lt;dbl&gt; 649.719, 655.670, 649.087, 362.702, 473.613, 495.410, 662.5…\n\n\nFrom the above output, we see that there are some rows with missing values. As PISA is an international survey, there could be some columns (i.e. questions) which are not applicable to Singapore and the entire column would be NA. Let us find out which are the columns that have the entire column filled with NA values in stu2 dataframe using the following code chunk.\n\n\nShow the code\nnames(which(colSums(is.na(stu2))== nrow(stu2)))\n\n\n[1] \"LEARRES\"    \"SCHSUST\"    \"PQSCHOOL\"   \"PA042Q01TA\" \"PA194Q01JA\"\n[6] \"PA195Q01JA\" \"PA041Q01TA\"\n\n\nFrom the above output, we see that there are 7 columns which contain all NA values. A quick check with the codebook showed these columns are:\n\nLEARRES: Types of learning resources used while school was closed (WLE)\nSCHSUST: School actions/activities to sustain learning (WLE)\nPQSCHOOL: School quality (WLE)\nPA042Q01TA: What is your annual household income?\nPA194Q01JA: How many [digital devices] with screens are there in your home?\nPA195Q01JA: How many books are there in your home?\nPA041Q01TA: In the last twelve months, about how much would you have paid to educational providers for services?\n\nLet us remove these columns from the stu2 dataframe. From the output we see that the number of columns dropped from 68 to 61, which is slightly more manageable than the earlier dataframe. The number of rows remained the same.\n\n\nShow the code\nstu2 &lt;- stu2[, (colSums(is.na(stu2))&lt;nrow(stu2))]\nglimpse(stu2)\n\n\nRows: 6,606\nColumns: 61\n$ CNTSTUID  &lt;dbl&gt; 70200001, 70200002, 70200003, 70200004, 70200005, 70200006, …\n$ CNTSCHID  &lt;dbl&gt; 70200052, 70200134, 70200112, 70200004, 70200152, 70200043, …\n$ ST004D01T &lt;dbl&gt; 1, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 1, 1, 2, 1, 2, 1, 2, 1, 1, …\n$ PROGN     &lt;chr&gt; \"07020002\", \"07020002\", \"07020002\", \"07020002\", \"07020002\", …\n$ AGE       &lt;dbl&gt; 15.50, 15.83, 15.75, 16.17, 15.58, 15.58, 16.08, 16.00, 15.6…\n$ ST003D02T &lt;dbl&gt; 10, 6, 7, 2, 9, 9, 3, 4, 8, 6, 10, 7, 9, 11, 5, 10, 11, 4, 7…\n$ ST003D03T &lt;dbl&gt; 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, …\n$ ESCS      &lt;dbl&gt; 0.1836, 0.8261, -1.0357, -0.9606, 0.0856, 0.1268, -0.0154, 1…\n$ PAREDINT  &lt;dbl&gt; 16.0, 14.5, 12.0, 12.0, 14.5, 16.0, 12.0, 16.0, 16.0, 16.0, …\n$ HISEI     &lt;dbl&gt; 30.34, 77.10, 17.00, 43.33, 75.54, 57.64, 70.34, 80.78, 65.1…\n$ HOMEPOS   &lt;dbl&gt; 0.7524, 0.7842, 0.0666, -0.9300, -0.8949, -0.5988, 0.0975, 0…\n$ MISCED    &lt;dbl&gt; 8, 7, 4, 6, 7, 7, 6, 9, 8, 8, 4, 9, 10, 6, 4, 9, 8, 8, 6, 6,…\n$ FISCED    &lt;dbl&gt; 7, 7, 4, 6, 7, 9, 2, 8, 8, 7, 4, 9, 10, 9, 6, 4, 9, 8, 6, 7,…\n$ HISCED    &lt;dbl&gt; 8, 7, 4, 6, 7, 9, 6, 9, 8, 8, 4, 9, 10, 9, 6, 9, 9, 8, 6, 7,…\n$ FAMSUP    &lt;dbl&gt; -0.3780, -0.5969, -1.0537, -0.8521, 1.7459, 1.7327, -0.8815,…\n$ WORKHOME  &lt;dbl&gt; 10, 2, 0, 10, 5, 5, 7, 0, 0, 4, 2, 2, 10, 0, 10, 0, 5, 5, 0,…\n$ WORKPAY   &lt;dbl&gt; 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ STUDYHMW  &lt;dbl&gt; 4, 7, 3, 5, 7, 10, 0, 10, 5, 3, 5, 5, 10, 0, 8, 5, 5, 5, 10,…\n$ EXERPRAC  &lt;dbl&gt; 1, 4, 2, 5, 9, 1, 2, 0, 3, 5, 1, 2, 5, 2, 4, 0, 2, 3, 2, 2, …\n$ LANGN     &lt;dbl&gt; 998, 998, 998, 998, 998, 998, 998, 998, 998, 998, 998, 998, …\n$ ICTOUT    &lt;dbl&gt; 0.2260, -0.8080, 0.1088, -1.2894, 2.9804, 0.3464, -0.4834, 2…\n$ ICTAVHOM  &lt;dbl&gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6, 6, 6, 5, 6, 6, 6, 6, …\n$ ICTHOME   &lt;dbl&gt; 0.3346, 0.3346, 0.3346, 0.3346, 0.3346, 0.3346, 0.3346, 0.33…\n$ ICTAVSCH  &lt;dbl&gt; 7, 7, 7, 7, 5, 6, 7, 6, 7, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, …\n$ ICTSCH    &lt;dbl&gt; 0.4062, 0.4062, 0.4062, 0.4062, -1.6647, -0.8411, 0.4062, -0…\n$ ICTRES    &lt;dbl&gt; 0.1940, 0.6249, -0.3987, -0.9028, 0.2514, -0.4733, 0.9904, 1…\n$ COBN_S    &lt;chr&gt; \"070200\", \"070200\", \"070200\", \"070200\", \"070200\", \"970200\", …\n$ COBN_M    &lt;chr&gt; \"070200\", \"070200\", \"970200\", \"070200\", \"070200\", \"970200\", …\n$ COBN_F    &lt;chr&gt; \"070200\", \"070200\", \"070200\", \"070200\", \"070200\", \"970200\", …\n$ OCOD1     &lt;chr&gt; \"9701\", \"31\", \"9701\", \"41\", \"23\", \"9701\", \"11\", \"23\", \"1\", \"…\n$ OCOD2     &lt;chr&gt; \"83\", \"21\", \"9704\", \"9705\", \"83\", \"34\", \"31\", \"21\", \"14\", \"3…\n$ PV1MATH   &lt;dbl&gt; 639.004, 697.191, 693.710, 427.317, 436.462, 569.982, 771.62…\n$ PV2MATH   &lt;dbl&gt; 601.251, 754.277, 654.450, 410.376, 453.450, 539.609, 672.81…\n$ PV3MATH   &lt;dbl&gt; 621.480, 671.940, 696.938, 423.586, 392.315, 531.648, 653.74…\n$ PV4MATH   &lt;dbl&gt; 631.596, 657.300, 646.187, 388.935, 439.986, 534.368, 734.81…\n$ PV5MATH   &lt;dbl&gt; 579.276, 621.126, 678.119, 330.962, 443.125, 465.815, 727.52…\n$ PV6MATH   &lt;dbl&gt; 591.791, 655.729, 644.019, 379.988, 452.648, 528.509, 729.83…\n$ PV7MATH   &lt;dbl&gt; 600.709, 747.934, 720.531, 398.535, 396.970, 514.326, 597.22…\n$ PV8MATH   &lt;dbl&gt; 587.322, 694.365, 671.425, 422.127, 459.945, 521.029, 772.27…\n$ PV9MATH   &lt;dbl&gt; 618.131, 742.732, 694.085, 375.354, 438.166, 472.382, 694.39…\n$ PV10MATH  &lt;dbl&gt; 581.973, 656.934, 668.304, 453.348, 448.084, 503.387, 725.29…\n$ PV1READ   &lt;dbl&gt; 676.298, 625.585, 620.116, 381.495, 448.199, 469.441, 744.53…\n$ PV2READ   &lt;dbl&gt; 692.247, 686.716, 559.078, 400.815, 560.636, 500.350, 679.85…\n$ PV3READ   &lt;dbl&gt; 690.981, 663.147, 554.767, 374.911, 365.478, 375.703, 635.12…\n$ PV4READ   &lt;dbl&gt; 643.067, 567.435, 587.026, 367.484, 469.970, 377.452, 725.55…\n$ PV5READ   &lt;dbl&gt; 627.908, 614.500, 591.806, 336.009, 503.664, 470.781, 731.16…\n$ PV6READ   &lt;dbl&gt; 684.676, 604.745, 570.547, 324.630, 481.215, 415.448, 684.68…\n$ PV7READ   &lt;dbl&gt; 661.380, 669.375, 599.078, 396.242, 436.800, 448.547, 646.02…\n$ PV8READ   &lt;dbl&gt; 674.070, 623.735, 545.610, 374.723, 531.226, 434.381, 756.80…\n$ PV9READ   &lt;dbl&gt; 666.282, 649.579, 610.466, 314.704, 480.997, 411.703, 653.76…\n$ PV10READ  &lt;dbl&gt; 657.387, 571.261, 590.758, 342.956, 478.578, 410.846, 784.71…\n$ PV1SCIE   &lt;dbl&gt; 710.634, 670.646, 666.095, 340.308, 456.333, 475.158, 693.80…\n$ PV2SCIE   &lt;dbl&gt; 618.739, 748.839, 604.771, 329.889, 453.400, 470.030, 626.98…\n$ PV3SCIE   &lt;dbl&gt; 591.623, 635.443, 704.217, 411.353, 498.937, 461.218, 627.38…\n$ PV4SCIE   &lt;dbl&gt; 659.770, 639.735, 687.659, 327.974, 532.324, 504.199, 676.79…\n$ PV5SCIE   &lt;dbl&gt; 635.892, 608.385, 690.974, 292.183, 508.231, 486.930, 661.84…\n$ PV6SCIE   &lt;dbl&gt; 646.901, 670.662, 617.175, 355.423, 504.461, 493.011, 618.39…\n$ PV7SCIE   &lt;dbl&gt; 603.569, 734.807, 692.886, 400.182, 404.572, 469.950, 602.07…\n$ PV8SCIE   &lt;dbl&gt; 621.352, 639.748, 630.900, 317.518, 549.457, 464.012, 653.97…\n$ PV9SCIE   &lt;dbl&gt; 659.674, 716.768, 656.620, 298.893, 411.062, 440.113, 645.47…\n$ PV10SCIE  &lt;dbl&gt; 649.719, 655.670, 649.087, 362.702, 473.613, 495.410, 662.55…",
    "crumbs": [
      "Take-home Exercises",
      "Take-home Exercise 1"
    ]
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#checking-for-missing-values",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#checking-for-missing-values",
    "title": "Take-home Exercise 1",
    "section": "3.2 Checking for missing values",
    "text": "3.2 Checking for missing values\nWe use the following code chunk to check for missing value(s) and drop rows where there are missing value(s).\n\nColumns with Missing Value(s)Dropping Rows with Missing Value(s)\n\n\n\nmissingv &lt;- stu2 %&gt;% \n  map(is.na) %&gt;%\n  map(sum) \n  \nmissingv &lt;- as_tibble(missingv)\n\nkable(missingv)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCNTSTUID\nCNTSCHID\nST004D01T\nPROGN\nAGE\nST003D02T\nST003D03T\nESCS\nPAREDINT\nHISEI\nHOMEPOS\nMISCED\nFISCED\nHISCED\nFAMSUP\nWORKHOME\nWORKPAY\nSTUDYHMW\nEXERPRAC\nLANGN\nICTOUT\nICTAVHOM\nICTHOME\nICTAVSCH\nICTSCH\nICTRES\nCOBN_S\nCOBN_M\nCOBN_F\nOCOD1\nOCOD2\nPV1MATH\nPV2MATH\nPV3MATH\nPV4MATH\nPV5MATH\nPV6MATH\nPV7MATH\nPV8MATH\nPV9MATH\nPV10MATH\nPV1READ\nPV2READ\nPV3READ\nPV4READ\nPV5READ\nPV6READ\nPV7READ\nPV8READ\nPV9READ\nPV10READ\nPV1SCIE\nPV2SCIE\nPV3SCIE\nPV4SCIE\nPV5SCIE\nPV6SCIE\nPV7SCIE\nPV8SCIE\nPV9SCIE\nPV10SCIE\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n47\n57\n310\n40\n74\n97\n57\n105\n51\n51\n46\n47\n0\n157\n131\n138\n119\n131\n40\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\nFor the purpose of this exercise, we will use the following code chunk to drop rows with missing value(s).\n\nstu2 &lt;- stu2 %&gt;%\n  drop_na()\n\nglimpse(stu2)\n\nRows: 6,094\nColumns: 61\n$ CNTSTUID  &lt;dbl&gt; 70200001, 70200002, 70200003, 70200004, 70200005, 70200006, …\n$ CNTSCHID  &lt;dbl&gt; 70200052, 70200134, 70200112, 70200004, 70200152, 70200043, …\n$ ST004D01T &lt;dbl&gt; 1, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 1, 1, 2, 1, 2, 1, 2, 1, 1, …\n$ PROGN     &lt;chr&gt; \"07020002\", \"07020002\", \"07020002\", \"07020002\", \"07020002\", …\n$ AGE       &lt;dbl&gt; 15.50, 15.83, 15.75, 16.17, 15.58, 15.58, 16.08, 16.00, 15.6…\n$ ST003D02T &lt;dbl&gt; 10, 6, 7, 2, 9, 9, 3, 4, 8, 6, 10, 7, 9, 11, 5, 10, 11, 4, 7…\n$ ST003D03T &lt;dbl&gt; 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, …\n$ ESCS      &lt;dbl&gt; 0.1836, 0.8261, -1.0357, -0.9606, 0.0856, 0.1268, -0.0154, 1…\n$ PAREDINT  &lt;dbl&gt; 16.0, 14.5, 12.0, 12.0, 14.5, 16.0, 12.0, 16.0, 16.0, 16.0, …\n$ HISEI     &lt;dbl&gt; 30.34, 77.10, 17.00, 43.33, 75.54, 57.64, 70.34, 80.78, 65.1…\n$ HOMEPOS   &lt;dbl&gt; 0.7524, 0.7842, 0.0666, -0.9300, -0.8949, -0.5988, 0.0975, 0…\n$ MISCED    &lt;dbl&gt; 8, 7, 4, 6, 7, 7, 6, 9, 8, 8, 4, 9, 10, 6, 4, 9, 8, 8, 6, 6,…\n$ FISCED    &lt;dbl&gt; 7, 7, 4, 6, 7, 9, 2, 8, 8, 7, 4, 9, 10, 9, 6, 4, 9, 8, 6, 7,…\n$ HISCED    &lt;dbl&gt; 8, 7, 4, 6, 7, 9, 6, 9, 8, 8, 4, 9, 10, 9, 6, 9, 9, 8, 6, 7,…\n$ FAMSUP    &lt;dbl&gt; -0.3780, -0.5969, -1.0537, -0.8521, 1.7459, 1.7327, -0.8815,…\n$ WORKHOME  &lt;dbl&gt; 10, 2, 0, 10, 5, 5, 7, 0, 0, 4, 2, 2, 10, 0, 10, 0, 5, 5, 0,…\n$ WORKPAY   &lt;dbl&gt; 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ STUDYHMW  &lt;dbl&gt; 4, 7, 3, 5, 7, 10, 0, 10, 5, 3, 5, 5, 10, 0, 8, 5, 5, 5, 10,…\n$ EXERPRAC  &lt;dbl&gt; 1, 4, 2, 5, 9, 1, 2, 0, 3, 5, 1, 2, 5, 2, 4, 0, 2, 3, 2, 2, …\n$ LANGN     &lt;dbl&gt; 998, 998, 998, 998, 998, 998, 998, 998, 998, 998, 998, 998, …\n$ ICTOUT    &lt;dbl&gt; 0.2260, -0.8080, 0.1088, -1.2894, 2.9804, 0.3464, -0.4834, 2…\n$ ICTAVHOM  &lt;dbl&gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6, 6, 6, 5, 6, 6, 6, 6, …\n$ ICTHOME   &lt;dbl&gt; 0.3346, 0.3346, 0.3346, 0.3346, 0.3346, 0.3346, 0.3346, 0.33…\n$ ICTAVSCH  &lt;dbl&gt; 7, 7, 7, 7, 5, 6, 7, 6, 7, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, …\n$ ICTSCH    &lt;dbl&gt; 0.4062, 0.4062, 0.4062, 0.4062, -1.6647, -0.8411, 0.4062, -0…\n$ ICTRES    &lt;dbl&gt; 0.1940, 0.6249, -0.3987, -0.9028, 0.2514, -0.4733, 0.9904, 1…\n$ COBN_S    &lt;chr&gt; \"070200\", \"070200\", \"070200\", \"070200\", \"070200\", \"970200\", …\n$ COBN_M    &lt;chr&gt; \"070200\", \"070200\", \"970200\", \"070200\", \"070200\", \"970200\", …\n$ COBN_F    &lt;chr&gt; \"070200\", \"070200\", \"070200\", \"070200\", \"070200\", \"970200\", …\n$ OCOD1     &lt;chr&gt; \"9701\", \"31\", \"9701\", \"41\", \"23\", \"9701\", \"11\", \"23\", \"1\", \"…\n$ OCOD2     &lt;chr&gt; \"83\", \"21\", \"9704\", \"9705\", \"83\", \"34\", \"31\", \"21\", \"14\", \"3…\n$ PV1MATH   &lt;dbl&gt; 639.004, 697.191, 693.710, 427.317, 436.462, 569.982, 771.62…\n$ PV2MATH   &lt;dbl&gt; 601.251, 754.277, 654.450, 410.376, 453.450, 539.609, 672.81…\n$ PV3MATH   &lt;dbl&gt; 621.480, 671.940, 696.938, 423.586, 392.315, 531.648, 653.74…\n$ PV4MATH   &lt;dbl&gt; 631.596, 657.300, 646.187, 388.935, 439.986, 534.368, 734.81…\n$ PV5MATH   &lt;dbl&gt; 579.276, 621.126, 678.119, 330.962, 443.125, 465.815, 727.52…\n$ PV6MATH   &lt;dbl&gt; 591.791, 655.729, 644.019, 379.988, 452.648, 528.509, 729.83…\n$ PV7MATH   &lt;dbl&gt; 600.709, 747.934, 720.531, 398.535, 396.970, 514.326, 597.22…\n$ PV8MATH   &lt;dbl&gt; 587.322, 694.365, 671.425, 422.127, 459.945, 521.029, 772.27…\n$ PV9MATH   &lt;dbl&gt; 618.131, 742.732, 694.085, 375.354, 438.166, 472.382, 694.39…\n$ PV10MATH  &lt;dbl&gt; 581.973, 656.934, 668.304, 453.348, 448.084, 503.387, 725.29…\n$ PV1READ   &lt;dbl&gt; 676.298, 625.585, 620.116, 381.495, 448.199, 469.441, 744.53…\n$ PV2READ   &lt;dbl&gt; 692.247, 686.716, 559.078, 400.815, 560.636, 500.350, 679.85…\n$ PV3READ   &lt;dbl&gt; 690.981, 663.147, 554.767, 374.911, 365.478, 375.703, 635.12…\n$ PV4READ   &lt;dbl&gt; 643.067, 567.435, 587.026, 367.484, 469.970, 377.452, 725.55…\n$ PV5READ   &lt;dbl&gt; 627.908, 614.500, 591.806, 336.009, 503.664, 470.781, 731.16…\n$ PV6READ   &lt;dbl&gt; 684.676, 604.745, 570.547, 324.630, 481.215, 415.448, 684.68…\n$ PV7READ   &lt;dbl&gt; 661.380, 669.375, 599.078, 396.242, 436.800, 448.547, 646.02…\n$ PV8READ   &lt;dbl&gt; 674.070, 623.735, 545.610, 374.723, 531.226, 434.381, 756.80…\n$ PV9READ   &lt;dbl&gt; 666.282, 649.579, 610.466, 314.704, 480.997, 411.703, 653.76…\n$ PV10READ  &lt;dbl&gt; 657.387, 571.261, 590.758, 342.956, 478.578, 410.846, 784.71…\n$ PV1SCIE   &lt;dbl&gt; 710.634, 670.646, 666.095, 340.308, 456.333, 475.158, 693.80…\n$ PV2SCIE   &lt;dbl&gt; 618.739, 748.839, 604.771, 329.889, 453.400, 470.030, 626.98…\n$ PV3SCIE   &lt;dbl&gt; 591.623, 635.443, 704.217, 411.353, 498.937, 461.218, 627.38…\n$ PV4SCIE   &lt;dbl&gt; 659.770, 639.735, 687.659, 327.974, 532.324, 504.199, 676.79…\n$ PV5SCIE   &lt;dbl&gt; 635.892, 608.385, 690.974, 292.183, 508.231, 486.930, 661.84…\n$ PV6SCIE   &lt;dbl&gt; 646.901, 670.662, 617.175, 355.423, 504.461, 493.011, 618.39…\n$ PV7SCIE   &lt;dbl&gt; 603.569, 734.807, 692.886, 400.182, 404.572, 469.950, 602.07…\n$ PV8SCIE   &lt;dbl&gt; 621.352, 639.748, 630.900, 317.518, 549.457, 464.012, 653.97…\n$ PV9SCIE   &lt;dbl&gt; 659.674, 716.768, 656.620, 298.893, 411.062, 440.113, 645.47…\n$ PV10SCIE  &lt;dbl&gt; 649.719, 655.670, 649.087, 362.702, 473.613, 495.410, 662.55…\n\n\n\n\n\nAfter dropping the rows with missing value(s), we saw that the number of rows dropped from 6,606 to 6,094.",
    "crumbs": [
      "Take-home Exercises",
      "Take-home Exercise 1"
    ]
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#checking-data-type",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#checking-data-type",
    "title": "Take-home Exercise 1",
    "section": "3.3 Checking data type",
    "text": "3.3 Checking data type\nFrom the above output, we also noted that some variables are in character data type. Since these columns contain levels (e.g. survey scales) or they actually take on a limited number of different values (e.g. student ID, school ID), we convert them into factor data type using as.factor().\n\n\nShow the code\nstu2 &lt;- stu2 %&gt;%\n  mutate_at(c('CNTSTUID', 'CNTSCHID', 'ST004D01T', 'PROGN','ST003D02T','ST003D03T','MISCED','FISCED','HISCED', \n              'WORKHOME','WORKPAY', 'STUDYHMW','EXERPRAC','LANGN','ICTAVHOM','ICTHOME','ICTAVSCH','ICTSCH',\n              'COBN_S','COBN_M','COBN_F','OCOD1','OCOD2'), as.factor) \n\nglimpse(stu2)\n\n\nRows: 6,094\nColumns: 61\n$ CNTSTUID  &lt;fct&gt; 70200001, 70200002, 70200003, 70200004, 70200005, 70200006, …\n$ CNTSCHID  &lt;fct&gt; 70200052, 70200134, 70200112, 70200004, 70200152, 70200043, …\n$ ST004D01T &lt;fct&gt; 1, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 1, 1, 2, 1, 2, 1, 2, 1, 1, …\n$ PROGN     &lt;fct&gt; 07020002, 07020002, 07020002, 07020002, 07020002, 07020002, …\n$ AGE       &lt;dbl&gt; 15.50, 15.83, 15.75, 16.17, 15.58, 15.58, 16.08, 16.00, 15.6…\n$ ST003D02T &lt;fct&gt; 10, 6, 7, 2, 9, 9, 3, 4, 8, 6, 10, 7, 9, 11, 5, 10, 11, 4, 7…\n$ ST003D03T &lt;fct&gt; 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, …\n$ ESCS      &lt;dbl&gt; 0.1836, 0.8261, -1.0357, -0.9606, 0.0856, 0.1268, -0.0154, 1…\n$ PAREDINT  &lt;dbl&gt; 16.0, 14.5, 12.0, 12.0, 14.5, 16.0, 12.0, 16.0, 16.0, 16.0, …\n$ HISEI     &lt;dbl&gt; 30.34, 77.10, 17.00, 43.33, 75.54, 57.64, 70.34, 80.78, 65.1…\n$ HOMEPOS   &lt;dbl&gt; 0.7524, 0.7842, 0.0666, -0.9300, -0.8949, -0.5988, 0.0975, 0…\n$ MISCED    &lt;fct&gt; 8, 7, 4, 6, 7, 7, 6, 9, 8, 8, 4, 9, 10, 6, 4, 9, 8, 8, 6, 6,…\n$ FISCED    &lt;fct&gt; 7, 7, 4, 6, 7, 9, 2, 8, 8, 7, 4, 9, 10, 9, 6, 4, 9, 8, 6, 7,…\n$ HISCED    &lt;fct&gt; 8, 7, 4, 6, 7, 9, 6, 9, 8, 8, 4, 9, 10, 9, 6, 9, 9, 8, 6, 7,…\n$ FAMSUP    &lt;dbl&gt; -0.3780, -0.5969, -1.0537, -0.8521, 1.7459, 1.7327, -0.8815,…\n$ WORKHOME  &lt;fct&gt; 10, 2, 0, 10, 5, 5, 7, 0, 0, 4, 2, 2, 10, 0, 10, 0, 5, 5, 0,…\n$ WORKPAY   &lt;fct&gt; 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ STUDYHMW  &lt;fct&gt; 4, 7, 3, 5, 7, 10, 0, 10, 5, 3, 5, 5, 10, 0, 8, 5, 5, 5, 10,…\n$ EXERPRAC  &lt;fct&gt; 1, 4, 2, 5, 9, 1, 2, 0, 3, 5, 1, 2, 5, 2, 4, 0, 2, 3, 2, 2, …\n$ LANGN     &lt;fct&gt; 998, 998, 998, 998, 998, 998, 998, 998, 998, 998, 998, 998, …\n$ ICTOUT    &lt;dbl&gt; 0.2260, -0.8080, 0.1088, -1.2894, 2.9804, 0.3464, -0.4834, 2…\n$ ICTAVHOM  &lt;fct&gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6, 6, 6, 5, 6, 6, 6, 6, …\n$ ICTHOME   &lt;fct&gt; 0.3346, 0.3346, 0.3346, 0.3346, 0.3346, 0.3346, 0.3346, 0.33…\n$ ICTAVSCH  &lt;fct&gt; 7, 7, 7, 7, 5, 6, 7, 6, 7, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, …\n$ ICTSCH    &lt;fct&gt; 0.4062, 0.4062, 0.4062, 0.4062, -1.6647, -0.8411, 0.4062, -0…\n$ ICTRES    &lt;dbl&gt; 0.1940, 0.6249, -0.3987, -0.9028, 0.2514, -0.4733, 0.9904, 1…\n$ COBN_S    &lt;fct&gt; 070200, 070200, 070200, 070200, 070200, 970200, 070200, 9702…\n$ COBN_M    &lt;fct&gt; 070200, 070200, 970200, 070200, 070200, 970200, 070200, 9702…\n$ COBN_F    &lt;fct&gt; 070200, 070200, 070200, 070200, 070200, 970200, 070200, 9702…\n$ OCOD1     &lt;fct&gt; 9701, 31, 9701, 41, 23, 9701, 11, 23, 1, 34, 9999, 13, 43, 9…\n$ OCOD2     &lt;fct&gt; 83, 21, 9704, 9705, 83, 34, 31, 21, 14, 31, 24, 33, 0, 9705,…\n$ PV1MATH   &lt;dbl&gt; 639.004, 697.191, 693.710, 427.317, 436.462, 569.982, 771.62…\n$ PV2MATH   &lt;dbl&gt; 601.251, 754.277, 654.450, 410.376, 453.450, 539.609, 672.81…\n$ PV3MATH   &lt;dbl&gt; 621.480, 671.940, 696.938, 423.586, 392.315, 531.648, 653.74…\n$ PV4MATH   &lt;dbl&gt; 631.596, 657.300, 646.187, 388.935, 439.986, 534.368, 734.81…\n$ PV5MATH   &lt;dbl&gt; 579.276, 621.126, 678.119, 330.962, 443.125, 465.815, 727.52…\n$ PV6MATH   &lt;dbl&gt; 591.791, 655.729, 644.019, 379.988, 452.648, 528.509, 729.83…\n$ PV7MATH   &lt;dbl&gt; 600.709, 747.934, 720.531, 398.535, 396.970, 514.326, 597.22…\n$ PV8MATH   &lt;dbl&gt; 587.322, 694.365, 671.425, 422.127, 459.945, 521.029, 772.27…\n$ PV9MATH   &lt;dbl&gt; 618.131, 742.732, 694.085, 375.354, 438.166, 472.382, 694.39…\n$ PV10MATH  &lt;dbl&gt; 581.973, 656.934, 668.304, 453.348, 448.084, 503.387, 725.29…\n$ PV1READ   &lt;dbl&gt; 676.298, 625.585, 620.116, 381.495, 448.199, 469.441, 744.53…\n$ PV2READ   &lt;dbl&gt; 692.247, 686.716, 559.078, 400.815, 560.636, 500.350, 679.85…\n$ PV3READ   &lt;dbl&gt; 690.981, 663.147, 554.767, 374.911, 365.478, 375.703, 635.12…\n$ PV4READ   &lt;dbl&gt; 643.067, 567.435, 587.026, 367.484, 469.970, 377.452, 725.55…\n$ PV5READ   &lt;dbl&gt; 627.908, 614.500, 591.806, 336.009, 503.664, 470.781, 731.16…\n$ PV6READ   &lt;dbl&gt; 684.676, 604.745, 570.547, 324.630, 481.215, 415.448, 684.68…\n$ PV7READ   &lt;dbl&gt; 661.380, 669.375, 599.078, 396.242, 436.800, 448.547, 646.02…\n$ PV8READ   &lt;dbl&gt; 674.070, 623.735, 545.610, 374.723, 531.226, 434.381, 756.80…\n$ PV9READ   &lt;dbl&gt; 666.282, 649.579, 610.466, 314.704, 480.997, 411.703, 653.76…\n$ PV10READ  &lt;dbl&gt; 657.387, 571.261, 590.758, 342.956, 478.578, 410.846, 784.71…\n$ PV1SCIE   &lt;dbl&gt; 710.634, 670.646, 666.095, 340.308, 456.333, 475.158, 693.80…\n$ PV2SCIE   &lt;dbl&gt; 618.739, 748.839, 604.771, 329.889, 453.400, 470.030, 626.98…\n$ PV3SCIE   &lt;dbl&gt; 591.623, 635.443, 704.217, 411.353, 498.937, 461.218, 627.38…\n$ PV4SCIE   &lt;dbl&gt; 659.770, 639.735, 687.659, 327.974, 532.324, 504.199, 676.79…\n$ PV5SCIE   &lt;dbl&gt; 635.892, 608.385, 690.974, 292.183, 508.231, 486.930, 661.84…\n$ PV6SCIE   &lt;dbl&gt; 646.901, 670.662, 617.175, 355.423, 504.461, 493.011, 618.39…\n$ PV7SCIE   &lt;dbl&gt; 603.569, 734.807, 692.886, 400.182, 404.572, 469.950, 602.07…\n$ PV8SCIE   &lt;dbl&gt; 621.352, 639.748, 630.900, 317.518, 549.457, 464.012, 653.97…\n$ PV9SCIE   &lt;dbl&gt; 659.674, 716.768, 656.620, 298.893, 411.062, 440.113, 645.47…\n$ PV10SCIE  &lt;dbl&gt; 649.719, 655.670, 649.087, 362.702, 473.613, 495.410, 662.55…",
    "crumbs": [
      "Take-home Exercises",
      "Take-home Exercise 1"
    ]
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#renaming-variable-names",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#renaming-variable-names",
    "title": "Take-home Exercise 1",
    "section": "3.4 Renaming Variable Names",
    "text": "3.4 Renaming Variable Names\nLet us rename the gender and school variable name to make it more intuitive, and change the gender’s code 1 to female and 2 to male using the following code chunk.\n\n\nShow the code\nstu2 &lt;- stu2 %&gt;% \n  rename(GENDER = ST004D01T) %&gt;%\n  mutate(GENDER = ifelse(GENDER == 1, 'FEMALE', 'MALE'))\n\nstu2 &lt;- stu2 %&gt;% \n  rename(SCHOOL = CNTSCHID) \n\nkable(head(stu2)) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCNTSTUID\nSCHOOL\nGENDER\nPROGN\nAGE\nST003D02T\nST003D03T\nESCS\nPAREDINT\nHISEI\nHOMEPOS\nMISCED\nFISCED\nHISCED\nFAMSUP\nWORKHOME\nWORKPAY\nSTUDYHMW\nEXERPRAC\nLANGN\nICTOUT\nICTAVHOM\nICTHOME\nICTAVSCH\nICTSCH\nICTRES\nCOBN_S\nCOBN_M\nCOBN_F\nOCOD1\nOCOD2\nPV1MATH\nPV2MATH\nPV3MATH\nPV4MATH\nPV5MATH\nPV6MATH\nPV7MATH\nPV8MATH\nPV9MATH\nPV10MATH\nPV1READ\nPV2READ\nPV3READ\nPV4READ\nPV5READ\nPV6READ\nPV7READ\nPV8READ\nPV9READ\nPV10READ\nPV1SCIE\nPV2SCIE\nPV3SCIE\nPV4SCIE\nPV5SCIE\nPV6SCIE\nPV7SCIE\nPV8SCIE\nPV9SCIE\nPV10SCIE\n\n\n\n\n70200001\n70200052\nFEMALE\n07020002\n15.50\n10\n2006\n0.1836\n16.0\n30.34\n0.7524\n8\n7\n8\n-0.3780\n10\n0\n4\n1\n998\n0.2260\n6\n0.3346\n7\n0.4062\n0.1940\n070200\n070200\n070200\n9701\n83\n639.004\n601.251\n621.480\n631.596\n579.276\n591.791\n600.709\n587.322\n618.131\n581.973\n676.298\n692.247\n690.981\n643.067\n627.908\n684.676\n661.380\n674.070\n666.282\n657.387\n710.634\n618.739\n591.623\n659.770\n635.892\n646.901\n603.569\n621.352\n659.674\n649.719\n\n\n70200002\n70200134\nMALE\n07020002\n15.83\n6\n2006\n0.8261\n14.5\n77.10\n0.7842\n7\n7\n7\n-0.5969\n2\n0\n7\n4\n998\n-0.8080\n6\n0.3346\n7\n0.4062\n0.6249\n070200\n070200\n070200\n31\n21\n697.191\n754.277\n671.940\n657.300\n621.126\n655.729\n747.934\n694.365\n742.732\n656.934\n625.585\n686.716\n663.147\n567.435\n614.500\n604.745\n669.375\n623.735\n649.579\n571.261\n670.646\n748.839\n635.443\n639.735\n608.385\n670.662\n734.807\n639.748\n716.768\n655.670\n\n\n70200003\n70200112\nMALE\n07020002\n15.75\n7\n2006\n-1.0357\n12.0\n17.00\n0.0666\n4\n4\n4\n-1.0537\n0\n0\n3\n2\n998\n0.1088\n6\n0.3346\n7\n0.4062\n-0.3987\n070200\n970200\n070200\n9701\n9704\n693.710\n654.450\n696.938\n646.187\n678.119\n644.019\n720.531\n671.425\n694.085\n668.304\n620.116\n559.078\n554.767\n587.026\n591.806\n570.547\n599.078\n545.610\n610.466\n590.758\n666.095\n604.771\n704.217\n687.659\n690.974\n617.175\n692.886\n630.900\n656.620\n649.087\n\n\n70200004\n70200004\nMALE\n07020002\n16.17\n2\n2006\n-0.9606\n12.0\n43.33\n-0.9300\n6\n6\n6\n-0.8521\n10\n6\n5\n5\n998\n-1.2894\n6\n0.3346\n7\n0.4062\n-0.9028\n070200\n070200\n070200\n41\n9705\n427.317\n410.376\n423.586\n388.935\n330.962\n379.988\n398.535\n422.127\n375.354\n453.348\n381.495\n400.815\n374.911\n367.484\n336.009\n324.630\n396.242\n374.723\n314.704\n342.956\n340.308\n329.889\n411.353\n327.974\n292.183\n355.423\n400.182\n317.518\n298.893\n362.702\n\n\n70200005\n70200152\nFEMALE\n07020002\n15.58\n9\n2006\n0.0856\n14.5\n75.54\n-0.8949\n7\n7\n7\n1.7459\n5\n0\n7\n9\n998\n2.9804\n6\n0.3346\n5\n-1.6647\n0.2514\n070200\n070200\n070200\n23\n83\n436.462\n453.450\n392.315\n439.986\n443.125\n452.648\n396.970\n459.945\n438.166\n448.084\n448.199\n560.636\n365.478\n469.970\n503.664\n481.215\n436.800\n531.226\n480.997\n478.578\n456.333\n453.400\n498.937\n532.324\n508.231\n504.461\n404.572\n549.457\n411.062\n473.613\n\n\n70200006\n70200043\nFEMALE\n07020002\n15.58\n9\n2006\n0.1268\n16.0\n57.64\n-0.5988\n7\n9\n9\n1.7327\n5\n0\n10\n1\n998\n0.3464\n6\n0.3346\n6\n-0.8411\n-0.4733\n970200\n970200\n970200\n9701\n34\n569.982\n539.609\n531.648\n534.368\n465.815\n528.509\n514.326\n521.029\n472.382\n503.387\n469.441\n500.350\n375.703\n377.452\n470.781\n415.448\n448.547\n434.381\n411.703\n410.846\n475.158\n470.030\n461.218\n504.199\n486.930\n493.011\n469.950\n464.012\n440.113\n495.410",
    "crumbs": [
      "Take-home Exercises",
      "Take-home Exercise 1"
    ]
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#checking-the-number-of-unique-students-their-age-and-schools",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#checking-the-number-of-unique-students-their-age-and-schools",
    "title": "Take-home Exercise 1",
    "section": "3.5 Checking the number of unique students, their age and schools",
    "text": "3.5 Checking the number of unique students, their age and schools\nLet us check number of unique students, their age and the schools the students were in using the following code chunk.\n\nNumber of Unique StudentsStudents’ AgeNumber of Unique Schools\n\n\n\nn_distinct(stu2$CNTSTUID)\n\n[1] 6094\n\n\nFrom the output, we noted that there are 6,094 unique students and the stu2 dataframe has 6094 observations, meaning that each observation is a unique student.\n\n\n\nsummary(stu2$AGE)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  15.33   15.50   15.75   15.78   16.00   16.33 \n\n\nFrom the above output we noted that there was a maximum age of 16.33 years old.\nLet us check the students’ birth year to ensure that all of them are born in 2006 (i.e. 15 years old).\n\nunique(stu2$ST003D03T)\n\n[1] 2006\nLevels: 2006\n\n\nFrom the above output, we noted that all students were born in 2006 so we will retain those students who were older than 15 years old for now, until we have more information on how this age being derived.\n\n\n\nunique(stu2$SCHOOL)\n\n  [1] 70200052 70200134 70200112 70200004 70200152 70200043 70200049 70200107\n  [9] 70200012 70200061 70200110 70200108 70200067 70200113 70200093 70200035\n [17] 70200027 70200102 70200163 70200097 70200139 70200081 70200040 70200140\n [25] 70200008 70200066 70200118 70200078 70200135 70200161 70200046 70200116\n [33] 70200144 70200136 70200127 70200092 70200158 70200156 70200100 70200147\n [41] 70200137 70200060 70200086 70200031 70200062 70200145 70200096 70200148\n [49] 70200103 70200003 70200059 70200021 70200001 70200098 70200038 70200154\n [57] 70200026 70200085 70200153 70200013 70200159 70200053 70200146 70200117\n [65] 70200084 70200044 70200090 70200089 70200121 70200165 70200020 70200036\n [73] 70200065 70200037 70200162 70200149 70200048 70200023 70200132 70200119\n [81] 70200064 70200091 70200099 70200080 70200160 70200122 70200017 70200128\n [89] 70200105 70200055 70200068 70200042 70200082 70200030 70200123 70200079\n [97] 70200057 70200056 70200131 70200130 70200143 70200025 70200006 70200063\n[105] 70200033 70200058 70200070 70200029 70200016 70200071 70200074 70200129\n[113] 70200101 70200069 70200114 70200028 70200050 70200007 70200022 70200083\n[121] 70200024 70200125 70200039 70200034 70200019 70200077 70200075 70200051\n[129] 70200072 70200104 70200141 70200142 70200076 70200018 70200009 70200106\n[137] 70200032 70200124 70200002 70200111 70200054 70200045 70200047 70200126\n[145] 70200041 70200088 70200014 70200133 70200109 70200095 70200138 70200155\n[153] 70200151 70200087 70200015 70200011 70200010 70200164 70200094 70200157\n[161] 70200120 70200005 70200115 70200073\n164 Levels: 70200001 70200002 70200003 70200004 70200005 70200006 ... 70200165\n\n\nFrom the above output, we noted that there were 164 schools in this dataset.",
    "crumbs": [
      "Take-home Exercises",
      "Take-home Exercise 1"
    ]
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#computing-mean-pv-scores-for-each-student",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#computing-mean-pv-scores-for-each-student",
    "title": "Take-home Exercise 1",
    "section": "3.6 Computing Mean PV Scores for each student",
    "text": "3.6 Computing Mean PV Scores for each student\nCurrently there are 10 PV Scores for each subject (i.e., Maths, Reading and Science). We will calculate each student’s mean PV scores for each subject using the following code chunk.\n\nCodesOutput\n\n\n\nstu2$AVG_PVMATH &lt;- rowMeans(stu2[,c(\"PV1MATH\", \"PV2MATH\", \"PV3MATH\", \"PV4MATH\", \"PV5MATH\", \n                             \"PV6MATH\", \"PV7MATH\", \"PV8MATH\", \"PV9MATH\", \"PV10MATH\")])\n\nstu2$AVG_PVREAD &lt;- rowMeans(stu2[,c(\"PV1READ\", \"PV2READ\", \"PV3READ\", \"PV4READ\", \"PV5READ\", \n                             \"PV6READ\", \"PV7READ\", \"PV8READ\", \"PV9READ\", \"PV10READ\")])\n\nstu2$AVG_PVSCI &lt;- rowMeans(stu2[,c(\"PV1SCIE\", \"PV2SCIE\", \"PV3SCIE\", \"PV4SCIE\", \"PV5SCIE\", \n                             \"PV6SCIE\", \"PV7SCIE\", \"PV8SCIE\", \"PV9SCIE\", \"PV10SCIE\")])\n\n\n\n\nkable(head(stu2))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCNTSTUID\nSCHOOL\nGENDER\nPROGN\nAGE\nST003D02T\nST003D03T\nESCS\nPAREDINT\nHISEI\nHOMEPOS\nMISCED\nFISCED\nHISCED\nFAMSUP\nWORKHOME\nWORKPAY\nSTUDYHMW\nEXERPRAC\nLANGN\nICTOUT\nICTAVHOM\nICTHOME\nICTAVSCH\nICTSCH\nICTRES\nCOBN_S\nCOBN_M\nCOBN_F\nOCOD1\nOCOD2\nPV1MATH\nPV2MATH\nPV3MATH\nPV4MATH\nPV5MATH\nPV6MATH\nPV7MATH\nPV8MATH\nPV9MATH\nPV10MATH\nPV1READ\nPV2READ\nPV3READ\nPV4READ\nPV5READ\nPV6READ\nPV7READ\nPV8READ\nPV9READ\nPV10READ\nPV1SCIE\nPV2SCIE\nPV3SCIE\nPV4SCIE\nPV5SCIE\nPV6SCIE\nPV7SCIE\nPV8SCIE\nPV9SCIE\nPV10SCIE\nAVG_PVMATH\nAVG_PVREAD\nAVG_PVSCI\n\n\n\n\n70200001\n70200052\nFEMALE\n07020002\n15.50\n10\n2006\n0.1836\n16.0\n30.34\n0.7524\n8\n7\n8\n-0.3780\n10\n0\n4\n1\n998\n0.2260\n6\n0.3346\n7\n0.4062\n0.1940\n070200\n070200\n070200\n9701\n83\n639.004\n601.251\n621.480\n631.596\n579.276\n591.791\n600.709\n587.322\n618.131\n581.973\n676.298\n692.247\n690.981\n643.067\n627.908\n684.676\n661.380\n674.070\n666.282\n657.387\n710.634\n618.739\n591.623\n659.770\n635.892\n646.901\n603.569\n621.352\n659.674\n649.719\n605.2533\n667.4296\n639.7873\n\n\n70200002\n70200134\nMALE\n07020002\n15.83\n6\n2006\n0.8261\n14.5\n77.10\n0.7842\n7\n7\n7\n-0.5969\n2\n0\n7\n4\n998\n-0.8080\n6\n0.3346\n7\n0.4062\n0.6249\n070200\n070200\n070200\n31\n21\n697.191\n754.277\n671.940\n657.300\n621.126\n655.729\n747.934\n694.365\n742.732\n656.934\n625.585\n686.716\n663.147\n567.435\n614.500\n604.745\n669.375\n623.735\n649.579\n571.261\n670.646\n748.839\n635.443\n639.735\n608.385\n670.662\n734.807\n639.748\n716.768\n655.670\n689.9528\n627.6078\n672.0703\n\n\n70200003\n70200112\nMALE\n07020002\n15.75\n7\n2006\n-1.0357\n12.0\n17.00\n0.0666\n4\n4\n4\n-1.0537\n0\n0\n3\n2\n998\n0.1088\n6\n0.3346\n7\n0.4062\n-0.3987\n070200\n970200\n070200\n9701\n9704\n693.710\n654.450\n696.938\n646.187\n678.119\n644.019\n720.531\n671.425\n694.085\n668.304\n620.116\n559.078\n554.767\n587.026\n591.806\n570.547\n599.078\n545.610\n610.466\n590.758\n666.095\n604.771\n704.217\n687.659\n690.974\n617.175\n692.886\n630.900\n656.620\n649.087\n676.7768\n582.9252\n660.0384\n\n\n70200004\n70200004\nMALE\n07020002\n16.17\n2\n2006\n-0.9606\n12.0\n43.33\n-0.9300\n6\n6\n6\n-0.8521\n10\n6\n5\n5\n998\n-1.2894\n6\n0.3346\n7\n0.4062\n-0.9028\n070200\n070200\n070200\n41\n9705\n427.317\n410.376\n423.586\n388.935\n330.962\n379.988\n398.535\n422.127\n375.354\n453.348\n381.495\n400.815\n374.911\n367.484\n336.009\n324.630\n396.242\n374.723\n314.704\n342.956\n340.308\n329.889\n411.353\n327.974\n292.183\n355.423\n400.182\n317.518\n298.893\n362.702\n401.0528\n361.3969\n343.6425\n\n\n70200005\n70200152\nFEMALE\n07020002\n15.58\n9\n2006\n0.0856\n14.5\n75.54\n-0.8949\n7\n7\n7\n1.7459\n5\n0\n7\n9\n998\n2.9804\n6\n0.3346\n5\n-1.6647\n0.2514\n070200\n070200\n070200\n23\n83\n436.462\n453.450\n392.315\n439.986\n443.125\n452.648\n396.970\n459.945\n438.166\n448.084\n448.199\n560.636\n365.478\n469.970\n503.664\n481.215\n436.800\n531.226\n480.997\n478.578\n456.333\n453.400\n498.937\n532.324\n508.231\n504.461\n404.572\n549.457\n411.062\n473.613\n436.1151\n475.6763\n479.2390\n\n\n70200006\n70200043\nFEMALE\n07020002\n15.58\n9\n2006\n0.1268\n16.0\n57.64\n-0.5988\n7\n9\n9\n1.7327\n5\n0\n10\n1\n998\n0.3464\n6\n0.3346\n6\n-0.8411\n-0.4733\n970200\n970200\n970200\n9701\n34\n569.982\n539.609\n531.648\n534.368\n465.815\n528.509\n514.326\n521.029\n472.382\n503.387\n469.441\n500.350\n375.703\n377.452\n470.781\n415.448\n448.547\n434.381\n411.703\n410.846\n475.158\n470.030\n461.218\n504.199\n486.930\n493.011\n469.950\n464.012\n440.113\n495.410\n518.1055\n431.4652\n476.0031\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote: For the purpose of this exercise, we calculated the individual students’ subject scores. PISA advised that students’ scores should not be interpreted at the individual level. For more details, please refer to this website.",
    "crumbs": [
      "Take-home Exercises",
      "Take-home Exercise 1"
    ]
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#distribution-of-students-scores-for-maths-reading-and-science",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#distribution-of-students-scores-for-maths-reading-and-science",
    "title": "Take-home Exercise 1",
    "section": "4.1 Distribution of Students’ Scores for Maths, Reading and Science",
    "text": "4.1 Distribution of Students’ Scores for Maths, Reading and Science\nLet us take a look at the distribution of the Students’ Scores for the three subjects: Maths, Reading and Science using ridgeplot.\n\nPreparing the DataPlot\n\n\nAs the current dataframe is large, we will create another dataframe that only contain the student ID and the average subject scores, then pivot longer the table.\n\nstu2s &lt;- stu2 %&gt;%\n  select(CNTSTUID, AVG_PVMATH, AVG_PVREAD, AVG_PVSCI) %&gt;%\n  pivot_longer(-CNTSTUID) %&gt;%\n  rename(subj = name, scores = value)\n\nmed_math &lt;- round(median(stu2$AVG_PVMATH), 0)\nmed_read &lt;- round(median(stu2$AVG_PVREAD), 0)\nmed_sci &lt;- round(median(stu2$AVG_PVSCI), 0)\n\n\n\n\nggplot(stu2s, \n       aes(x = scores,\n           y = subj,\n           fill = factor(stat(quantile)))) + \n  #geom_text(aes(label = med_math), color = \"black\") + \n  stat_density_ridges(geom = \"density_ridges_gradient\",\n                      calc_ecdf = TRUE,\n                      quantiles = c(0.025, 0.975)) +\n  scale_fill_manual(name = \"Probability\",\n    values = c(\"#FF0000A0\", \"#A0A0A0A0\", \"#0000FFA0\"),\n    labels = c(\"(0, 0.025]\", \"(0.025, 0.975]\", \"(0.975, 1]\")) + \n  theme_ridges()+\n  labs(title = \"Distribution of Average Subject Scores\", caption = \"Data from PISA 2022\",\n       x= \"Scores\", y = \"Subjects\") + theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nFrom the above plot, despite the median scores for each subject being relatively similar to other subjects, we see that there was a wide range of scores for each subject. This means that for a particular subject, some students could score as low as 300 points while some students can score as high as 750 points.\nLet us explore further in the subsequent plots!",
    "crumbs": [
      "Take-home Exercises",
      "Take-home Exercise 1"
    ]
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#distribution-of-scores-by-gender",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#distribution-of-scores-by-gender",
    "title": "Take-home Exercise 1",
    "section": "4.2 Distribution of Scores By Gender",
    "text": "4.2 Distribution of Scores By Gender\nLet us take a look if there is any difference in scores by gender using ggdist’s raincloud plots. However, as there are a large number of data points, we would not be plotting the dots because the dots would end up become very small. There are other suggestions at the ggdist website on handling data with large samples.\nWe will first prepare one plot for each subject using the following code chunks.\n\nMaths Scores Between GenderReading Scores Between GenderScience Scores Between Gender\n\n\n\ng1 &lt;- ggplot(stu2, aes(x = GENDER, y = AVG_PVMATH, fill = GENDER)) +\n  stat_halfeye(adjust = 0.5,\n               width = 0.3, \n               justification = -0.1,\n               point_color = NA,\n               scale = 1.5) +\n  geom_boxplot(width = 0.05,\n               outlier.shape = NA) + coord_flip() +\n   stat_summary(fun = median, geom = \"text\", aes(label = round(after_stat(y), )),\n               position = position_nudge(x = 0.05), vjust = -0.5,size=3) +  \n  labs(title = \"Distribution of Maths Scores\",\n       y = \"Scores\", x = \"Gender\") +\n  theme_minimal() + scale_y_continuous(limits = c(100,900)) + theme(plot.title = element_text(size = 10))\n\nggsave(\"g1.png\", plot = g1)\ng1\n\n\n\n\n\n\n\n\n\n\n\ng2 &lt;- ggplot(stu2, aes(x = GENDER, y = AVG_PVREAD, fill = GENDER)) +\n  stat_halfeye(adjust = 0.5,\n               width = 0.3, \n               justification = -0.1,\n               point_color = NA,\n               scale = 1.5) +\n  geom_boxplot(width = 0.05,\n               outlier.shape = NA) + coord_flip() +\n   stat_summary(fun = median, geom = \"text\", aes(label = round(after_stat(y), )),\n               position = position_nudge(x = 0.05), vjust = -0.5,size=3) +  \n labs(title = \"Distribution of Reading Scores\",\n       y = \"Scores\", x = \"Gender\") + theme_minimal()+\n  scale_y_continuous(limits = c(100,900)) + theme(plot.title = element_text(size = 10))\n\n\ng2\n\n\n\n\n\n\n\n\n\n\n\ng3 &lt;- ggplot(stu2, aes(x = GENDER, y = AVG_PVSCI, fill=GENDER)) +\n  stat_halfeye(adjust = 0.5,\n               width = 0.3, \n               justification = -0.1,\n               point_color = NA,\n               scale = 1.5) +\n  geom_boxplot(width = 0.05,\n               outlier.shape = NA) +coord_flip() +\n stat_summary(fun = median, geom = \"text\", aes(label = round(after_stat(y), )),\n               position = position_nudge(x = 0.05), vjust = -0.5,size=3) +  \n labs(title = \"Distribution of Science Scores\",\n       y = \"Scores\", x = \"Gender\") + theme_minimal() + \n  scale_y_continuous(limits = c(100,900)) + theme(plot.title = element_text(size = 10))\n\ng3\n\n\n\n\n\n\n\n\n\n\n\nWe will use patchwork to combine these three plots into one with the following code chunk.\n\n\nShow the code\npatch1 &lt;- g1 /  g2 / g3\npatch1+ plot_layout(guides = \"collect\") \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe difference in median scores between Male and Female Students ranges from: Math: 19 points with male students scoring slightly higher than female students Reading: 12 points difference with females students scoring slightly higher than male students Science: 8 points difference with male students scoring slightly higher than female students.\nFor further investigations, we could also test if the difference between gender is statistically significant.",
    "crumbs": [
      "Take-home Exercises",
      "Take-home Exercise 1"
    ]
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#differences-in-subject-scores-among-schools",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#differences-in-subject-scores-among-schools",
    "title": "Take-home Exercise 1",
    "section": "4.3 Differences in Subject Scores Among Schools",
    "text": "4.3 Differences in Subject Scores Among Schools\nAs there are more than 160 schools in this dataset, we will focus only on the differences in Scores among the top 10% and bottom 10% of schools for each subject.\nFirst, for each subject score, we use the following code chunk to find the minimum and maximum subject scores for each school using group_by and summarise functions.\n\n\nShow the code\nmath_sch &lt;- stu2 %&gt;%\n  group_by(SCHOOL) %&gt;%\n  summarise_at(vars(AVG_PVMATH), \n               list(minscore = min, maxscore = max)) %&gt;%\n  mutate(difference = maxscore - minscore)\n\nread_sch &lt;- stu2 %&gt;%\n  group_by(SCHOOL) %&gt;%\n  summarise_at(vars(AVG_PVREAD), \n               list(minscore = min, maxscore = max)) %&gt;%\n  mutate(difference = maxscore - minscore)\n\nsci_sch &lt;- stu2 %&gt;%\n  group_by(SCHOOL) %&gt;%\n  summarise_at(vars(AVG_PVSCI), \n               list(minscore = min, maxscore = max)) %&gt;%\n  mutate(difference = maxscore - minscore)\n\n\nThen, we will use the the follow code chunks to (i) get the top 5% schools for each subject using slice_max, (ii) create a table with the top 5% schools’ minimum score of the subject, (iii) create a table with the top 5% schools’ maximum score of the subject then pivot the table using pivot_longer.\n\nMathsReadingScience\n\n\n\n\nShow the code\nmath_sch_high5 &lt;- math_sch %&gt;% slice_max(maxscore, prop = 0.05)\n\n\nmath_sch_min &lt;- math_sch_high5 %&gt;%\n  select(-maxscore)\n\n\nmath_sch_max &lt;- math_sch_high5%&gt;%\n  select(-minscore)\n\n\nmath_long &lt;- math_sch_high5%&gt;% \n  select(-difference)%&gt;%\n  pivot_longer(-SCHOOL) \n\n\nh1 &lt;- ggplot(math_long) +\n  geom_segment(data = math_sch_min,\n               aes(x = minscore, y = SCHOOL,\n                   yend = math_sch_max$SCHOOL, xend=math_sch_max$maxscore),\n               color= \"#aeb6bf\",\n               size = 4,\n               alpha = 0.5) +\n  geom_point(aes(x = value, y = SCHOOL, color = name), size = 4, show.legend = TRUE) + \n  labs(title = \"Differences in Students Scores for Top 5% Schools in Maths\", x = \"Maths Scores\", y = \"School\", color = \"Min/Max Scores\") +\n  geom_text(data = math_sch_min,\n            aes(label= paste(\"Diff: \", difference), x= minscore+100, y= SCHOOL),\n             color = \"black\",\n             size = 2.5)+ theme_minimal()+\n  theme(panel.grid.minor.y = element_blank(),\n        panel.grid.minor.x = element_blank(), \n        axis.ticks.x = element_line(color = \"#4a4e4d\"),\n        axis.text.y = element_text(size = 8),\n        axis.title.y = element_text(size = 8), \n        axis.text.x = element_text(size = 8),\n        axis.title.x = element_text(size = 8))\n\nh1 \n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nread_sch_high5 &lt;- read_sch %&gt;% slice_max(maxscore, prop = 0.05)\n\nread_sch_min &lt;- read_sch_high5 %&gt;%\n  select(-maxscore)\n\nread_sch_max &lt;- read_sch_high5%&gt;%\n  select(-minscore)\n\nread_long &lt;- read_sch_high5%&gt;% \n  select(-difference)%&gt;%\n  pivot_longer(-SCHOOL) \n\nh2 &lt;- ggplot(read_long) +\n  geom_segment(data = read_sch_min,\n               aes(x = minscore, y = SCHOOL,\n                   yend = read_sch_max$SCHOOL, xend=read_sch_max$maxscore),\n               color= \"#aeb6bf\",\n               size = 4,\n               alpha = 0.5) +\n  geom_point(aes(x = value, y = SCHOOL, color = name), size = 4, show.legend = TRUE) + \n  labs(title = \"Differences in Students Scores for Top 5% Schools in Reading\", x = \"Reading Scores\", y = \"School\", color = \"Min/Max Scores\") +\n  geom_text(data = read_sch_min,\n            aes(label= paste(\"Diff: \", difference), x= minscore+100, y= SCHOOL),\n             color = \"black\",\n             size = 2.5)+ theme_minimal()+\n  theme(panel.grid.minor.y = element_blank(),\n        panel.grid.minor.x = element_blank(), \n        axis.ticks.x = element_line(color = \"#4a4e4d\"),\n        axis.text.y = element_text(size = 8),\n        axis.title.y = element_text(size = 8), \n        axis.text.x = element_text(size = 8),\n        axis.title.x = element_text(size = 8))\nh2\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nsci_sch_high5 &lt;- sci_sch %&gt;% slice_max(maxscore, prop = 0.05)\n\nsci_sch_min &lt;- sci_sch_high5 %&gt;%\n  select(-maxscore)\n\nsci_sch_max &lt;- sci_sch_high5%&gt;%\n  select(-minscore)\n\nsci_long &lt;- sci_sch_high5%&gt;% \n  select(-difference)%&gt;%\n  pivot_longer(-SCHOOL) \n\nh3 &lt;- ggplot(sci_long) +\n  geom_segment(data = sci_sch_min,\n               aes(x = minscore, y = SCHOOL,\n                   yend = sci_sch_max$SCHOOL, xend=sci_sch_max$maxscore),\n               color= \"#aeb6bf\",\n               size = 4,\n               alpha = 0.5) +\n  geom_point(aes(x = value, y = SCHOOL, color = name), size = 4, show.legend = TRUE) + \n  labs(title = \"Differences in Students Scores for Top 5% Schools in Science\", caption = \"Data from PISA 2022 Study\", x = \"Science Scores\", y = \"School\", color = \"Min/Max Scores\") +\n  geom_text(data = sci_sch_min,\n            aes(label= paste(\"Diff: \", difference), x= minscore+100, y= SCHOOL),\n             color = \"black\",\n             size = 2.5) + theme_minimal()+\n  theme(panel.grid.minor.y = element_blank(),\n        panel.grid.minor.x = element_blank(), \n        axis.ticks.x = element_line(color = \"#4a4e4d\"), \n        axis.text.y = element_text(size = 8),\n        axis.title.y = element_text(size = 8), \n        axis.text.x = element_text(size = 8),\n        axis.title.x = element_text(size = 8))\n\nh3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nh1+ h2 + h3 + plot_layout(nrow = 3, guides = \"collect\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nAmongst the schools that scored the top 5% for each subject, the difference between their top scorers and low scorers ranges from 161 marks (school 70200003 for Science) to 458 marks (school 70200068 for Reading). In addition, we noted that there were five schools which had top 5% of the scores for at least two subjects: - For Science, Reading and Maths: 70200155, 70200101,70200003, 70200001 - For Reading and Maths: 70200062\nWe can do further exploration with the schools questionnaire data provided by PISA to find out about these schools. For further investigations, we could also test if the difference between the top and low scorers is statistically significant.",
    "crumbs": [
      "Take-home Exercises",
      "Take-home Exercise 1"
    ]
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#students-scores-and-socioeconomic-factors",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#students-scores-and-socioeconomic-factors",
    "title": "Take-home Exercise 1",
    "section": "4.4 Students’ Scores and Socioeconomic Factors",
    "text": "4.4 Students’ Scores and Socioeconomic Factors\nWe will now explore the students’ scores and the PISA index of economic, social and cultural status (ESCS) using the following code chunks.\n\nMaths Scores and ESCSReading and ESCSScience Scores and ESCS\n\n\n\nz1 &lt;- ggplot(stu2, \n       aes(x = AVG_PVMATH, y = ESCS)) +\n  geom_point() + \n  geom_smooth(method = \"lm\", size=0.5, fill = \"blue\")+ \n   coord_cartesian(xlim=c(150,850),\n                  ylim=c(-3,3)) + theme_minimal()+\n      theme(legend.position=\"none\") + \n  labs(title = \"Relationship between Maths Scores and ESCS\", \n       x = 'Maths Scores', y = \"ESCS\")\n\nz1\n\n\n\n\n\n\n\n\n\n\n\nz2 &lt;- ggplot(stu2, \n       aes(x = AVG_PVREAD, y = ESCS)) +\n  geom_point()+\n  geom_smooth(method = \"lm\", size=0.5, fill = \"blue\")+ \n   coord_cartesian(xlim=c(150,850),\n                  ylim=c(-3,3)) + theme_minimal()+\n      theme(legend.position=\"none\") +   \n  labs(title = \"Relationship between Reading Scores and ESCS\", \n       x = 'Reading Scores', y = \"ESCS\")\n\n\nz2\n\n\n\n\n\n\n\n\n\n\n\nz3 &lt;- ggplot(stu2, \n       aes(x = AVG_PVSCI, y = ESCS)) +\n  geom_point() + \n    geom_smooth(method = \"lm\", size=0.5, fill = \"blue\")+ \n   coord_cartesian(xlim=c(150,850),\n                  ylim=c(-3,3)) + theme_minimal()+\n      theme(legend.position=\"none\") + \n  labs(title = \"Relationship between Science Scores and ESCS\", \n       caption = \"Data from PISA 2022\", \n       x = 'Science Scores', y = \"ESCS\")\nz3\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\npatch3 &lt;- z1 +  z2 + z3\npatch3 + plot_layout(nrow = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nBased on the above plot, across all three subjects, it seems that scores seem to have a positive relationship with ESCS. In future analysis, we should investigate the relationship between students’ scores with indices that make up ESCS. We can also explore how scores are related to other PISA items, such as whether the student has to work after school hours (i.e. WORKPAY), and whether the student has to assist in housework or assist in taking care of family members (i.e.WORKHOME).",
    "crumbs": [
      "Take-home Exercises",
      "Take-home Exercise 1"
    ]
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03b.html",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03b.html",
    "title": "Take-home Exercise 3: Be Weatherwise or Otherwise",
    "section": "",
    "text": "According to Ministry of Sustainability and the Environment, the daily mean temperature is projected to increase by 1.4 to 4.6 Degree Celsius by the end of the century.\nIn this take-home exercise, I will be using the visual analytics techniques learnt in ISSS 608 to visualise uncertainty methods and create interactive visual analytics to validate the projection if the daily mean temperature is increasing from 1983 to 2023.",
    "crumbs": [
      "Take-home Exercises",
      "Take-home Exercise 3: Be Weatherwise or Otherwise"
    ]
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03b.html#loading-r-packages",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03b.html#loading-r-packages",
    "title": "Take-home Exercise 3: Be Weatherwise or Otherwise",
    "section": "2.1 Loading R packages",
    "text": "2.1 Loading R packages\nFor this exercise, we will be using the following packages:\n\ntidyverse : to load the core tidyverse packages, which includes ggplot2 and dplyr.\nggdis: provides stats and geoms for visualising distributions and uncertainty.\nggiraph: to make interactive ggplot2 plots\nplotly: to plot interactive statistical graphs\nDT: to create interactive tables using the JavaScript library DataTables\ncrosstalk: to implement cross-widget interactions\nggstatsplot:an extension of ggplot2 package for creating graphics with details from statistical tests included in the information-rich plots themselves.\nnortest: to test the normality assumption.\n\nThe code chunk below uses p_load() of pacman package to check if the abovementioned packages are installed in the computer. If they are, they will be launched in R. Otherwise, pacman will install the relevant packages before launching them.\n\n\nShow the code\npacman::p_load(tidyverse, ggdist, plotly, gganimate, DT, crosstalk, ggstatsplot, nortest, ggiraph, hrbrthemes)",
    "crumbs": [
      "Take-home Exercises",
      "Take-home Exercise 3: Be Weatherwise or Otherwise"
    ]
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03b.html#importing-the-data",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03b.html#importing-the-data",
    "title": "Take-home Exercise 3: Be Weatherwise or Otherwise",
    "section": "2.2 Importing the Data",
    "text": "2.2 Importing the Data\nFor this exercise, we will be using the historical daily temperature data from Meteorological Service Singapore. As there are more than 60 weather stations in Singapore, for the purpose of this exercise, we will be using the temperature data from Changi weather station in August 1983, 1993, 2003, 2013 and 2023.\n\nAug 1983Aug 1993Aug 2003Aug 2013Aug 2023\n\n\n\naug1983 &lt;- read_csv(\"data/DAILYDATA_S24_198308.csv\")\nhead(aug1983,10)\n\n# A tibble: 10 × 13\n   Station  Year Month   Day `Daily Rainfall Total (mm)` Highest 30 Min Rainfa…¹\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;                       &lt;dbl&gt; &lt;lgl&gt;                  \n 1 Changi   1983     8     1                         0   NA                     \n 2 Changi   1983     8     2                         6.4 NA                     \n 3 Changi   1983     8     3                         2.8 NA                     \n 4 Changi   1983     8     4                         3.7 NA                     \n 5 Changi   1983     8     5                        18.7 NA                     \n 6 Changi   1983     8     6                         0   NA                     \n 7 Changi   1983     8     7                         0   NA                     \n 8 Changi   1983     8     8                         0   NA                     \n 9 Changi   1983     8     9                         0   NA                     \n10 Changi   1983     8    10                         0   NA                     \n# ℹ abbreviated name: ¹​`Highest 30 Min Rainfall (mm)`\n# ℹ 7 more variables: `Highest 60 Min Rainfall (mm)` &lt;lgl&gt;,\n#   `Highest 120 Min Rainfall (mm)` &lt;lgl&gt;,\n#   `Mean Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Maximum Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Minimum Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Mean Wind Speed (km/h)` &lt;dbl&gt;, `Max Wind Speed (km/h)` &lt;dbl&gt;\n\n\n\n\n\naug1993 &lt;- read_csv(\"data/DAILYDATA_S24_199308.csv\")\nhead(aug1993,10)\n\n# A tibble: 10 × 13\n   Station  Year Month   Day `Daily Rainfall Total (mm)` Highest 30 Min Rainfa…¹\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;                       &lt;dbl&gt; &lt;lgl&gt;                  \n 1 Changi   1993     8     1                         0.4 NA                     \n 2 Changi   1993     8     2                         0   NA                     \n 3 Changi   1993     8     3                         0   NA                     \n 4 Changi   1993     8     4                         0   NA                     \n 5 Changi   1993     8     5                         0   NA                     \n 6 Changi   1993     8     6                         0   NA                     \n 7 Changi   1993     8     7                         0   NA                     \n 8 Changi   1993     8     8                         0   NA                     \n 9 Changi   1993     8     9                         0.4 NA                     \n10 Changi   1993     8    10                         0   NA                     \n# ℹ abbreviated name: ¹​`Highest 30 Min Rainfall (mm)`\n# ℹ 7 more variables: `Highest 60 Min Rainfall (mm)` &lt;lgl&gt;,\n#   `Highest 120 Min Rainfall (mm)` &lt;lgl&gt;,\n#   `Mean Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Maximum Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Minimum Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Mean Wind Speed (km/h)` &lt;dbl&gt;, `Max Wind Speed (km/h)` &lt;dbl&gt;\n\n\n\n\n\naug2003 &lt;- read_csv(\"data/DAILYDATA_S24_200308.csv\")\nhead(aug2003,10)\n\n# A tibble: 10 × 13\n   Station  Year Month   Day `Daily Rainfall Total (mm)` Highest 30 Min Rainfa…¹\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;                       &lt;dbl&gt; &lt;lgl&gt;                  \n 1 Changi   2003     8     1                         0   NA                     \n 2 Changi   2003     8     2                        21.1 NA                     \n 3 Changi   2003     8     3                         0   NA                     \n 4 Changi   2003     8     4                         0   NA                     \n 5 Changi   2003     8     5                        64.4 NA                     \n 6 Changi   2003     8     6                         9.2 NA                     \n 7 Changi   2003     8     7                         0   NA                     \n 8 Changi   2003     8     8                         0   NA                     \n 9 Changi   2003     8     9                         1.3 NA                     \n10 Changi   2003     8    10                         0   NA                     \n# ℹ abbreviated name: ¹​`Highest 30 Min Rainfall (mm)`\n# ℹ 7 more variables: `Highest 60 Min Rainfall (mm)` &lt;lgl&gt;,\n#   `Highest 120 Min Rainfall (mm)` &lt;lgl&gt;,\n#   `Mean Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Maximum Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Minimum Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Mean Wind Speed (km/h)` &lt;dbl&gt;, `Max Wind Speed (km/h)` &lt;dbl&gt;\n\n\n\n\n\naug2013 &lt;- read_csv(\"data/DAILYDATA_S24_201308.csv\")\nhead(aug2013,10)\n\n# A tibble: 10 × 13\n   Station  Year Month   Day `Daily Rainfall Total (mm)` Highest 30 Min Rainfa…¹\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;                       &lt;dbl&gt; &lt;lgl&gt;                  \n 1 Changi   2013     8     1                         8.2 NA                     \n 2 Changi   2013     8     2                         0   NA                     \n 3 Changi   2013     8     3                         0   NA                     \n 4 Changi   2013     8     4                         0   NA                     \n 5 Changi   2013     8     5                        43.2 NA                     \n 6 Changi   2013     8     6                         1   NA                     \n 7 Changi   2013     8     7                        19.2 NA                     \n 8 Changi   2013     8     8                         0.6 NA                     \n 9 Changi   2013     8     9                         1.8 NA                     \n10 Changi   2013     8    10                        40   NA                     \n# ℹ abbreviated name: ¹​`Highest 30 Min Rainfall (mm)`\n# ℹ 7 more variables: `Highest 60 Min Rainfall (mm)` &lt;lgl&gt;,\n#   `Highest 120 Min Rainfall (mm)` &lt;lgl&gt;,\n#   `Mean Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Maximum Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Minimum Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Mean Wind Speed (km/h)` &lt;dbl&gt;, `Max Wind Speed (km/h)` &lt;dbl&gt;\n\n\n\n\n\naug2023 &lt;- read_csv(\"data/DAILYDATA_S24_202308.csv\")\nhead(aug2023,10)\n\n# A tibble: 10 × 13\n   Station  Year Month   Day `Daily Rainfall Total (mm)` Highest 30 min Rainfa…¹\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;                       &lt;dbl&gt;                   &lt;dbl&gt;\n 1 Changi   2023     8     1                         0                       0  \n 2 Changi   2023     8     2                         0                       0  \n 3 Changi   2023     8     3                         9.2                     2.2\n 4 Changi   2023     8     4                         0                       0  \n 5 Changi   2023     8     5                         0                       0  \n 6 Changi   2023     8     6                         0                       0  \n 7 Changi   2023     8     7                         0                       0  \n 8 Changi   2023     8     8                        24.4                    16.4\n 9 Changi   2023     8     9                         0                       0  \n10 Changi   2023     8    10                         0                       0  \n# ℹ abbreviated name: ¹​`Highest 30 min Rainfall (mm)`\n# ℹ 7 more variables: `Highest 60 min Rainfall (mm)` &lt;dbl&gt;,\n#   `Highest 120 min Rainfall (mm)` &lt;dbl&gt;,\n#   `Mean Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Maximum Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Minimum Temperature (degrees celsius)` &lt;dbl&gt;,\n#   `Mean Wind Speed (km/h)` &lt;dbl&gt;, `Max Wind Speed (km/h)` &lt;dbl&gt;",
    "crumbs": [
      "Take-home Exercises",
      "Take-home Exercise 3: Be Weatherwise or Otherwise"
    ]
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03b.html#data-preparation",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03b.html#data-preparation",
    "title": "Take-home Exercise 3: Be Weatherwise or Otherwise",
    "section": "2.3 Data Preparation",
    "text": "2.3 Data Preparation\nAs the downloaded data are in different tables, we will join them into one table Changi first.\n\n\nShow the code\nchangi &lt;- full_join(aug1983, aug1993)\nchangi &lt;- full_join(changi, aug2003)\nchangi &lt;- full_join(changi, aug2003)\nchangi &lt;- full_join(changi, aug2013)\nchangi &lt;- full_join(changi, aug2023)\n\n\ndatatable(changi, caption = \"Table 1 - Observatons from Changi Weather Station\", class='compact')\n\n\n\n\n\n\nFrom the output above, we see that there are columns such as rainfall and wind that we do not need for this exercise. Since we want to verify if daily mean temperature is indeed rising over the years, we will retain the mean temperature column, which we assumed is the daily mean temperature.\n\n\n\n\n\n\nNote\n\n\n\nAt the time of this take-home exercise, we cannot find information on how Weather.gov.sg calculated “mean temperature” or “daily mean temperature”.\nAccording to this Source, the mean daily temperature is the mean of the temperatures observed at 24 equidistant times in the course of a continuous 24-hour period (normally the mean solar day from midnight to midnight according to the zonal time of the station).The data from Weather.gov.sg also provided us with the daily maximum temperature and daily minimum temperature, which is the maximum and minimum temperature in the course of a continuous time interval of 24 hours. However, if we do not have hourly temperature of the day, we would not be able to verify the mean temperature given and we are still unable to determine the distribution of the temperature throughout the day.Hence, we will drop the minimum and maximum temperature columns.\n\n\nWe will first drop the columns that we do not need (i.e. those related to rainfall and wind) and retain only the Day, Year, Mean Temperature using select. We also check the output using glimpse().\n\n\nShow the code\nchangitemp &lt;- changi %&gt;%\n  select(Day, Year, `Mean Temperature (degrees celsius)`)\n\nglimpse(changitemp)\n\n\nRows: 155\nColumns: 3\n$ Day                                  &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11…\n$ Year                                 &lt;dbl&gt; 1983, 1983, 1983, 1983, 1983, 198…\n$ `Mean Temperature (degrees celsius)` &lt;dbl&gt; 28.9, 28.4, 28.5, 26.3, 27.3, 27.…\n\n\nWe also use the following code chunk to check for missing data.\n\n\nShow the code\nany(is.na(changitemp))\n\n\n[1] FALSE\n\n\nFrom the above output, we verified that there is no missing data.\n\nRename Mean TemperatureChange Variables from character to factor\n\n\n\nchangitemp &lt;- rename(changitemp, \n       DailyTemp = `Mean Temperature (degrees celsius)`)\n\nglimpse(changitemp)\n\nRows: 155\nColumns: 3\n$ Day       &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n$ Year      &lt;dbl&gt; 1983, 1983, 1983, 1983, 1983, 1983, 1983, 1983, 1983, 1983, …\n$ DailyTemp &lt;dbl&gt; 28.9, 28.4, 28.5, 26.3, 27.3, 27.5, 27.9, 28.6, 28.8, 29.0, …\n\n\n\n\n\nchangitemp$Day &lt;- as.factor(changitemp$Day)\nchangitemp$Year &lt;- as.factor(changitemp$Year)\n\nglimpse(changitemp)\n\nRows: 155\nColumns: 3\n$ Day       &lt;fct&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n$ Year      &lt;fct&gt; 1983, 1983, 1983, 1983, 1983, 1983, 1983, 1983, 1983, 1983, …\n$ DailyTemp &lt;dbl&gt; 28.9, 28.4, 28.5, 26.3, 27.3, 27.5, 27.9, 28.6, 28.8, 29.0, …\n\n\n\n\n\nCurrently, we see that the temperatures for all years are in one column. To facilitate the visualisation and filtering of the charts in subsequent steps, we transform the table to make each Year a column using pivot_wider(). By doing so, the temperature for each year is in one column.\n\n\nShow the code\n#Make every year's temperature a column\nchangitemp_transformed &lt;- changitemp %&gt;%\n   pivot_wider(names_from = Year, values_from = DailyTemp)\n\n#Rename the Columns\nchangitemp_transformed &lt;- changitemp_transformed %&gt;%\n  rename(\"Year1983\" = `1983`,\n         \"Year1993\" = `1993`,\n         \"Year2003\" = `2003`,\n         \"Year2013\" = `2013`,\n         \"Year2023\" = `2023`)\n\n#Checking the end product \ndatatable(changitemp_transformed, caption = \"Table 2 - Daily Temperature by Years\", class='compact', rownames = FALSE)",
    "crumbs": [
      "Take-home Exercises",
      "Take-home Exercise 3: Be Weatherwise or Otherwise"
    ]
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03b.html#normality-test",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03b.html#normality-test",
    "title": "Take-home Exercise 3: Be Weatherwise or Otherwise",
    "section": "4.1 Normality test",
    "text": "4.1 Normality test\nBefore we can perform confirmatory data analysis to find out if the daily temperatures were indeed increasing over the years, we need to decide if parametric or non-parametric test should be used. As such, we will use ad.test() from nortest package to perform Anderson-Darling test with a confidence level of 95% to test the null hypothesis that the daily temperature for each year is normally distributed.\nIn the following code chunk, we will loop through each year (i.e. 1983, 1993, 2003, 2013 and 2023) to create a list called testresultlist containing a list of Anderson-Darling test results. Then we will create a tibble resultlist to contain the Year and the p-value result of the Anderson-Darling test. Then we will display the tibble using datatable() from DT package.\n\n\nShow the code\ntestresultlist &lt;- list()\n\nfor (i in unique(changitemp$Year)){\n  subdf &lt;- subset(x = changitemp, subset=Year==i)\n  testresultlist[[i]] &lt;- ad.test(subdf$DailyTemp)\n}\n\nresultlist &lt;- tibble(Year = unique(changitemp$Year),\n                     p_value = unlist(lapply(testresultlist, `[[`, 2)))\n\ndatatable(resultlist, rownames = FALSE, caption = \"Table 3 - p-value for each Year's Anderson-Darling test\", class='compact')\n\n\n\n\n\n\nBased on the result above, the null hypothesis (i.e. distribution is normally distributed) is rejected because the p-value for Years 1983, 1993, 2013 and 2023 are below the 0.05 critical value. As such, we are unable to confirm normality assumption for the distribution of daily temperature.",
    "crumbs": [
      "Take-home Exercises",
      "Take-home Exercise 3: Be Weatherwise or Otherwise"
    ]
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03b.html#anova-test",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03b.html#anova-test",
    "title": "Take-home Exercise 3: Be Weatherwise or Otherwise",
    "section": "4.2 ANOVA Test",
    "text": "4.2 ANOVA Test\nAs we are comparing the point estimates between more than 2 groups, we will use ggstatsplot’s ggbetweenstats() to visualise the ANOVA Test results. When visualising the ANOVA test results using ggbetweenstats(), non-parametric test is considered, hence type = \"np\" argument because we were unable to confirm the normality assumption for the distribution of daily temperature. In addition, we wanted to make pairwise comparisons e between significant pairs since we are interested to know those pairs with significant difference between them. Hence, pairwise.display = \"s\".\n\n\nShow the code\ntest1 &lt;- ggbetweenstats(\n  data = changitemp,\n  x = Year, \n  y = DailyTemp,\n  type = \"np\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE, \n  title = \"One-Way ANOVA Shows Differences in Daily Temperature Across Years\",\n  caption = \"Data from Weather.gov.sg\", \n  ylab = \"Daily Temperature\", \n  theme = theme_ipsum_rc(plot_title_size = 13, plot_title_margin=4,\n                         subtitle_size=11, subtitle_margin=4, \n                         axis_title_size = 8, axis_text_size=8, axis_title_face=\n                           \"bold\", plot_margin = margin(4, 4, 4, 4))) \n\n\ntest1\n\n\n\n\n\n\n\n\n\nAs noted above, the hypothesis testing is done using Kruskal-Wallis test with 95% confidence level. The hypothesis is:\nH0 : There is no difference between median daily temperatures between years.\nH1 : There is difference between median daily temperatures between years.\n\n\nShow the code\nextract_stats(test1)\n\n\n$subtitle_data\n# A tibble: 1 × 15\n  parameter1 parameter2 statistic df.error  p.value method                      \n  &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt;    &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;                       \n1 DailyTemp  Year            18.8        4 0.000844 Kruskal-Wallis rank sum test\n  effectsize      estimate conf.level conf.low conf.high conf.method         \n  &lt;chr&gt;              &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;               \n1 Epsilon2 (rank)    0.122       0.95   0.0730         1 percentile bootstrap\n  conf.iterations n.obs expression\n            &lt;int&gt; &lt;int&gt; &lt;list&gt;    \n1             100   155 &lt;language&gt;\n\n$caption_data\nNULL\n\n$pairwise_comparisons_data\n# A tibble: 10 × 9\n   group1 group2 statistic p.value alternative distribution p.adjust.method\n   &lt;chr&gt;  &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;        &lt;chr&gt;          \n 1 1983   1993       2.29  0.0553  two.sided   z            FDR            \n 2 1983   2003       0.803 0.469   two.sided   z            FDR            \n 3 1983   2013       0.947 0.429   two.sided   z            FDR            \n 4 1983   2023       2.71  0.0222  two.sided   z            FDR            \n 5 1993   2003       1.49  0.196   two.sided   z            FDR            \n 6 1993   2013       3.24  0.00607 two.sided   z            FDR            \n 7 1993   2023       0.425 0.671   two.sided   z            FDR            \n 8 2003   2013       1.75  0.133   two.sided   z            FDR            \n 9 2003   2023       1.91  0.112   two.sided   z            FDR            \n10 2013   2023       3.66  0.00252 two.sided   z            FDR            \n   test  expression\n   &lt;chr&gt; &lt;list&gt;    \n 1 Dunn  &lt;language&gt;\n 2 Dunn  &lt;language&gt;\n 3 Dunn  &lt;language&gt;\n 4 Dunn  &lt;language&gt;\n 5 Dunn  &lt;language&gt;\n 6 Dunn  &lt;language&gt;\n 7 Dunn  &lt;language&gt;\n 8 Dunn  &lt;language&gt;\n 9 Dunn  &lt;language&gt;\n10 Dunn  &lt;language&gt;\n\n$descriptive_data\nNULL\n\n$one_sample_data\nNULL\n\n$tidy_data\nNULL\n\n$glance_data\nNULL\n\n\n\n\n\n\n\n\nObservations\n\n\n\nSince the p-value (0.0008444241) is less than the critical value of 0.05, there is statistical evidence to reject the null hypothesis. We can conclude that there is difference between median daily temperature.\nIn the above plot, we observed that there are certain pairs of years with p-value less than 0.05. These pairs are: 1983 and 2023, 1993 and 2013, 2013 and 2023. This suggests that the differences between the medians of these pairs are statistically significant.\nLooking at the significant differences between 1983 and 2023 and 2013 and 2023, based on the medians, it seems that there was indeed a temperature rise from 1983 to 2023 and from 2013 and 2023. However, the differences between the medians were small, which were different from the figures quoted. We would need more years of data to ascertain the projectation that daily mean temperature would increase by 1.4 to 4.6 Degree Celsius.",
    "crumbs": [
      "Take-home Exercises",
      "Take-home Exercise 3: Be Weatherwise or Otherwise"
    ]
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03b.html#visualising-uncertainty",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03b.html#visualising-uncertainty",
    "title": "Take-home Exercise 3: Be Weatherwise or Otherwise",
    "section": "4.3 Visualising Uncertainty",
    "text": "4.3 Visualising Uncertainty\nAs we are using point estimates (i.e. median), there are uncertainties surrounding the point estimates since each estimate is derived from a bunch of figures. In our case, each median daily temperature for the year is derived from the 31 daily temperatures in August for the year. As such, it would be more accurate and informative to show the target quantile confidence levels (e.g. 95% or 99%) that the true (unknown) estimate would lie within the interval, given the evidence provided by the observed data.\nFor the following plot, we will use median point estimates instead of mean due to outliers and skewness of data. In addition, using median also allows to user to relate to the above one-way ANOVA analysis\nWith median as the point estimate, quantile intervals are used instead of confidence interval. We use 95% and 99% intervals because they are commonly associated with 5% and 1% error rate, which are commonly used in hypothesis testing.\n\n\nShow the code\n#Base ggplot\np2 &lt;- ggplot(\n  data = changitemp,\n  aes(x = Year,\n      y = DailyTemp)) +\n  \n  #Using stat_pointinterval to plot the points and intervals\n  stat_pointinterval(\n    aes(interval_color = stat(level)),\n    .width = c(0.95, 0.99),\n    .point = median,\n    .interval = qi,\n    point_color = \"black\",\n    show.legend = TRUE) + \n   stat_summary(fun = median, geom = \"text\", aes(label = round(after_stat(y), 1)), position = position_nudge(x = 0.15), vjust = -0.5,size=3)+\n  \n  #Add title, subtitle, x-axis labels \n  labs(title = \"Uncertainty in Median Daily Temperature in the Month of August\",\nsubtitle = \"Years: 1983, 1993, 2003, 2013, 2023 \\nWeather Station: Changi \\nQuantile Intervals: 95% and 99% of Daily Temperature\") +\n  xlab(\"Year\") + \n  ylab(\"Daily Temperature (Degree Celsius)\")+\n  \n  #Add a theme \n  theme_ipsum_rc(plot_title_size = 13, plot_title_margin=4, subtitle_size=11, subtitle_margin=4, \n                 axis_title_size = 8, axis_text_size=8, axis_title_face= \"bold\", plot_margin = margin(4, 4, 4, 4)) \n\np2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe length of the error bars indicates the amount of uncertainty. For those years with more outliers or more varied temperatures, they have higher uncertainities, hence longer length of error bar. For example Year 1993 had a wide range of daily temperatures as compared to other years. In contrast, 2013 had a relatively smaller range of temperatures and less outliers, hence a shorter length of error bar because it had lower uncertainties.",
    "crumbs": [
      "Take-home Exercises",
      "Take-home Exercise 3: Be Weatherwise or Otherwise"
    ]
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html",
    "title": "Hands-on Exercise 8: Modelling, Visualising and Analysing Network data with R",
    "section": "",
    "text": "In this hands- on exercise, we will learn how to model, analyse and visualise network data using R. In particular, we will learn how to:\n\ncreate graph object data frames, manipulate them using appropriate functions of dplyr, lubridate and tidygraph,\nbuild network visualisation using appropriate functions of ggraph,\ncompute network geometrics using tidygraph,\nbuild advanced graph visualisation by incorporating the network geometrics, and\nbuild interactive network visualisation using visNetwork package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#installing-and-loading-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#installing-and-loading-r-packages",
    "title": "Hands-on Exercise 8: Modelling, Visualising and Analysing Network data with R",
    "section": "2.1 Installing and Loading R Packages",
    "text": "2.1 Installing and Loading R Packages\nFor this exercise, other than tidyverse and lubridate, we will use the following packages:\n\nigraph:Routines for simple graphs and network analysis. It can handle large graphs very well and provides functions for generating random and regular graphs, graph visualization, centrality methods and much more.\ntidygraph:provides a tidy API for graph/network manipulation.\nggraph:an extension of ggplot2 aimed at supporting relational data structures such as networks, graphs, and trees.\nvisNetwork:for network visualization, using vis.js javascript library\n\nThe code chunk below uses p_load() of pacman package to check if the abovementioned packages are installed in the computer. If they are, they will be launched in R. Otherwise, pacman will install the relevant packages before launching them.\n\npacman::p_load(tidyverse, lubridate, igraph, tidygraph, ggraph, visNetwork)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#importing-data-into-r",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#importing-data-into-r",
    "title": "Hands-on Exercise 8: Modelling, Visualising and Analysing Network data with R",
    "section": "2.2 Importing Data into R",
    "text": "2.2 Importing Data into R\nThe data sets used in this hands-on exercise are from an oil exploration and extraction company. There are two data sets: one data set contains the nodes data and the other contains the edges (also know as link) data. We will import them using read_csv() of readr package.\n\nnodeedge\n\n\n\nnode &lt;- read_csv(\"data/GAStech_email_node.csv\")\nhead(node)\n\n# A tibble: 6 × 4\n     id label               Department     Title                                \n  &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt;          &lt;chr&gt;                                \n1     1 Mat.Bramar          Administration Assistant to CEO                     \n2     2 Anda.Ribera         Administration Assistant to CFO                     \n3     3 Rachel.Pantanal     Administration Assistant to CIO                     \n4     4 Linda.Lagos         Administration Assistant to COO                     \n5     5 Ruscella.Mies.Haber Administration Assistant to Engineering Group Manag…\n6     6 Carla.Forluniau     Administration Assistant to IT Group Manager        \n\n\nGAStech_email_nodes.csv consist of the name, department and title of the employees.\n\n\n\nedge &lt;- read_csv(\"data/GAStech_email_edge-v2.csv\")\nhead(edge)\n\n# A tibble: 6 × 8\n  source target SentDate SentTime Subject    MainSubject sourceLabel targetLabel\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;    &lt;time&gt;   &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt;       &lt;chr&gt;      \n1     43     41 6/1/2014 08:39    GT-Seismi… Work relat… Sven.Flecha Isak.Baza  \n2     43     40 6/1/2014 08:39    GT-Seismi… Work relat… Sven.Flecha Lucas.Alca…\n3     44     51 6/1/2014 08:58    Inspectio… Work relat… Kanon.Herr… Felix.Resu…\n4     44     52 6/1/2014 08:58    Inspectio… Work relat… Kanon.Herr… Hideki.Coc…\n5     44     53 6/1/2014 08:58    Inspectio… Work relat… Kanon.Herr… Inga.Ferro \n6     44     45 6/1/2014 08:58    Inspectio… Work relat… Kanon.Herr… Varja.Lagos\n\n\nGAStech-email_edges.csv consists of emails correspondances between the employees."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#data-checks",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#data-checks",
    "title": "Hands-on Exercise 8: Modelling, Visualising and Analysing Network data with R",
    "section": "2.3 Data Checks",
    "text": "2.3 Data Checks\nWe will now examine the structure of the data frame using glimpse() of dplyr.\n\nnodeedge\n\n\n\nglimpse(node)\n\nRows: 54\nColumns: 4\n$ id         &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 44, 45, 46, 8, 9, 10, 11, 12, 13, 14, …\n$ label      &lt;chr&gt; \"Mat.Bramar\", \"Anda.Ribera\", \"Rachel.Pantanal\", \"Linda.Lago…\n$ Department &lt;chr&gt; \"Administration\", \"Administration\", \"Administration\", \"Admi…\n$ Title      &lt;chr&gt; \"Assistant to CEO\", \"Assistant to CFO\", \"Assistant to CIO\",…\n\n\n\n\n\nglimpse(edge)\n\nRows: 9,063\nColumns: 8\n$ source      &lt;dbl&gt; 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      &lt;dbl&gt; 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    &lt;chr&gt; \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    &lt;time&gt; 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     &lt;chr&gt; \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject &lt;chr&gt; \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel &lt;chr&gt; \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel &lt;chr&gt; \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n\n\n\n\n\n:::{.callout-note} ## Observations from the abouve"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#tbl_graph-object",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#tbl_graph-object",
    "title": "Hands-on Exercise 8: Modelling, Visualising and Analysing Network data with R",
    "section": "4.1 tbl_graph object",
    "text": "4.1 tbl_graph object\nGraphs and networks can come from many sources, or be created by simulation or deterministacally. Tidygraph provides conversions from all well-known structures in R, as well as a range of create_() and play_*() functions for creating well-defined or simulated graphs.\nWe can make use of tbl_graph() and as_tbl_graph() to create network objects.\n\ntbl_graph() creates a tbl_graph network object from nodes and edges data.\nas_tbl_graph() converts network data and objects to a tbl_graph network. Below are network data and objects supported by as_tbl_graph()\na node data.frame and an edge data.frame,\ndata.frame, list, matrix from base,\nigraph from igraph,\nnetwork from network,\ndendrogram and hclust from stats,\nNode from data.tree,\nphylo and evonet from ape, and\ngraphNEL, graphAM, graphBAM from graph (in Bioconductor)."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#dplyr-verbs-in-tidygraph",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#dplyr-verbs-in-tidygraph",
    "title": "Hands-on Exercise 8: Modelling, Visualising and Analysing Network data with R",
    "section": "4.2 dplyr verbs in tidygraph",
    "text": "4.2 dplyr verbs in tidygraph\n\nactivate() verb from tidygraph serves as a switch between tibbles for nodes and edges. All dplyr verbs applied to tbl_graph object are applied to the active tibble.\n.N() function is used to gain access to the node data while manipulating the edge data.\n.E() function is used to gain access to the edge data.\n.G() function is used to give us the tbl_graph object itself."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#building-tidygraph-data-model",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#building-tidygraph-data-model",
    "title": "Hands-on Exercise 8: Modelling, Visualising and Analysing Network data with R",
    "section": "4.3 Building tidygraph data model",
    "text": "4.3 Building tidygraph data model\nIn this section, we will use tbl_graph() of tinygraph package to build an tidygraph’s network graph data.frame.\n\ngraph &lt;- tbl_graph(nodes = node,\n                   edges = edge_aggregated,\n                   directed = TRUE)\n\ngraph \n\n# A tbl_graph: 54 nodes and 1456 edges\n#\n# A directed multigraph with 1 component\n#\n# Node Data: 54 × 4 (active)\n      id label               Department     Title                               \n   &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt;          &lt;chr&gt;                               \n 1     1 Mat.Bramar          Administration Assistant to CEO                    \n 2     2 Anda.Ribera         Administration Assistant to CFO                    \n 3     3 Rachel.Pantanal     Administration Assistant to CIO                    \n 4     4 Linda.Lagos         Administration Assistant to COO                    \n 5     5 Ruscella.Mies.Haber Administration Assistant to Engineering Group Mana…\n 6     6 Carla.Forluniau     Administration Assistant to IT Group Manager       \n 7     7 Cornelia.Lais       Administration Assistant to Security Group Manager \n 8    44 Kanon.Herrero       Security       Badging Office                      \n 9    45 Varja.Lagos         Security       Badging Office                      \n10    46 Stenig.Fusil        Security       Building Control                    \n# ℹ 44 more rows\n#\n# Edge Data: 1,456 × 4\n   from    to Weekday   Weight\n  &lt;int&gt; &lt;int&gt; &lt;ord&gt;      &lt;int&gt;\n1     1     2 Monday         4\n2     1     2 Tuesday        3\n3     1     2 Wednesday      5\n# ℹ 1,453 more rows\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nThe output above reveals that GAStech_graph is a tbl_graph object with 54 nodes and 4541 edges.\nThe command also prints the first six rows of “Node Data” and the first three of “Edge Data”.\nIt states that the Node Data is active. The notion of an active tibble within a tbl_graph object makes it possible to manipulate the data in one tibble at a time."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#changing-the-active-object",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#changing-the-active-object",
    "title": "Hands-on Exercise 8: Modelling, Visualising and Analysing Network data with R",
    "section": "4.4 Changing the active object",
    "text": "4.4 Changing the active object\nThe nodes tibble data frame is activated by default, but you can change which tibble data frame is active with the activate() function. Thus, if we wanted to rearrange the rows in the edges tibble to list those with the highest “weight” first, we could use activate() and then arrange().\n\ngraph %&gt;% \n  activate(edges) %&gt;%\n  arrange(desc(Weight))\n\n# A tbl_graph: 54 nodes and 1456 edges\n#\n# A directed multigraph with 1 component\n#\n# Edge Data: 1,456 × 4 (active)\n    from    to Weekday   Weight\n   &lt;int&gt; &lt;int&gt; &lt;ord&gt;      &lt;int&gt;\n 1    40    41 Tuesday       23\n 2    40    43 Tuesday       19\n 3    41    43 Tuesday       15\n 4    41    40 Tuesday       14\n 5    42    41 Tuesday       13\n 6    42    40 Tuesday       12\n 7    42    43 Tuesday       11\n 8    43    42 Wednesday     11\n 9    36    32 Wednesday      9\n10    40    41 Monday         9\n# ℹ 1,446 more rows\n#\n# Node Data: 54 × 4\n     id label           Department     Title           \n  &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;          &lt;chr&gt;           \n1     1 Mat.Bramar      Administration Assistant to CEO\n2     2 Anda.Ribera     Administration Assistant to CFO\n3     3 Rachel.Pantanal Administration Assistant to CIO\n# ℹ 51 more rows"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#plotting-a-basic-network-graph",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#plotting-a-basic-network-graph",
    "title": "Hands-on Exercise 8: Modelling, Visualising and Analysing Network data with R",
    "section": "5.1 Plotting a basic network graph",
    "text": "5.1 Plotting a basic network graph\nThe code chunk below uses ggraph(), geom-edge_link() and geom_node_point() to plot a network graph by using graph which is the tbl_graph object that we created in previous section.\n\nggraph(graph) +\n  geom_edge_link() +\n  geom_node_point()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe basic plotting function is ggraph(), which takes the data to be used for the graph and the type of layout desired.\nBoth of the arguments for ggraph() are built around igraph. Therefore, ggraph() can use either an igraph object or a tbl_graph object."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#customising-ggraphs-network-graphs",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#customising-ggraphs-network-graphs",
    "title": "Hands-on Exercise 8: Modelling, Visualising and Analysing Network data with R",
    "section": "5.2 Customising ggraph’s network graphs",
    "text": "5.2 Customising ggraph’s network graphs\nWe can use ggraph’s theme_graph() to further customise our network graph. This theme_graph() is tuned for graph visualisations because the coordinate values are often of no importance and axes are thus a distraction. Hence, ggraph has theme_graph(), a built-in theme to remove redundant elements. Furthermore the default behaviour is to use a narrow font so text takes up less space. Theme colour is defined by a background and foreground colour where the background defines the colour of the whole graphics area and the foreground defines the colour of the strip and border. By default strip and border is turned off as it is an unnecessary element unless facetting is used. To add a foreground colour to a plot that is already using theme_graph the th_foreground helper is provided. In order to use this appearance as default use the set_graph_style function. An added benefit of this is that it also changes the default text-related values in the different geoms for a completely coherent look. unset_graph_style can be used to revert the defaults back to their default settings (that is, they are not necessarily reverted back to what they were prior to calling set_graph_style). The th_no_axes() helper is provided to modify an existing theme so that grid and axes are removed.\n\n#create the graph\ng &lt;- ggraph(graph) +\n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\n#adding the theme\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n5.2.1 Changing the plot colour\ntheme_graph() makes it easy to change the coloring of the plot.\n\ng &lt;- ggraph(graph) + \n  geom_edge_link(aes(colour = 'grey50')) +\n  geom_node_point(aes(colour = 'grey40'))\n\ng + theme_graph(background = 'grey10',\n                text_colour = 'white')"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#working-with-ggraphs-layouts",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#working-with-ggraphs-layouts",
    "title": "Hands-on Exercise 8: Modelling, Visualising and Analysing Network data with R",
    "section": "5.3 Working with ggraph’s layouts",
    "text": "5.3 Working with ggraph’s layouts\nggraph support many layout for standard used, they are: star, circle, nicely (default), dh, gem, graphopt, grid, mds, spahere, randomly, fr, kk, drl and lgl. Figures below and on the right show layouts supported by ggraph().\n\nFruchterman and Reingold layoutstar layouteigen layout\n\n\n\ng &lt;- ggraph(graph, \n            layout = \"fr\") +\n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\n\ng &lt;- ggraph(graph, \n            layout = \"star\") +\n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\n\ng &lt;- ggraph(graph, \n            layout = \"eigen\") +\n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#modifying-network-nodes",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#modifying-network-nodes",
    "title": "Hands-on Exercise 8: Modelling, Visualising and Analysing Network data with R",
    "section": "5.4 Modifying network nodes",
    "text": "5.4 Modifying network nodes\nWe can also define the colour each node by referring to their respective departments.\n\ng &lt;- ggraph(graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes()) +\n  geom_node_point(aes(colour = Department, \n                      size = 3))\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\ngeom_node_point is equivalent in functionality to geo_point of ggplot2. It allows for simple plotting of nodes in different shapes, colours and sizes. In the codes chnuks above colour and size are used."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#modifying-edges",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#modifying-edges",
    "title": "Hands-on Exercise 8: Modelling, Visualising and Analysing Network data with R",
    "section": "5.5 Modifying Edges",
    "text": "5.5 Modifying Edges\nWe can also modify the edges such that the thickness of the edges will be mapped with the Weight variable.\n\ng &lt;- ggraph(graph, \n            layout = \"nicely\") +\n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 3)\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\ngeom_edge_link draws edges in the simplest way - as straight lines between the start and end nodes. But, it can do more that that. In the example above, argument width is used to map the width of the line in proportional to the Weight attribute and argument alpha is used to introduce opacity on the line."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#creating-facet-graphs",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#creating-facet-graphs",
    "title": "Hands-on Exercise 8: Modelling, Visualising and Analysing Network data with R",
    "section": "5.6 Creating Facet Graphs",
    "text": "5.6 Creating Facet Graphs\nAnother very useful feature of ggraph is faceting. In visualising network data, this technique can be used to reduce edge over-plotting in a very meaning way by spreading nodes and edges out based on their attributes.\nThere are three functions in ggraph to implement faceting, they are:\n\nfacet_nodes() whereby edges are only draw in a panel if both terminal nodes are present here,\nfacet_edges() whereby nodes are always drawn in a panels even if the node data contains an attribute named the same as the one used for the edge facetting, and\nfacet_graph() faceting on two variables simultaneously.\n\n\n5.6.1 facet_edges()\nThis function creates small multiples based on edge attributes. It is equivalent to ggplot::facet_wrap() but only facet edges. Nodes are repeated in every panel.\n\nset_graph_style()\n\ng &lt;- ggraph(graph, layout = \"nicely\") +\n  geom_edge_link(aes(width=Weight),\n                 alpha = 0.2) +\n  scale_edge_width(range = c(0.1, 5)) + \n  geom_node_point(aes(color = Department),\n                  size = 2)\n\ng + facet_edges(~Weekday)\n\n\n\n\n\n\n\n\nCurrently the legend is on the right side. We can use theme() to change the position of the legend.\n\nset_graph_style()\n\ng &lt;- ggraph(graph, layout = \"nicely\") +\n  geom_edge_link(aes(width=Weight),\n                 alpha = 0.2) +\n  scale_edge_width(range = c(0.1, 5)) + \n  geom_node_point(aes(color = Department),\n                  size = 2) +\n  theme(legend.position = \"bottom\")\n\ng + facet_edges(~Weekday)\n\n\n\n\n\n\n\n\nWe can also add frame to each facet graph using th_foreground().\n\nset_graph_style()\n\ng &lt;- ggraph(graph, layout = \"nicely\") +\n  geom_edge_link(aes(width=Weight),\n                 alpha = 0.2) +\n  scale_edge_width(range = c(0.1, 5)) + \n  geom_node_point(aes(color = Department),\n                  size = 2) +\n  theme(legend.position = \"bottom\")\n\ng + facet_edges(~Weekday) +\n  th_foreground(foreground = \"skyblue\",\n                border = TRUE)\n\n\n\n\n\n\n\n\n\n\n5.6.2 facet_nodes()\nfacet_nodes() creates small multiples based on node attributes. This function is equivalent to ggplot2::facet_wrap() but only facets nodes. Edges are drawn if their terminal nodes are both present in a panel.\n\nset_graph_style()\n\ng &lt;- ggraph(graph, layout = \"nicely\") +\n  geom_edge_link(aes(width=Weight),\n                 alpha = 0.2) +\n  scale_edge_width(range = c(0.1, 5)) + \n  geom_node_point(aes(color = Department),\n                  size = 2) \n\ng + facet_nodes(~Department) +\n  th_foreground(foreground = \"skyblue\",\n                border = TRUE) +\n  theme(legend.position = 'bottom')"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#computing-centrality-indices",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#computing-centrality-indices",
    "title": "Hands-on Exercise 8: Modelling, Visualising and Analysing Network data with R",
    "section": "6.1 Computing Centrality Indices",
    "text": "6.1 Computing Centrality Indices"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#visualising-centrality-indices",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#visualising-centrality-indices",
    "title": "Hands-on Exercise 8: Modelling, Visualising and Analysing Network data with R",
    "section": "6.2 Visualising Centrality Indices",
    "text": "6.2 Visualising Centrality Indices"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#visualising-community",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#visualising-community",
    "title": "Hands-on Exercise 8: Modelling, Visualising and Analysing Network data with R",
    "section": "6.2 Visualising Community",
    "text": "6.2 Visualising Community\ntidygraph package inherits many of the community detection algorithms imbedded into igraph and makes them available to us, including Edge-betweenness (group_edge_betweenness), Leading eigenvector (group_leading_eigen), Fast-greedy (group_fast_greedy), Louvain (group_louvain), Walktrap (group_walktrap), Label propagation (group_label_prop), InfoMAP (group_infomap), Spinglass (group_spinglass), and Optimal (group_optimal). Some community algorithms are designed to take into account direction or weight, while others ignore it. Use this link to find out more about community detection functions provided by tidygraph.\nIn the code chunk below group_edge_betweenness() is used.\n\ng &lt;- graph %&gt;%\n  mutate(community = as.factor(group_edge_betweenness(weights = Weight, directed = TRUE))) %&gt;%\n  ggraph(layout = \"fr\") +\n  geom_edge_link(aes(width = Weight),\n                 alpha = 0.2) +\n  scale_edge_width(range = c(0.1, 5)) + \n  geom_node_point(aes(color = community))\n\n  g + theme_graph()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#data-preparation",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#data-preparation",
    "title": "Hands-on Exercise 8: Modelling, Visualising and Analysing Network data with R",
    "section": "7.1 Data Preparation",
    "text": "7.1 Data Preparation\nBefore we can plot the interactive graph, let us prepare the data using the following code chunk so that it has the required columns.\n\nedge_aggregated2 &lt;- edge %&gt;%\n  left_join(node, by = c(\"sourceLabel\" = \"label\")) %&gt;%\n  rename(from = id) %&gt;%\n  left_join(node, by = c(\"targetLabel\" = \"label\")) %&gt;%\n  rename(to = id) %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(from, to) %&gt;%\n  summarise(weight = n()) %&gt;%\n  filter(from != to) %&gt;%\n  filter(weight &gt;1) %&gt;%\n  ungroup()\n\nhead(edge_aggregated2)\n\n# A tibble: 6 × 3\n   from    to weight\n  &lt;dbl&gt; &lt;dbl&gt;  &lt;int&gt;\n1     1     2     21\n2     1     3     21\n3     1     4     21\n4     1     5     21\n5     1     6     21\n6     1     7     21\n\n\n\n\n\n\n\n\nAbout the above code chunk\n\n\n\n\nPerform left join between edge dataframe and node dataframe using the sourceLabel and label columns from the respective dataframe. By doing so, the resulting dataframe have additional information (i.e. id, group and title) about the source employee.\nrename id to from so that we can use this from column later.\nPerform left join between edge dataframe and node dataframe using the targetLabel and label columns from the respective dataframe. By doing so, the resulting dataframe have additional information (i.e. id, group and title) about the target employee.\nrename id to to so that we can use this to column later.\nfilter the rows where the MainSubject is related to work\nThen perform group_by using from and to.\nThen we create a new field Weight by counting the number of emails resulting from the group_by.\nWe also remove those emails where the source and target are the same employees and those weights that are less than 1 to get the resultant table edge_aggregated2."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#plotting-the-first-interactive-network-graph",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#plotting-the-first-interactive-network-graph",
    "title": "Hands-on Exercise 8: Modelling, Visualising and Analysing Network data with R",
    "section": "7.2 Plotting the first interactive network graph",
    "text": "7.2 Plotting the first interactive network graph\nLet us plot an interactive network graph using the data prepared.\n\nvisNetwork(node, \n           edge_aggregated2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#working-with-layout",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#working-with-layout",
    "title": "Hands-on Exercise 8: Modelling, Visualising and Analysing Network data with R",
    "section": "7.3 Working with layout",
    "text": "7.3 Working with layout\nWe can change the layout of the network graph by specifying the type of layout we want using the function visIgraphLayout()\n\nvisNetwork(node, \n           edge_aggregated2) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#working-with-visual-attributes---notes",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#working-with-visual-attributes---notes",
    "title": "Hands-on Exercise 8: Modelling, Visualising and Analysing Network data with R",
    "section": "7.4 Working with visual attributes - Notes",
    "text": "7.4 Working with visual attributes - Notes\nvisNetwork() looks for a field called “group” in the nodes object and colour the nodes according to the values of the group field.\nThe code chunk below rename Department field to group.\n\nnode &lt;- node %&gt;%\n  rename(group = Department)\n\nWhen we rerun the code chunk below, visNetwork shades the nodes by assigning unique colour to each category in the group field.\n\nvisNetwork(node, \n           edge_aggregated2) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 2024)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#working-with-visual-attributes---edges",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#working-with-visual-attributes---edges",
    "title": "Hands-on Exercise 8: Modelling, Visualising and Analysing Network data with R",
    "section": "7.5 Working with visual attributes - Edges",
    "text": "7.5 Working with visual attributes - Edges\nIn the code run below visEdges() is used to symbolise the edges. - The argument arrows is used to define where to place the arrow. - The smooth argument is used to plot the edges using a smooth curve.\n\nvisNetwork(node, \n           edge_aggregated2) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visEdges(arrows = \"to\",\n           smooth = list(enables = TRUE,  \n                         type = \"curvedCW\")) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 2024)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#interactivity",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#interactivity",
    "title": "Hands-on Exercise 8: Modelling, Visualising and Analysing Network data with R",
    "section": "7.6 Interactivity",
    "text": "7.6 Interactivity\nIn the code chunk below, visOptions() is used to incorporate interactivity features in the data visualisation.\n\n\n\n\n\n\nAbout the code chunk\n\n\n\nThe argument highlightNearest highlights nearest when clicking a node. The argument nodesIdSelection adds an id node selection creating an HTML select element.\n\n\n\nvisNetwork(node, \n           edge_aggregated2) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visOptions(highlightNearest = TRUE,\n             nodesIdSelection = TRUE) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 2024)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#computing-and-visualising-centrality-indices",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#computing-and-visualising-centrality-indices",
    "title": "Hands-on Exercise 8: Modelling, Visualising and Analysing Network data with R",
    "section": "6.1 Computing and Visualising Centrality Indices",
    "text": "6.1 Computing and Visualising Centrality Indices\nIt is important to note that from ggraph v2.0 onward tidygraph algorithms such as centrality measures can be accessed directly in ggraph calls. This means that it is no longer necessary to precompute and store derived node and edge centrality measures on the graph in order to use them in a plot.\nThe code chunks below show how to compute and visualise graphs based on centrality indices before ggraph v2.0 and after ggraph v2.0.\n\nManual Computation (before ggraph v2.0)No Computation (ggraph v2.0 onwards)\n\n\n\ng &lt;- graph %&gt;%\n  mutate(betweenness_centrality = centrality_betweenness()) %&gt;%\n  ggraph(layout = \"fr\") +\n  geom_edge_link(aes(width=Weight),\n                 alpha = 0.2) +\n  scale_edge_width(range = c(0.1, 5)) + \n  geom_node_point(aes(color = Department,\n                  size = betweenness_centrality)) \n\ng + theme_graph() +\n  labs(title = 'The email relationship between employees based on betweenness centrality')\n\n\n\n\n\n\n\n\n\nmutate() of dplyr is used to perform the computation.\nthe algorithm used, on the other hand, is the centrality_betweenness() of tidygraph.\n\n\n\n\ng &lt;- graph %&gt;%\n  ggraph(layout = \"fr\") +\n  geom_edge_link(aes(width=Weight),\n                 alpha = 0.2) +\n  scale_edge_width(range = c(0.1, 5)) + \n  geom_node_point(aes(color = Department,\n                  size = centrality_betweenness()))\n\ng + theme_graph() +\n   labs(title = 'The email relationship between employees based on betweenness centrality')"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html",
    "title": "Take-home Exercise 4: Prototyping Time Series Module for Visual Analytics Shiny Application",
    "section": "",
    "text": "In this exercise, we will be preparing the time-series forecasting module of the proposed Shiny Application and complete the following task:\n\nEvaluate and determine the necessary R packages needed for my group project’s Shiny application;\nPrepare and test the specific R codes can be run and returned the correct output as expected;\nDetermine the parameters and outputs that will be exposed on the Shiny applications; and\nSelect the appropriate Shiny UI components for exposing the parameters determined above."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#creating-a-date-column",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#creating-a-date-column",
    "title": "Take-home Exercise 4: Prototyping Time Series Module for Visual Analytics Shiny Application",
    "section": "2.3 Creating a date column",
    "text": "2.3 Creating a date column\n\ndata$tdate &lt;- paste(data$year, \"-\", data$month, \"-\", data$day)\ndata &lt;- data %&gt;%\n  mutate(tdate = ymd(tdate))\n\nglimpse(data)\n\nRows: 329,156\nColumns: 14\n$ station                  &lt;chr&gt; \"Macritchie Reservoir\", \"Macritchie Reservoir…\n$ year                     &lt;dbl&gt; 1980, 1980, 1980, 1980, 1980, 1980, 1980, 198…\n$ month                    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ day                      &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14…\n$ daily_rainfall_total     &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 22.6, 49.6, 2.4, 0.0, 0.0…\n$ highest_30_min_rainfall  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ highest_60_min_rainfall  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ highest_120_min_rainfall &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ mean_temperature         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ maximum_temperature      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ minimum_temperature      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ mean_wind_speed          &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ max_wind_speed           &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ tdate                    &lt;date&gt; 1980-01-01, 1980-01-02, 1980-01-03, 1980-01-…"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#selecting-the-relevant-columns-for-temperature-data",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#selecting-the-relevant-columns-for-temperature-data",
    "title": "Take-home Exercise 4: Prototyping Time Series Module for Visual Analytics Shiny Application",
    "section": "3.1 Selecting the relevant columns for Temperature Data",
    "text": "3.1 Selecting the relevant columns for Temperature Data\n\ntemp &lt;- data %&gt;%\n  select(station, tdate, mean_temperature, maximum_temperature, minimum_temperature) \n\nglimpse(temp)\n\nRows: 329,156\nColumns: 5\n$ station             &lt;chr&gt; \"Macritchie Reservoir\", \"Macritchie Reservoir\", \"M…\n$ tdate               &lt;date&gt; 1980-01-01, 1980-01-02, 1980-01-03, 1980-01-04, 1…\n$ mean_temperature    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ maximum_temperature &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ minimum_temperature &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#checking-for-missing-values",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#checking-for-missing-values",
    "title": "Take-home Exercise 4: Prototyping Time Series Module for Visual Analytics Shiny Application",
    "section": "3.2 Checking for missing values",
    "text": "3.2 Checking for missing values\n\nsummary(temp)\n\n   station              tdate            mean_temperature maximum_temperature\n Length:329156      Min.   :1980-01-01   Min.   :22.20    Min.   :22.80      \n Class :character   1st Qu.:1997-04-29   1st Qu.:27.10    1st Qu.:30.50      \n Mode  :character   Median :2011-09-18   Median :27.90    Median :31.70      \n                    Mean   :2007-07-02   Mean   :27.87    Mean   :31.49      \n                    3rd Qu.:2017-11-27   3rd Qu.:28.80    3rd Qu.:32.60      \n                    Max.   :2023-12-31   Max.   :31.50    Max.   :38.00      \n                    NA's   :58           NA's   :255645   NA's   :255282     \n minimum_temperature\n Min.   :20.0       \n 1st Qu.:24.3       \n Median :25.2       \n Mean   :25.3       \n 3rd Qu.:26.3       \n Max.   :30.0       \n NA's   :255283     \n\n\ndrop those rows where date is missing.\n\ntemp &lt;- temp %&gt;%\n  drop_na(tdate)\n\nsummary(temp)\n\n   station              tdate            mean_temperature maximum_temperature\n Length:329098      Min.   :1980-01-01   Min.   :22.20    Min.   :22.80      \n Class :character   1st Qu.:1997-04-29   1st Qu.:27.10    1st Qu.:30.50      \n Mode  :character   Median :2011-09-18   Median :27.90    Median :31.70      \n                    Mean   :2007-07-02   Mean   :27.87    Mean   :31.49      \n                    3rd Qu.:2017-11-27   3rd Qu.:28.80    3rd Qu.:32.60      \n                    Max.   :2023-12-31   Max.   :31.50    Max.   :38.00      \n                                         NA's   :255587   NA's   :255224     \n minimum_temperature\n Min.   :20.0       \n 1st Qu.:24.3       \n Median :25.2       \n Mean   :25.3       \n 3rd Qu.:26.3       \n Max.   :30.0       \n NA's   :255225     \n\n\n\nunique(temp$station)\n\n [1] \"Macritchie Reservoir\"    \"Lower Peirce Reservoir\" \n [3] \"Admiralty\"               \"East Coast Parkway\"     \n [5] \"Ang Mo Kio\"              \"Newton\"                 \n [7] \"Lim Chu Kang\"            \"Marine Parade\"          \n [9] \"Choa Chu Kang (Central)\" \"Tuas South\"             \n[11] \"Pasir Panjang\"           \"Jurong Island\"          \n[13] \"Nicoll Highway\"          \"Botanic Garden\"         \n[15] \"Choa Chu Kang (South)\"   \"Whampoa\"                \n[17] \"Changi\"                  \"Jurong Pier\"            \n[19] \"Ulu Pandan\"              \"Mandai\"                 \n[21] \"Tai Seng\"                \"Jurong (West)\"          \n[23] \"Clementi\"                \"Sentosa Island\"         \n[25] \"Bukit Panjang\"           \"Kranji Reservoir\"       \n[27] \"Upper Peirce Reservoir\"  \"Kent Ridge\"             \n[29] \"Queenstown\"              \"Tanjong Katong\"         \n[31] \"Somerset (Road)\"         \"Punggol\"                \n[33] \"Simei\"                   \"Toa Payoh\"              \n[35] \"Tuas\"                    \"Bukit Timah\"            \n[37] \"Pasir Ris (Central)\"    \n\n\n\nmissing.values &lt;- temp %&gt;%\n  gather(key = \"key\", value = \"val\") %&gt;%\n  mutate(isna = is.na(val)) %&gt;%\n  group_by(key) %&gt;%\n  mutate(total = n()) %&gt;%\n  group_by(key, total, isna) %&gt;%\n  summarise(num.isna = n()) %&gt;%\n  mutate(pct = num.isna / total * 100)\n\nlevels &lt;-\n    (missing.values  %&gt;% filter(isna == T) %&gt;% arrange(desc(pct)))$key\n\npercentage.plot &lt;- missing.values %&gt;%\n      ggplot() +\n        geom_bar(aes(x = reorder(key, desc(pct)), \n                     y = pct, fill=isna), \n                 stat = 'identity', alpha=0.8) +\n      scale_x_discrete(limits = levels) +\n      scale_fill_manual(name = \"\", \n                        values = c('steelblue', 'tomato3'), labels = c(\"Present\", \"Missing\")) +\n      coord_flip() +\n      labs(title = \"Percentage of missing values\", x =\n             'Variable', y = \"% of missing values\")\n\npercentage.plot"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#further-exploration-of-missing-mean_temperature-using-plotly",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#further-exploration-of-missing-mean_temperature-using-plotly",
    "title": "Take-home Exercise 4: Prototyping Time Series Module for Visual Analytics Shiny Application",
    "section": "3.3 Further exploration of missing mean_temperature using plotly",
    "text": "3.3 Further exploration of missing mean_temperature using plotly\n\ntemp_mean_wide &lt;- temp %&gt;%\n  select(tdate, station, mean_temperature) %&gt;%\n  pivot_wider(names_from = station, values_from = mean_temperature)\n\nglimpse(temp_mean_wide)\n\nRows: 16,071\nColumns: 38\n$ tdate                     &lt;date&gt; 1980-01-01, 1980-01-02, 1980-01-03, 1980-01…\n$ `Macritchie Reservoir`    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Lower Peirce Reservoir`  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ Admiralty                 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `East Coast Parkway`      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Ang Mo Kio`              &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ Newton                    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Lim Chu Kang`            &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Marine Parade`           &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Choa Chu Kang (Central)` &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Tuas South`              &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Pasir Panjang`           &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Jurong Island`           &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Nicoll Highway`          &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Botanic Garden`          &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Choa Chu Kang (South)`   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ Whampoa                   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ Changi                    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Jurong Pier`             &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Ulu Pandan`              &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ Mandai                    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Tai Seng`                &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Jurong (West)`           &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ Clementi                  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Sentosa Island`          &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Bukit Panjang`           &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Kranji Reservoir`        &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Upper Peirce Reservoir`  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Kent Ridge`              &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ Queenstown                &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Tanjong Katong`          &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Somerset (Road)`         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ Punggol                   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ Simei                     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Toa Payoh`               &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ Tuas                      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Bukit Timah`             &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Pasir Ris (Central)`     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n\n\n\nplot_ly(data = temp_mean_wide, \n        x = ~tdate, \n        y = ~ Admiralty, \n        type = \"scatter\", \n        mode = \"lines+markers\") |&gt; \n  layout(title = \"Temperature observed by Weather Station\", \n       xaxis = list(title = \"Date\"), \n       yaxis = list(title = \"\", range = c(0,40)), \n      theme_ipsum_rc(plot_title_size = 13, plot_title_margin=4, subtitle_size=11, subtitle_margin=4,  \n                 axis_title_size = 8, axis_text_size=8, axis_title_face= \"bold\", plot_margin = margin(4, 4, 4, 4)),  \n       updatemenus = list(list(type = 'dropdown', \n                               xref = \"paper\", \n                               yref = \"paper\", \n                               xanchor = \"left\",\n                               x = 0.04,\n                               y = 0.95, \n                               buttons = list(\n                                 list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_wide$Admiralty)), \n                                                    list(yaxis = list(title = \"Temperature in Admiralty\", range = c(0,40)))),label = \"Admiralty\"),\n                                 list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_wide$`East Coast Parkway`)), \n                                                    list(yaxis = list(title = \"Temperature in East Coast Parkway\", range = c(0,40)))),label = \"East Coast Parkway\"), \n                                 list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_wide$`Ang Mo Kio`)), \n                                                    list(yaxis = list(title = \"Temperature in Ang Mo Kio\", range = c(0,40)))),label = \"Ang Mo Kio\"), \n                                 list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_wide$Newton)), \n                                                    list(yaxis = list(title = \"Temperature in Newton\", range = c(0,40)))),label = \"Newton\"), \n                                 list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_wide$`Tuas South`)), \n                                                    list(yaxis = list(title = \"Temperature in Tuas South\", range = c(0,40)))),label = \"Tuas South\"),\n                                  list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_wide$`Pasir Panjang`)), \n                                                    list(yaxis = list(title = \"Temperature in Pasir Panjang\", range = c(0,40)))),label = \"Pasir Panjang\"), \n                                  list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_wide$`Jurong Island`)), \n                                                    list(yaxis = list(title = \"Temperature in Jurong Island\", range = c(0,40)))),label = \"Jurong Island\"), \n                                 list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_wide$`Choa Chu Kang (South)`)), \n                                                    list(yaxis = list(title = \"Temperature in Choa Chu Kang (South)\", range = c(0,40)))),label = \"Choa Chu Kang (South)\"), \n                                 list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$Changi)), \n                                                    list(yaxis = list(title = \"Temperature in Changi\", range = c(0,40)))),label = \"Changi\"),\n                                  list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_wide$`Tai Seng`)), \n                                                    list(yaxis = list(title = \"Temperature in Tai Seng\", range = c(0,40)))),label = \"Tai Seng\"),\n                                  list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_wide$`Jurong (West)`)), \n                                                    list(yaxis = list(title = \"Temperature in Jurong West\", range = c(0,40)))),label = \"Jurong West\"), \n                                   list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$Clementi)), \n                                                    list(yaxis = list(title = \"Temperature  in Clementi\", range = c(0,40)))),label = \"Clementi\"), \n                                   list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Sentosa Island`)), \n                                                    list(yaxis = list(title = \"Temperature  in Sentosa\", range = c(0,40)))),label = \"Sentosa\"), \n                                 list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Macritchie Reservoir`)), \n                                                    list(yaxis = list(title = \"Temperature  at Macritchie Reservoir\", range = c(0,40)))),label = \"Macritchie Reservoir\"), \n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Lower Peirce Reservoir`)), \n                                                    list(yaxis = list(title = \"Temperature  at Lower Peirce Reservoir\", range = c(0,40)))),label = \"Lower Peirce Reservoir\"),\n                                 list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Lim Chu Kang`)), \n                                                    list(yaxis = list(title = \"Temperature at Lim Chu Kang\", range = c(0,40)))),label = \"Lim Chu Kang\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Marine Parade`)), \n                                                    list(yaxis = list(title = \"Temperature at Marine Parade\", range = c(0,40)))),label = \"Marine Parade\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Choa Chu Kang (Central)`)), \n                                                    list(yaxis = list(title = \"Temperature at Choa Chu Kang (Central)\", range = c(0,40)))),label = \"Choa Chu Kang (Central)\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Choa Chu Kang (Central)`)), \n                                                    list(yaxis = list(title = \"Temperature at Choa Chu Kang (Central)\", range = c(0,40)))),label = \"Choa Chu Kang (Central)\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Nicoll Highway`)), \n                                                    list(yaxis = list(title = \"Temperature at Nicoll Highway\", range = c(0,40)))),label = \"Nicoll Highway\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Botanic Garden`)), \n                                                    list(yaxis = list(title = \"Temperature at Botanic Garden\", range = c(0,40)))),label = \"Botanic Garden\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$Whampoa)), \n                                                    list(yaxis = list(title = \"Temperature at Whampoa\", range = c(0,40)))),label = \"Whampoa\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Jurong Pier`)), \n                                                    list(yaxis = list(title = \"Temperature at Jurong Pier\", range = c(0,40)))),label = \"Jurong Pier\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Ulu Pandan`)), \n                                                    list(yaxis = list(title = \"Temperature at Ulu Pandan\", range = c(0,40)))),label = \"Ulu Pandan\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$Mandai)), \n                                                    list(yaxis = list(title = \"Temperature at Mandai\", range = c(0,40)))),label = \"Mandai\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Bukit Panjang`)), \n                                                    list(yaxis = list(title = \"Temperature at Bukit Panjang\", range = c(0,40)))),label = \"Bukit Panjang\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Kranji Reservoir`)), \n                                                    list(yaxis = list(title = \"Temperature at Kranji Reservoir\", range = c(0,40)))),label = \"Kranji Reservoir\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Upper Peirce Reservoir`)), \n                                                    list(yaxis = list(title = \"Temperature at Upper Peirce Reservoir\", range = c(0,40)))),label = \"Upper Peirce Reservoir\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Kent Ridge`)), \n                                                    list(yaxis = list(title = \"Temperature at Kent Ridge\", range = c(0,40)))),label = \"Kent Ridge\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$Queenstown)), \n                                                    list(yaxis = list(title = \"Temperature at Queenstown\", range = c(0,40)))),label = \"Queenstown\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Tanjong Katong`)), \n                                                    list(yaxis = list(title = \"Temperature at Tanjong Katong\", range = c(0,40)))),label = \"Tanjong Katong\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Somerset (Road)`)), \n                                                    list(yaxis = list(title = \"Temperature at Somerset (Road)\", range = c(0,40)))),label = \"Somerset (Road)\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Punggol`)), \n                                                    list(yaxis = list(title = \"Temperature at Punggol\", range = c(0,40)))),label = \"Punggol\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Simei`)), \n                                                    list(yaxis = list(title = \"Temperature at Simei\", range = c(0,40)))),label = \"Simei\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Toa Payoh`)), \n                                                    list(yaxis = list(title = \"Temperature at Toa Payoh\", range = c(0,40)))),label = \"Toa Payoh\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Tuas`)), \n                                                    list(yaxis = list(title = \"Temperature at Tuas\", range = c(0,40)))),label = \"Tuas\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Bukit Timah`)), \n                                                    list(yaxis = list(title = \"Temperature at Bukit Timah\", range = c(0,40)))),label = \"Bukit Timah\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Pasir Ris (Central)`)), \n                                                    list(yaxis = list(title = \"Temperature at Pasir Ris (Central)\", range = c(0,40)))),label = \"Pasir Ris (Central)\")\n                               ))))  \n\n\n\n\n\nSeems like there are some weather stations with no temperature data, some weather stations have temperature data for certain years, and some stations have temperature data from 1984 onwards.\nLet’s explore further.\n\nmissing.values &lt;- temp_mean_wide %&gt;%\n  gather(key = \"key\", value = \"val\") %&gt;%\n  mutate(isna = is.na(val)) %&gt;%\n  group_by(key) %&gt;%\n  mutate(total = n()) %&gt;%\n  group_by(key, total, isna) %&gt;%\n  summarise(num.isna = n()) %&gt;%\n  mutate(pct = num.isna / total * 100)\n\nlevels &lt;-\n    (missing.values  %&gt;% filter(isna == T) %&gt;% arrange(desc(pct)))$key\n\npercentage.plot &lt;- missing.values %&gt;%\n      ggplot() +\n        geom_bar(aes(x = reorder(key, desc(pct)), \n                     y = pct, fill=isna), \n                 stat = 'identity', alpha=0.8) +\n      scale_x_discrete(limits = levels) +\n      scale_fill_manual(name = \"\", \n                        values = c('steelblue', 'tomato3'), labels = c(\"Present\", \"Missing\")) +\n      coord_flip() +\n      labs(title = \"Percentage of missing values\", x =\n             'Variable', y = \"% of missing values\")\n\npercentage.plot\n\n\n\n\n\n\n\n\nDrop the stations with no weather data at all.\n\nnotempdata &lt;- missing.values %&gt;%\n  filter(isna == TRUE & pct==100)\n\nnotempdata$key\n\n [1] \"Botanic Garden\"          \"Bukit Panjang\"          \n [3] \"Bukit Timah\"             \"Choa Chu Kang (Central)\"\n [5] \"Jurong Pier\"             \"Kent Ridge\"             \n [7] \"Kranji Reservoir\"        \"Lim Chu Kang\"           \n [9] \"Lower Peirce Reservoir\"  \"Macritchie Reservoir\"   \n[11] \"Mandai\"                  \"Marine Parade\"          \n[13] \"Nicoll Highway\"          \"Pasir Ris (Central)\"    \n[15] \"Punggol\"                 \"Queenstown\"             \n[17] \"Simei\"                   \"Somerset (Road)\"        \n[19] \"Tanjong Katong\"          \"Toa Payoh\"              \n[21] \"Tuas\"                    \"Ulu Pandan\"             \n[23] \"Upper Peirce Reservoir\"  \"Whampoa\"                \n\n\n\nstationstoremove &lt;- c(\"Botanic Garden\",\"Bukit Panjang\",\"Bukit Timah\",\"Choa Chu Kang (Central)\",\"Jurong Pier\",\"Kent Ridge\", \"Kranji Reservoir\", \"Lim Chu Kang\", \"Lower Peirce Reservoir\", \"Macritchie Reservoir\",\"Mandai\", \"Marine Parade\",\"Nicoll Highway\", \"Pasir Ris (Central)\", \"Punggol\", \"Queenstown\",\"Simei\", \"Somerset (Road)\",\"Tanjong Katong\", \"Toa Payoh\", \"Tuas\", \"Ulu Pandan\", \"Upper Peirce Reservoir\",\"Whampoa\")\n\n#create a operator to exclude things \n'%!in%' &lt;- function(x,y)!('%in%'(x,y))\n\n#excluded stations that have no temp data at all \ntemp_clean &lt;- temp %&gt;%\n  filter(station %!in% stationstoremove)\n\nglimpse(temp_clean)\n\nRows: 120,139\nColumns: 5\n$ station             &lt;chr&gt; \"Admiralty\", \"Admiralty\", \"Admiralty\", \"Admiralty\"…\n$ tdate               &lt;date&gt; 2009-01-01, 2009-01-02, 2009-01-03, 2009-01-04, 2…\n$ mean_temperature    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ maximum_temperature &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ minimum_temperature &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n\n\n\ntemp_mean_widec &lt;- temp_clean %&gt;%\n  select(tdate, station, mean_temperature) %&gt;%\n  pivot_wider(names_from = station, values_from = mean_temperature)\n\nglimpse(temp_mean_widec)\n\nRows: 16,071\nColumns: 14\n$ tdate                   &lt;date&gt; 2009-01-01, 2009-01-02, 2009-01-03, 2009-01-0…\n$ Admiralty               &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ `East Coast Parkway`    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ `Ang Mo Kio`            &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ Newton                  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ `Tuas South`            &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ `Pasir Panjang`         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ `Jurong Island`         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ `Choa Chu Kang (South)` &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ Changi                  &lt;dbl&gt; 26.6, 26.4, 26.5, 26.3, 27.0, 27.4, 27.1, 27.0…\n$ `Tai Seng`              &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ `Jurong (West)`         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ Clementi                &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ `Sentosa Island`        &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n\n\n\nplot_ly(data = temp_mean_widec, \n        x = ~tdate, \n        y = ~ Admiralty, \n        type = \"scatter\", \n        mode = \"lines+markers\") |&gt; \n  layout(title = \"Temperature observed by Weather Station\", \n       xaxis = list(title = \"Date\"), \n       yaxis = list(title = \"\", range = c(0,40)), \n      theme_ipsum_rc(plot_title_size = 13, plot_title_margin=4, subtitle_size=11, subtitle_margin=4,  \n                 axis_title_size = 8, axis_text_size=8, axis_title_face= \"bold\", plot_margin = margin(4, 4, 4, 4)),  \n       updatemenus = list(list(type = 'dropdown', \n                               xref = \"paper\", \n                               yref = \"paper\", \n                               xanchor = \"left\",\n                               x = 0.04,\n                               y = 0.95, \n                               buttons = list(\n                                 list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_widec$Admiralty)), \n                                                    list(yaxis = list(title = \"Temperature in Admiralty\", range = c(0,40)))),label = \"Admiralty\"),\n                                 list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_widec$`East Coast Parkway`)), \n                                                    list(yaxis = list(title = \"Temperature in East Coast Parkway\", range = c(0,40)))),label = \"East Coast Parkway\"), \n                                 list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_widec$`Ang Mo Kio`)), \n                                                    list(yaxis = list(title = \"Temperature in Ang Mo Kio\", range = c(0,40)))),label = \"Ang Mo Kio\"), \n                                 list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_widec$Newton)), \n                                                    list(yaxis = list(title = \"Temperature in Newton\", range = c(0,40)))),label = \"Newton\"), \n                                 list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_widec$`Tuas South`)), \n                                                    list(yaxis = list(title = \"Temperature in Tuas South\", range = c(0,40)))),label = \"Tuas South\"),\n                                  list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_widec$`Pasir Panjang`)), \n                                                    list(yaxis = list(title = \"Temperature in Pasir Panjang\", range = c(0,40)))),label = \"Pasir Panjang\"), \n                                  list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_widec$`Jurong Island`)), \n                                                    list(yaxis = list(title = \"Temperature in Jurong Island\", range = c(0,40)))),label = \"Jurong Island\"), \n                                 list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_widec$`Choa Chu Kang (South)`)), \n                                                    list(yaxis = list(title = \"Temperature in Choa Chu Kang\", range = c(0,40)))),label = \"Choa Chu Kang\"), \n                                 list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_widec$Changi)), \n                                                    list(yaxis = list(title = \"Temperature in Changi\", range = c(0,40)))),label = \"Changi\"),\n                                  list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_widec$`Tai Seng`)), \n                                                    list(yaxis = list(title = \"Temperature in Tai Seng\", range = c(0,40)))),label = \"Tai Seng\"),\n                                  list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_widec$`Jurong (West)`)), \n                                                    list(yaxis = list(title = \"Temperature in Jurong West\", range = c(0,40)))),label = \"Jurong West\"), \n                                   list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_widec$Clementi)), \n                                                    list(yaxis = list(title = \"Temperature  in Clementi\", range = c(0,40)))),label = \"Clementi\"), \n                                   list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_widec$`Sentosa Island`)), \n                                                    list(yaxis = list(title = \"Temperature  in Sentosa\", range = c(0,40)))),label = \"Sentosa\")\n                                   \n                               ))))  \n\n\n\n\n\nThere are some missing time gaps in the data. Daily data for data ranging over more than 20 years is too frequent for time series forecasting."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#preparing-for-time-series-forecasting",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#preparing-for-time-series-forecasting",
    "title": "Take-home Exercise 4: Prototyping Time Series Module for Visual Analytics Shiny Application",
    "section": "3.4 Preparing for time series forecasting",
    "text": "3.4 Preparing for time series forecasting\nWe are using changi weather station for now\n\nchangi &lt;- temp %&gt;%\n  filter(station == \"Changi\")\n\nglimpse(changi)\n\nRows: 16,071\nColumns: 5\n$ station             &lt;chr&gt; \"Changi\", \"Changi\", \"Changi\", \"Changi\", \"Changi\", …\n$ tdate               &lt;date&gt; 1980-01-01, 1980-01-02, 1980-01-03, 1980-01-04, 1…\n$ mean_temperature    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ maximum_temperature &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ minimum_temperature &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n\n\n\nchangi &lt;- changi %&gt;%\n  drop_na()%&gt;%\n  select(tdate, mean_temperature, maximum_temperature, minimum_temperature)\n\nglimpse(changi)\n\nRows: 15,340\nColumns: 4\n$ tdate               &lt;date&gt; 1982-01-01, 1982-01-02, 1982-01-03, 1982-01-04, 1…\n$ mean_temperature    &lt;dbl&gt; 25.3, 24.7, 25.7, 26.3, 25.8, 23.7, 23.7, 24.4, 25…\n$ maximum_temperature &lt;dbl&gt; 29.4, 26.2, 27.2, 29.8, 28.8, 24.9, 25.2, 27.6, 28…\n$ minimum_temperature &lt;dbl&gt; 23.0, 23.5, 24.0, 24.1, 23.5, 21.9, 22.4, 22.8, 23…\n\n\n\nrange(changi$tdate)\n\n[1] \"1982-01-01\" \"2023-12-31\""
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#convert-df-to-timeseries-objec",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#convert-df-to-timeseries-objec",
    "title": "Take-home Exercise 4: Prototyping Time Series Module for Visual Analytics Shiny Application",
    "section": "3.5 Convert df to timeseries objec",
    "text": "3.5 Convert df to timeseries objec\nts_regular gives the time series a regular interval by adding NA values for missing dates na.fill function fills those missing dates by extending values from previous days The window() function clips off the starting and ending dates so the number of years covered is a multiple of four. This will be needed later when the data needs to be aggregated into monthly periods.\n\nchangi_ts &lt;- xts(changi[,c(\"mean_temperature\", \"maximum_temperature\", \"minimum_temperature\")], order.by=as.Date(changi$tdate))\nchangi_ts &lt;- ts_regular(changi_ts)\nchangi_ts &lt;- na.fill(changi_ts, \"extend\")\nchangi_ts &lt;- window(changi_ts, start = as.Date(\"1983-01-01\"), end = as.Date(\"2023-12-31\"))\n\nCheck the class of changi_ts\n\nclass(changi_ts)\n\n[1] \"xts\" \"zoo\"\n\n\n\nstr(changi_ts)\n\nAn xts object on 1983-01-01 / 2023-12-31 containing: \n  Data:    double [14975, 3]\n  Columns: mean_temperature, maximum_temperature, minimum_temperature\n  Index:   Date [14975] (TZ: \"UTC\")\n\n\nA plot() of mean, max and min temperatures show the annual cycle of temperatures as well as extreme temperature events that spike above the general curve.\nThe ts_ts() function is used to plot ts objects rather than xts objects so that more control is available over the formatting of the charts.\n\nplot(ts_ts(changi_ts$maximum_temperature), col=\"darkred\", bty=\"n\", las=1, fg=NA, \n    ylim=c(20, 40), ylab=\"Temperature (C)\")\n\nlines(ts_ts(changi_ts$minimum_temperature), col=\"navy\")\n\nlines(ts_ts(changi_ts$mean_temperature), col=\"darkgreen\")\n\ngrid(nx=NA, ny=NULL, lty=1, col=\"gray\")\n\nlegend(\"topright\", fill=c(\"darkred\", \"darkgreen\", \"navy\"), cex=0.7,\n    legend=c(\"MAX\", \"MEAN\", \"MIN\"), bg=\"white\")\n\n\n\n\n\n\n\n\n\n3.5.1 Summary Statistics\nRunning the summary() function will give basic descriptive statistics for the time series fields. You can subset the time series with logical conditions to find the dates for extreme conditions.\n\nsummary(changi_ts)\n\n     Index            mean_temperature maximum_temperature minimum_temperature\n Min.   :1983-01-01   Min.   :22.8     Min.   :23.60       Min.   :20.20      \n 1st Qu.:1993-04-01   1st Qu.:26.9     1st Qu.:30.80       1st Qu.:24.00      \n Median :2003-07-02   Median :27.7     Median :31.80       Median :24.90      \n Mean   :2003-07-02   Mean   :27.7     Mean   :31.54       Mean   :24.95      \n 3rd Qu.:2013-09-30   3rd Qu.:28.6     3rd Qu.:32.50       3rd Qu.:25.80      \n Max.   :2023-12-31   Max.   :30.9     Max.   :36.00       Max.   :29.10      \n\n\n\n\n3.5.2 Seasonal statistics\nTime series representation also facilitates summary statistics of recurring periods, like months.\nThe format() function with the %m format descriptor isolates the month portion of the date. The split() function breaks a vector into a data frame with separate fields based on a specific factor, in this case the month. The as.numeric() conversion is used to remove the time series date index so that dates are not included in the summaries. The sapply() function runs the summary() function on each field in the split data frame.\n\nchangi_ts$MONTH &lt;- format(index(changi_ts), \"%m\")\n\nmonths &lt;- split(as.numeric(changi_ts$maximum_temperature), changi_ts$MONTH)\n\nsapply(months, summary)\n\n               1        2        3        4        5        6        7        8\nMin.    23.60000 24.90000 24.60000 26.50000 26.30000 26.50000 25.50000 25.90000\n1st Qu. 29.80000 30.92500 31.30000 31.70000 31.60000 31.20000 30.90000 30.70000\nMedian  30.90000 31.70000 32.40000 32.60000 32.40000 32.10000 31.70000 31.70000\nMean    30.49103 31.58377 32.13698 32.41984 32.24194 31.86537 31.41542 31.38631\n3rd Qu. 31.60000 32.40000 33.20000 33.30000 33.10000 32.80000 32.40000 32.30000\nMax.    35.20000 35.20000 36.00000 35.80000 35.40000 35.00000 34.00000 34.20000\n               9       10       11       12\nMin.    26.30000 26.40000 24.20000 23.60000\n1st Qu. 30.90000 31.00000 30.50000 29.80000\nMedian  31.70000 31.90000 31.50000 30.90000\nMean    31.47854 31.74673 31.22528 30.48521\n3rd Qu. 32.30000 32.70000 32.20000 31.60000\nMax.    34.40000 34.60000 34.60000 33.90000\n\n\n\nmonths &lt;- split(as.numeric(changi_ts$minimum_temperature), changi_ts$MONTH)\n\nsapply(months, summary)\n\n               1        2      3        4        5        6        7        8\nMin.    21.20000 21.10000 20.200 21.40000 21.20000 21.10000 20.50000 21.10000\n1st Qu. 23.60000 23.90000 24.100 24.50000 24.90000 24.60000 24.30000 24.20000\nMedian  24.20000 24.50000 24.900 25.25000 25.70000 25.60000 25.40000 25.40000\nMean    24.20543 24.50017 24.838 25.23528 25.66452 25.57211 25.35083 25.26609\n3rd Qu. 24.80000 25.10000 25.600 26.00000 26.60000 26.60000 26.50000 26.40000\nMax.    27.00000 26.90000 27.500 28.40000 29.10000 28.40000 28.60000 28.20000\n               9       10       11       12\nMin.    21.00000 20.90000 21.30000 21.20000\n1st Qu. 24.10000 24.20000 23.90000 23.70000\nMedian  25.10000 25.00000 24.50000 24.30000\nMean    25.04301 24.94154 24.49407 24.28521\n3rd Qu. 26.10000 25.70000 25.10000 24.90000\nMax.    27.90000 28.20000 27.30000 26.60000\n\n\n\nmonths &lt;- split(as.numeric(changi_ts$mean_temperature), changi_ts$MONTH)\n\nsapply(months, summary)\n\n               1        2        3        4        5       6        7        8\nMin.    22.80000 22.90000 23.70000 25.30000 24.60000 25.3000 24.00000 24.60000\n1st Qu. 26.10000 26.70000 27.00000 27.50000 27.80000 27.7000 27.35000 27.30000\nMedian  26.80000 27.30000 27.80000 28.20000 28.50000 28.5000 28.30000 28.10000\nMean    26.69575 27.24542 27.71243 28.16374 28.52258 28.4239 28.09677 27.99622\n3rd Qu. 27.40000 27.80000 28.50000 28.80000 29.30000 29.2000 29.00000 28.80000\nMax.    29.70000 29.90000 30.50000 30.70000 30.90000 30.8000 30.20000 30.20000\n               9       10       11       12\nMin.    24.10000 24.70000 23.30000 23.00000\n1st Qu. 27.20000 27.10000 26.52500 26.10000\nMedian  28.00000 27.90000 27.10000 26.80000\nMean    27.85667 27.80913 27.15317 26.69984\n3rd Qu. 28.70000 28.50000 27.70000 27.40000\nMax.    29.80000 30.40000 30.00000 29.60000\n\n\n\n\n3.5.3 Time Series Decomposition\nTime series decomposition separates a time series into three fundamental components that can be added together to create the original data:\nA seasonal component A long-term trend component A random component When using historical data to anticipate what might happen in the future, we often wish to separates the seasonal and random components to see the long term historical trends.\nThe stl() function performs this decomposition.\n\ndecomposition &lt;- stl(ts_ts(changi_ts$mean_temperature), s.window=365, t.window = 14001)\n\nsummary(decomposition)\n\n Call:\n stl(x = ts_ts(changi_ts$mean_temperature), s.window = 365, t.window = 14001)\n\n Time.series components:\n    seasonal              trend            remainder        \n Min.   :-1.3289742   Min.   :27.17374   Min.   :-4.145266  \n 1st Qu.:-0.5143364   1st Qu.:27.47242   1st Qu.:-0.606672  \n Median : 0.1954581   Median :27.74174   Median : 0.089437  \n Mean   :-0.0006473   Mean   :27.69026   Mean   : 0.009567  \n 3rd Qu.: 0.4367594   3rd Qu.:27.91279   3rd Qu.: 0.703569  \n Max.   : 1.1109584   Max.   :28.07372   Max.   : 2.905331  \n IQR:\n     STL.seasonal STL.trend STL.remainder data  \n     0.9511       0.4404    1.3102        1.7000\n   %  55.9         25.9      77.1         100.0 \n\n Weights: all == 1\n\n Other components: List of 5\n $ win  : Named num [1:3] 365 14001 365\n $ deg  : Named int [1:3] 0 1 1\n $ jump : Named num [1:3] 37 1401 37\n $ inner: int 2\n $ outer: int 0\n\n\n\ndecomposition %&gt;%\n  autoplot()\n\n\n\n\n\n\n\n\nin this example, the trend indicates an increase of around 0.4 degrees celsius in daily maximum temperatures over the period 1983 to 2023. We also see there is some seasonality in (dry to wet seasons) of around 2.5 degrees celsius and random daily variation of around 8 degrees.\nNote: try changing the t.window and see how this would affect!\n\ndecomposition_max &lt;- stl(ts_ts(changi_ts$maximum_temperature), s.window=365, t.window = 14001)\n\nsummary(decomposition_max)\n\n Call:\n stl(x = ts_ts(changi_ts$maximum_temperature), s.window = 365, \n    t.window = 14001)\n\n Time.series components:\n    seasonal              trend            remainder        \n Min.   :-1.7090322   Min.   :31.34394   Min.   :-7.175268  \n 1st Qu.:-0.3690903   1st Qu.:31.45873   1st Qu.:-0.687818  \n Median : 0.0233567   Median :31.54728   Median : 0.249546  \n Mean   :-0.0007481   Mean   :31.53549   Mean   : 0.002352  \n 3rd Qu.: 0.5299052   3rd Qu.:31.61432   3rd Qu.: 0.952924  \n Max.   : 1.2867955   Max.   :31.69634   Max.   : 4.540526  \n IQR:\n     STL.seasonal STL.trend STL.remainder data  \n     0.8990       0.1556    1.6407        1.7000\n   %  52.9          9.2      96.5         100.0 \n\n Weights: all == 1\n\n Other components: List of 5\n $ win  : Named num [1:3] 365 14001 365\n $ deg  : Named int [1:3] 0 1 1\n $ jump : Named num [1:3] 37 1401 37\n $ inner: int 2\n $ outer: int 0\n\n\n\ndecomposition_max %&gt;%\n  autoplot()\n\n\n\n\n\n\n\n\n\ndecomposition_min &lt;- stl(ts_ts(changi_ts$minimum_temperature), s.window=365, t.window = 14001)\n\nsummary(decomposition_min)\n\n Call:\n stl(x = ts_ts(changi_ts$minimum_temperature), s.window = 365, \n    t.window = 14001)\n\n Time.series components:\n    seasonal              trend            remainder        \n Min.   :-0.9994999   Min.   :24.39684   Min.   :-4.653954  \n 1st Qu.:-0.4866771   1st Qu.:24.67177   1st Qu.:-0.696582  \n Median : 0.0508366   Median :24.94177   Median : 0.055812  \n Mean   :-0.0004214   Mean   :24.95344   Mean   :-0.001433  \n 3rd Qu.: 0.4047768   3rd Qu.:25.23570   3rd Qu.: 0.771620  \n Max.   : 1.1033537   Max.   :25.53458   Max.   : 3.964983  \n IQR:\n     STL.seasonal STL.trend STL.remainder data  \n     0.8915       0.5639    1.4682        1.8000\n   %  49.5         31.3      81.6         100.0 \n\n Weights: all == 1\n\n Other components: List of 5\n $ win  : Named num [1:3] 365 14001 365\n $ deg  : Named int [1:3] 0 1 1\n $ jump : Named num [1:3] 37 1401 37\n $ inner: int 2\n $ outer: int 0\n\n\n\ndecomposition_min %&gt;%\n  autoplot()\n\n\n\n\n\n\n\n\n\n\n3.5.4 Aggregating values by months\nAlthough having daily weather observations is extremely useful for analysis, direct visualization of daily observations relies on inexact (and occasionally deceptive) visual identification of patterns and trends. Visualization of decomposition captures trends.\nOne common technique to address this issue with time series data is to aggregate values by months or years.\nWith xts time series objects, the period.apply() function can be used to perform operations on blocks of observations across the time series. Those operations are specified with the FUN= parameter and commonly include min(), max(), and mean(). period.apply() requires a regular (even) interval between periods. Because months are uneven, a seq() of regularly spaced index values is passed to the INDEX= parameter. The INDEX value is is a regular spacing that approximates 12 months per year when leap years are considered (365 + 365 + 365 + 366 / 48 = 30.4375). Because the periods need an even number of observations, the window() dates should be chosen so they encompass even multiples of four years to account for leap years.\n\nmonthly_avgmeantemp &lt;- period.apply(changi_ts$mean_temperature, INDEX = seq(1, nrow(changi_ts) - 1, 30.4375), FUN = colMeans)\n\nplot(ts_ts(monthly_avgmeantemp), col=\"darkred\", ylim=c(20, 40), \n    lwd=3, bty=\"n\", las=1, fg=NA, ylab=\"TMAX (C)\")\n\ngrid(nx=NA, ny=NULL, lty=1)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#naive-method",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#naive-method",
    "title": "Take-home Exercise 4: Prototyping Time Series Module for Visual Analytics Shiny Application",
    "section": "4.1 Naive method",
    "text": "4.1 Naive method\n\nnaive_model &lt;- naive(training, h = length(validation))\nMAPE(naive_model$mean, validation) * 100\n\n[1] 2.163031\n\nsummary(naive_model)\n\n\nForecast method: Naive method\n\nModel Information:\nCall: naive(y = training, h = length(validation)) \n\nResidual sd: 0.6067 \n\nError measures:\n                     ME     RMSE       MAE         MPE     MAPE      MASE\nTraining set 0.00364557 0.606687 0.4865316 -0.01086379 1.765102 0.9689419\n                    ACF1\nTraining set -0.01453341\n\nForecasts:\n         Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\nJan 2016          27.94 27.16250 28.71750 26.75092 29.12908\nFeb 2016          27.94 26.84045 29.03955 26.25838 29.62162\nMar 2016          27.94 26.59333 29.28667 25.88044 29.99956\nApr 2016          27.94 26.38500 29.49500 25.56183 30.31817\nMay 2016          27.94 26.20146 29.67854 25.28113 30.59887\nJun 2016          27.94 26.03552 29.84448 25.02735 30.85265\nJul 2016          27.94 25.88293 29.99707 24.79398 31.08602\nAug 2016          27.94 25.74090 30.13910 24.57676 31.30324\nSep 2016          27.94 25.60750 30.27250 24.37275 31.50725\nOct 2016          27.94 25.48133 30.39867 24.17978 31.70022\nNov 2016          27.94 25.36132 30.51868 23.99625 31.88375\nDec 2016          27.94 25.24666 30.63334 23.82089 32.05911\nJan 2017          27.94 25.13668 30.74332 23.65269 32.22731\nFeb 2017          27.94 25.03086 30.84914 23.49085 32.38915\nMar 2017          27.94 24.92875 30.95125 23.33469 32.54531\nApr 2017          27.94 24.83000 31.05000 23.18366 32.69634\nMay 2017          27.94 24.73428 31.14572 23.03728 32.84272\nJun 2017          27.94 24.64134 31.23866 22.89514 32.98486\nJul 2017          27.94 24.55095 31.32905 22.75690 33.12310\nAug 2017          27.94 24.46291 31.41709 22.62225 33.25775\nSep 2017          27.94 24.37704 31.50296 22.49093 33.38907\nOct 2017          27.94 24.29320 31.58680 22.36270 33.51730\nNov 2017          27.94 24.21124 31.66876 22.23735 33.64265\nDec 2017          27.94 24.13104 31.74896 22.11470 33.76530\nJan 2018          27.94 24.05250 31.82750 21.99458 33.88542\nFeb 2018          27.94 23.97551 31.90449 21.87683 34.00317\nMar 2018          27.94 23.89999 31.98001 21.76133 34.11867\nApr 2018          27.94 23.82585 32.05415 21.64796 34.23204\nMay 2018          27.94 23.75303 32.12697 21.53658 34.34342\nJun 2018          27.94 23.68145 32.19855 21.42711 34.45289\nJul 2018          27.94 23.61106 32.26894 21.31946 34.56054\nAug 2018          27.94 23.54179 32.33821 21.21352 34.66648\nSep 2018          27.94 23.47360 32.40640 21.10923 34.77077\nOct 2018          27.94 23.40643 32.47357 21.00650 34.87350\nNov 2018          27.94 23.34024 32.53976 20.90528 34.97472\nDec 2018          27.94 23.27500 32.60500 20.80549 35.07451\nJan 2019          27.94 23.21065 32.66935 20.70708 35.17292\nFeb 2019          27.94 23.14716 32.73284 20.60999 35.27001\nMar 2019          27.94 23.08451 32.79549 20.51417 35.36583\nApr 2019          27.94 23.02265 32.85735 20.41957 35.46043\nMay 2019          27.94 22.96157 32.91843 20.32614 35.55386\nJun 2019          27.94 22.90122 32.97878 20.23385 35.64615\nJul 2019          27.94 22.84159 33.03841 20.14265 35.73735\nAug 2019          27.94 22.78264 33.09736 20.05250 35.82750\nSep 2019          27.94 22.72437 33.15563 19.96338 35.91662\nOct 2019          27.94 22.66673 33.21327 19.87524 36.00476\nNov 2019          27.94 22.60972 33.27028 19.78805 36.09195\nDec 2019          27.94 22.55332 33.32668 19.70178 36.17822\nJan 2020          27.94 22.49750 33.38250 19.61641 36.26359\nFeb 2020          27.94 22.44224 33.43776 19.53190 36.34810\nMar 2020          27.94 22.38753 33.49247 19.44824 36.43176\nApr 2020          27.94 22.33336 33.54664 19.36539 36.51461\nMay 2020          27.94 22.27971 33.60029 19.28333 36.59667\nJun 2020          27.94 22.22656 33.65344 19.20205 36.67795\nJul 2020          27.94 22.17390 33.70610 19.12151 36.75849\nAug 2020          27.94 22.12172 33.75828 19.04170 36.83830\nSep 2020          27.94 22.07000 33.81000 18.96261 36.91739\nOct 2020          27.94 22.01873 33.86127 18.88420 36.99580\nNov 2020          27.94 21.96790 33.91210 18.80647 37.07353\nDec 2020          27.94 21.91751 33.96249 18.72939 37.15061\nJan 2021          27.94 21.86753 34.01247 18.65295 37.22705\nFeb 2021          27.94 21.81795 34.06205 18.57714 37.30286\nMar 2021          27.94 21.76878 34.11122 18.50193 37.37807\nApr 2021          27.94 21.71999 34.16001 18.42732 37.45268\nMay 2021          27.94 21.67159 34.20841 18.35329 37.52671\nJun 2021          27.94 21.62355 34.25645 18.27983 37.60017\nJul 2021          27.94 21.57588 34.30412 18.20692 37.67308\nAug 2021          27.94 21.52856 34.35144 18.13456 37.74544\nSep 2021          27.94 21.48159 34.39841 18.06272 37.81728\nOct 2021          27.94 21.43496 34.44504 17.99140 37.88860\nNov 2021          27.94 21.38866 34.49134 17.92059 37.95941\nDec 2021          27.94 21.34269 34.53731 17.85028 38.02972\nJan 2022          27.94 21.29703 34.58297 17.78046 38.09954\nFeb 2022          27.94 21.25169 34.62831 17.71111 38.16889\nMar 2022          27.94 21.20665 34.67335 17.64222 38.23778\nApr 2022          27.94 21.16191 34.71809 17.57380 38.30620\nMay 2022          27.94 21.11746 34.76254 17.50582 38.37418\nJun 2022          27.94 21.07330 34.80670 17.43829 38.44171\nJul 2022          27.94 21.02942 34.85058 17.37118 38.50882\nAug 2022          27.94 20.98582 34.89418 17.30450 38.57550\nSep 2022          27.94 20.94249 34.93751 17.23824 38.64176\nOct 2022          27.94 20.89943 34.98057 17.17238 38.70762\nNov 2022          27.94 20.85663 35.02337 17.10692 38.77308\nDec 2022          27.94 20.81409 35.06591 17.04186 38.83814\nJan 2023          27.94 20.77180 35.10820 16.97718 38.90282\nFeb 2023          27.94 20.72976 35.15024 16.91288 38.96712\nMar 2023          27.94 20.68796 35.19204 16.84896 39.03104\nApr 2023          27.94 20.64640 35.23360 16.78540 39.09460\nMay 2023          27.94 20.60507 35.27493 16.72220 39.15780\nJun 2023          27.94 20.56398 35.31602 16.65935 39.22065\nJul 2023          27.94 20.52312 35.35688 16.59685 39.28315\nAug 2023          27.94 20.48248 35.39752 16.53470 39.34530\nSep 2023          27.94 20.44205 35.43795 16.47288 39.40712\nOct 2023          27.94 20.40185 35.47815 16.41140 39.46860\nNov 2023          27.94 20.36186 35.51814 16.35024 39.52976\nDec 2023          27.94 20.32208 35.55792 16.28940 39.59060\nJan 2024          27.94 20.28251 35.59749 16.22887 39.65113"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#seasonal-naive-method",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#seasonal-naive-method",
    "title": "Take-home Exercise 4: Prototyping Time Series Module for Visual Analytics Shiny Application",
    "section": "4.2 Seasonal Naive method",
    "text": "4.2 Seasonal Naive method\n\nsnaive_model &lt;- snaive(training, h = length(validation))\nMAPE(snaive_model$mean, validation) * 100\n\n[1] 1.719337\n\nsummary(snaive_model)\n\n\nForecast method: Seasonal naive method\n\nModel Information:\nCall: snaive(y = training, h = length(validation)) \n\nResidual sd: 0.6436 \n\nError measures:\n                    ME      RMSE       MAE        MPE    MAPE MASE      ACF1\nTraining set 0.0140191 0.6435697 0.5021267 0.02344714 1.82024    1 0.5350252\n\nForecasts:\n         Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\nJan 2016       27.08000 26.25523 27.90477 25.81863 28.34137\nFeb 2016       26.89667 26.07190 27.72143 25.63529 28.15804\nMar 2016       27.06667 26.24190 27.89143 25.80529 28.32804\nApr 2016       28.10667 27.28190 28.93143 26.84529 29.36804\nMay 2016       28.65333 27.82857 29.47810 27.39196 29.91471\nJun 2016       28.81667 27.99190 29.64143 27.55529 30.07804\nJul 2016       28.95333 28.12857 29.77810 27.69196 30.21471\nAug 2016       28.98667 28.16190 29.81143 27.72529 30.24804\nSep 2016       28.47667 27.65190 29.30143 27.21529 29.73804\nOct 2016       28.66000 27.83523 29.48477 27.39863 29.92137\nNov 2016       28.67333 27.84857 29.49810 27.41196 29.93471\nDec 2016       27.94000 27.11523 28.76477 26.67863 29.20137\nJan 2017       27.08000 25.91360 28.24640 25.29615 28.86385\nFeb 2017       26.89667 25.73027 28.06306 25.11282 28.68052\nMar 2017       27.06667 25.90027 28.23306 25.28282 28.85052\nApr 2017       28.10667 26.94027 29.27306 26.32282 29.89052\nMay 2017       28.65333 27.48694 29.81973 26.86948 30.43718\nJun 2017       28.81667 27.65027 29.98306 27.03282 30.60052\nJul 2017       28.95333 27.78694 30.11973 27.16948 30.73718\nAug 2017       28.98667 27.82027 30.15306 27.20282 30.77052\nSep 2017       28.47667 27.31027 29.64306 26.69282 30.26052\nOct 2017       28.66000 27.49360 29.82640 26.87615 30.44385\nNov 2017       28.67333 27.50694 29.83973 26.88948 30.45718\nDec 2017       27.94000 26.77360 29.10640 26.15615 29.72385\nJan 2018       27.08000 25.65146 28.50854 24.89524 29.26476\nFeb 2018       26.89667 25.46813 28.32521 24.71190 29.08143\nMar 2018       27.06667 25.63813 28.49521 24.88190 29.25143\nApr 2018       28.10667 26.67813 29.53521 25.92190 30.29143\nMay 2018       28.65333 27.22479 30.08187 26.46857 30.83810\nJun 2018       28.81667 27.38813 30.24521 26.63190 31.00143\nJul 2018       28.95333 27.52479 30.38187 26.76857 31.13810\nAug 2018       28.98667 27.55813 30.41521 26.80190 31.17143\nSep 2018       28.47667 27.04813 29.90521 26.29190 30.66143\nOct 2018       28.66000 27.23146 30.08854 26.47524 30.84476\nNov 2018       28.67333 27.24479 30.10187 26.48857 30.85810\nDec 2018       27.94000 26.51146 29.36854 25.75524 30.12476\nJan 2019       27.08000 25.43046 28.72954 24.55725 29.60275\nFeb 2019       26.89667 25.24713 28.54620 24.37392 29.41941\nMar 2019       27.06667 25.41713 28.71620 24.54392 29.58941\nApr 2019       28.10667 26.45713 29.75620 25.58392 30.62941\nMay 2019       28.65333 27.00380 30.30287 26.13059 31.17608\nJun 2019       28.81667 27.16713 30.46620 26.29392 31.33941\nJul 2019       28.95333 27.30380 30.60287 26.43059 31.47608\nAug 2019       28.98667 27.33713 30.63620 26.46392 31.50941\nSep 2019       28.47667 26.82713 30.12620 25.95392 30.99941\nOct 2019       28.66000 27.01046 30.30954 26.13725 31.18275\nNov 2019       28.67333 27.02380 30.32287 26.15059 31.19608\nDec 2019       27.94000 26.29046 29.58954 25.41725 30.46275\nJan 2020       27.08000 25.23576 28.92424 24.25948 29.90052\nFeb 2020       26.89667 25.05243 28.74090 24.07615 29.71718\nMar 2020       27.06667 25.22243 28.91090 24.24615 29.88718\nApr 2020       28.10667 26.26243 29.95090 25.28615 30.92718\nMay 2020       28.65333 26.80910 30.49757 25.83282 31.47385\nJun 2020       28.81667 26.97243 30.66090 25.99615 31.63718\nJul 2020       28.95333 27.10910 30.79757 26.13282 31.77385\nAug 2020       28.98667 27.14243 30.83090 26.16615 31.80718\nSep 2020       28.47667 26.63243 30.32090 25.65615 31.29718\nOct 2020       28.66000 26.81576 30.50424 25.83948 31.48052\nNov 2020       28.67333 26.82910 30.51757 25.85282 31.49385\nDec 2020       27.94000 26.09576 29.78424 25.11948 30.76052\nJan 2021       27.08000 25.05974 29.10026 23.99028 30.16972\nFeb 2021       26.89667 24.87641 28.91693 23.80695 29.98639\nMar 2021       27.06667 25.04641 29.08693 23.97695 30.15639\nApr 2021       28.10667 26.08641 30.12693 25.01695 31.19639\nMay 2021       28.65333 26.63307 30.67359 25.56361 31.74305\nJun 2021       28.81667 26.79641 30.83693 25.72695 31.90639\nJul 2021       28.95333 26.93307 30.97359 25.86361 32.04305\nAug 2021       28.98667 26.96641 31.00693 25.89695 32.07639\nSep 2021       28.47667 26.45641 30.49693 25.38695 31.56639\nOct 2021       28.66000 26.63974 30.68026 25.57028 31.74972\nNov 2021       28.67333 26.65307 30.69359 25.58361 31.76305\nDec 2021       27.94000 25.91974 29.96026 24.85028 31.02972\nJan 2022       27.08000 24.89787 29.26213 23.74272 30.41728\nFeb 2022       26.89667 24.71454 29.07880 23.55939 30.23395\nMar 2022       27.06667 24.88454 29.24880 23.72939 30.40395\nApr 2022       28.10667 25.92454 30.28880 24.76939 31.44395\nMay 2022       28.65333 26.47120 30.83546 25.31605 31.99061\nJun 2022       28.81667 26.63454 30.99880 25.47939 32.15395\nJul 2022       28.95333 26.77120 31.13546 25.61605 32.29061\nAug 2022       28.98667 26.80454 31.16880 25.64939 32.32395\nSep 2022       28.47667 26.29454 30.65880 25.13939 31.81395\nOct 2022       28.66000 26.47787 30.84213 25.32272 31.99728\nNov 2022       28.67333 26.49120 30.85546 25.33605 32.01061\nDec 2022       27.94000 25.75787 30.12213 24.60272 31.27728\nJan 2023       27.08000 24.74720 29.41280 23.51230 30.64770\nFeb 2023       26.89667 24.56387 29.22946 23.32896 30.46437\nMar 2023       27.06667 24.73387 29.39946 23.49896 30.63437\nApr 2023       28.10667 25.77387 30.43946 24.53896 31.67437\nMay 2023       28.65333 26.32054 30.98613 25.08563 32.22104\nJun 2023       28.81667 26.48387 31.14946 25.24896 32.38437\nJul 2023       28.95333 26.62054 31.28613 25.38563 32.52104\nAug 2023       28.98667 26.65387 31.31946 25.41896 32.55437\nSep 2023       28.47667 26.14387 30.80946 24.90896 32.04437\nOct 2023       28.66000 26.32720 30.99280 25.09230 32.22770\nNov 2023       28.67333 26.34054 31.00613 25.10563 32.24104\nDec 2023       27.94000 25.60720 30.27280 24.37230 31.50770\nJan 2024       27.08000 24.60570 29.55430 23.29588 30.86412"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#state-space-models",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#state-space-models",
    "title": "Take-home Exercise 4: Prototyping Time Series Module for Visual Analytics Shiny Application",
    "section": "4.3 State Space Models",
    "text": "4.3 State Space Models\n\nets_modelT &lt;- ets(training, allow.multiplicative.trend = TRUE)\nsummary(ets_modelT)\n\nETS(A,N,A) \n\nCall:\n ets(y = training, allow.multiplicative.trend = TRUE) \n\n  Smoothing parameters:\n    alpha = 0.4445 \n    gamma = 1e-04 \n\n  Initial states:\n    l = 27.6201 \n    s = -0.5352 0.0884 0.155 0.28 0.3454 0.7757\n           0.8227 0.4922 0.0699 -0.4203 -1.0135 -1.0602\n\n  sigma:  0.4191\n\n     AIC     AICc      BIC \n1695.638 1696.901 1755.359 \n\nTraining set error measures:\n                      ME      RMSE       MAE           MPE     MAPE      MASE\nTraining set 0.004836528 0.4116305 0.3275042 -0.0002696186 1.186142 0.6522342\n                    ACF1\nTraining set 0.001489326\n\n\nMultiplicative errors No trend Addictive seasonnality\n\nets_forecastT &lt;- forecast(ets_modelT, h=length(validation))\nMAPE(ets_forecastT$mean, validation) *100\n\n[1] 1.894772\n\nets_forecastT\n\n         Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\nJan 2016       27.41122 26.87412 27.94833 26.58979 28.23265\nFeb 2016       27.45798 26.87020 28.04575 26.55905 28.35691\nMar 2016       28.05119 27.41678 28.68561 27.08094 29.02145\nApr 2016       28.54114 27.86329 29.21899 27.50446 29.57783\nMay 2016       28.96364 28.24497 29.68231 27.86453 30.06275\nJun 2016       29.29409 28.53680 30.05138 28.13592 30.45226\nJul 2016       29.24717 28.45314 30.04120 28.03281 30.46154\nAug 2016       28.81692 27.98777 29.64607 27.54885 30.08499\nSep 2016       28.75134 27.88851 29.61418 27.43175 30.07094\nOct 2016       28.62637 27.73111 29.52163 27.25719 29.99555\nNov 2016       28.55976 27.63322 29.48631 27.14273 29.97679\nDec 2016       27.93617 26.97935 28.89299 26.47283 29.39950\nJan 2017       27.41122 26.42506 28.39738 25.90302 28.91942\nFeb 2017       27.45798 26.44333 28.47263 25.90621 29.00975\nMar 2017       28.05119 27.00883 29.09355 26.45704 29.64534\nApr 2017       28.54114 27.47179 29.61049 26.90571 30.17657\nMay 2017       28.96364 27.86796 30.05932 27.28794 30.63933\nJun 2017       29.29409 28.17270 30.41548 27.57907 31.00911\nJul 2017       29.24717 28.10065 30.39369 27.49372 31.00063\nAug 2017       28.81692 27.64580 29.98803 27.02585 30.60799\nSep 2017       28.75134 27.55614 29.94655 26.92344 30.57925\nOct 2017       28.62637 27.40755 29.84519 26.76235 30.49039\nNov 2017       28.55976 27.31778 29.80174 26.66032 30.45921\nDec 2017       27.93617 26.67144 29.20090 26.00193 29.87041\nJan 2018       27.41122 26.12415 28.69829 25.44282 29.37962\nFeb 2018       27.45798 26.14895 28.76700 25.45600 29.45996\nMar 2018       28.05119 26.72058 29.38181 26.01619 30.08620\nApr 2018       28.54114 27.18927 29.89301 26.47364 30.60864\nMay 2018       28.96364 27.59085 30.33642 26.86414 31.06313\nJun 2018       29.29409 27.90070 30.68748 27.16308 31.42510\nJul 2018       29.24717 27.83347 30.66087 27.08511 31.40924\nAug 2018       28.81692 27.38320 30.25064 26.62424 31.00960\nSep 2018       28.75134 27.29788 30.20480 26.52847 30.97422\nOct 2018       28.62637 27.15343 30.09931 26.37371 30.87903\nNov 2018       28.55976 27.06760 30.05192 26.27770 30.84182\nDec 2018       27.93617 26.42502 29.44732 25.62507 30.24727\nJan 2019       27.41122 25.88133 28.94111 25.07145 29.75099\nFeb 2019       27.45798 25.90957 29.00639 25.08989 29.82606\nMar 2019       28.05119 26.48449 29.61790 25.65512 30.44726\nApr 2019       28.54114 26.95635 30.12593 26.11741 30.96487\nMay 2019       28.96364 27.36096 30.56631 26.51256 31.41472\nJun 2019       29.29409 27.67373 30.91445 26.81596 31.77222\nJul 2019       29.24717 27.60931 30.88503 26.74229 31.75206\nAug 2019       28.81692 27.16175 30.47208 26.28556 31.34828\nSep 2019       28.75134 27.07905 30.42364 26.19379 31.30890\nOct 2019       28.62637 26.93712 30.31562 26.04288 31.20986\nNov 2019       28.55976 26.85372 30.26580 25.95060 31.16892\nDec 2019       27.93617 26.21350 29.65884 25.30157 30.57077\nJan 2020       27.41122 25.67208 29.15036 24.75144 30.07100\nFeb 2020       27.45798 25.70253 29.21343 24.77325 30.14270\nMar 2020       28.05119 26.27958 29.82280 25.34175 30.76064\nApr 2020       28.54114 26.75352 30.32877 25.80720 31.27508\nMay 2020       28.96364 27.16014 30.76714 26.20543 31.72185\nJun 2020       29.29409 27.47486 31.11332 26.51181 32.07636\nJul 2020       29.24717 27.41234 31.08200 26.44104 32.05330\nAug 2020       28.81692 26.96662 30.66722 25.98713 31.64671\nSep 2020       28.75134 26.88571 30.61698 25.89810 31.60459\nOct 2020       28.62637 26.74552 30.50722 25.74986 31.50289\nNov 2020       28.55976 26.66382 30.45570 25.66016 31.45936\nDec 2020       27.93617 26.02525 29.84709 25.01366 30.85868\nJan 2021       27.41122 25.48544 29.33700 24.46599 30.35645\nFeb 2021       27.45798 25.51745 29.39850 24.49020 30.42575\nMar 2021       28.05119 26.09604 30.00635 25.06104 31.04134\nApr 2021       28.54114 26.57146 30.51082 25.52878 31.55350\nMay 2021       28.96364 26.97954 30.94773 25.92923 31.99805\nJun 2021       29.29409 27.29568 31.29250 26.23779 32.35039\nJul 2021       29.24717 27.23455 31.25979 26.16914 32.32520\nAug 2021       28.81692 26.79019 30.84365 25.71730 31.91653\nSep 2021       28.75134 26.71060 30.79209 25.63030 31.87239\nOct 2021       28.62637 26.57171 30.68103 25.48404 31.76871\nNov 2021       28.55976 26.49128 30.62825 25.39628 31.72324\nDec 2021       27.93617 25.85394 30.01839 24.75168 31.12066\nJan 2022       27.41122 25.31535 29.50709 24.20587 30.61657\nFeb 2022       27.45798 25.34855 29.56740 24.23189 30.68406\nMar 2022       28.05119 25.92830 30.17408 24.80451 31.29787\nApr 2022       28.54114 26.40487 30.67741 25.27399 31.80829\nMay 2022       28.96364 26.81407 31.11321 25.67615 32.25113\nJun 2022       29.29409 27.13130 31.45688 25.98639 32.60179\nJul 2022       29.24717 27.07124 31.42310 25.91938 32.57497\nAug 2022       28.81692 26.62793 31.00591 25.46915 32.16468\nSep 2022       28.75134 26.54938 30.95331 25.38372 32.11896\nOct 2022       28.62637 26.41150 30.84124 25.23902 32.01373\nNov 2022       28.55976 26.33206 30.78746 25.15278 31.96674\nDec 2022       27.93617 25.69570 30.17664 24.50967 31.36267\nJan 2023       27.41122 25.15807 29.66437 23.96532 30.85712\nFeb 2023       27.45798 25.19221 29.72374 23.99279 30.92317\nMar 2023       28.05119 25.77288 30.32950 24.56682 31.53556\nApr 2023       28.54114 26.25036 30.83192 25.03769 32.04459\nMay 2023       28.96364 26.66045 31.26683 25.44121 32.48607\nJun 2023       29.29409 26.97856 31.60962 25.75279 32.83539\nJul 2023       29.24717 26.91936 31.57498 25.68710 32.80725\nAug 2023       28.81692 26.47690 31.15694 25.23817 32.39567\nSep 2023       28.75134 26.39918 31.10351 25.15401 32.34867\nOct 2023       28.62637 26.26212 30.99062 25.01056 32.24218\nNov 2023       28.55976 26.18348 30.93604 24.92556 32.19396\nDec 2023       27.93617 25.54792 30.32442 24.28366 31.58868\nJan 2024       27.41122 25.01107 29.81137 23.74051 31.08193\n\n\n\nplot(monthly_avgmeantemp, col=\"blue\", xlab=\"Year\", ylab=\"Daily Max Temp\", main=\"ETS Forecast\", type='l') \nlines(ets_forecastT$mean, col=\"red\", lwd=2)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#holt-winters",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#holt-winters",
    "title": "Take-home Exercise 4: Prototyping Time Series Module for Visual Analytics Shiny Application",
    "section": "4.4 Holt-Winters",
    "text": "4.4 Holt-Winters\n\nhw_model &lt;- hw(training, h = length(validation))\nMAPE(hw_model$mean, validation)*100\n\n[1] 1.829896"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#arima",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#arima",
    "title": "Take-home Exercise 4: Prototyping Time Series Module for Visual Analytics Shiny Application",
    "section": "4.5 ARIMA",
    "text": "4.5 ARIMA\n\narima_optimal &lt;- auto.arima(training)\nsummary(arima_optimal)\n\nSeries: training \nARIMA(0,0,1)(2,1,0)[12] with drift \n\nCoefficients:\n         ma1     sar1     sar2   drift\n      0.4103  -0.5036  -0.3495  0.0018\ns.e.  0.0402   0.0498   0.0500  0.0017\n\nsigma^2 = 0.2527:  log likelihood = -281.3\nAIC=572.6   AICc=572.76   BIC=592.36\n\nTraining set error measures:\n                       ME      RMSE      MAE         MPE     MAPE      MASE\nTraining set -0.009596971 0.4924452 0.386962 -0.05926227 1.401805 0.7706462\n                  ACF1\nTraining set 0.1099722\n\n\n\nsarima_forecast &lt;- sarima.for(training, n.ahead=length(validation), \n                               p=0,d=0,q=1,P=2,D=1,Q=0,S=12)\n\n\n\n\n\n\n\nsarima_forecast\n\n$pred\n          Jan      Feb      Mar      Apr      May      Jun      Jul      Aug\n2016 27.06569 26.87211 26.98105 28.19028 28.47709 28.57984 29.03199 28.54374\n2017 26.90637 26.68068 27.13374 28.13427 28.46456 28.63742 29.06235 28.71326\n2018 27.03127 26.82533 27.12645 28.17293 28.57214 28.73087 29.05924 28.82237\n2019 27.06373 26.85907 27.11643 28.21271 28.56202 28.70336 29.08987 28.74785\n2020 27.04341 26.83120 27.16370 28.21884 28.56919 28.72423 29.11521 28.78692\n2021 27.08198 26.87312 27.18307 28.24152 28.60879 28.76301 29.13142 28.83296\n2022 27.10933 26.90142 27.19647 28.26763 28.62602 28.77586 29.15408 28.83580\n2023 27.12175 26.91219 27.22263 28.28623 28.64318 28.79551 29.17668 28.85795\n2024 27.14561                                                               \n          Sep      Oct      Nov      Dec\n2016 28.19265 28.29693 28.31795 27.62436\n2017 28.07712 28.46468 28.43057 27.64590\n2018 28.27423 28.54677 28.53774 27.78504\n2019 28.25502 28.48648 28.48409 27.74712\n2020 28.23548 28.52782 28.51333 27.75726\n2021 28.29171 28.56775 28.55703 27.80508\n2022 28.30990 28.57287 28.56448 27.81713\n2023 28.32076 28.59601 28.58513 27.83403\n2024                                    \n\n$se\n           Jan       Feb       Mar       Apr       May       Jun       Jul\n2016 0.5000565 0.5405175 0.5405175 0.5405175 0.5405175 0.5405175 0.5405175\n2017 0.5948006 0.6034603 0.6034603 0.6034603 0.6034603 0.6034603 0.6034603\n2018 0.6358294 0.6411187 0.6411187 0.6411187 0.6411187 0.6411187 0.6411187\n2019 0.7131990 0.7246305 0.7246305 0.7246305 0.7246305 0.7246305 0.7246305\n2020 0.7742545 0.7823003 0.7823003 0.7823003 0.7823003 0.7823003 0.7823003\n2021 0.8223570 0.8289110 0.8289110 0.8289110 0.8289110 0.8289110 0.8289110\n2022 0.8739936 0.8813574 0.8813574 0.8813574 0.8813574 0.8813574 0.8813574\n2023 0.9223536 0.9290784 0.9290784 0.9290784 0.9290784 0.9290784 0.9290784\n2024 0.9664874                                                            \n           Aug       Sep       Oct       Nov       Dec\n2016 0.5405175 0.5405175 0.5405175 0.5405175 0.5405175\n2017 0.6034603 0.6034603 0.6034603 0.6034603 0.6034603\n2018 0.6411187 0.6411187 0.6411187 0.6411187 0.6411187\n2019 0.7246305 0.7246305 0.7246305 0.7246305 0.7246305\n2020 0.7823003 0.7823003 0.7823003 0.7823003 0.7823003\n2021 0.8289110 0.8289110 0.8289110 0.8289110 0.8289110\n2022 0.8813574 0.8813574 0.8813574 0.8813574 0.8813574\n2023 0.9290784 0.9290784 0.9290784 0.9290784 0.9290784\n2024                                                  \n\nMAPE(sarima_forecast$pred, validation)*100\n\n[1] 1.616764"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#selecting-the-relevant-columns-for-temperature-data-1",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#selecting-the-relevant-columns-for-temperature-data-1",
    "title": "Take-home Exercise 4: Prototyping Time Series Module for Visual Analytics Shiny Application",
    "section": "5.1 Selecting the relevant columns for Temperature Data",
    "text": "5.1 Selecting the relevant columns for Temperature Data\n\nrain &lt;- data %&gt;%\n  select(tdate, station, daily_rainfall_total) \n\nglimpse(rain)\n\nRows: 329,156\nColumns: 3\n$ tdate                &lt;date&gt; 1980-01-01, 1980-01-02, 1980-01-03, 1980-01-04, …\n$ station              &lt;chr&gt; \"Macritchie Reservoir\", \"Macritchie Reservoir\", \"…\n$ daily_rainfall_total &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 22.6, 49.6, 2.4, 0.0, 0.0, 0.…"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#checking-for-missing-values-1",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#checking-for-missing-values-1",
    "title": "Take-home Exercise 4: Prototyping Time Series Module for Visual Analytics Shiny Application",
    "section": "5.2 Checking for missing values",
    "text": "5.2 Checking for missing values\n\nsummary(rain)\n\n     tdate              station          daily_rainfall_total\n Min.   :1980-01-01   Length:329156      Min.   :  0.000     \n 1st Qu.:1997-04-29   Class :character   1st Qu.:  0.000     \n Median :2011-09-18   Mode  :character   Median :  0.200     \n Mean   :2007-07-02                      Mean   :  6.822     \n 3rd Qu.:2017-11-27                      3rd Qu.:  6.500     \n Max.   :2023-12-31                      Max.   :278.600     \n NA's   :58                              NA's   :5136        \n\n\n\nrain &lt;- rain %&gt;%\n  drop_na(c(tdate, station))\n\nsummary(rain)\n\n     tdate              station          daily_rainfall_total\n Min.   :1980-01-01   Length:329098      Min.   :  0.000     \n 1st Qu.:1997-04-29   Class :character   1st Qu.:  0.000     \n Median :2011-09-18   Mode  :character   Median :  0.200     \n Mean   :2007-07-02                      Mean   :  6.822     \n 3rd Qu.:2017-11-27                      3rd Qu.:  6.500     \n Max.   :2023-12-31                      Max.   :278.600     \n                                         NA's   :5078        \n\n\n\nunique(rain$station)\n\n [1] \"Macritchie Reservoir\"    \"Lower Peirce Reservoir\" \n [3] \"Admiralty\"               \"East Coast Parkway\"     \n [5] \"Ang Mo Kio\"              \"Newton\"                 \n [7] \"Lim Chu Kang\"            \"Marine Parade\"          \n [9] \"Choa Chu Kang (Central)\" \"Tuas South\"             \n[11] \"Pasir Panjang\"           \"Jurong Island\"          \n[13] \"Nicoll Highway\"          \"Botanic Garden\"         \n[15] \"Choa Chu Kang (South)\"   \"Whampoa\"                \n[17] \"Changi\"                  \"Jurong Pier\"            \n[19] \"Ulu Pandan\"              \"Mandai\"                 \n[21] \"Tai Seng\"                \"Jurong (West)\"          \n[23] \"Clementi\"                \"Sentosa Island\"         \n[25] \"Bukit Panjang\"           \"Kranji Reservoir\"       \n[27] \"Upper Peirce Reservoir\"  \"Kent Ridge\"             \n[29] \"Queenstown\"              \"Tanjong Katong\"         \n[31] \"Somerset (Road)\"         \"Punggol\"                \n[33] \"Simei\"                   \"Toa Payoh\"              \n[35] \"Tuas\"                    \"Bukit Timah\"            \n[37] \"Pasir Ris (Central)\""
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#further-exploration-of-total-rainfall-using-plotly",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#further-exploration-of-total-rainfall-using-plotly",
    "title": "Take-home Exercise 4: Prototyping Time Series Module for Visual Analytics Shiny Application",
    "section": "5.3 Further exploration of total rainfall using plotly",
    "text": "5.3 Further exploration of total rainfall using plotly\n\nrain_wide &lt;- rain %&gt;%\n  pivot_wider(names_from = station, values_from = daily_rainfall_total)\n\nglimpse(rain_wide)\n\nRows: 16,071\nColumns: 38\n$ tdate                     &lt;date&gt; 1980-01-01, 1980-01-02, 1980-01-03, 1980-01…\n$ `Macritchie Reservoir`    &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 22.6, 49.6, 2.4, 0.0, 0.…\n$ `Lower Peirce Reservoir`  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ Admiralty                 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `East Coast Parkway`      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Ang Mo Kio`              &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ Newton                    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Lim Chu Kang`            &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Marine Parade`           &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Choa Chu Kang (Central)` &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Tuas South`              &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Pasir Panjang`           &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Jurong Island`           &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Nicoll Highway`          &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Botanic Garden`          &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Choa Chu Kang (South)`   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ Whampoa                   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ Changi                    &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 8.0, 9.1, 7.9, 0.0, 0.0,…\n$ `Jurong Pier`             &lt;dbl&gt; 0.0, 29.2, 0.1, 0.0, 11.1, 27.6, 0.8, 0.0, 0…\n$ `Ulu Pandan`              &lt;dbl&gt; 0.0, 0.9, 0.0, 0.0, 18.8, 17.0, 5.4, 0.0, 0.…\n$ Mandai                    &lt;dbl&gt; 0.0, 0.0, 0.0, 0.2, 15.8, 40.4, 2.5, 0.0, 0.…\n$ `Tai Seng`                &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 26.1, 49.7, 4.9, 0.0, 0.…\n$ `Jurong (West)`           &lt;dbl&gt; 0.0, 0.4, 0.0, 0.0, 2.9, 20.9, 0.0, 0.1, 0.0…\n$ Clementi                  &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 34.1, NA, NA, NA, NA, NA…\n$ `Sentosa Island`          &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 13.6, 45.5, 0.4, 0.0, 0.…\n$ `Bukit Panjang`           &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 12.8, 28.5, 5.5, 0.0, 0.…\n$ `Kranji Reservoir`        &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 6.4, 18.3, 0.0, 0.0, 0.0…\n$ `Upper Peirce Reservoir`  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Kent Ridge`              &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ Queenstown                &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 28.3, 46.0, 1.5, 0.0, 0.…\n$ `Tanjong Katong`          &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 8.7, 30.2, 2.4, 0.0, 0.0…\n$ `Somerset (Road)`         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ Punggol                   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ Simei                     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Toa Payoh`               &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ Tuas                      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Bukit Timah`             &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Pasir Ris (Central)`     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n\n\n\nplot_ly(data = rain_wide, \n        x = ~tdate, \n        y = ~ Admiralty, \n        type = \"bar\") |&gt; \n  layout(title = \"Total Rain Fall observed by Weather Station\", \n       xaxis = list(title = \"Date\"), \n       yaxis = list(title = \"\", range = c(0,300)), \n      theme_ipsum_rc(plot_title_size = 13, plot_title_margin=4, subtitle_size=11, subtitle_margin=4,  \n                 axis_title_size = 8, axis_text_size=8, axis_title_face= \"bold\", plot_margin = margin(4, 4, 4, 4)),  \n       updatemenus = list(list(type = 'dropdown', \n                               xref = \"paper\", \n                               yref = \"paper\", \n                               xanchor = \"left\",\n                               x = 0.04,\n                               y = 0.95, \n                               buttons = list(\n                                 list(method = \"update\",\n                                      args = list(list(y = list(rain_wide$Admiralty)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed in Admiralty\", range = c(0,300)))),label = \"Admiralty\"),\n                                 list(method = \"update\",\n                                      args = list(list(y = list(rain_wide$`East Coast Parkway`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed in East Coast Parkway\", range = c(0,300)))),label = \"East Coast Parkway\"), \n                                 list(method = \"update\",\n                                      args = list(list(y = list(rain_wide$`Ang Mo Kio`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed in Ang Mo Kio\", range = c(0,300)))),label = \"Ang Mo Kio\"), \n                                 list(method = \"update\",\n                                      args = list(list(y = list(rain_wide$Newton)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed in Newton\", range = c(0,300)))),label = \"Newton\"), \n                                 list(method = \"update\",\n                                      args = list(list(y = list(rain_wide$`Tuas South`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed in Tuas South\", range = c(0,300)))),label = \"Tuas South\"),\n                                  list(method = \"update\",\n                                      args = list(list(y = list(rain_wide$`Pasir Panjang`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed in Pasir Panjang\", range = c(0,300)))),label = \"Pasir Panjang\"), \n                                  list(method = \"update\",\n                                      args = list(list(y = list(rain_wide$`Jurong Island`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed in Jurong Island\", range = c(0,300)))),label = \"Jurong Island\"), \n                                 list(method = \"update\",\n                                      args = list(list(y = list(rain_wide$`Choa Chu Kang (South)`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed in Choa Chu Kang (South)\", range = c(0,300)))),label = \"Choa Chu Kang (South)\"), \n                                 list(method = \"update\", \n                                        args = list(list(y = list(rain_wide$Changi)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed in Changi\", range = c(0,300)))),label = \"Changi\"),\n                                  list(method = \"update\",\n                                      args = list(list(y = list(rain_wide$`Tai Seng`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed in Tai Seng\", range = c(0,300)))),label = \"Tai Seng\"),\n                                  list(method = \"update\",\n                                      args = list(list(y = list(rain_wide$`Jurong (West)`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed in Jurong West\", range = c(0,300)))),label = \"Jurong West\"), \n                                   list(method = \"update\", \n                                        args = list(list(y = list(rain_wide$Clementi)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed in Clementi\", range = c(0,300)))),label = \"Clementi\"), \n                                   list(method = \"update\", \n                                        args = list(list(y = list(rain_wide$`Sentosa Island`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed  in Sentosa\", range = c(0,300)))),label = \"Sentosa\"), \n                                 list(method = \"update\", \n                                        args = list(list(y = list(rain_wide$`Macritchie Reservoir`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed  at Macritchie Reservoir\", range = c(0,300)))),label = \"Macritchie Reservoir\"), \n                                list(method = \"update\", \n                                        args = list(list(y = list(rain_wide$`Lower Peirce Reservoir`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed  at Lower Peirce Reservoir\", range = c(0,300)))),label = \"Lower Peirce Reservoir\"),\n                                 list(method = \"update\", \n                                        args = list(list(y = list(rain_wide$`Lim Chu Kang`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed at Lim Chu Kang\", range = c(0,300)))),label = \"Lim Chu Kang\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(rain_wide$`Marine Parade`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed at Marine Parade\", range = c(0,300)))),label = \"Marine Parade\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(rain_wide$`Choa Chu Kang (Central)`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed at Choa Chu Kang (Central)\", range = c(0,300)))),label = \"Choa Chu Kang (Central)\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(rain_wide$`Nicoll Highway`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed at Nicoll Highway\", range = c(0,300)))),label = \"Nicoll Highway\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(rain_wide$`Botanic Garden`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed at Botanic Garden\", range = c(0,300)))),label = \"Botanic Garden\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(rain_wide$Whampoa)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed at Whampoa\", range = c(0,300)))),label = \"Whampoa\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(rain_wide$`Jurong Pier`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed at Jurong Pier\", range = c(0,300)))),label = \"Jurong Pier\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(rain_wide$`Ulu Pandan`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed at Ulu Pandan\", range = c(0,300)))),label = \"Ulu Pandan\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(rain_wide$Mandai)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed at Mandai\", range = c(0,300)))),label = \"Mandai\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(rain_wide$`Bukit Panjang`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed at Bukit Panjang\", range = c(0,300)))),label = \"Bukit Panjang\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(rain_wide$`Kranji Reservoir`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed at Kranji Reservoir\", range = c(0,300)))),label = \"Kranji Reservoir\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(rain_wide$`Upper Peirce Reservoir`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed at Upper Peirce Reservoir\", range = c(0,300)))),label = \"Upper Peirce Reservoir\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(rain_wide$`Kent Ridge`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed at Kent Ridge\", range = c(0,300)))),label = \"Kent Ridge\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(rain_wide$Queenstown)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed at Queenstown\", range = c(0,300)))),label = \"Queenstown\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(rain_wide$`Tanjong Katong`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed at Tanjong Katong\", range = c(0,300)))),label = \"Tanjong Katong\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(rain_wide$`Somerset (Road)`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed at Somerset (Road)\", range = c(0,300)))),label = \"Somerset (Road)\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(rain_wide$`Punggol`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed at Punggol\", range = c(0,300)))),label = \"Punggol\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(rain_wide$`Simei`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed at Simei\", range = c(0,300)))),label = \"Simei\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(rain_wide$`Toa Payoh`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed at Toa Payoh\", range = c(0,300)))),label = \"Toa Payoh\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(rain_wide$`Tuas`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed at Tuas\", range = c(0,300)))),label = \"Tuas\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(rain_wide$`Bukit Timah`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed at Bukit Timah\", range = c(0,300)))),label = \"Bukit Timah\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(rain_wide$`Pasir Ris (Central)`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed at Pasir Ris (Central)\", range = c(0,300)))),label = \"Pasir Ris (Central)\")\n                               ))))  \n\n\n\n\n\nSeems like there are some stations with no rainfall data in all years, while some have rainfall data from certain years onwards. Let’s explore further.\n\nmissing.values &lt;- rain_wide %&gt;%\n  gather(key = \"key\", value = \"val\") %&gt;%\n  mutate(isna = is.na(val)) %&gt;%\n  group_by(key) %&gt;%\n  mutate(total = n()) %&gt;%\n  group_by(key, total, isna) %&gt;%\n  summarise(num.isna = n()) %&gt;%\n  mutate(pct = num.isna / total * 100)\n\nlevels &lt;-\n    (missing.values  %&gt;% filter(isna == T) %&gt;% arrange(desc(pct)))$key\n\npercentage.plot &lt;- missing.values %&gt;%\n      ggplot() +\n        geom_bar(aes(x = reorder(key, desc(pct)), \n                     y = pct, fill=isna), \n                 stat = 'identity', alpha=0.8) +\n      scale_x_discrete(limits = levels) +\n      scale_fill_manual(name = \"\", \n                        values = c('steelblue', 'tomato3'), labels = c(\"Present\", \"Missing\")) +\n      coord_flip() +\n      labs(title = \"Percentage of missing values\", x =\n             'Variable', y = \"% of missing values\")\n\npercentage.plot"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#preparing-for-time-series-forecasting-1",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#preparing-for-time-series-forecasting-1",
    "title": "Take-home Exercise 4: Prototyping Time Series Module for Visual Analytics Shiny Application",
    "section": "5.4 Preparing for time series forecasting",
    "text": "5.4 Preparing for time series forecasting\n\nchangirf &lt;- rain %&gt;%\n  filter(station == \"Changi\") %&gt;%\n  select(tdate, daily_rainfall_total)\n\nglimpse(changirf)\n\nRows: 16,071\nColumns: 2\n$ tdate                &lt;date&gt; 1980-01-01, 1980-01-02, 1980-01-03, 1980-01-04, …\n$ daily_rainfall_total &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 8.0, 9.1, 7.9, 0.0, 0.0, 0.0,…\n\n\nconvert df to timeseries data\n\nchangirf_ts &lt;- xts(changirf[,\"daily_rainfall_total\"], order.by=as.Date(changirf$tdate))\nchangirf_ts &lt;- ts_regular(changirf_ts)\nchangirf_ts &lt;- na.fill(changirf_ts, \"extend\")\nchangirf_ts &lt;- window(changirf_ts, start = as.Date(\"1983-01-01\"), end = as.Date(\"2023-12-31\"))\n\nconvert this to a time series object ts_regular gives the time series a regular interval by adding NA values for missing dates na.fill function fills those missing dates by extending values from previous days The window() function clips off the starting and ending dates so the number of years covered is a multiple of four. This will be needed later when the data needs to be aggregated into monthly periods.\n\nchangirf_ts_mth &lt;- period.apply(changirf_ts$value, INDEX = seq(1, nrow(changirf_ts) - 1, 30.4375), FUN = sum)\nplot(ts_ts(changirf_ts_mth), col=\"darkgreen\", \n    lwd=3, bty=\"n\", las=1, fg=NA, ylab=\"Monthly Rainfall (mm)\")\n\ngrid(nx=NA, ny=NULL, lty=1)\n\n\n\n\n\n\n\n\ndecompsition of rainfall\n\nfit &lt;- stl(ts_ts(changirf_ts_mth), s.window=365, t.window = 14001)\nplot(fit)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#building-models-1",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#building-models-1",
    "title": "Take-home Exercise 4: Prototyping Time Series Module for Visual Analytics Shiny Application",
    "section": "5.5 Building Models",
    "text": "5.5 Building Models\n\n# create samples \ntrainingrf &lt;- window(changirf_ts_mth, start = as.Date('1983-01-01'), end = as.Date('2015-12-31'))\nvalidationrf &lt;- window(changirf_ts_mth, start = as.Date('2016-01-01'))"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#naive-method-1",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#naive-method-1",
    "title": "Take-home Exercise 4: Prototyping Time Series Module for Visual Analytics Shiny Application",
    "section": "5.6 Naive method",
    "text": "5.6 Naive method\n\nnaive_model &lt;- naive(trainingrf, h = length(validationrf))\nsummary(naive_model)\n\n\nForecast method: Naive method\n\nModel Information:\nCall: naive(y = trainingrf, h = length(validationrf)) \n\nResidual sd: 148.1979 \n\nError measures:\n                   ME     RMSE      MAE       MPE     MAPE MASE       ACF1\nTraining set 0.255443 148.1979 109.2408 -163.5134 208.4867    1 -0.4617884\n\nForecasts:\n    Point Forecast       Lo 80     Hi 80      Lo 95     Hi 95\n397          101.2   -88.72328  291.1233  -189.2626  391.6626\n398          101.2  -167.39207  369.7921  -309.5761  511.9761\n399          101.2  -227.75677  430.1568  -401.8960  604.2960\n400          101.2  -278.64656  481.0466  -479.7252  682.1252\n401          101.2  -323.48136  525.8814  -548.2941  750.6941\n402          101.2  -364.01512  566.4151  -610.2851  812.6851\n403          101.2  -401.28976  603.6898  -667.2918  869.6918\n404          101.2  -435.98415  638.3841  -720.3523  922.7523\n405          101.2  -468.56983  670.9698  -770.1878  972.5878\n406          101.2  -499.39014  701.7901  -817.3234 1019.7234\n407          101.2  -528.70425  731.1043  -862.1554 1064.5554\n408          101.2  -556.71353  759.1135  -904.9919 1107.3919\n409          101.2  -583.57812  785.9781  -946.0778 1148.4778\n410          101.2  -609.42783  811.8278  -985.6115 1188.0115\n411          101.2  -634.36969  836.7697 -1023.7568 1226.1568\n412          101.2  -658.49311  860.8931 -1060.6504 1263.0504\n413          101.2  -681.87373  884.2737 -1096.4079 1298.8079\n414          101.2  -704.57622  906.9762 -1131.1284 1333.5284\n415          101.2  -726.65637  929.0564 -1164.8971 1367.2971\n416          101.2  -748.16272  950.5627 -1197.7882 1400.1882\n417          101.2  -769.13780  971.5378 -1229.8668 1432.2668\n418          101.2  -789.61913  992.0191 -1261.1903 1463.5903\n419          101.2  -809.64004 1012.0400 -1291.8096 1494.2096\n420          101.2  -829.23024 1031.6302 -1321.7703 1524.1703\n421          101.2  -848.41639 1050.8164 -1351.1129 1553.5129\n422          101.2  -867.22250 1069.6225 -1379.8744 1582.2744\n423          101.2  -885.67030 1088.0703 -1408.0879 1610.4879\n424          101.2  -903.77952 1106.1795 -1435.7835 1638.1835\n425          101.2  -921.56815 1123.9682 -1462.9889 1665.3889\n426          101.2  -939.05263 1141.4526 -1489.7291 1692.1291\n427          101.2  -956.24806 1158.6481 -1516.0272 1718.4272\n428          101.2  -973.16830 1175.5683 -1541.9045 1744.3045\n429          101.2  -989.82617 1192.2262 -1567.3805 1769.7805\n430          101.2 -1006.23350 1208.6335 -1592.4734 1794.8734\n431          101.2 -1022.40126 1224.8013 -1617.1998 1819.5998\n432          101.2 -1038.33967 1240.7397 -1641.5755 1843.9755\n433          101.2 -1054.05820 1256.4582 -1665.6149 1868.0149\n434          101.2 -1069.56571 1271.9657 -1689.3316 1891.7316\n435          101.2 -1084.87049 1287.2705 -1712.7383 1915.1383\n436          101.2 -1099.98028 1302.3803 -1735.8467 1938.2467\n437          101.2 -1114.90234 1317.3023 -1758.6680 1961.0680\n438          101.2 -1129.64351 1332.0435 -1781.2127 1983.6127\n439          101.2 -1144.21022 1346.6102 -1803.4906 2005.8906\n440          101.2 -1158.60850 1361.0085 -1825.5108 2027.9108\n441          101.2 -1172.84408 1375.2441 -1847.2823 2049.6823\n442          101.2 -1186.92234 1389.3223 -1868.8131 2071.2131\n443          101.2 -1200.84839 1403.2484 -1890.1112 2092.5112\n444          101.2 -1214.62707 1417.0271 -1911.1838 2113.5838\n445          101.2 -1228.26294 1430.6629 -1932.0381 2134.4381\n446          101.2 -1241.76037 1444.1604 -1952.6807 2155.0807\n447          101.2 -1255.12349 1457.5235 -1973.1178 2175.5178\n448          101.2 -1268.35623 1470.7562 -1993.3555 2195.7555\n449          101.2 -1281.46233 1483.8623 -2013.3996 2215.7996\n450          101.2 -1294.44536 1496.8454 -2033.2554 2235.6554\n451          101.2 -1307.30872 1509.7087 -2052.9282 2255.3282\n452          101.2 -1320.05567 1522.4557 -2072.4230 2274.8230\n453          101.2 -1332.68930 1535.0893 -2091.7444 2294.1444\n454          101.2 -1345.21259 1547.6126 -2110.8972 2313.2972\n455          101.2 -1357.62838 1560.0284 -2129.8855 2332.2855\n456          101.2 -1369.93938 1572.3394 -2148.7135 2351.1135\n457          101.2 -1382.14822 1584.5482 -2167.3853 2369.7853\n458          101.2 -1394.25738 1596.6574 -2185.9047 2388.3047\n459          101.2 -1406.26928 1608.6693 -2204.2753 2406.6753\n460          101.2 -1418.18622 1620.5862 -2222.5007 2424.9007\n461          101.2 -1430.01042 1632.4104 -2240.5842 2442.9842\n462          101.2 -1441.74400 1644.1440 -2258.5292 2460.9292\n463          101.2 -1453.38903 1655.7890 -2276.3387 2478.7387\n464          101.2 -1464.94747 1667.3475 -2294.0159 2496.4159\n465          101.2 -1476.42123 1678.8212 -2311.5635 2513.9635\n466          101.2 -1487.81214 1690.2121 -2328.9844 2531.3844\n467          101.2 -1499.12198 1701.5220 -2346.2813 2548.6813\n468          101.2 -1510.35245 1712.7524 -2363.4568 2565.8568\n469          101.2 -1521.50520 1723.9052 -2380.5134 2582.9134\n470          101.2 -1532.58181 1734.9818 -2397.4537 2599.8537\n471          101.2 -1543.58383 1745.9838 -2414.2798 2616.6798\n472          101.2 -1554.51275 1756.9127 -2430.9941 2633.3941\n473          101.2 -1565.37000 1767.7700 -2447.5989 2649.9989\n474          101.2 -1576.15697 1778.5570 -2464.0961 2666.4961\n475          101.2 -1586.87502 1789.2750 -2480.4879 2682.8879\n476          101.2 -1597.52544 1799.9254 -2496.7764 2699.1764\n477          101.2 -1608.10950 1810.5095 -2512.9633 2715.3633\n478          101.2 -1618.62843 1821.0284 -2529.0506 2731.4506\n479          101.2 -1629.08341 1831.4834 -2545.0401 2747.4401\n480          101.2 -1639.47559 1841.8756 -2560.9336 2763.3336\n481          101.2 -1649.80610 1852.2061 -2576.7327 2779.1327\n482          101.2 -1660.07602 1862.4760 -2592.4392 2794.8392\n483          101.2 -1670.28640 1872.6864 -2608.0547 2810.4547\n484          101.2 -1680.43827 1882.8383 -2623.5806 2825.9806\n485          101.2 -1690.53262 1892.9326 -2639.0186 2841.4186\n486          101.2 -1700.57041 1902.9704 -2654.3701 2856.7701\n487          101.2 -1710.55260 1912.9526 -2669.6365 2872.0365\n488          101.2 -1720.48008 1922.8801 -2684.8193 2887.2193\n489          101.2 -1730.35376 1932.7538 -2699.9198 2902.3198\n490          101.2 -1740.17449 1942.5745 -2714.9393 2917.3393\n491          101.2 -1749.94313 1952.3431 -2729.8791 2932.2791\n492          101.2 -1759.66048 1962.0605 -2744.7405 2947.1405\n493          101.2 -1769.32735 1971.7274 -2759.5247 2961.9247"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#seasonal-naive-method-1",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#seasonal-naive-method-1",
    "title": "Take-home Exercise 4: Prototyping Time Series Module for Visual Analytics Shiny Application",
    "section": "5.7 Seasonal Naive method",
    "text": "5.7 Seasonal Naive method\n\nsnaive_model &lt;- snaive(trainingrf, h = length(validationrf))\nsummary(snaive_model)\n\n\nForecast method: Seasonal naive method\n\nModel Information:\nCall: snaive(y = trainingrf, h = length(validationrf)) \n\nResidual sd: 148.1979 \n\nError measures:\n                   ME     RMSE      MAE       MPE     MAPE MASE       ACF1\nTraining set 0.255443 148.1979 109.2408 -163.5134 208.4867    1 -0.4617884\n\nForecasts:\n    Point Forecast       Lo 80     Hi 80      Lo 95     Hi 95\n397          101.2   -88.72328  291.1233  -189.2626  391.6626\n398          101.2  -167.39207  369.7921  -309.5761  511.9761\n399          101.2  -227.75677  430.1568  -401.8960  604.2960\n400          101.2  -278.64656  481.0466  -479.7252  682.1252\n401          101.2  -323.48136  525.8814  -548.2941  750.6941\n402          101.2  -364.01512  566.4151  -610.2851  812.6851\n403          101.2  -401.28976  603.6898  -667.2918  869.6918\n404          101.2  -435.98415  638.3841  -720.3523  922.7523\n405          101.2  -468.56983  670.9698  -770.1878  972.5878\n406          101.2  -499.39014  701.7901  -817.3234 1019.7234\n407          101.2  -528.70425  731.1043  -862.1554 1064.5554\n408          101.2  -556.71353  759.1135  -904.9919 1107.3919\n409          101.2  -583.57812  785.9781  -946.0778 1148.4778\n410          101.2  -609.42783  811.8278  -985.6115 1188.0115\n411          101.2  -634.36969  836.7697 -1023.7568 1226.1568\n412          101.2  -658.49311  860.8931 -1060.6504 1263.0504\n413          101.2  -681.87373  884.2737 -1096.4079 1298.8079\n414          101.2  -704.57622  906.9762 -1131.1284 1333.5284\n415          101.2  -726.65637  929.0564 -1164.8971 1367.2971\n416          101.2  -748.16272  950.5627 -1197.7882 1400.1882\n417          101.2  -769.13780  971.5378 -1229.8668 1432.2668\n418          101.2  -789.61913  992.0191 -1261.1903 1463.5903\n419          101.2  -809.64004 1012.0400 -1291.8096 1494.2096\n420          101.2  -829.23024 1031.6302 -1321.7703 1524.1703\n421          101.2  -848.41639 1050.8164 -1351.1129 1553.5129\n422          101.2  -867.22250 1069.6225 -1379.8744 1582.2744\n423          101.2  -885.67030 1088.0703 -1408.0879 1610.4879\n424          101.2  -903.77952 1106.1795 -1435.7835 1638.1835\n425          101.2  -921.56815 1123.9682 -1462.9889 1665.3889\n426          101.2  -939.05263 1141.4526 -1489.7291 1692.1291\n427          101.2  -956.24806 1158.6481 -1516.0272 1718.4272\n428          101.2  -973.16830 1175.5683 -1541.9045 1744.3045\n429          101.2  -989.82617 1192.2262 -1567.3805 1769.7805\n430          101.2 -1006.23350 1208.6335 -1592.4734 1794.8734\n431          101.2 -1022.40126 1224.8013 -1617.1998 1819.5998\n432          101.2 -1038.33967 1240.7397 -1641.5755 1843.9755\n433          101.2 -1054.05820 1256.4582 -1665.6149 1868.0149\n434          101.2 -1069.56571 1271.9657 -1689.3316 1891.7316\n435          101.2 -1084.87049 1287.2705 -1712.7383 1915.1383\n436          101.2 -1099.98028 1302.3803 -1735.8467 1938.2467\n437          101.2 -1114.90234 1317.3023 -1758.6680 1961.0680\n438          101.2 -1129.64351 1332.0435 -1781.2127 1983.6127\n439          101.2 -1144.21022 1346.6102 -1803.4906 2005.8906\n440          101.2 -1158.60850 1361.0085 -1825.5108 2027.9108\n441          101.2 -1172.84408 1375.2441 -1847.2823 2049.6823\n442          101.2 -1186.92234 1389.3223 -1868.8131 2071.2131\n443          101.2 -1200.84839 1403.2484 -1890.1112 2092.5112\n444          101.2 -1214.62707 1417.0271 -1911.1838 2113.5838\n445          101.2 -1228.26294 1430.6629 -1932.0381 2134.4381\n446          101.2 -1241.76037 1444.1604 -1952.6807 2155.0807\n447          101.2 -1255.12349 1457.5235 -1973.1178 2175.5178\n448          101.2 -1268.35623 1470.7562 -1993.3555 2195.7555\n449          101.2 -1281.46233 1483.8623 -2013.3996 2215.7996\n450          101.2 -1294.44536 1496.8454 -2033.2554 2235.6554\n451          101.2 -1307.30872 1509.7087 -2052.9282 2255.3282\n452          101.2 -1320.05567 1522.4557 -2072.4230 2274.8230\n453          101.2 -1332.68930 1535.0893 -2091.7444 2294.1444\n454          101.2 -1345.21259 1547.6126 -2110.8972 2313.2972\n455          101.2 -1357.62838 1560.0284 -2129.8855 2332.2855\n456          101.2 -1369.93938 1572.3394 -2148.7135 2351.1135\n457          101.2 -1382.14822 1584.5482 -2167.3853 2369.7853\n458          101.2 -1394.25738 1596.6574 -2185.9047 2388.3047\n459          101.2 -1406.26928 1608.6693 -2204.2753 2406.6753\n460          101.2 -1418.18622 1620.5862 -2222.5007 2424.9007\n461          101.2 -1430.01042 1632.4104 -2240.5842 2442.9842\n462          101.2 -1441.74400 1644.1440 -2258.5292 2460.9292\n463          101.2 -1453.38903 1655.7890 -2276.3387 2478.7387\n464          101.2 -1464.94747 1667.3475 -2294.0159 2496.4159\n465          101.2 -1476.42123 1678.8212 -2311.5635 2513.9635\n466          101.2 -1487.81214 1690.2121 -2328.9844 2531.3844\n467          101.2 -1499.12198 1701.5220 -2346.2813 2548.6813\n468          101.2 -1510.35245 1712.7524 -2363.4568 2565.8568\n469          101.2 -1521.50520 1723.9052 -2380.5134 2582.9134\n470          101.2 -1532.58181 1734.9818 -2397.4537 2599.8537\n471          101.2 -1543.58383 1745.9838 -2414.2798 2616.6798\n472          101.2 -1554.51275 1756.9127 -2430.9941 2633.3941\n473          101.2 -1565.37000 1767.7700 -2447.5989 2649.9989\n474          101.2 -1576.15697 1778.5570 -2464.0961 2666.4961\n475          101.2 -1586.87502 1789.2750 -2480.4879 2682.8879\n476          101.2 -1597.52544 1799.9254 -2496.7764 2699.1764\n477          101.2 -1608.10950 1810.5095 -2512.9633 2715.3633\n478          101.2 -1618.62843 1821.0284 -2529.0506 2731.4506\n479          101.2 -1629.08341 1831.4834 -2545.0401 2747.4401\n480          101.2 -1639.47559 1841.8756 -2560.9336 2763.3336\n481          101.2 -1649.80610 1852.2061 -2576.7327 2779.1327\n482          101.2 -1660.07602 1862.4760 -2592.4392 2794.8392\n483          101.2 -1670.28640 1872.6864 -2608.0547 2810.4547\n484          101.2 -1680.43827 1882.8383 -2623.5806 2825.9806\n485          101.2 -1690.53262 1892.9326 -2639.0186 2841.4186\n486          101.2 -1700.57041 1902.9704 -2654.3701 2856.7701\n487          101.2 -1710.55260 1912.9526 -2669.6365 2872.0365\n488          101.2 -1720.48008 1922.8801 -2684.8193 2887.2193\n489          101.2 -1730.35376 1932.7538 -2699.9198 2902.3198\n490          101.2 -1740.17449 1942.5745 -2714.9393 2917.3393\n491          101.2 -1749.94313 1952.3431 -2729.8791 2932.2791\n492          101.2 -1759.66048 1962.0605 -2744.7405 2947.1405\n493          101.2 -1769.32735 1971.7274 -2759.5247 2961.9247"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#state-space-models-1",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#state-space-models-1",
    "title": "Take-home Exercise 4: Prototyping Time Series Module for Visual Analytics Shiny Application",
    "section": "5.8 State Space Models",
    "text": "5.8 State Space Models\n\nets_modelrf &lt;- ets(trainingrf, allow.multiplicative.trend = TRUE)\nsummary(ets_modelrf)\n\nETS(A,N,N) \n\nCall:\n ets(y = trainingrf, allow.multiplicative.trend = TRUE) \n\n  Smoothing parameters:\n    alpha = 1e-04 \n\n  Initial states:\n    l = 177.9223 \n\n  sigma:  114.7456\n\n     AIC     AICc      BIC \n6128.867 6128.928 6140.811 \n\nTraining set error measures:\n                      ME     RMSE      MAE       MPE     MAPE      MASE\nTraining set 0.004551363 114.4554 87.41135 -454.4325 480.2861 0.8001716\n                  ACF1\nTraining set 0.1601591\n\n\nadditive errors No trend No seasonality\n\nets_forecastrf &lt;- forecast(ets_modelrf, h=length(validationrf))\nets_forecastrf\n\n    Point Forecast    Lo 80    Hi 80     Lo 95    Hi 95\n397       177.9225 30.87011 324.9748 -46.97469 402.8196\n398       177.9225 30.87011 324.9748 -46.97469 402.8196\n399       177.9225 30.87011 324.9748 -46.97469 402.8196\n400       177.9225 30.87011 324.9748 -46.97470 402.8196\n401       177.9225 30.87011 324.9748 -46.97470 402.8196\n402       177.9225 30.87011 324.9748 -46.97470 402.8196\n403       177.9225 30.87011 324.9748 -46.97470 402.8196\n404       177.9225 30.87011 324.9748 -46.97470 402.8196\n405       177.9225 30.87011 324.9748 -46.97470 402.8196\n406       177.9225 30.87011 324.9748 -46.97470 402.8196\n407       177.9225 30.87011 324.9748 -46.97470 402.8196\n408       177.9225 30.87011 324.9748 -46.97470 402.8196\n409       177.9225 30.87011 324.9748 -46.97471 402.8196\n410       177.9225 30.87010 324.9748 -46.97471 402.8196\n411       177.9225 30.87010 324.9748 -46.97471 402.8196\n412       177.9225 30.87010 324.9748 -46.97471 402.8196\n413       177.9225 30.87010 324.9748 -46.97471 402.8196\n414       177.9225 30.87010 324.9748 -46.97471 402.8196\n415       177.9225 30.87010 324.9748 -46.97471 402.8196\n416       177.9225 30.87010 324.9748 -46.97471 402.8196\n417       177.9225 30.87010 324.9748 -46.97471 402.8196\n418       177.9225 30.87010 324.9748 -46.97472 402.8196\n419       177.9225 30.87010 324.9748 -46.97472 402.8196\n420       177.9225 30.87010 324.9748 -46.97472 402.8196\n421       177.9225 30.87010 324.9748 -46.97472 402.8196\n422       177.9225 30.87010 324.9748 -46.97472 402.8196\n423       177.9225 30.87010 324.9748 -46.97472 402.8196\n424       177.9225 30.87009 324.9748 -46.97472 402.8196\n425       177.9225 30.87009 324.9748 -46.97472 402.8196\n426       177.9225 30.87009 324.9748 -46.97472 402.8196\n427       177.9225 30.87009 324.9748 -46.97473 402.8196\n428       177.9225 30.87009 324.9748 -46.97473 402.8196\n429       177.9225 30.87009 324.9748 -46.97473 402.8196\n430       177.9225 30.87009 324.9748 -46.97473 402.8196\n431       177.9225 30.87009 324.9748 -46.97473 402.8196\n432       177.9225 30.87009 324.9748 -46.97473 402.8196\n433       177.9225 30.87009 324.9748 -46.97473 402.8196\n434       177.9225 30.87009 324.9748 -46.97473 402.8196\n435       177.9225 30.87009 324.9748 -46.97473 402.8196\n436       177.9225 30.87009 324.9748 -46.97474 402.8196\n437       177.9225 30.87009 324.9748 -46.97474 402.8196\n438       177.9225 30.87008 324.9748 -46.97474 402.8196\n439       177.9225 30.87008 324.9748 -46.97474 402.8196\n440       177.9225 30.87008 324.9748 -46.97474 402.8196\n441       177.9225 30.87008 324.9748 -46.97474 402.8196\n442       177.9225 30.87008 324.9748 -46.97474 402.8197\n443       177.9225 30.87008 324.9748 -46.97474 402.8197\n444       177.9225 30.87008 324.9748 -46.97474 402.8197\n445       177.9225 30.87008 324.9748 -46.97475 402.8197\n446       177.9225 30.87008 324.9748 -46.97475 402.8197\n447       177.9225 30.87008 324.9748 -46.97475 402.8197\n448       177.9225 30.87008 324.9748 -46.97475 402.8197\n449       177.9225 30.87008 324.9748 -46.97475 402.8197\n450       177.9225 30.87008 324.9748 -46.97475 402.8197\n451       177.9225 30.87007 324.9748 -46.97475 402.8197\n452       177.9225 30.87007 324.9748 -46.97475 402.8197\n453       177.9225 30.87007 324.9748 -46.97475 402.8197\n454       177.9225 30.87007 324.9748 -46.97476 402.8197\n455       177.9225 30.87007 324.9748 -46.97476 402.8197\n456       177.9225 30.87007 324.9748 -46.97476 402.8197\n457       177.9225 30.87007 324.9748 -46.97476 402.8197\n458       177.9225 30.87007 324.9748 -46.97476 402.8197\n459       177.9225 30.87007 324.9748 -46.97476 402.8197\n460       177.9225 30.87007 324.9748 -46.97476 402.8197\n461       177.9225 30.87007 324.9748 -46.97476 402.8197\n462       177.9225 30.87007 324.9748 -46.97477 402.8197\n463       177.9225 30.87007 324.9748 -46.97477 402.8197\n464       177.9225 30.87007 324.9748 -46.97477 402.8197\n465       177.9225 30.87006 324.9748 -46.97477 402.8197\n466       177.9225 30.87006 324.9748 -46.97477 402.8197\n467       177.9225 30.87006 324.9748 -46.97477 402.8197\n468       177.9225 30.87006 324.9748 -46.97477 402.8197\n469       177.9225 30.87006 324.9748 -46.97477 402.8197\n470       177.9225 30.87006 324.9748 -46.97477 402.8197\n471       177.9225 30.87006 324.9748 -46.97478 402.8197\n472       177.9225 30.87006 324.9748 -46.97478 402.8197\n473       177.9225 30.87006 324.9748 -46.97478 402.8197\n474       177.9225 30.87006 324.9749 -46.97478 402.8197\n475       177.9225 30.87006 324.9749 -46.97478 402.8197\n476       177.9225 30.87006 324.9749 -46.97478 402.8197\n477       177.9225 30.87006 324.9749 -46.97478 402.8197\n478       177.9225 30.87005 324.9749 -46.97478 402.8197\n479       177.9225 30.87005 324.9749 -46.97478 402.8197\n480       177.9225 30.87005 324.9749 -46.97479 402.8197\n481       177.9225 30.87005 324.9749 -46.97479 402.8197\n482       177.9225 30.87005 324.9749 -46.97479 402.8197\n483       177.9225 30.87005 324.9749 -46.97479 402.8197\n484       177.9225 30.87005 324.9749 -46.97479 402.8197\n485       177.9225 30.87005 324.9749 -46.97479 402.8197\n486       177.9225 30.87005 324.9749 -46.97479 402.8197\n487       177.9225 30.87005 324.9749 -46.97479 402.8197\n488       177.9225 30.87005 324.9749 -46.97479 402.8197\n489       177.9225 30.87005 324.9749 -46.97480 402.8197\n490       177.9225 30.87005 324.9749 -46.97480 402.8197\n491       177.9225 30.87005 324.9749 -46.97480 402.8197\n492       177.9225 30.87004 324.9749 -46.97480 402.8197\n493       177.9225 30.87004 324.9749 -46.97480 402.8197"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#holt-winters-1",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#holt-winters-1",
    "title": "Take-home Exercise 4: Prototyping Time Series Module for Visual Analytics Shiny Application",
    "section": "5.9 Holt-Winters",
    "text": "5.9 Holt-Winters"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#arima-1",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#arima-1",
    "title": "Take-home Exercise 4: Prototyping Time Series Module for Visual Analytics Shiny Application",
    "section": "5.10 ARIMA",
    "text": "5.10 ARIMA\n\narima_optimal &lt;- auto.arima(trainingrf)\nsummary(arima_optimal)\n\nSeries: trainingrf \nARIMA(4,0,2) with non-zero mean \n\nCoefficients:\n         ar1     ar2      ar3      ar4      ma1      ma2      mean\n      0.1935  0.9521  -0.2109  -0.0492  -0.0349  -0.8948  179.6007\ns.e.  0.0772  0.0710   0.0513   0.0547   0.0587   0.0536    3.6884\n\nsigma^2 = 12584:  log likelihood = -2427.68\nAIC=4871.36   AICc=4871.73   BIC=4903.21\n\nTraining set error measures:\n                     ME    RMSE      MAE      MPE     MAPE      MASE\nTraining set -0.6514072 111.181 84.55129 -410.529 435.8642 0.7739903\n                      ACF1\nTraining set -0.0008009035"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#ui",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#ui",
    "title": "Take-home Exercise 4: Prototyping Time Series Module for Visual Analytics Shiny Application",
    "section": "6.1 UI",
    "text": "6.1 UI\n\n6.1.1 Sketch\n\n\n6.1.2 Details"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#server",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04.html#server",
    "title": "Take-home Exercise 4: Prototyping Time Series Module for Visual Analytics Shiny Application",
    "section": "6.2 Server",
    "text": "6.2 Server\n\n6.2.1 Details"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html",
    "title": "Take-home Exercise 4: Prototyping Time Series Module for Visual Analytics Shiny Application",
    "section": "",
    "text": "In this exercise, we will be preparing the time-series forecasting module of the proposed Shiny Application and complete the following task:\n\nEvaluate and determine the necessary R packages needed for my group project’s Shiny application;\nPrepare and test the specific R codes can be run and returned the correct output as expected;\nDetermine the parameters and outputs that will be exposed on the Shiny applications; and\nSelect the appropriate Shiny UI components for exposing the parameters determined above.",
    "crumbs": [
      "Take-home Exercises",
      "Take-home Exercise 4: Prototyping Time Series Module for Shiny Application"
    ]
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#loading-r-packages",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#loading-r-packages",
    "title": "Take-home Exercise 4: Prototyping Time Series Module for Visual Analytics Shiny Application",
    "section": "2.1 Loading R Packages",
    "text": "2.1 Loading R Packages\nFor this exercise, we will be using the following packages:\n\ntidyverse\nxts\nlubridate\ntsbox\nimputeTS\nDT\nggplot2\nplotly\nggthemes\nhrbrthemes\nforecast\nMLmetrics\n\nThe code chunk below uses p_load() of pacman package to check if the packages are installed in the computer. If they are, they will be launched in R. Otherwise, pacman will install the relevant packages before launching them.\n\n\nShow the code\npacman::p_load(tidyverse, lubridate, knitr, DT, ggplot2, plotly, ggthemes, ggfortify, forecast, MLmetrics, tsbox, xts, imputeTS, tseries, hrbrthemes, autoplotly)",
    "crumbs": [
      "Take-home Exercises",
      "Take-home Exercise 4: Prototyping Time Series Module for Shiny Application"
    ]
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#importing-data-into-r",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#importing-data-into-r",
    "title": "Take-home Exercise 4: Prototyping Time Series Module for Visual Analytics Shiny Application",
    "section": "2.2 Importing Data Into R",
    "text": "2.2 Importing Data Into R\nFor this exercise, we will be using the historical daily weather records from weather.gov.sg. We retrieved the daily records data from Jan 1980 to Dec 2023 via data.gov.sg’s API. The daily historical weather records is in csv file format.\nWe use read_csv() function of readr to import the daily_historical csv file into R then we will use glimpse() of dplyr to learn about the associated attribute information in the dataframe.\n\n\nShow the code\ndata &lt;- read_csv(\"data/daily_historical.csv\")\nglimpse(data)\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nThere is no date but there are columns year, month and day. In addition, we also note that R currently read columns year, month and day as numeric data.\nThere are different columns for rainfall and temperature so we will need to select and filter the relevant columns that we want in subsequent steps.\nThe entire dataset daily_historical.csv is very large. We should save the filtered data into an R data format (RDS) so that we can easily retrieve it in future without importing the entire daily_historical.csv dataset again.",
    "crumbs": [
      "Take-home Exercises",
      "Take-home Exercise 4: Prototyping Time Series Module for Shiny Application"
    ]
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#creating-a-date-column",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#creating-a-date-column",
    "title": "Take-home Exercise 4: Prototyping Time Series Module for Visual Analytics Shiny Application",
    "section": "2.3 Creating a date column",
    "text": "2.3 Creating a date column\nLet us first create a date column, called tdate using paste(), mutate() and lubridate’s ymd() to convert the numeric data type into a date data type and year-month-day format.\n\n\nShow the code\ndata$tdate &lt;- paste(data$year, \"-\", data$month, \"-\", data$day)\ndata &lt;- data %&gt;%\n  mutate(tdate = ymd(tdate))\n\nglimpse(data)",
    "crumbs": [
      "Take-home Exercises",
      "Take-home Exercise 4: Prototyping Time Series Module for Shiny Application"
    ]
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#investigating-missing-values",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#investigating-missing-values",
    "title": "Take-home Exercise 4: Prototyping Time Series Module for Visual Analytics Shiny Application",
    "section": "3.1 Investigating missing values",
    "text": "3.1 Investigating missing values\nFirst, let us use summary to have a sense of the missing data.\n\n\nShow the code\nsummary(temp)\n\n\n   station              tdate            mean_temperature maximum_temperature\n Length:329156      Min.   :1980-01-01   Min.   :22.20    Min.   :22.80      \n Class :character   1st Qu.:1997-04-29   1st Qu.:27.10    1st Qu.:30.50      \n Mode  :character   Median :2011-09-18   Median :27.90    Median :31.70      \n                    Mean   :2007-07-02   Mean   :27.87    Mean   :31.49      \n                    3rd Qu.:2017-11-27   3rd Qu.:28.80    3rd Qu.:32.60      \n                    Max.   :2023-12-31   Max.   :31.50    Max.   :38.00      \n                    NA's   :58           NA's   :255645   NA's   :255282     \n minimum_temperature\n Min.   :20.0       \n 1st Qu.:24.3       \n Median :25.2       \n Mean   :25.3       \n 3rd Qu.:26.3       \n Max.   :30.0       \n NA's   :255283     \n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nThe observations ranged from 1 Jan 1980 to 31 Dec 2023. There are 58 rows with missing dates. We should drop these rows since they are unable to tell us which day the observations were made (even if they have temperature readings).\nThere are 255,645 rows of NAs in daily mean temperature.\nThere are 255,282 rows of NAs in daily maximum temperature.\nThere are 255,283 rows of NAs in daily minimum temperature.\nWe noted that there are a lot of missing values. As the aim of this task is to forecast future temperatures, missing value treatment is not so straight-forward. Imputation using mean, median & mode might hide trends or seasonal patterns whereas removing missing data points altogether might reduce information contained in other features. Let’s understand more about the missing values before we proceed to do imputation for missing values.\n\n\n\nFirst, let us drop those rows where date is missing because we would not be able to definitively identify when the temperature(s) were collected (even if there were temperature readings for these rows.\n\n\nShow the code\ntemp &lt;- temp %&gt;%\n  drop_na(tdate)\n\nsummary(temp)\n\n\n   station              tdate            mean_temperature maximum_temperature\n Length:329098      Min.   :1980-01-01   Min.   :22.20    Min.   :22.80      \n Class :character   1st Qu.:1997-04-29   1st Qu.:27.10    1st Qu.:30.50      \n Mode  :character   Median :2011-09-18   Median :27.90    Median :31.70      \n                    Mean   :2007-07-02   Mean   :27.87    Mean   :31.49      \n                    3rd Qu.:2017-11-27   3rd Qu.:28.80    3rd Qu.:32.60      \n                    Max.   :2023-12-31   Max.   :31.50    Max.   :38.00      \n                                         NA's   :255587   NA's   :255224     \n minimum_temperature\n Min.   :20.0       \n 1st Qu.:24.3       \n Median :25.2       \n Mean   :25.3       \n 3rd Qu.:26.3       \n Max.   :30.0       \n NA's   :255225",
    "crumbs": [
      "Take-home Exercises",
      "Take-home Exercise 4: Prototyping Time Series Module for Shiny Application"
    ]
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#further-exploration-of-missing-temperatures-using-plotly",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#further-exploration-of-missing-temperatures-using-plotly",
    "title": "Take-home Exercise 4: Prototyping Time Series Module for Visual Analytics Shiny Application",
    "section": "3.2 Further exploration of missing temperatures using plotly",
    "text": "3.2 Further exploration of missing temperatures using plotly\nWe noted that there are many weather stations in the temp dataframe. Hence, we will make use of plotly to further explore the missing temperatures.\n\n3.2.1 Daily Mean Temperatures\nLet us first explore the daily mean temperatures by selecting the relevant columns and pivot the dataframe wider.\n\n\nShow the code\ntemp_mean_wide &lt;- temp %&gt;%\n  select(tdate, station, mean_temperature) %&gt;%\n  pivot_wider(names_from = station, values_from = mean_temperature)\n\nglimpse(temp_mean_wide)\n\n\nRows: 16,071\nColumns: 38\n$ tdate                     &lt;date&gt; 1980-01-01, 1980-01-02, 1980-01-03, 1980-01…\n$ `Macritchie Reservoir`    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Lower Peirce Reservoir`  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ Admiralty                 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `East Coast Parkway`      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Ang Mo Kio`              &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ Newton                    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Lim Chu Kang`            &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Marine Parade`           &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Choa Chu Kang (Central)` &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Tuas South`              &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Pasir Panjang`           &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Jurong Island`           &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Nicoll Highway`          &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Botanic Garden`          &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Choa Chu Kang (South)`   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ Whampoa                   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ Changi                    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Jurong Pier`             &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Ulu Pandan`              &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ Mandai                    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Tai Seng`                &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Jurong (West)`           &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ Clementi                  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Sentosa Island`          &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Bukit Panjang`           &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Kranji Reservoir`        &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Upper Peirce Reservoir`  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Kent Ridge`              &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ Queenstown                &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Tanjong Katong`          &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Somerset (Road)`         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ Punggol                   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ Simei                     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Toa Payoh`               &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ Tuas                      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Bukit Timah`             &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Pasir Ris (Central)`     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n\n\nWe will make use of plotly to explore the missing daily temperatures for each station using a dropdown list.\n\nplot_ly(data = temp_mean_wide, \n        x = ~tdate, \n        y = ~ Admiralty, \n        type = \"scatter\", \n        mode = \"lines\") |&gt; \n  layout(title = \"Temperature observed by Weather Stations\", \n       xaxis = list(title = \"Date\"), \n       yaxis = list(title = \"\", range = c(0,40)), \n      theme_ipsum_rc(plot_title_size = 13, plot_title_margin=4, subtitle_size=11, subtitle_margin=4,  \n                 axis_title_size = 8, axis_text_size=8, axis_title_face= \"bold\", plot_margin = margin(4, 4, 4, 4)),  \n       updatemenus = list(list(type = 'dropdown', \n                               xref = \"paper\", \n                               yref = \"paper\", \n                               xanchor = \"left\",\n                               x = 0.04,\n                               y = 0.95, \n                               buttons = list(\n                                 list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_wide$Admiralty)), \n                                                    list(yaxis = list(title = \"Temperature in Admiralty\", range = c(0,40)))),label = \"Admiralty\"),\n                                 list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_wide$`East Coast Parkway`)), \n                                                    list(yaxis = list(title = \"Temperature in East Coast Parkway\", range = c(0,40)))),label = \"East Coast Parkway\"), \n                                 list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_wide$`Ang Mo Kio`)), \n                                                    list(yaxis = list(title = \"Temperature in Ang Mo Kio\", range = c(0,40)))),label = \"Ang Mo Kio\"), \n                                 list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_wide$Newton)), \n                                                    list(yaxis = list(title = \"Temperature in Newton\", range = c(0,40)))),label = \"Newton\"), \n                                 list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_wide$`Tuas South`)), \n                                                    list(yaxis = list(title = \"Temperature in Tuas South\", range = c(0,40)))),label = \"Tuas South\"),\n                                  list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_wide$`Pasir Panjang`)), \n                                                    list(yaxis = list(title = \"Temperature in Pasir Panjang\", range = c(0,40)))),label = \"Pasir Panjang\"), \n                                  list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_wide$`Jurong Island`)), \n                                                    list(yaxis = list(title = \"Temperature in Jurong Island\", range = c(0,40)))),label = \"Jurong Island\"), \n                                 list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_wide$`Choa Chu Kang (South)`)), \n                                                    list(yaxis = list(title = \"Temperature in Choa Chu Kang (South)\", range = c(0,40)))),label = \"Choa Chu Kang (South)\"), \n                                 list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$Changi)), \n                                                    list(yaxis = list(title = \"Temperature in Changi\", range = c(0,40)))),label = \"Changi\"),\n                                  list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_wide$`Tai Seng`)), \n                                                    list(yaxis = list(title = \"Temperature in Tai Seng\", range = c(0,40)))),label = \"Tai Seng\"),\n                                  list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_wide$`Jurong (West)`)), \n                                                    list(yaxis = list(title = \"Temperature in Jurong West\", range = c(0,40)))),label = \"Jurong West\"), \n                                   list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$Clementi)), \n                                                    list(yaxis = list(title = \"Temperature  in Clementi\", range = c(0,40)))),label = \"Clementi\"), \n                                   list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Sentosa Island`)), \n                                                    list(yaxis = list(title = \"Temperature  in Sentosa\", range = c(0,40)))),label = \"Sentosa\"), \n                                 list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Macritchie Reservoir`)), \n                                                    list(yaxis = list(title = \"Temperature  at Macritchie Reservoir\", range = c(0,40)))),label = \"Macritchie Reservoir\"), \n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Lower Peirce Reservoir`)), \n                                                    list(yaxis = list(title = \"Temperature  at Lower Peirce Reservoir\", range = c(0,40)))),label = \"Lower Peirce Reservoir\"),\n                                 list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Lim Chu Kang`)), \n                                                    list(yaxis = list(title = \"Temperature at Lim Chu Kang\", range = c(0,40)))),label = \"Lim Chu Kang\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Marine Parade`)), \n                                                    list(yaxis = list(title = \"Temperature at Marine Parade\", range = c(0,40)))),label = \"Marine Parade\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Choa Chu Kang (Central)`)), \n                                                    list(yaxis = list(title = \"Temperature at Choa Chu Kang (Central)\", range = c(0,40)))),label = \"Choa Chu Kang (Central)\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Choa Chu Kang (Central)`)), \n                                                    list(yaxis = list(title = \"Temperature at Choa Chu Kang (Central)\", range = c(0,40)))),label = \"Choa Chu Kang (Central)\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Nicoll Highway`)), \n                                                    list(yaxis = list(title = \"Temperature at Nicoll Highway\", range = c(0,40)))),label = \"Nicoll Highway\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Botanic Garden`)), \n                                                    list(yaxis = list(title = \"Temperature at Botanic Garden\", range = c(0,40)))),label = \"Botanic Garden\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$Whampoa)), \n                                                    list(yaxis = list(title = \"Temperature at Whampoa\", range = c(0,40)))),label = \"Whampoa\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Jurong Pier`)), \n                                                    list(yaxis = list(title = \"Temperature at Jurong Pier\", range = c(0,40)))),label = \"Jurong Pier\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Ulu Pandan`)), \n                                                    list(yaxis = list(title = \"Temperature at Ulu Pandan\", range = c(0,40)))),label = \"Ulu Pandan\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$Mandai)), \n                                                    list(yaxis = list(title = \"Temperature at Mandai\", range = c(0,40)))),label = \"Mandai\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Bukit Panjang`)), \n                                                    list(yaxis = list(title = \"Temperature at Bukit Panjang\", range = c(0,40)))),label = \"Bukit Panjang\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Kranji Reservoir`)), \n                                                    list(yaxis = list(title = \"Temperature at Kranji Reservoir\", range = c(0,40)))),label = \"Kranji Reservoir\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Upper Peirce Reservoir`)), \n                                                    list(yaxis = list(title = \"Temperature at Upper Peirce Reservoir\", range = c(0,40)))),label = \"Upper Peirce Reservoir\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Kent Ridge`)), \n                                                    list(yaxis = list(title = \"Temperature at Kent Ridge\", range = c(0,40)))),label = \"Kent Ridge\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$Queenstown)), \n                                                    list(yaxis = list(title = \"Temperature at Queenstown\", range = c(0,40)))),label = \"Queenstown\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Tanjong Katong`)), \n                                                    list(yaxis = list(title = \"Temperature at Tanjong Katong\", range = c(0,40)))),label = \"Tanjong Katong\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Somerset (Road)`)), \n                                                    list(yaxis = list(title = \"Temperature at Somerset (Road)\", range = c(0,40)))),label = \"Somerset (Road)\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Punggol`)), \n                                                    list(yaxis = list(title = \"Temperature at Punggol\", range = c(0,40)))),label = \"Punggol\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Simei`)), \n                                                    list(yaxis = list(title = \"Temperature at Simei\", range = c(0,40)))),label = \"Simei\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Toa Payoh`)), \n                                                    list(yaxis = list(title = \"Temperature at Toa Payoh\", range = c(0,40)))),label = \"Toa Payoh\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Tuas`)), \n                                                    list(yaxis = list(title = \"Temperature at Tuas\", range = c(0,40)))),label = \"Tuas\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Bukit Timah`)), \n                                                    list(yaxis = list(title = \"Temperature at Bukit Timah\", range = c(0,40)))),label = \"Bukit Timah\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Pasir Ris (Central)`)), \n                                                    list(yaxis = list(title = \"Temperature at Pasir Ris (Central)\", range = c(0,40)))),label = \"Pasir Ris (Central)\")\n                               ))))  \n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nIt seems like there are some weather stations with no temperature data at all. We should remove them from the temp dataframe.\nThere are some weather stations (e.g. Admiralty) have temperature data only from a certain year onwards (e.g. 2009), and some stations (e.g. Changi) have temperature data as early as 1980s.\nFor those weather stations with temperature data, they also have missing values over a given time period. So we will need to decide how to handle these missing values in subsequent sections.\n\n\n\nLet us identify the amount of missing values for each weather station using the following code chunk.\n\n\nShow the code\nmissing_values &lt;- temp_mean_wide %&gt;%\n  gather(key = \"key\", value = \"val\") %&gt;%\n  mutate(isna = is.na(val)) %&gt;%\n  group_by(key) %&gt;%\n  mutate(total = n()) %&gt;%\n  group_by(key, total, isna) %&gt;%\n  summarise(num.isna = n()) %&gt;%\n  mutate(pct = num.isna / total * 100)\n\nlevels &lt;-\n    (missing_values  %&gt;% filter(isna == T) %&gt;% arrange(desc(pct)))$key\n\npercentage_plot &lt;- missing_values %&gt;%\n      ggplot() +\n        geom_bar(aes(x = reorder(key, desc(pct)), \n                     y = pct, fill=isna), \n                 stat = 'identity', alpha=0.8) +\n      scale_x_discrete(limits = levels) +\n      scale_fill_manual(name = \"\", \n                        values = c('steelblue', 'tomato3'), labels = c(\"Present\", \"Missing\")) +\n      coord_flip() +\n      labs(title = \"Percentage of missing values\", x =\n             'Variable', y = \"% of missing values\")\n\npercentage_plot\n\n\n\n\n\n\n\n\n\nThe above output is consistent with what we observed when exploring the data using plotly. There are numerous stations without temperature readings throughout all years and there are certain stations with temperature readings during certain time periods.\nLet us find out which stations that have no temperature readings throughout the entire time period using filter().We will filter out those weather stations that have 100% NAs.\n\nnotempdata &lt;- missing_values %&gt;%\n  filter(isna == TRUE & pct==100)\n\nnotempdata$key\n\n [1] \"Botanic Garden\"          \"Bukit Panjang\"          \n [3] \"Bukit Timah\"             \"Choa Chu Kang (Central)\"\n [5] \"Jurong Pier\"             \"Kent Ridge\"             \n [7] \"Kranji Reservoir\"        \"Lim Chu Kang\"           \n [9] \"Lower Peirce Reservoir\"  \"Macritchie Reservoir\"   \n[11] \"Mandai\"                  \"Marine Parade\"          \n[13] \"Nicoll Highway\"          \"Pasir Ris (Central)\"    \n[15] \"Punggol\"                 \"Queenstown\"             \n[17] \"Simei\"                   \"Somerset (Road)\"        \n[19] \"Tanjong Katong\"          \"Toa Payoh\"              \n[21] \"Tuas\"                    \"Ulu Pandan\"             \n[23] \"Upper Peirce Reservoir\"  \"Whampoa\"                \n\n\nFrom the above output, we know that these 24 weather stations have no temperature readings. We will put them into a list and create an operator to exclude them from the temp data using filter().\n\nstationstoremove &lt;- c(\"Botanic Garden\",\"Bukit Panjang\",\"Bukit Timah\",\"Choa Chu Kang (Central)\",\"Jurong Pier\",\"Kent Ridge\", \"Kranji Reservoir\", \"Lim Chu Kang\", \"Lower Peirce Reservoir\", \"Macritchie Reservoir\",\"Mandai\", \"Marine Parade\",\"Nicoll Highway\", \"Pasir Ris (Central)\", \"Punggol\", \"Queenstown\",\"Simei\", \"Somerset (Road)\",\"Tanjong Katong\", \"Toa Payoh\", \"Tuas\", \"Ulu Pandan\", \"Upper Peirce Reservoir\",\"Whampoa\")\n\n#create a operator to exclude things \n'%!in%' &lt;- function(x,y)!('%in%'(x,y))\n\n#excluded stations that have no temp data at all \ntemp_clean &lt;- temp %&gt;%\n  filter(station %!in% stationstoremove)\n\nglimpse(temp_clean)\n\nRows: 120,139\nColumns: 5\n$ station             &lt;chr&gt; \"Admiralty\", \"Admiralty\", \"Admiralty\", \"Admiralty\"…\n$ tdate               &lt;date&gt; 2009-01-01, 2009-01-02, 2009-01-03, 2009-01-04, 2…\n$ mean_temperature    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ maximum_temperature &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ minimum_temperature &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n\n\nWe will then pivot the temp_clean dataframe wider and plot the daily temperature for the remaining weather stations using plotly.\n\ntemp_mean_widec &lt;- temp_clean %&gt;%\n  select(tdate, station, mean_temperature) %&gt;%\n  pivot_wider(names_from = station, values_from = mean_temperature)\n\nglimpse(temp_mean_widec)\n\nRows: 16,071\nColumns: 14\n$ tdate                   &lt;date&gt; 2009-01-01, 2009-01-02, 2009-01-03, 2009-01-0…\n$ Admiralty               &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ `East Coast Parkway`    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ `Ang Mo Kio`            &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ Newton                  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ `Tuas South`            &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ `Pasir Panjang`         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ `Jurong Island`         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ `Choa Chu Kang (South)` &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ Changi                  &lt;dbl&gt; 26.6, 26.4, 26.5, 26.3, 27.0, 27.4, 27.1, 27.0…\n$ `Tai Seng`              &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ `Jurong (West)`         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ Clementi                &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ `Sentosa Island`        &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n\n\n\nplot_ly(data = temp_mean_widec, \n        x = ~tdate, \n        y = ~ Admiralty, \n        type = \"scatter\", \n        mode = \"lines+markers\") |&gt; \n  layout(title = \"Temperature observed by Weather Station\", \n       xaxis = list(title = \"Date\"), \n       yaxis = list(title = \"\", range = c(0,40)), \n      theme_ipsum_rc(plot_title_size = 13, plot_title_margin=4, subtitle_size=11, subtitle_margin=4,  \n                 axis_title_size = 8, axis_text_size=8, axis_title_face= \"bold\", plot_margin = margin(4, 4, 4, 4)),  \n       updatemenus = list(list(type = 'dropdown', \n                               xref = \"paper\", \n                               yref = \"paper\", \n                               xanchor = \"left\",\n                               x = 0.04,\n                               y = 0.95, \n                               buttons = list(\n                                 list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_widec$Admiralty)), \n                                                    list(yaxis = list(title = \"Temperature in Admiralty\", range = c(0,40)))),label = \"Admiralty\"),\n                                 list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_widec$`East Coast Parkway`)), \n                                                    list(yaxis = list(title = \"Temperature in East Coast Parkway\", range = c(0,40)))),label = \"East Coast Parkway\"), \n                                 list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_widec$`Ang Mo Kio`)), \n                                                    list(yaxis = list(title = \"Temperature in Ang Mo Kio\", range = c(0,40)))),label = \"Ang Mo Kio\"), \n                                 list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_widec$Newton)), \n                                                    list(yaxis = list(title = \"Temperature in Newton\", range = c(0,40)))),label = \"Newton\"), \n                                 list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_widec$`Tuas South`)), \n                                                    list(yaxis = list(title = \"Temperature in Tuas South\", range = c(0,40)))),label = \"Tuas South\"),\n                                  list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_widec$`Pasir Panjang`)), \n                                                    list(yaxis = list(title = \"Temperature in Pasir Panjang\", range = c(0,40)))),label = \"Pasir Panjang\"), \n                                  list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_widec$`Jurong Island`)), \n                                                    list(yaxis = list(title = \"Temperature in Jurong Island\", range = c(0,40)))),label = \"Jurong Island\"), \n                                 list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_widec$`Choa Chu Kang (South)`)), \n                                                    list(yaxis = list(title = \"Temperature in Choa Chu Kang\", range = c(0,40)))),label = \"Choa Chu Kang\"), \n                                 list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_widec$Changi)), \n                                                    list(yaxis = list(title = \"Temperature in Changi\", range = c(0,40)))),label = \"Changi\"),\n                                  list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_widec$`Tai Seng`)), \n                                                    list(yaxis = list(title = \"Temperature in Tai Seng\", range = c(0,40)))),label = \"Tai Seng\"),\n                                  list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_widec$`Jurong (West)`)), \n                                                    list(yaxis = list(title = \"Temperature in Jurong West\", range = c(0,40)))),label = \"Jurong West\"), \n                                   list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_widec$Clementi)), \n                                                    list(yaxis = list(title = \"Temperature  in Clementi\", range = c(0,40)))),label = \"Clementi\"), \n                                   list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_widec$`Sentosa Island`)), \n                                                    list(yaxis = list(title = \"Temperature  in Sentosa\", range = c(0,40)))),label = \"Sentosa\")\n                                   \n                               ))))  \n\n\n\n\n\n:::{.callout-note} ## Observations\n\nFrom the above interactive chart, we note that some stations have a longer time period with temperature readings (e.g. Changi). Almost all stations have some missing time gaps in the data, hence we will need to do imputation for this missing values to ensure better accuracy of our temperature forecasting.\nAlso, we noted that daily temperature readings that range more than 20 years is too frequent for time series forecasting. Hence, we will aggregate the daily temperature readings to monthly temperature readings by calculating the mean in subsequent section."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#creating-time-series-object",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#creating-time-series-object",
    "title": "Take-home Exercise 4: Prototyping Time Series Module for Visual Analytics Shiny Application",
    "section": "3.3 Creating Time Series Object",
    "text": "3.3 Creating Time Series Object\nIn the previous sections, we noted that the dataframes were all tibble dataframes. For us to make use of the time series forecasting packages and their functions, we would need to convert the tibble dataframe into a time series object.\nBefore we create the time series object, let us first aggregate the daily temperature readings to monthly temperature readings by (1) creating the year-month column for each observation using floor_date() and specifying it to derive the year and month of each observation, and (2) aggregate the temperature readings by station and year_month then use summarise() to compute the monthly averages for mean_temperature, maximum_temperature and minimum_temperature.\n\n\nShow the code\n#create year-month col\ntemp_clean$year_month &lt;- floor_date(temp_clean$tdate, \"month\")\nglimpse(temp_clean)\n\n\nRows: 120,139\nColumns: 6\n$ station             &lt;chr&gt; \"Admiralty\", \"Admiralty\", \"Admiralty\", \"Admiralty\"…\n$ tdate               &lt;date&gt; 2009-01-01, 2009-01-02, 2009-01-03, 2009-01-04, 2…\n$ mean_temperature    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ maximum_temperature &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ minimum_temperature &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ year_month          &lt;date&gt; 2009-01-01, 2009-01-01, 2009-01-01, 2009-01-01, 2…\n\n\n\n\nShow the code\nmonthly_temp &lt;- temp_clean %&gt;%                         \n  group_by(station, year_month) %&gt;% \n  summarise(across(c(mean_temperature, maximum_temperature, minimum_temperature), mean))\n\nglimpse(monthly_temp)\n\n\nRows: 3,947\nColumns: 5\nGroups: station [13]\n$ station             &lt;chr&gt; \"Admiralty\", \"Admiralty\", \"Admiralty\", \"Admiralty\"…\n$ year_month          &lt;date&gt; 2009-01-01, 2009-02-01, 2009-03-01, 2009-04-01, 2…\n$ mean_temperature    &lt;dbl&gt; NA, 26.76786, NA, 28.12000, 28.48387, 28.89667, 28…\n$ maximum_temperature &lt;dbl&gt; NA, 31.44286, NA, 32.19667, 32.59032, 32.87000, 31…\n$ minimum_temperature &lt;dbl&gt; NA, 24.26071, NA, 25.06667, 25.09355, 25.95000, 25…\n\n\nShow the code\nwrite_rds(monthly_temp, \"data/monthly_temp.rds\")\n\n\nWith the monthly temperature of all weather stations, let us filter out one weather station (e.g. Admiralty) to create a tibble data frame adm so that we can convert it into an xts object, which is a type of time series object.\n\n\nShow the code\nmonthly_temp &lt;- read_rds(\"data/monthly_temp.rds\")\n\n#filter out Admiralty weather station \nadm &lt;- monthly_temp %&gt;%\n  filter(station == \"Admiralty\")\n\n#check the resultant dataframe\nsummary(adm)\n\n\n   station            year_month         mean_temperature maximum_temperature\n Length:179         Min.   :2009-01-01   Min.   :25.61    Min.   :28.86      \n Class :character   1st Qu.:2012-09-16   1st Qu.:27.10    1st Qu.:31.33      \n Mode  :character   Median :2016-07-01   Median :27.74    Median :31.85      \n                    Mean   :2016-06-19   Mean   :27.68    Mean   :31.83      \n                    3rd Qu.:2020-03-16   3rd Qu.:28.29    3rd Qu.:32.45      \n                    Max.   :2023-12-01   Max.   :29.15    Max.   :33.87      \n                                         NA's   :22       NA's   :18         \n minimum_temperature\n Min.   :23.63      \n 1st Qu.:24.52      \n Median :24.92      \n Mean   :24.98      \n 3rd Qu.:25.46      \n Max.   :26.45      \n NA's   :18         \n\n\nWe will use xts() from xts package to create a time series object. The order.by parameter uses the dates from the adm dataframe. We then use the ts_regular() function to give the time series object adm_xts a regular interval by adding NA values for missing dates.\nJust in case there are missing months which we did not detected, we use the na.fill() function fills in those missing dates by extending values from previous days.\n\n\nShow the code\nadm_xts &lt;- xts(adm[,c(\"mean_temperature\", \"maximum_temperature\", \"minimum_temperature\")], order.by=as.Date(adm$year_month))\nadm_xts&lt;- ts_regular(adm_xts)\nadm_xts &lt;- na.fill(adm_xts, \"extend\")\n\n\nLet us plot out the monthly mean temperature of Admiralty weather station using ggplotly.\n\n\nShow the code\np1 &lt;- ggplot(adm_xts, aes(x = Index, y = mean_temperature)) + \n  geom_line() + theme_clean() +\n  labs(title = \"Monthly Mean Temperature of Admiralty Weather Station\", caption = \"Data from Weather.gov.sg\") +\n  xlab(\"Month-Year\") +\n  ylab(\"Temperature in degrees celsius\") +\n  theme_ipsum_rc()\n\nggplotly(p1)\n\n\n\n\n\n\nFrom the above output, we see that there are missing temperatures for numerous time periods. As a result, the line for the above chart is not continuous.\nLet us investigate further using imputeTS package’s ggplot_na_distribution, which highlights the missing values in our data. For the following example, we focus on the mean temperature of the adm time series object.\n\n\nShow the code\nggplot_na_distribution(x = adm$mean_temperature,\n                       x_axis_labels = adm$year_month,\n                       ylab = \"Temperature in degrees celsius\")\n\n\n\n\n\n\n\n\n\nWe also use the imputeTS package’s statsNA to have a report on the number of missing mean temperature readings.\n\n\nShow the code\nstatsNA(adm_xts$mean_temperature)\n\n\n[1] \"Length of time series:\"\n[1] 180\n[1] \"-------------------------\"\n[1] \"Number of Missing Values:\"\n[1] 23\n[1] \"-------------------------\"\n[1] \"Percentage of Missing Values:\"\n[1] \"12.8%\"\n[1] \"-------------------------\"\n[1] \"Number of Gaps:\"\n[1] 12\n[1] \"-------------------------\"\n[1] \"Average Gap Size:\"\n[1] 1.916667\n[1] \"-------------------------\"\n[1] \"Stats for Bins\"\n[1] \"  Bin 1 (45 values from 1 to 45) :      3 NAs (6.67%)\"\n[1] \"  Bin 2 (45 values from 46 to 90) :      4 NAs (8.89%)\"\n[1] \"  Bin 3 (45 values from 91 to 135) :      11 NAs (24.4%)\"\n[1] \"  Bin 4 (45 values from 136 to 180) :      5 NAs (11.1%)\"\n[1] \"-------------------------\"\n[1] \"Longest NA gap (series of consecutive NAs)\"\n[1] \"6 in a row\"\n[1] \"-------------------------\"\n[1] \"Most frequent gap size (series of consecutive NA series)\"\n[1] \"1 NA in a row (occurring 7 times)\"\n[1] \"-------------------------\"\n[1] \"Gap size accounting for most NAs\"\n[1] \"1 NA in a row (occurring 7 times, making up for overall 7 NAs)\"\n[1] \"-------------------------\"\n[1] \"Overview NA series\"\n[1] \"  1 NA in a row: 7 times\"\n[1] \"  2 NA in a row: 2 times\"\n[1] \"  3 NA in a row: 2 times\"\n[1] \"  6 NA in a row: 1 times\"\n\n\n\n3.3.1 Missing Value Imputation\nThere are several ways to impute missing data in time series objects. We need to impute missing values because some of the models cannot handle NAs in Time Series objects.\n\n3.3.1.1 Moving Averages\nAs this function calculates moving averages based on the last n observations, it will generally be performing better than using mean, mode and median imputation. Moving averages work well when data has a linear trend. This function also allows us to use linear-weighted and exponentially-weighted moving averages.\n\n\nShow the code\nadm_imp_movingavg &lt;- na_ma(adm_xts, weighting = \"exponential\") #default is exponential. Other options are \"simple\" and \"linear\". We can allow users to choose if the option they want. \n\n#plot chart \n#ggplot(adm_imp_movingavg, aes(x = Index, y = mean_temperature)) + \n  #geom_line()\n\nplot_ma&lt;- ggplot(adm_imp_movingavg, aes(x = Index, y = mean_temperature)) + \n  geom_line() + theme_clean() +\n  labs(title = \"Monthly Mean Temperature of Admiralty Weather Station \\n(missing values imputed using moving average)\") +\n  xlab(\"Month-Year\") +\n  ylab(\"Temperature in degrees celsius\") +\n  theme_ipsum_rc()\n\nggplotly(plot_ma)\n\n\n\n\n\n\n\n\n3.3.1.2 Kalman smoothing\nWe can also use Kalman Smoothing on ARIMA model to impute the missing values.\n\n\nShow the code\nadm_imp_kalman &lt;- na_kalman(adm_xts, model = \"auto.arima\")\n\n#plot chart \n\nggplot(adm_imp_kalman, aes(x = Index, y = mean_temperature)) + \n  geom_line()\n\n\n\n\n\n\n\n\n\nFrom the above output, we see that some of the imputed values are below 0 degrees celsius which is impossible in Singapore. As such, we will not be using this method to impute missing values for temperature readings.\nKalman Smoothing also has a “StrucTS” option. Let us try and see how it works for our temperature data.\n\n\nShow the code\nadm_imp_kalman_ts &lt;- na_kalman(adm_xts, model = \"StructTS\")\n\n#plot chart \n\nggplot(adm_imp_kalman_ts, aes(x = Index, y = mean_temperature)) + \n  geom_line()\n\n\n\n\n\n\n\n\n\nFrom the above output, it seems like using the “StrucTS” model works better than auto.arima model since the imputed results were reasonable. Again, we can also let users choose which model they want to use.",
    "crumbs": [
      "Take-home Exercises",
      "Take-home Exercise 4: Prototyping Time Series Module for Shiny Application"
    ]
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#testing-if-the-time-series-is-stationary",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#testing-if-the-time-series-is-stationary",
    "title": "Take-home Exercise 4: Prototyping Time Series Module for Visual Analytics Shiny Application",
    "section": "3.4 Testing if the time series is stationary",
    "text": "3.4 Testing if the time series is stationary\nBefore we model the time series forecasting model, let us test is our time series data is stationary. Stationarity signifies that the statistical properties of time series, such as mean, variance, and covariance, remain constant over time, which is the fundamental assumption for many time series modeling techniques.It simplifies the complex dynamics within the data, making it more amenable to analysis, modeling, and forecasting.\nThere are two tests we are use to test for stationarity: - Augmented Dickey-Fuller (ADF) Test; and - Kwiatkowski-Phillips-Schmidt-Shin (KPSS) Test\n\n3.4.1 Augmented Dickey-Fuller Test\nNull Hypothesis: Series is non-stationary, or series has a unit root. Alternative Hypothesis: Series is stationary, or series has no unit root.\nIf the null hypothesis fails to be rejected, this test may provide evidence that the series is non-stationary.\n\n\nShow the code\nadf.test(adm_imp_movingavg$mean_temperature)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  adm_imp_movingavg$mean_temperature\nDickey-Fuller = -7.3405, Lag order = 5, p-value = 0.01\nalternative hypothesis: stationary\n\n\nSince the p-value is 0.01, which is less than critical value of 0.05, we reject the null hypothesis. This means that the time series does not have a unit root, meaning it is stationary. It does not have a time-dependent structure.\n\n\n3.4.2 Kwiatkowski-Phillips-Schmidt-Shin Test\nNull Hypothesis: Series is trend stationary or series has no unit root. Alternative Hypothesis: Series is non-stationary, or series has a unit root.\n\n\n\n\n\n\nNote\n\n\n\nNote: The hypothesis is reversed in the KPSS test compared to ADF Test.\n\n\n\n\nShow the code\nkpss.test(adm_imp_movingavg$mean_temperature)\n\n\n\n    KPSS Test for Level Stationarity\n\ndata:  adm_imp_movingavg$mean_temperature\nKPSS Level = 0.086074, Truncation lag parameter = 4, p-value = 0.1\n\n\nSince the p-value is 0.1, which is greater than the critical value of 0.05, we fail to reject the null hypothesis of the KPSS test.This means we can assume that the time series is trend stationary.\nBoth ADF and KPSS tests conclude that the given series is stationary. This means that we can make use of most of the time series forecasting models such as Exponential Smoothing and ARIMA.",
    "crumbs": [
      "Take-home Exercises",
      "Take-home Exercise 4: Prototyping Time Series Module for Shiny Application"
    ]
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#decomposition-of-time-series-object",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#decomposition-of-time-series-object",
    "title": "Take-home Exercise 4: Prototyping Time Series Module for Visual Analytics Shiny Application",
    "section": "3.5 Decomposition of Time Series Object",
    "text": "3.5 Decomposition of Time Series Object\nTime series data can exhibit a variety of patterns, and it is often helpful to split a time series into several component to help us improve our understanding of the time series and forecast accuracy.\nFirst, let us plot the monthly mean temperature of the Admiralty weather station.\n\n\nShow the code\np2 &lt;- ggplot(adm_imp_movingavg, aes(x = Index, y = mean_temperature)) + \n  geom_line() + \n  geom_smooth(method=lm) \n\nggplotly(p2)\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFrom the above output, it seems like there were fluctuations in monthly mean temperature but there was no increasing trend.\nIt also seems like for each year, the mean temperature would usually be the highest in May/Jun of each year as indicated by the peaks. Also, for each year, the lowest mean temperature would usually be around Dec/ Jan.\n\n\nTo find out if there is a seasonality, trend and cycle, we can decompose a time series object using stl()from xts package. STL is a versatile and robust method for decomposing time series. STL is an acronym for “Seasonal and Trend decomposition using Loess”, while Loess is a method for estimating nonlinear relationships. The STL method was developed by R. B. Cleveland, Cleveland, McRae, & Terpenning (1990).\nIn the following code chunk, we use: - stl() to decompose the time series object - ts_ts() function from the library converts an xts field to a ts object that can be used with stl().\n\n\nShow the code\nadm_decomposition &lt;- stl(ts_ts(adm_imp_movingavg$mean_temperature), s.window = \"periodic\")\n\n## plot out the decomposition results \nautoplot(adm_decomposition)+ \n  ggtitle(\"Decomposition for Monthly Mean Temperature\") +\n  xlab(\"Month-Year\") + \n  theme_clean()\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nsummary(adm_decomposition)\n\n\n Call:\n stl(x = ts_ts(adm_imp_movingavg$mean_temperature), s.window = \"periodic\")\n\n Time.series components:\n    seasonal              trend            remainder         \n Min.   :-0.9176109   Min.   :27.30177   Min.   :-0.9341622  \n 1st Qu.:-0.5109855   1st Qu.:27.43056   1st Qu.:-0.2329592  \n Median : 0.1857624   Median :27.65282   Median :-0.0124042  \n Mean   : 0.0000000   Mean   :27.66756   Mean   :-0.0019506  \n 3rd Qu.: 0.4489919   3rd Qu.:27.85388   3rd Qu.: 0.2320894  \n Max.   : 0.7298073   Max.   :28.19072   Max.   : 0.9402058  \n IQR:\n     STL.seasonal STL.trend STL.remainder data  \n     0.9600       0.4233    0.4650        1.0218\n   %  94.0         41.4      45.5         100.0 \n\n Weights: all == 1\n\n Other components: List of 5\n $ win  : Named num [1:3] 1801 19 13\n $ deg  : Named int [1:3] 0 1 1\n $ jump : Named num [1:3] 181 2 2\n $ inner: int 2\n $ outer: int 0\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nFrom the above output, we noted that there is no clear linear trend for the monthly mean temperature over the years. However, we do note that the monthly mean temperature ranges from 27.3 degrees celsius to ~28.2 degree celsius.\nWe observed seasonality in the monthly mean temperature over the years.",
    "crumbs": [
      "Take-home Exercises",
      "Take-home Exercise 4: Prototyping Time Series Module for Shiny Application"
    ]
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#building-models",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#building-models",
    "title": "Take-home Exercise 4: Prototyping Time Series Module for Visual Analytics Shiny Application",
    "section": "3.6 Building Models",
    "text": "3.6 Building Models\nFirst we will split the data into training and validation data.\nWhen choosing models, it is common practice to separate the available data into two portions, training and test data, where the training data is used to estimate any parameters of a forecasting method and the test data is used to evaluate its accuracy. Because the test data is not used in determining the forecasts, it should provide a reliable indication of how well the model is likely to forecast on new data.\nThe size of the test set is typically about 20% of the total sample, although this value depends on how long the sample is and how far ahead you want to forecast. The test set should ideally be at least as large as the maximum forecast horizon required.\nFor this section, we will set the test set ~20% of the dataframe we have. However when building the Shiny dashboard, we should allow user input on the duration they want to forecast (e.g. next few months or next few years) because this would affect the size of the test dataset.\n\n\nShow the code\n# create samples \ntrainingtemp &lt;- ts_ts(head(adm_imp_movingavg$mean_temperature, (length(adm_imp_movingavg$mean_temperature)-24))) \nvalidationtemp &lt;- ts_ts(tail(adm_imp_movingavg$mean_temperature, 24)) # we are going to predict the \n\n\n\n3.6.1 Benchmark models\nSome forecasting methods are extremely simple and surprisingly effective. We will use the following two forecasting methods (i.e. naive and seasonal naive) as benchmarks.\n\n3.6.1.1 Naive method\nFor naïve forecasts, we simply set all forecasts to be the value of the last observation.\n\nData TableMean Absolute Percentage Error (MAPE)Plot of Forecasted Results\n\n\n\nnaive_model &lt;- naive(trainingtemp, h = length(validationtemp))\ndatatable(data.frame(naive_model))\n\n\n\n\n\n\n\nMean absolute percentage error (MAPE) is the percentage equivalent of mean absolute error (MAE). Mean absolute percentage error measures the average magnitude of error produced by a model, or how far off predictions are on average.\nTo measure the performance of how well the model’s forecasted values as compared to the test dataset, we use MAPE() of MLmetrics package to calculate the MAPE.\n\nMAPE(naive_model$mean, validationtemp) * 100\n\n[1] 2.79179\n\n\nFrom the above output, we have a MAPE of 2.79% meaning that the average difference between the forecasted value and the actual value is 2.79%.For a simple model, this forecasting accuracy is very good! It means that other models introduced would need to have a even lower MAPE in order for us to consider them.\n\n\n\nautoplot(naive_model) +\n  ggtitle(\"Naive Forecasts for Monthly Mean Temperature\") +\n  xlab(\"Month-Year\") + \n  ylab(\"Temperature (degree celsius)\") + \n  theme_ipsum_rc()\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.6.1.2 Seasonal naive method\nA similar method is useful for highly seasonal data. In this case, we set each forecast to be equal to the last observed value from the same season (e.g.,the same month of the previous year).\n\nData TableMAPEPlot of Forecasted Results\n\n\n\nsnaive_model &lt;- snaive(trainingtemp, h = length(validationtemp))\ndatatable(data.frame(snaive_model))\n\n\n\n\n\n\n\n\nMAPE(snaive_model$mean, validationtemp) * 100\n\n[1] 2.06697\n\n\nFrom the above output, we have a MAPE of 2.07% meaning that the average difference between the forecasted value and the actual value is 2.07%.For a simple model, this forecasting accuracy is even better than the naive model! It means that other models introduced would need to have a even lower MAPE in order for us to consider them.\n\n\n\nautoplot(trainingtemp) +\n  autolayer(snaive(trainingtemp, h = length(validationtemp),\n                   series=\"Seasonal Naive\", PI=FALSE)) +\n  ggtitle(\"Seasonal Naive Forecasts for Monthly Mean Temperature\") +\n  xlab(\"Month-Year\") + \n  ylab(\"Temperature (degree celsius)\") + \n  hrbrthemes::theme_ipsum_rc()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.6.2 Exponential Smoothing Methods\n\n3.6.2.1 Simple exponential smoothing\nThe simplest of the exponentially smoothing methods is naturally called simple exponential smoothing (SES). This method is suitable for forecasting data with no clear trend or seasonal pattern.\n\nData TableMAPEPlot of Forecasted Results\n\n\n\nses_modelT &lt;- ses(trainingtemp, h = length(validationtemp))\ndatatable(data.frame(ses_modelT))\n\n\n\n\n\n\n\n\nMAPE(ses_modelT$mean, validationtemp) * 100\n\n[1] 2.784014\n\n\nFrom the above output, we have a MAPE of 2.78% meaning that the average difference between the forecasted value and the actual value is 2.78%, which is slightly better than the naive model and poorer performance than the seasonal naive model.\n\n\n\nautoplot(ses_modelT) +\n  ggtitle(\"Simple exponential smoothing Forecasts for \\nMonthly Mean Temperature\") +\n  xlab(\"Month-Year\") + \n  ylab(\"Temperature (degree celsius)\") + \n  theme_ipsum_rc()\n\n\n\n\n\n\n\n\nThis output resembles the results from the naive model.\n\n\n\n\n\n3.6.2.2 State Space Model\nState space models provide a flexible framework for modeling time series data. They consist of two components: the state equation and the observation equation. The state equation describes how the underlying states of the system evolve over time, while the observation equation relates the observed data to the underlying states.\nState space models allow us to capture complex dynamics and dependencies in the data, making them suitable for a wide range of applications, including finance, economics and engineering.\n\nFitting the modelGetting the Forcasted ResultsMAPEPlotting the Forecasted Results\n\n\nWe use ets() from forecast package to find out the optimal model.\n\nets_modelT &lt;- ets(trainingtemp)\nsummary(ets_modelT)\n\nETS(M,N,A) \n\nCall:\n ets(y = trainingtemp) \n\n  Smoothing parameters:\n    alpha = 0.2716 \n    gamma = 1e-04 \n\n  Initial states:\n    l = 27.8746 \n    s = -0.8262 -0.4477 0.1394 0.235 0.4386 0.5042\n           0.6772 0.6073 0.3135 -0.0647 -0.631 -0.9455\n\n  sigma:  0.0158\n\n     AIC     AICc      BIC \n544.5750 548.0035 590.3228 \n\nTraining set error measures:\n                       ME      RMSE      MAE         MPE     MAPE      MASE\nTraining set -0.005080769 0.4168655 0.336926 -0.03791676 1.219642 0.7034614\n                  ACF1\nTraining set 0.1037638\n\n\nWe see ETS (M,N,A). This means we have an ets model with multiplicative errors, no trend and a additive seasonality. Additive seasonality means there aren’t any changes to widths or heights of seasonal periods over time.\n\n\n\nets_forecastT &lt;- forecast(ets_modelT, h=length(validationtemp))\ndatatable(data.frame(ets_forecastT))\n\n\n\n\n\n\n\n\nMAPE(ets_forecastT$mean, validationtemp) *100\n\n[1] 1.471135\n\n\nFrom the above output, we have a MAPE of 1.47% meaning that the average difference between the forecasted value and the actual value is 1.47%, which is so far the best performing model.\n\n\n\nautoplot(ets_forecastT) +\n  ggtitle(\"ETS Forecasts for Monthly Mean Temperature\") +\n  xlab(\"Month-Year\") + \n  ylab(\"Temperature (degree celsius)\") + \n  theme_ipsum_rc()\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.6.2.3 Holt-Winters\nSince time series analysis decomposes past weather observations into seasonal, trend, and random components, that data can be used to create forecasts based on an assumption that the observed patterns will continue into the future. This type of forecasting is especially useful in business for anticipating potential demand for seasonal products.\nTo forecast future temperatures based on historical observations, we can use Holt-Winters model that considers past seasonal cycles, trends, and random variation.\nNote that for Holt-Winters’ method, we can choose additive or multiplicative seasonality to forecast the monthly mean temperature, which can be part of the user input.\n\n3.6.2.3.1 Additive Seasonality\n\nData TableMAPEPlotting Forecasted Results\n\n\n\nhw_modela &lt;- hw(trainingtemp, h = length(validationtemp), seasonal = \"additive\") \ndatatable(data.frame(hw_modela))\n\n\n\n\n\n\n\n\nMAPE(hw_modela$mean, validationtemp)*100\n\n[1] 1.467264\n\n\nFrom the above output, we have a MAPE of 1.47% meaning that the average difference between the forecasted value and the actual value is 1.47%, which gives us as good performance as the State Space Model.\n\n\n\nautoplot(hw_modela) +\n  ggtitle(\"Holt-Winters (Additive Seasonality) Forecasts for \\nMonthly Mean Temperature\") +\n  xlab(\"Month-Year\") + \n  ylab(\"Temperature (degree celsius)\") + \n  theme_ipsum_rc()\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.6.2.3.2 Multiplicative Seasonality\n\nData TableMAPEPlotting Forecasted Results\n\n\n\nhw_modelm &lt;- hw(trainingtemp, h = length(validationtemp), seasonal = \"multiplicative\") \ndatatable(data.frame(hw_modelm))\n\n\n\n\n\n\n\n\nMAPE(hw_modelm$mean, validationtemp)*100\n\n[1] 1.525166\n\n\nFrom the above output, we have a MAPE of 1.53% meaning that the average difference between the forecasted value and the actual value is 1.53%, which is relatively slightly poorer than the Holt-Winters’ Additive Seasonality Model.\n\n\n\nautoplot(hw_modelm) +\n  ggtitle(\"Holt-Winters (Multiplicative Seasonality) Forecasts for \\nMonthly Mean Temperature\") +\n  xlab(\"Month-Year\") + \n  ylab(\"Temperature (degree celsius)\") + \n  theme_ipsum_rc()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.6.3 ARIMA\nARIMA models provide another approach to time series forecasting. While exponential smoothing models are based on a description of the trend and seasonality in the data, auto regressive integrated moving average (ARIMA) modeling involves a more detailed analysis of the training data using lags and lagged forecast errors.\n\nFitting the modelGetting the Forecasted ResultsMAPEPlotting the forecasted results\n\n\nThe first step is to use a function like auto.arima() to analyze the data and find appropriate model configuration parameters.\n\narima_optimal &lt;- auto.arima(trainingtemp)\narima_optimal\n\nSeries: trainingtemp \nARIMA(1,0,1)(2,1,1)[12] with drift \n\nCoefficients:\n         ar1      ma1    sar1     sar2     sma1   drift\n      0.7912  -0.4842  0.0130  -0.1503  -0.8839  0.0013\ns.e.  0.1180   0.1735  0.1349   0.1298   0.1996  0.0018\n\nsigma^2 = 0.1948:  log likelihood = -93.88\nAIC=201.76   AICc=202.59   BIC=222.55\n\n\nThe function returned the following model: ARIMA (1,0,1)(2,1,1)[12] with drift.\n\n\n\narima_model &lt;- forecast(arima_optimal)\ndatatable(data.frame(arima_model))\n\n\n\n\n\n\n\n\nMAPE(arima_model$mean, validationtemp)*100\n\n[1] 1.552931\n\n\nFrom the above output, we have a MAPE of 1.55% meaning that the average difference between the forecasted value and the actual value is 1.55%, which gives us better performance than the benchmark models.\n\n\n\nautoplot(arima_model) +\n  ggtitle(\"ARIMA Forecasts for Monthly Mean Temperature\") +\n  xlab(\"Month-Year\") + \n  ylab(\"Temperature (degree celsius)\") + \n  theme_ipsum_rc()\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.6.4 Comparisons of Models Tried\n\nMAPE Results of Models Tried\n\n\nModel\nMAPE (%)\n\n\n\n\nNaive\n2.79\n\n\nSeasonal Naive\n2.07\n\n\nSimple Exponential Smoothing\n2.78\n\n\nState Space Model\n1.47\n\n\nHolt-Winters’ Model (Additive Seasonality)\n1.47\n\n\nHolt-Winters’ Model (Multiplicative Seasonality)\n1.53\n\n\nARIMA\n1.55\n\n\n\nFrom the above table, the state space model, Holt-Winters’ model and ARIMA model all outperformed the benchmark models (i.e. naive Model and Seasonal Naive Model) for temperature data. We can consider letting users to choose to use these models when forecasting temperature data.",
    "crumbs": [
      "Take-home Exercises",
      "Take-home Exercise 4: Prototyping Time Series Module for Shiny Application"
    ]
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#selecting-the-relevant-columns-for-temperature-data",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#selecting-the-relevant-columns-for-temperature-data",
    "title": "Take-home Exercise 4: Prototyping Time Series Module for Visual Analytics Shiny Application",
    "section": "4.1 Selecting the relevant columns for Temperature Data",
    "text": "4.1 Selecting the relevant columns for Temperature Data\n\n\nShow the code\nrain &lt;- data %&gt;%\n  select(tdate, station, daily_rainfall_total) \n\nstr(rain)\n\n\n\n\nShow the code\nwrite_rds(rain, \"data/rainfall.rds\")\n\n\n\nrain &lt;- read_rds(\"data/rainfall.rds\")\nstr(rain)\n\ntibble [329,156 × 3] (S3: tbl_df/tbl/data.frame)\n $ tdate               : Date[1:329156], format: \"1980-01-01\" \"1980-01-02\" ...\n $ station             : chr [1:329156] \"Macritchie Reservoir\" \"Macritchie Reservoir\" \"Macritchie Reservoir\" \"Macritchie Reservoir\" ...\n $ daily_rainfall_total: num [1:329156] 0 0 0 0 22.6 49.6 2.4 0 0 0 ..."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#checking-for-missing-values",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#checking-for-missing-values",
    "title": "Take-home Exercise 4: Prototyping Time Series Module for Visual Analytics Shiny Application",
    "section": "4.2 Checking for missing values",
    "text": "4.2 Checking for missing values\n\nsummary(rain)\n\n     tdate              station          daily_rainfall_total\n Min.   :1980-01-01   Length:329156      Min.   :  0.000     \n 1st Qu.:1997-04-29   Class :character   1st Qu.:  0.000     \n Median :2011-09-18   Mode  :character   Median :  0.200     \n Mean   :2007-07-02                      Mean   :  6.822     \n 3rd Qu.:2017-11-27                      3rd Qu.:  6.500     \n Max.   :2023-12-31                      Max.   :278.600     \n NA's   :58                              NA's   :5136        \n\n\n\nrain &lt;- rain %&gt;%\n  drop_na(c(tdate, station))\n\nsummary(rain)\n\n     tdate              station          daily_rainfall_total\n Min.   :1980-01-01   Length:329098      Min.   :  0.000     \n 1st Qu.:1997-04-29   Class :character   1st Qu.:  0.000     \n Median :2011-09-18   Mode  :character   Median :  0.200     \n Mean   :2007-07-02                      Mean   :  6.822     \n 3rd Qu.:2017-11-27                      3rd Qu.:  6.500     \n Max.   :2023-12-31                      Max.   :278.600     \n                                         NA's   :5078        \n\n\n\nunique(rain$station)\n\n [1] \"Macritchie Reservoir\"    \"Lower Peirce Reservoir\" \n [3] \"Admiralty\"               \"East Coast Parkway\"     \n [5] \"Ang Mo Kio\"              \"Newton\"                 \n [7] \"Lim Chu Kang\"            \"Marine Parade\"          \n [9] \"Choa Chu Kang (Central)\" \"Tuas South\"             \n[11] \"Pasir Panjang\"           \"Jurong Island\"          \n[13] \"Nicoll Highway\"          \"Botanic Garden\"         \n[15] \"Choa Chu Kang (South)\"   \"Whampoa\"                \n[17] \"Changi\"                  \"Jurong Pier\"            \n[19] \"Ulu Pandan\"              \"Mandai\"                 \n[21] \"Tai Seng\"                \"Jurong (West)\"          \n[23] \"Clementi\"                \"Sentosa Island\"         \n[25] \"Bukit Panjang\"           \"Kranji Reservoir\"       \n[27] \"Upper Peirce Reservoir\"  \"Kent Ridge\"             \n[29] \"Queenstown\"              \"Tanjong Katong\"         \n[31] \"Somerset (Road)\"         \"Punggol\"                \n[33] \"Simei\"                   \"Toa Payoh\"              \n[35] \"Tuas\"                    \"Bukit Timah\"            \n[37] \"Pasir Ris (Central)\""
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#further-exploration-of-total-rainfall-using-plotly",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#further-exploration-of-total-rainfall-using-plotly",
    "title": "Take-home Exercise 4: Prototyping Time Series Module for Visual Analytics Shiny Application",
    "section": "4.2 Further exploration of total rainfall using plotly",
    "text": "4.2 Further exploration of total rainfall using plotly\nFrom the previous section, we noted that there are many weather stations in the rainfall dataframe. Hence, we will make use of plotly to further explore the missing temperatures.\nFirst, we will pivot the dataframe wider.\n\n\nShow the code\nrain_wide &lt;- rain %&gt;%\n  pivot_wider(names_from = station, values_from = daily_rainfall_total)\n\nsummary(rain_wide)\n\n\n     tdate            Macritchie Reservoir Lower Peirce Reservoir\n Min.   :1980-01-01   Min.   :  0.000      Min.   :  0.000       \n 1st Qu.:1990-12-31   1st Qu.:  0.000      1st Qu.:  0.000       \n Median :2001-12-31   Median :  0.200      Median :  0.400       \n Mean   :2001-12-31   Mean   :  7.145      Mean   :  7.706       \n 3rd Qu.:2012-12-30   3rd Qu.:  7.100      3rd Qu.:  8.200       \n Max.   :2023-12-31   Max.   :256.000      Max.   :227.600       \n                      NA's   :121          NA's   :11141         \n   Admiralty       East Coast Parkway   Ang Mo Kio          Newton       \n Min.   :  0.000   Min.   :  0.000    Min.   :  0.000   Min.   :  0.000  \n 1st Qu.:  0.000   1st Qu.:  0.000    1st Qu.:  0.000   1st Qu.:  0.000  \n Median :  0.400   Median :  0.000    Median :  0.400   Median :  0.200  \n Mean   :  6.735   Mean   :  4.977    Mean   :  7.218   Mean   :  6.625  \n 3rd Qu.:  6.800   3rd Qu.:  3.800    3rd Qu.:  7.800   3rd Qu.:  6.800  \n Max.   :142.000   Max.   :192.600    Max.   :164.400   Max.   :150.400  \n NA's   :10785     NA's   :10778      NA's   :10901     NA's   :11112    \n  Lim Chu Kang     Marine Parade    Choa Chu Kang (Central)   Tuas South     \n Min.   :  0.000   Min.   :  0.00   Min.   :  0.00          Min.   :  0.000  \n 1st Qu.:  0.000   1st Qu.:  0.00   1st Qu.:  0.00          1st Qu.:  0.000  \n Median :  0.400   Median :  0.00   Median :  0.40          Median :  0.200  \n Mean   :  6.759   Mean   :  5.61   Mean   :  7.51          Mean   :  6.892  \n 3rd Qu.:  7.000   3rd Qu.:  4.95   3rd Qu.:  8.60          3rd Qu.:  6.200  \n Max.   :158.600   Max.   :195.40   Max.   :144.20          Max.   :208.200  \n NA's   :11214     NA's   :10933    NA's   :10994           NA's   :11472    \n Pasir Panjang    Jurong Island     Nicoll Highway    Botanic Garden   \n Min.   :  0.00   Min.   :  0.000   Min.   :  0.000   Min.   :  0.000  \n 1st Qu.:  0.00   1st Qu.:  0.000   1st Qu.:  0.000   1st Qu.:  0.000  \n Median :  0.20   Median :  0.000   Median :  0.200   Median :  0.200  \n Mean   :  6.29   Mean   :  6.287   Mean   :  6.548   Mean   :  7.283  \n 3rd Qu.:  6.00   3rd Qu.:  5.600   3rd Qu.:  6.200   3rd Qu.:  7.800  \n Max.   :151.60   Max.   :185.200   Max.   :163.800   Max.   :160.400  \n NA's   :11031    NA's   :11582     NA's   :11269     NA's   :11228    \n Choa Chu Kang (South)    Whampoa            Changi        Jurong Pier     \n Min.   :  0.000       Min.   :  0.000   Min.   :  0.00   Min.   :  0.000  \n 1st Qu.:  0.000       1st Qu.:  0.000   1st Qu.:  0.00   1st Qu.:  0.000  \n Median :  0.400       Median :  0.200   Median :  0.00   Median :  0.200  \n Mean   :  7.469       Mean   :  6.888   Mean   :  5.81   Mean   :  7.131  \n 3rd Qu.:  8.200       3rd Qu.:  7.100   3rd Qu.:  4.40   3rd Qu.:  7.000  \n Max.   :143.800       Max.   :154.600   Max.   :216.20   Max.   :226.400  \n NA's   :11491         NA's   :11606                      NA's   :275      \n   Ulu Pandan          Mandai           Tai Seng       Jurong (West)    \n Min.   :  0.000   Min.   :  0.000   Min.   :  0.000   Min.   :  0.000  \n 1st Qu.:  0.000   1st Qu.:  0.000   1st Qu.:  0.000   1st Qu.:  0.000  \n Median :  0.200   Median :  0.400   Median :  0.000   Median :  0.300  \n Mean   :  7.201   Mean   :  7.218   Mean   :  6.757   Mean   :  7.224  \n 3rd Qu.:  6.900   3rd Qu.:  7.150   3rd Qu.:  5.900   3rd Qu.:  7.000  \n Max.   :230.400   Max.   :247.200   Max.   :217.200   Max.   :226.200  \n NA's   :355       NA's   :828       NA's   :5         NA's   :445      \n    Clementi       Sentosa Island    Bukit Panjang     Kranji Reservoir \n Min.   :  0.000   Min.   :  0.000   Min.   :  0.000   Min.   :  0.000  \n 1st Qu.:  0.000   1st Qu.:  0.000   1st Qu.:  0.000   1st Qu.:  0.000  \n Median :  0.300   Median :  0.000   Median :  0.400   Median :  0.300  \n Mean   :  7.216   Mean   :  6.166   Mean   :  7.316   Mean   :  7.041  \n 3rd Qu.:  7.200   3rd Qu.:  5.100   3rd Qu.:  7.400   3rd Qu.:  7.100  \n Max.   :239.500   Max.   :220.800   Max.   :235.600   Max.   :239.800  \n NA's   :207       NA's   :365       NA's   :227       NA's   :234      \n Upper Peirce Reservoir   Kent Ridge        Queenstown      Tanjong Katong   \n Min.   :  0.000        Min.   :  0.000   Min.   :  0.000   Min.   :  0.000  \n 1st Qu.:  0.000        1st Qu.:  0.000   1st Qu.:  0.000   1st Qu.:  0.000  \n Median :  0.400        Median :  0.200   Median :  0.200   Median :  0.100  \n Mean   :  7.527        Mean   :  7.365   Mean   :  6.805   Mean   :  6.136  \n 3rd Qu.:  7.900        3rd Qu.:  7.400   3rd Qu.:  6.100   3rd Qu.:  5.100  \n Max.   :202.800        Max.   :179.600   Max.   :278.600   Max.   :226.000  \n NA's   :11168          NA's   :10858     NA's   :773       NA's   :392      \n Somerset (Road)      Punggol            Simei           Toa Payoh      \n Min.   :  0.000   Min.   :  0.000   Min.   :  0.000   Min.   :  0.000  \n 1st Qu.:  0.000   1st Qu.:  0.000   1st Qu.:  0.000   1st Qu.:  0.000  \n Median :  0.200   Median :  0.200   Median :  0.000   Median :  0.200  \n Mean   :  6.387   Mean   :  6.594   Mean   :  5.987   Mean   :  7.211  \n 3rd Qu.:  6.200   3rd Qu.:  6.400   3rd Qu.:  5.200   3rd Qu.:  7.600  \n Max.   :155.800   Max.   :168.800   Max.   :182.600   Max.   :150.200  \n NA's   :11427     NA's   :10767     NA's   :11013     NA's   :11037    \n      Tuas          Bukit Timah     Pasir Ris (Central)\n Min.   :  0.000   Min.   :  0.00   Min.   :  0.000    \n 1st Qu.:  0.000   1st Qu.:  0.00   1st Qu.:  0.000    \n Median :  0.200   Median :  0.40   Median :  0.200    \n Mean   :  7.198   Mean   :  7.13   Mean   :  6.165    \n 3rd Qu.:  7.600   3rd Qu.:  7.40   3rd Qu.:  5.400    \n Max.   :217.000   Max.   :156.80   Max.   :185.800    \n NA's   :10690     NA's   :10826    NA's   :11057      \n\n\nWe will make use of plotly to explore the daily rainfall for each station using a dropdown list.\n\n\nShow the code\nplot_ly(data = rain_wide, \n        x = ~tdate, \n        y = ~ Admiralty, \n        type = \"scatter\",\n        mode = \"lines\") |&gt; \n  layout(title = \"Total Rain Fall observed by Weather Station\", \n       xaxis = list(title = \"Date\", range(as.Date(\"1980-01-01\"), as.Date(\"2023-12-31\"))), \n       yaxis = list(title = \"\"), \n      theme_ipsum_rc(plot_title_size = 13, plot_title_margin=4, subtitle_size=11, subtitle_margin=4,  \n                 axis_title_size = 8, axis_text_size=8, axis_title_face= \"bold\", plot_margin = margin(4, 4, 4, 4)),  \n       updatemenus = list(list(type = 'dropdown', \n                               xref = \"paper\", \n                               yref = \"paper\", \n                               xanchor = \"left\",\n                               x = 0.04,\n                               y = 0.95, \n                               buttons = list(\n                                 list(method = \"update\",\n                                      args = list(list(y = list(rain_wide$Admiralty)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed in Admiralty\"))),label = \"Admiralty\"),\n                                 list(method = \"update\",\n                                      args = list(list(y = list(rain_wide$`East Coast Parkway`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed in East Coast Parkway\"))),label = \"East Coast Parkway\"), \n                                 list(method = \"update\",\n                                      args = list(list(y = list(rain_wide$`Ang Mo Kio`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed in Ang Mo Kio\"))),label = \"Ang Mo Kio\"), \n                                 list(method = \"update\",\n                                      args = list(list(y = list(rain_wide$Newton)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed in Newton\"))),label = \"Newton\"), \n                                 list(method = \"update\",\n                                      args = list(list(y = list(rain_wide$`Tuas South`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed in Tuas South\"))),label = \"Tuas South\"),\n                                  list(method = \"update\",\n                                      args = list(list(y = list(rain_wide$`Pasir Panjang`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed in Pasir Panjang\"))),label = \"Pasir Panjang\"), \n                                  list(method = \"update\",\n                                      args = list(list(y = list(rain_wide$`Jurong Island`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed in Jurong Island\"))),label = \"Jurong Island\"), \n                                 list(method = \"update\",\n                                      args = list(list(y = list(rain_wide$`Choa Chu Kang (South)`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed in Choa Chu Kang (South)\"))),label = \"Choa Chu Kang (South)\"), \n                                 list(method = \"update\", \n                                        args = list(list(y = list(rain_wide$Changi)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed in Changi\"))),label = \"Changi\"),\n                                  list(method = \"update\",\n                                      args = list(list(y = list(rain_wide$`Tai Seng`)), \n                                                  list(yaxis = list(title = \"Total Rainfall observed in Tai Seng\"))),label = \"Tai Seng\"),\n                                  list(method = \"update\",\n                                      args = list(list(y = list(rain_wide$`Jurong (West)`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed in Jurong West\"))),label = \"Jurong West\"), \n                                   list(method = \"update\", \n                                        args = list(list(y = list(rain_wide$Clementi)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed in Clementi\"))),label = \"Clementi\"), \n                                   list(method = \"update\", \n                                        args = list(list(y = list(rain_wide$`Sentosa Island`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed in Sentosa\"))),label = \"Sentosa\"), \n                                 list(method = \"update\", \n                                        args = list(list(y = list(rain_wide$`Macritchie Reservoir`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed  at Macritchie Reservoir\"))),label = \"Macritchie Reservoir\"), \n                                list(method = \"update\", \n                                        args = list(list(y = list(rain_wide$`Lower Peirce Reservoir`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed  at Lower Peirce Reservoir\"))),label = \"Lower Peirce Reservoir\"),\n                                 list(method = \"update\", \n                                        args = list(list(y = list(rain_wide$`Lim Chu Kang`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed at Lim Chu Kang\"))),label = \"Lim Chu Kang\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(rain_wide$`Marine Parade`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed at Marine Parade\"))),label = \"Marine Parade\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(rain_wide$`Choa Chu Kang (Central)`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed at Choa Chu Kang (Central)\"))),label = \"Choa Chu Kang (Central)\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(rain_wide$`Nicoll Highway`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed at Nicoll Highway\"))),label = \"Nicoll Highway\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(rain_wide$`Botanic Garden`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed at Botanic Garden\"))),label = \"Botanic Garden\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(rain_wide$Whampoa)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed at Whampoa\"))),label = \"Whampoa\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(rain_wide$`Jurong Pier`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed at Jurong Pier\"))),label = \"Jurong Pier\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(rain_wide$`Ulu Pandan`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed at Ulu Pandan\"))),label = \"Ulu Pandan\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(rain_wide$Mandai)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed at Mandai\"))),label = \"Mandai\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(rain_wide$`Bukit Panjang`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed at Bukit Panjang\"))),label = \"Bukit Panjang\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(rain_wide$`Kranji Reservoir`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed at Kranji Reservoir\"))),label = \"Kranji Reservoir\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(rain_wide$`Upper Peirce Reservoir`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed at Upper Peirce Reservoir\"))),label = \"Upper Peirce Reservoir\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(rain_wide$`Kent Ridge`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed at Kent Ridge\"))),label = \"Kent Ridge\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(rain_wide$Queenstown)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed at Queenstown\"))),label = \"Queenstown\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(rain_wide$`Tanjong Katong`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed at Tanjong Katong\"))),label = \"Tanjong Katong\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(rain_wide$`Somerset (Road)`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed at Somerset (Road)\"))),label = \"Somerset (Road)\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(rain_wide$`Punggol`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed at Punggol\"))),label = \"Punggol\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(rain_wide$`Simei`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed at Simei\"))),label = \"Simei\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(rain_wide$`Toa Payoh`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed at Toa Payoh\"))),label = \"Toa Payoh\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(rain_wide$`Tuas`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed at Tuas\"))),label = \"Tuas\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(rain_wide$`Bukit Timah`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed at Bukit Timah\"))),label = \"Bukit Timah\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(rain_wide$`Pasir Ris (Central)`)), \n                                                    list(yaxis = list(title = \"Total Rainfall observed at Pasir Ris (Central)\"))),label = \"Pasir Ris (Central)\")\n                               ))))  \n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nIt seems like there are some stations with no rainfall data in all years, while some have rainfall data from certain years onwards. Let’s explore further.\n\n\n\nLet us find out the amount of missing values for each weather station using the following code chunk.\n\nmissing_values &lt;- rain_wide %&gt;%\n  gather(key = \"key\", value = \"val\") %&gt;%\n  mutate(isna = is.na(val)) %&gt;%\n  group_by(key) %&gt;%\n  mutate(total = n()) %&gt;%\n  group_by(key, total, isna) %&gt;%\n  summarise(num.isna = n()) %&gt;%\n  mutate(pct = num.isna / total * 100)\n\nlevels &lt;-\n    (missing_values  %&gt;% filter(isna == T) %&gt;% arrange(desc(pct)))$key\n\npercentage_plot &lt;- missing_values %&gt;%\n      ggplot() +\n        geom_bar(aes(x = reorder(key, desc(pct)), \n                     y = pct, fill=isna), \n                 stat = 'identity', alpha=0.8) +\n      scale_x_discrete(limits = levels) +\n      scale_fill_manual(name = \"\", \n                        values = c('steelblue', 'tomato3'), labels = c(\"Present\", \"Missing\")) +\n      coord_flip() +\n      labs(title = \"Percentage of missing values\", x =\n             'Variable', y = \"% of missing values\")\n\n\nTable of missing valuesPlot of amount of missing values\n\n\n\nmissing_values %&gt;%\n  filter(isna == TRUE) %&gt;% \n  datatable()\n\n\n\n\n\n\n\n\npercentage_plot\n\n\n\n\n\n\n\n\n\n\n\nLet us check if there are any stations where they have 100% missing values.\n\n\nShow the code\nnorfdata &lt;- missing_values %&gt;%\n  filter(isna == TRUE & pct==100)\n\nnorfdata$key\n\n\ncharacter(0)\n\n\nFrom the above output, it seems like no stations have 100% missing values.\nLet us check if there are any stations with no missing values.\n\n\nShow the code\nallrfdata &lt;- missing_values %&gt;%\n  filter(isna == FALSE & pct==100)\n\nallrfdata$key\n\n\n[1] \"Changi\" \"tdate\" \n\n\nFrom the above output, it seems like only Changi weather station has no missing values. This means that for other weather stations there are some amount of missing data for each weather station. Let us impute the missing values in the next section.",
    "crumbs": [
      "Take-home Exercises",
      "Take-home Exercise 4: Prototyping Time Series Module for Shiny Application"
    ]
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#preparing-for-time-series-forecasting",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#preparing-for-time-series-forecasting",
    "title": "Take-home Exercise 4: Prototyping Time Series Module for Visual Analytics Shiny Application",
    "section": "4.4 Preparing for time series forecasting",
    "text": "4.4 Preparing for time series forecasting\n\n#create year-month col\nrain$year_month &lt;- floor_date(rain$tdate, \"month\")\nglimpse(rain)\n\nRows: 329,098\nColumns: 4\n$ tdate                &lt;date&gt; 1980-01-01, 1980-01-02, 1980-01-03, 1980-01-04, …\n$ station              &lt;chr&gt; \"Macritchie Reservoir\", \"Macritchie Reservoir\", \"…\n$ daily_rainfall_total &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 22.6, 49.6, 2.4, 0.0, 0.0, 0.…\n$ year_month           &lt;date&gt; 1980-01-01, 1980-01-01, 1980-01-01, 1980-01-01, …\n\n\n\nmonthly_rain &lt;- rain %&gt;%                         # Aggregate data\n  group_by(station, year_month) %&gt;% \n  summarise(total_rf = sum(daily_rainfall_total))\n\nglimpse(monthly_rain)\n\nRows: 10,813\nColumns: 3\nGroups: station [37]\n$ station    &lt;chr&gt; \"Admiralty\", \"Admiralty\", \"Admiralty\", \"Admiralty\", \"Admira…\n$ year_month &lt;date&gt; 2009-01-01, 2009-02-01, 2009-03-01, 2009-04-01, 2009-05-01…\n$ total_rf   &lt;dbl&gt; NA, 148.0, NA, 148.8, 205.6, 92.0, 103.0, 90.2, 67.6, 160.0…\n\n\n\nmonthly_rain_wide &lt;- monthly_rain %&gt;%\n  select(year_month, station, total_rf) %&gt;%\n  pivot_wider(names_from = station, values_from = total_rf)\n\nglimpse(monthly_rain_wide)\n\nRows: 528\nColumns: 38\n$ year_month                &lt;date&gt; 2009-01-01, 2009-02-01, 2009-03-01, 2009-04…\n$ Admiralty                 &lt;dbl&gt; NA, 148.0, NA, 148.8, 205.6, 92.0, 103.0, 90…\n$ `Ang Mo Kio`              &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 181.…\n$ `Botanic Garden`          &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Bukit Panjang`           &lt;dbl&gt; 14.2, 168.9, 357.4, 164.0, 101.9, 89.1, 119.…\n$ `Bukit Timah`             &lt;dbl&gt; NA, NA, 329.0, 292.4, 217.8, 126.6, 93.0, 19…\n$ Changi                    &lt;dbl&gt; 38.3, 201.8, 223.3, 183.7, 198.6, 21.8, 161.…\n$ `Choa Chu Kang (Central)` &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Choa Chu Kang (South)`   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ Clementi                  &lt;dbl&gt; 16.7, 212.2, 250.9, NA, NA, 90.0, 57.1, NA, …\n$ `East Coast Parkway`      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, 163.0, 135.4, 87.2, …\n$ `Jurong (West)`           &lt;dbl&gt; 20.8, 111.6, 288.2, 298.1, 92.5, 95.7, 140.6…\n$ `Jurong Island`           &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Jurong Pier`             &lt;dbl&gt; 7.5, 88.0, NA, 316.2, 96.2, 53.2, 164.9, 262…\n$ `Kent Ridge`              &lt;dbl&gt; NA, NA, 272.5, 249.8, 191.7, 134.0, 159.2, 2…\n$ `Kranji Reservoir`        &lt;dbl&gt; 16.2, 145.3, 302.7, 237.8, 201.7, 83.3, 115.…\n$ `Lim Chu Kang`            &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Lower Peirce Reservoir`  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Macritchie Reservoir`    &lt;dbl&gt; 7.6, 243.0, 410.8, 340.3, 268.0, 93.8, 92.4,…\n$ Mandai                    &lt;dbl&gt; 10.9, 155.3, 375.8, 107.6, 173.2, 121.6, 119…\n$ `Marine Parade`           &lt;dbl&gt; NA, NA, NA, NA, NA, 25.8, 142.0, 149.0, 66.4…\n$ Newton                    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 247.…\n$ `Nicoll Highway`          &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Pasir Panjang`           &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Pasir Ris (Central)`     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ Punggol                   &lt;dbl&gt; NA, NA, NA, NA, NA, 35.9, 234.7, 216.4, 211.…\n$ Queenstown                &lt;dbl&gt; 23.5, 217.0, 249.5, 286.4, 167.2, 45.7, 149.…\n$ `Sentosa Island`          &lt;dbl&gt; 37.7, 143.6, 236.0, 313.4, 124.4, 57.5, 146.…\n$ Simei                     &lt;dbl&gt; NA, NA, NA, NA, NA, 27.6, 170.2, 170.8, 108.…\n$ `Somerset (Road)`         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Tai Seng`                &lt;dbl&gt; 7.5, 121.0, 262.4, 259.1, 261.2, 84.2, 121.3…\n$ `Tanjong Katong`          &lt;dbl&gt; 8.8, 150.3, 388.3, 258.5, 170.0, 38.0, 168.7…\n$ `Toa Payoh`               &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ Tuas                      &lt;dbl&gt; 55.2, 60.7, 211.4, 371.9, 147.5, 190.2, 122.…\n$ `Tuas South`              &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Ulu Pandan`              &lt;dbl&gt; 14.5, 175.3, 379.0, 165.3, 157.8, 79.5, 88.2…\n$ `Upper Peirce Reservoir`  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ Whampoa                   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n\n\n\nplot_ly(data = monthly_rain_wide, \n        x = ~year_month, \n        y = ~ Admiralty, \n        type = \"scatter\",\n        mode = \"lines+markers\")|&gt; \n  layout(title = \"Total Rainfall observed by Weather Station\", \n       xaxis = list(title = \"Date\"), \n       yaxis = list(title = \"\" ), \n      theme_ipsum_rc(plot_title_size = 13, plot_title_margin=4, subtitle_size=11, subtitle_margin=4,  \n                 axis_title_size = 8, axis_text_size=8, axis_title_face= \"bold\", plot_margin = margin(4, 4, 4, 4)),  \n       updatemenus = list(list(type = 'dropdown', \n                               xref = \"paper\", \n                               yref = \"paper\", \n                               xanchor = \"left\",\n                               x = 0.04,\n                               y = 0.95, \n                               buttons = list(\n                                 list(method = \"update\",\n                                      args = list(list(y = list(monthly_rain_wide$Admiralty)), \n                                                    list(yaxis = list(title = \"Temperature in Admiralty\"))),label = \"Admiralty\"),\n                                 list(method = \"update\",\n                                      args = list(list(y = list(monthly_rain_wide$`East Coast Parkway`)), \n                                                    list(yaxis = list(title = \"Temperature in East Coast Parkway\"))),label = \"East Coast Parkway\"), \n                                 list(method = \"update\",\n                                      args = list(list(y = list(monthly_rain_wide$`Ang Mo Kio`)), \n                                                    list(yaxis = list(title = \"Temperature in Ang Mo Kio\"))),label = \"Ang Mo Kio\"), \n                                 list(method = \"update\",\n                                      args = list(list(y = list(monthly_rain_wide$Newton)), \n                                                    list(yaxis = list(title = \"Temperature in Newton\"))),label = \"Newton\"), \n                                 list(method = \"update\",\n                                      args = list(list(y = list(monthly_rain_wide$`Tuas South`)), \n                                                    list(yaxis = list(title = \"Temperature in Tuas South\"))),label = \"Tuas South\"),\n                                  list(method = \"update\",\n                                      args = list(list(y = list(monthly_rain_wide$`Pasir Panjang`)), \n                                                    list(yaxis = list(title = \"Temperature in Pasir Panjang\"))),label = \"Pasir Panjang\"), \n                                  list(method = \"update\",\n                                      args = list(list(y = list(monthly_rain_wide$`Jurong Island`)), \n                                                    list(yaxis = list(title = \"Temperature in Jurong Island\"))),label = \"Jurong Island\"), \n                                 list(method = \"update\",\n                                      args = list(list(y = list(monthly_rain_wide$`Choa Chu Kang (South)`)), \n                                                    list(yaxis = list(title = \"Temperature in Choa Chu Kang\"))),label = \"Choa Chu Kang\"), \n                                 list(method = \"update\", \n                                        args = list(list(y = list(monthly_rain_wide$Changi)), \n                                                    list(yaxis = list(title = \"Temperature in Changi\"))),label = \"Changi\"),\n                                  list(method = \"update\",\n                                      args = list(list(y = list(monthly_rain_wide$`Tai Seng`)), \n                                                    list(yaxis = list(title = \"Temperature in Tai Seng\"))),label = \"Tai Seng\"),\n                                  list(method = \"update\",\n                                      args = list(list(y = list(monthly_rain_wide$`Jurong (West)`)), \n                                                    list(yaxis = list(title = \"Temperature in Jurong West\"))),label = \"Jurong West\"), \n                                   list(method = \"update\", \n                                        args = list(list(y = list(monthly_rain_wide$Clementi)), \n                                                    list(yaxis = list(title = \"Temperature  in Clementi\"))),label = \"Clementi\"), \n                                   list(method = \"update\", \n                                        args = list(list(y = list(monthly_rain_wide$`Sentosa Island`)), \n                                                    list(yaxis = list(title = \"Temperature  in Sentosa\"))),label = \"Sentosa\")\n                                   \n                               ))))  \n\n\n\n\n\nConvert the data to timeseries data\nWe are using Admiralty weather station for now\n\nadm_rf&lt;- monthly_rain %&gt;%\n  filter(station == \"Admiralty\")\n\nglimpse(adm_rf)\n\nRows: 179\nColumns: 3\nGroups: station [1]\n$ station    &lt;chr&gt; \"Admiralty\", \"Admiralty\", \"Admiralty\", \"Admiralty\", \"Admira…\n$ year_month &lt;date&gt; 2009-01-01, 2009-02-01, 2009-03-01, 2009-04-01, 2009-05-01…\n$ total_rf   &lt;dbl&gt; NA, 148.0, NA, 148.8, 205.6, 92.0, 103.0, 90.2, 67.6, 160.0…\n\n\n\nsummary(adm_rf)\n\n   station            year_month            total_rf    \n Length:179         Min.   :2009-01-01   Min.   : 15.8  \n Class :character   1st Qu.:2012-09-16   1st Qu.:124.4  \n Mode  :character   Median :2016-07-01   Median :189.0  \n                    Mean   :2016-06-19   Mean   :204.9  \n                    3rd Qu.:2020-03-16   3rd Qu.:270.4  \n                    Max.   :2023-12-01   Max.   :517.4  \n                                         NA's   :18     \n\n\n\nadm_rf_xts &lt;- xts(adm_rf[,\"total_rf\"], order.by=as.Date(adm_rf$year_month))\nadm_rf_xts&lt;- ts_regular(adm_rf_xts)\nadm_rf_xts &lt;- na.fill(adm_rf_xts, \"extend\")\nadm_rf_xts&lt;- window(adm_rf_xts, start = as.Date(\"2009-01-01\"), end = as.Date(\"2023-12-01\"))\n\n\nclass(adm_rf_xts)\n\n[1] \"xts\" \"zoo\"\n\nautoplot(adm_rf_xts)\n\n\n\n\n\n\n\n\n\n#check with ti does not work\nggplot(adm_rf_xts, aes(x = Index, y = value)) + \n  geom_line()\n\n\n\n\n\n\n\n\n\nggplot_na_distribution(adm_rf_xts)\n\n\n\n\n\n\n\n\n\nstatsNA(adm_rf_xts$value)\n\n[1] \"Length of time series:\"\n[1] 180\n[1] \"-------------------------\"\n[1] \"Number of Missing Values:\"\n[1] 19\n[1] \"-------------------------\"\n[1] \"Percentage of Missing Values:\"\n[1] \"10.6%\"\n[1] \"-------------------------\"\n[1] \"Number of Gaps:\"\n[1] 9\n[1] \"-------------------------\"\n[1] \"Average Gap Size:\"\n[1] 2.111111\n[1] \"-------------------------\"\n[1] \"Stats for Bins\"\n[1] \"  Bin 1 (45 values from 1 to 45) :      3 NAs (6.67%)\"\n[1] \"  Bin 2 (45 values from 46 to 90) :      3 NAs (6.67%)\"\n[1] \"  Bin 3 (45 values from 91 to 135) :      8 NAs (17.8%)\"\n[1] \"  Bin 4 (45 values from 136 to 180) :      5 NAs (11.1%)\"\n[1] \"-------------------------\"\n[1] \"Longest NA gap (series of consecutive NAs)\"\n[1] \"5 in a row\"\n[1] \"-------------------------\"\n[1] \"Most frequent gap size (series of consecutive NA series)\"\n[1] \"1 NA in a row (occurring 4 times)\"\n[1] \"-------------------------\"\n[1] \"Gap size accounting for most NAs\"\n[1] \"3 NA in a row (occurring 2 times, making up for overall 6 NAs)\"\n[1] \"-------------------------\"\n[1] \"Overview NA series\"\n[1] \"  1 NA in a row: 4 times\"\n[1] \"  2 NA in a row: 2 times\"\n[1] \"  3 NA in a row: 2 times\"\n[1] \"  5 NA in a row: 1 times\"\n\n\n\nadm_imp_kalman &lt;- na_kalman(adm_rf_xts, model = \"auto.arima\")\n\n#plot chart \n\nggplot(adm_imp_kalman, aes(x = Index, y = value)) + \n  geom_line()\n\n\n\n\n\n\n\n\n\nadmrf_imp_movingavg &lt;- na_ma(adm_rf_xts)\n#plot chart \nggplot(admrf_imp_movingavg, aes(x = Index, y = value)) + \n  geom_line()"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#decomposition-of-rainfall",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#decomposition-of-rainfall",
    "title": "Take-home Exercise 4: Prototyping Time Series Module for Visual Analytics Shiny Application",
    "section": "4.5 decomposition of rainfall",
    "text": "4.5 decomposition of rainfall\n\nfit &lt;- stl(ts_ts(admrf_imp_movingavg), s.window=365, t.window = 14001)\nplot(fit)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#building-models-1",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#building-models-1",
    "title": "Take-home Exercise 4: Prototyping Time Series Module for Visual Analytics Shiny Application",
    "section": "4.6 Building Models",
    "text": "4.6 Building Models\nFirst we will split the data into training and validation data. Again, similar to temperature data, we will let users decide how much training data and test data they would want, and the duration of the forecast.\n\n\nShow the code\n# create samples \ntrainingrf &lt;- ts_ts(head(admrf_imp_movingavg$value, (length(admrf_imp_movingavg$value)-24)))\nvalidationrf &lt;- ts_ts(tail(admrf_imp_movingavg$value, 24)) # the number of months would be the user input  \n\n\n\n4.6.1 Benchmark models\n\n\n4.6.2 Naive method\n\nData TableMAPEPlot of Forecasted Results\n\n\n\nnaive_model &lt;- naive(trainingrf, h = length(validationrf))\ndatatable(data.frame(naive_model))\n\n\n\n\n\n\n\n\nMAPE(naive_model$mean, validationrf) * 100\n\n[1] 39.95809\n\n\nFrom the above output, we have a MAPE of ~40% meaning that the average difference between the forecasted value and the actual value is ~40%. As compared to the temperature data, this MAPE is considered very high and there are a lot of room for improvement.\n\n\n\nautoplot(naive_model) +\n  ggtitle(\"Naive Forecasts for Monthly Total Rainfall\") +\n  xlab(\"Month-Year\") + \n  ylab(\"Amount of Rainfall (in mm)\") + \n  theme_ipsum_rc()\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.6.3 Seasonal Naive method\n\nData TableMAPEPlot of Forecasted Results\n\n\n\nsnaive_model &lt;- snaive(trainingrf, h = length(validationrf))\ndatatable(data.frame(snaive_model))\n\n\n\n\n\n\n\n\nMAPE(snaive_model$mean, validationrf) * 100\n\n[1] 69.78003\n\n\nThe MAPE for seasonal naive model is worse than the naive model with it have a MAPE of ~70%. Let’s try other models and see if there any improvements.\n\n\n\nautoplot(snaive_model) +\n  ggtitle(\"Seasonal Naive Forecasts for Monthly Total Rainfall\") +\n  xlab(\"Month-Year\") + \n  ylab(\"Amount of Rainfall (in mm)\") + \n  theme_ipsum_rc()\n\n\n\n\n\n\n\n\n\n4.6.4 Exponential Smoothing Methods\n\n4.6.4.1 Simple exponential smoothing\n\nData TableMAPEPlot of Forecasted Results\n\n\n\nses_modelrf &lt;- ses(trainingrf, h = length(validationrf))\ndatatable(data.frame(ses_modelrf))\n\n\n\n\n\n\n\n\nMAPE(ses_modelrf$mean, validationrf) * 100\n\n[1] 42.794\n\n\nFrom the above output, we have a MAPE of ~43% meaning that the average difference between the forecasted value and the actual value is ~43%. This MAPE is similar to the naive model.\n\n\n\nautoplot(ses_modelrf) +\n  ggtitle(\"Simple Expoential Smoothing forecasts for \\nMonthly Total Rainfall\") +\n  xlab(\"Month-Year\") + \n  ylab(\"Amount of Rainfall (in mm)\") + \n  theme_ipsum_rc()\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.6.4.2 State Space Models\n\nFitting the modelGetting the forecasted resultsMAPEPlot of Forecasted Results\n\n\n\nets_modelrf &lt;- ets(trainingrf)\nsummary(ets_modelrf)\n\nETS(M,N,A) \n\nCall:\n ets(y = trainingrf) \n\n  Smoothing parameters:\n    alpha = 0.2289 \n    gamma = 3e-04 \n\n  Initial states:\n    l = 211.8336 \n    s = 28.6946 69.9082 11.4358 -25.3137 -47.8125 -12.06\n           -50.4517 33.2959 61.8826 6.3161 -69.7022 -6.1932\n\n  sigma:  0.4492\n\n     AIC     AICc      BIC \n2185.839 2189.267 2231.587 \n\nTraining set error measures:\n                      ME     RMSE      MAE       MPE     MAPE      MASE\nTraining set -0.02588395 82.13912 65.51488 -21.92718 45.45668 0.6772009\n                   ACF1\nTraining set 0.03461945\n\n\nWe see ETS (M,N,A). This means we have an ets model with multiplicative errors, no trend and a additive seasonality. Additive seasonality means there aren’t any changes to widths or heights of seasonal periods over time.\n\n\n\nets_forecastrf &lt;- forecast(ets_modelrf, h=length(validationrf))\ndatatable(data.frame(ets_forecastrf))\n\n\n\n\n\n\n\n\nMAPE(ets_forecastrf$mean, validationrf)*100\n\n[1] 45.46367\n\n\nFrom the above output, we have a MAPE of 45.5%, which is worse than the naive model, our benchmark model.\n\n\n\nautoplot(ets_forecastrf) +\n  ggtitle(\"ETS Forecasts for Monthly Total Rainfall\") +\n  xlab(\"Month-Year\") + \n  ylab(\"Amount of Rainfall (in mm)\") + \n  theme_ipsum_rc()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.6.5 Holt-Winters\n\n4.6.5.1 Holt-Winters’ Additive Seasonality\n\nData TableMAPEPlot of Forecasted Results\n\n\n\nhw_modela &lt;- hw(trainingrf, h = length(validationrf), seasonal = \"additive\")\ndatatable(data.frame(hw_modela))\n\n\n\n\n\n\n\n\nMAPE(hw_modela$mean, validationrf)*100\n\n[1] 48.5378\n\n\n\n\n\nautoplot(hw_modela) +\n  ggtitle(\"Holt-Winters (Additive Seasonality) Forecasts for\\nMonthly Total Rainfall\") +\n  xlab(\"Month-Year\") + \n  ylab(\"Amount of Rainfall (in mm)\") + \n  theme_ipsum_rc()\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.6.5.2 Holt-Winters’ Multiplicative Seasonality\n\nData TableMAPEPlot of Forecasted Results\n\n\n\nhw_modelm &lt;- hw(trainingrf, h = length(validationrf), seasonal = \"multiplicative\")\ndatatable(data.frame(hw_modelm))\n\n\n\n\n\n\n\n\nMAPE(hw_modelm$mean, validationrf)*100\n\n[1] 63.30767\n\n\n\n\n\nautoplot(hw_modelm) +\n  ggtitle(\"Holt-Winters (Multiplicative Seasonality) Forecasts for\\nMonthly Total Rainfall\") +\n  xlab(\"Month-Year\") + \n  ylab(\"Amount of Rainfall (in mm)\") + \n  theme_ipsum_rc()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.6.6 ARIMA\n\nFitting the modelGetting the Forecasted ResultsMAPEPlotting the forecasted results\n\n\n\narima_optimal &lt;- auto.arima(trainingrf)\narima_optimal\n\nSeries: trainingrf \nARIMA(0,0,2) with non-zero mean \n\nCoefficients:\n         ma1     ma2      mean\n      0.2415  0.1251  196.2097\ns.e.  0.0809  0.0825    9.6921\n\nsigma^2 = 8037:  log likelihood = -921.24\nAIC=1850.48   AICc=1850.74   BIC=1862.68\n\n\n\n\n\narima_model &lt;- forecast(arima_optimal)\ndatatable(data.frame(arima_model))\n\n\n\n\n\n\n\n\nMAPE(arima_model$mean, validationrf)*100\n\n[1] 40.18849\n\n\n\n\n\nautoplot(arima_model) +\n  ggtitle(\"ARIMA Forecasts for Monthly Mean Temperature\") +\n  xlab(\"Month-Year\") + \n  ylab(\"Temperature (degree celsius)\") + \n  theme_ipsum_rc()\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.6.7 Comparison of Model Performance\n\nComparison of MAPE across Models\n\n\nModel\nMAPE (%)\n\n\n\n\nNaive\n39.96%\n\n\nSeasonal Naive\n69.78%\n\n\nSimple Exponential Smoothing\n42.79%\n\n\nHolt-Winters’ Additive Seasonality\n48.54%\n\n\nHolt-Winters’ Multiplicative Seasonality\n63.31%\n\n\nARIMA\n40.20%\n\n\n\nFrom the above table, we see that except for ARIMA model which has a MAPE of 40%, none of the models outperformed the naive model when it comes to forecasting of rainfall data. As such, we can consider exposing only Naive and ARIMA models for rainfall data prediction on our Shiny Application.",
    "crumbs": [
      "Take-home Exercises",
      "Take-home Exercise 4: Prototyping Time Series Module for Shiny Application"
    ]
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#ui",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#ui",
    "title": "Take-home Exercise 4: Prototyping Time Series Module for Visual Analytics Shiny Application",
    "section": "5.1 UI",
    "text": "5.1 UI\n\n5.1.1 Sketch\n\n\n5.1.2 Details"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#server",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#server",
    "title": "Take-home Exercise 4: Prototyping Time Series Module for Visual Analytics Shiny Application",
    "section": "5.2 Server",
    "text": "5.2 Server\n\n5.2.1 Details"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html",
    "title": "In-class Exercise 3",
    "section": "",
    "text": "To be updated with more codes soon!",
    "crumbs": [
      "In-class Exercises",
      "In-class Exercise 3"
    ]
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#installing-the-relevant-packages",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#installing-the-relevant-packages",
    "title": "In-class Exercise 3",
    "section": "2.1 Installing the Relevant Packages",
    "text": "2.1 Installing the Relevant Packages\nFor this exercise, other than tidyverse, we will be using the following packages:\n\nsf: allow us to data import and manipulate geospatial data\nterra: allow us to handle raster data\ngstat: to do spatial interprolation\ntmap:\nviridis: colour library\n\n\npacman::p_load(sf,terra, gstat, tmap, viridis, tidyverse)",
    "crumbs": [
      "In-class Exercises",
      "In-class Exercise 3"
    ]
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#importing-the-data",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#importing-the-data",
    "title": "In-class Exercise 3",
    "section": "2.2 Importing the Data",
    "text": "2.2 Importing the Data\n\nrfstations &lt;-read_csv(\"data/aspatial/RainfallStation.csv\")\nstr(rfstations)\n\nspc_tbl_ [63 × 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Station  : chr [1:63] \"Admiralty\" \"Admiralty (West)\" \"Ang Mo Kio\" \"Boon Lay (East)\" ...\n $ Latitude : num [1:63] 1.44 1.46 1.38 1.33 1.33 ...\n $ Longitude: num [1:63] 104 104 104 104 104 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   Station = col_character(),\n  ..   Latitude = col_double(),\n  ..   Longitude = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nuse readr’s read_csv, use\n\nrfdata &lt;- read_csv(\"data/aspatial/DAILYDATA_202402.csv\") %&gt;%\n  select(c(1,5)) %&gt;% \n  group_by(Station) %&gt;%\n  summarise(MONTHSUM = sum(`Daily Rainfall Total (mm)`)) %&gt;%\n  ungroup() \n\nglimpse(rfdata)\n\nRows: 43\nColumns: 2\n$ Station  &lt;chr&gt; \"Admiralty\", \"Ang Mo Kio\", \"Botanic Garden\", \"Bukit Panjang\",…\n$ MONTHSUM &lt;dbl&gt; 153.8, 150.0, 173.6, 173.6, 162.2, 130.8, 60.2, 187.8, 207.8,…\n\n\nDo a left join using rfdata as the reference layer, to ensure that those stations with temperature readings would have the longitude and latitude of the weather stations. join by Station column\n\nrfdata &lt;- rfdata %&gt;%\n  left_join(rfstations)\n\nglimpse(rfdata)\n\nRows: 43\nColumns: 4\n$ Station   &lt;chr&gt; \"Admiralty\", \"Ang Mo Kio\", \"Botanic Garden\", \"Bukit Panjang\"…\n$ MONTHSUM  &lt;dbl&gt; 153.8, 150.0, 173.6, 173.6, 162.2, 130.8, 60.2, 187.8, 207.8…\n$ Latitude  &lt;dbl&gt; 1.4439, 1.3764, 1.3087, 1.3824, 1.3191, 1.2841, 1.3678, 1.38…\n$ Longitude &lt;dbl&gt; 103.7854, 103.8492, 103.8180, 103.7603, 103.8191, 103.7886, …\n\n\n\n#check for missing values \n\nsummary(rfdata)\n\n   Station             MONTHSUM        Latitude       Longitude    \n Length:43          Min.   : 60.2   Min.   :1.250   Min.   :103.6  \n Class :character   1st Qu.:100.1   1st Qu.:1.307   1st Qu.:103.8  \n Mode  :character   Median :135.0   Median :1.340   Median :103.8  \n                    Mean   :131.3   Mean   :1.344   Mean   :103.8  \n                    3rd Qu.:159.3   3rd Qu.:1.379   3rd Qu.:103.9  \n                    Max.   :207.8   Max.   :1.444   Max.   :104.0  \n\n\n\nrfdata_sf &lt;- st_as_sf(rfdata, \n                      coords = c(\"Longitude\",\n                                 \"Latitude\"),\n                      crs = 4326) %&gt;%\n  st_transform(crs = 3414) # transform the data into SVY21 as we need distance in meters to do interprolation (otherwise, it will be in decimal degree!)\n\nrfdata_sf\n\nSimple feature collection with 43 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 4081.375 ymin: 25844.17 xmax: 44613.25 ymax: 47284.7\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 43 × 3\n   Station                 MONTHSUM            geometry\n * &lt;chr&gt;                      &lt;dbl&gt;         &lt;POINT [m]&gt;\n 1 Admiralty                  154.   (22667.41 47284.7)\n 2 Ang Mo Kio                 150   (29767.41 39820.84)\n 3 Botanic Garden             174.  (26295.19 32334.92)\n 4 Bukit Panjang              174.  (19873.96 40484.41)\n 5 Bukit Timah                162.   (26417.61 33484.9)\n 6 Buona Vista                131.  (23023.19 29614.82)\n 7 Changi                      60.2 (44613.25 38870.41)\n 8 Choa Chu Kang (Central)    188.  (17459.02 40429.21)\n 9 Choa Chu Kang (South)      208.  (15656.11 39434.11)\n10 Clementi                   163.  (21710.07 35099.36)\n# ℹ 33 more rows\n\n\n\nmpsz2019 &lt;- st_read(dsn = \"data/geospatial\",\n                    layer = \"MPSZ-2019\") %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `MPSZ-2019' from data source \n  `C:\\sihuihui\\VAA\\In-class_Ex\\In-class_Ex03\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\nqtm(mpsz2019)\n\n\n\n\n\n\n\n\n\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"view\")\n\ntm_shape(mpsz2019) + #plot boundary map first\n  tm_borders() + #Note: if we use tm_polygons, the entire polygon would be shaded, covering the background plot. \n  tm_shape(rfdata_sf) + #plot rainfall stations\n  tm_dots(col = \"MONTHSUM\") #colour the rainfall stations (i.e. the dots) based on the monthly rainfall values \n\n\n\n\ntmap_mode(\"plot\")\n\n\nmpsz2019\n\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n                 SUBZONE_N SUBZONE_C       PLN_AREA_N PLN_AREA_C       REGION_N\n1              MARINA EAST    MESZ01      MARINA EAST         ME CENTRAL REGION\n2         INSTITUTION HILL    RVSZ05     RIVER VALLEY         RV CENTRAL REGION\n3           ROBERTSON QUAY    SRSZ01  SINGAPORE RIVER         SR CENTRAL REGION\n4  JURONG ISLAND AND BUKOM    WISZ01  WESTERN ISLANDS         WI    WEST REGION\n5             FORT CANNING    MUSZ02           MUSEUM         MU CENTRAL REGION\n6         MARINA EAST (MP)    MPSZ05    MARINE PARADE         MP CENTRAL REGION\n7                   SUDONG    WISZ03  WESTERN ISLANDS         WI    WEST REGION\n8                  SEMAKAU    WISZ02  WESTERN ISLANDS         WI    WEST REGION\n9           SOUTHERN GROUP    SISZ02 SOUTHERN ISLANDS         SI CENTRAL REGION\n10                 SENTOSA    SISZ01 SOUTHERN ISLANDS         SI CENTRAL REGION\n   REGION_C                       geometry\n1        CR MULTIPOLYGON (((33222.98 29...\n2        CR MULTIPOLYGON (((28481.45 30...\n3        CR MULTIPOLYGON (((28087.34 30...\n4        WR MULTIPOLYGON (((14557.7 304...\n5        CR MULTIPOLYGON (((29542.53 31...\n6        CR MULTIPOLYGON (((35279.55 30...\n7        WR MULTIPOLYGON (((15772.59 21...\n8        WR MULTIPOLYGON (((19843.41 21...\n9        CR MULTIPOLYGON (((30870.53 22...\n10       CR MULTIPOLYGON (((26879.04 26...\n\n\n\ngrid &lt;- terra::rast(mpsz2019,\n                    nrows = 690,  #from the difference between xmax and xmin of mpsz2019 \n                    ncols = 1075) # from the difference bwtween ymax and ymin of mpsz2019 \n\nxy &lt;- terra::xyFromCell(grid, \n                        1:ncell(grid))\n\ncreate an sf layer\nthen use gstat to calculate the resolution. inverse distance (nmax is 15) niehgbour u choose,",
    "crumbs": [
      "In-class Exercises",
      "In-class Exercise 3"
    ]
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/data/geospatial/MPSZ-2019.html",
    "href": "In-class_Ex/In-class_Ex03/data/geospatial/MPSZ-2019.html",
    "title": "",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#further-investigation-of-missing-temperatures-using-plotly",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#further-investigation-of-missing-temperatures-using-plotly",
    "title": "Take-home Exercise 4: Prototyping Time Series Module for Visual Analytics Shiny Application",
    "section": "3.2 Further investigation of missing temperatures using plotly",
    "text": "3.2 Further investigation of missing temperatures using plotly\nWe noted that there are many weather stations in the temp dataframe. Hence, we will make use of plotly to further explore the missing temperatures.\n\n3.2.1 Daily Mean Temperatures\nLet us first explore the daily mean temperatures by selecting the relevant columns and pivot the dataframe wider.\n\n\nShow the code\ntemp_mean_wide &lt;- temp %&gt;%\n  select(tdate, station, mean_temperature) %&gt;%\n  pivot_wider(names_from = station, values_from = mean_temperature)\n\nsummary(temp_mean_wide)\n\n\n     tdate            Macritchie Reservoir Lower Peirce Reservoir\n Min.   :1980-01-01   Min.   : NA          Min.   : NA           \n 1st Qu.:1990-12-31   1st Qu.: NA          1st Qu.: NA           \n Median :2001-12-31   Median : NA          Median : NA           \n Mean   :2001-12-31   Mean   :NaN          Mean   :NaN           \n 3rd Qu.:2012-12-30   3rd Qu.: NA          3rd Qu.: NA           \n Max.   :2023-12-31   Max.   : NA          Max.   : NA           \n                      NA's   :16071        NA's   :16071         \n   Admiralty     East Coast Parkway   Ang Mo Kio        Newton     \n Min.   :22.50   Min.   :23.40      Min.   :22.50   Min.   :22.20  \n 1st Qu.:26.80   1st Qu.:27.50      1st Qu.:26.90   1st Qu.:26.80  \n Median :27.60   Median :28.20      Median :27.80   Median :27.70  \n Mean   :27.66   Mean   :28.13      Mean   :27.82   Mean   :27.58  \n 3rd Qu.:28.50   3rd Qu.:28.90      3rd Qu.:28.80   3rd Qu.:28.40  \n Max.   :30.80   Max.   :30.80      Max.   :31.20   Max.   :30.60  \n NA's   :10821   NA's   :10806      NA's   :10918   NA's   :11148  \n  Lim Chu Kang   Marine Parade   Choa Chu Kang (Central)   Tuas South   \n Min.   : NA     Min.   : NA     Min.   : NA             Min.   :23.10  \n 1st Qu.: NA     1st Qu.: NA     1st Qu.: NA             1st Qu.:27.40  \n Median : NA     Median : NA     Median : NA             Median :28.20  \n Mean   :NaN     Mean   :NaN     Mean   :NaN             Mean   :28.19  \n 3rd Qu.: NA     3rd Qu.: NA     3rd Qu.: NA             3rd Qu.:29.00  \n Max.   : NA     Max.   : NA     Max.   : NA             Max.   :31.00  \n NA's   :16071   NA's   :16071   NA's   :16071           NA's   :11609  \n Pasir Panjang   Jurong Island   Nicoll Highway  Botanic Garden \n Min.   :23.20   Min.   :23.40   Min.   : NA     Min.   : NA    \n 1st Qu.:27.50   1st Qu.:27.60   1st Qu.: NA     1st Qu.: NA    \n Median :28.30   Median :28.40   Median : NA     Median : NA    \n Mean   :28.25   Mean   :28.29   Mean   :NaN     Mean   :NaN    \n 3rd Qu.:29.10   3rd Qu.:29.10   3rd Qu.: NA     3rd Qu.: NA    \n Max.   :31.30   Max.   :31.10   Max.   : NA     Max.   : NA    \n NA's   :11049   NA's   :11748   NA's   :16071   NA's   :16071  \n Choa Chu Kang (South)    Whampoa          Changi       Jurong Pier   \n Min.   :22.70         Min.   : NA     Min.   :22.80   Min.   : NA    \n 1st Qu.:26.80         1st Qu.: NA     1st Qu.:26.90   1st Qu.: NA    \n Median :27.70         Median : NA     Median :27.70   Median : NA    \n Mean   :27.68         Mean   :NaN     Mean   :27.69   Mean   :NaN    \n 3rd Qu.:28.60         3rd Qu.: NA     3rd Qu.:28.60   3rd Qu.: NA    \n Max.   :31.00         Max.   : NA     Max.   :30.90   Max.   : NA    \n NA's   :11558         NA's   :16071   NA's   :731     NA's   :16071  \n   Ulu Pandan        Mandai         Tai Seng     Jurong (West)  \n Min.   : NA     Min.   : NA     Min.   :23.2    Min.   :22.20  \n 1st Qu.: NA     1st Qu.: NA     1st Qu.:27.6    1st Qu.:26.60  \n Median : NA     Median : NA     Median :28.4    Median :27.40  \n Mean   :NaN     Mean   :NaN     Mean   :28.4    Mean   :27.41  \n 3rd Qu.: NA     3rd Qu.: NA     3rd Qu.:29.3    3rd Qu.:28.30  \n Max.   : NA     Max.   : NA     Max.   :31.5    Max.   :30.60  \n NA's   :16071   NA's   :16071   NA's   :11454   NA's   :10995  \n    Clementi     Sentosa Island  Bukit Panjang   Kranji Reservoir\n Min.   :22.80   Min.   :23.00   Min.   : NA     Min.   : NA     \n 1st Qu.:26.90   1st Qu.:27.40   1st Qu.: NA     1st Qu.: NA     \n Median :27.70   Median :28.20   Median : NA     Median : NA     \n Mean   :27.63   Mean   :28.12   Mean   :NaN     Mean   :NaN     \n 3rd Qu.:28.50   3rd Qu.:28.90   3rd Qu.: NA     3rd Qu.: NA     \n Max.   :30.60   Max.   :31.10   Max.   : NA     Max.   : NA     \n NA's   :11258   NA's   :11317   NA's   :16071   NA's   :16071   \n Upper Peirce Reservoir   Kent Ridge      Queenstown    Tanjong Katong \n Min.   : NA            Min.   : NA     Min.   : NA     Min.   : NA    \n 1st Qu.: NA            1st Qu.: NA     1st Qu.: NA     1st Qu.: NA    \n Median : NA            Median : NA     Median : NA     Median : NA    \n Mean   :NaN            Mean   :NaN     Mean   :NaN     Mean   :NaN    \n 3rd Qu.: NA            3rd Qu.: NA     3rd Qu.: NA     3rd Qu.: NA    \n Max.   : NA            Max.   : NA     Max.   : NA     Max.   : NA    \n NA's   :16071          NA's   :16071   NA's   :16071   NA's   :16071  \n Somerset (Road)    Punggol          Simei         Toa Payoh    \n Min.   : NA     Min.   : NA     Min.   : NA     Min.   : NA    \n 1st Qu.: NA     1st Qu.: NA     1st Qu.: NA     1st Qu.: NA    \n Median : NA     Median : NA     Median : NA     Median : NA    \n Mean   :NaN     Mean   :NaN     Mean   :NaN     Mean   :NaN    \n 3rd Qu.: NA     3rd Qu.: NA     3rd Qu.: NA     3rd Qu.: NA    \n Max.   : NA     Max.   : NA     Max.   : NA     Max.   : NA    \n NA's   :16071   NA's   :16071   NA's   :16071   NA's   :16071  \n      Tuas        Bukit Timah    Pasir Ris (Central)\n Min.   : NA     Min.   : NA     Min.   : NA        \n 1st Qu.: NA     1st Qu.: NA     1st Qu.: NA        \n Median : NA     Median : NA     Median : NA        \n Mean   :NaN     Mean   :NaN     Mean   :NaN        \n 3rd Qu.: NA     3rd Qu.: NA     3rd Qu.: NA        \n Max.   : NA     Max.   : NA     Max.   : NA        \n NA's   :16071   NA's   :16071   NA's   :16071      \n\n\nWe will make use of plotly to explore the missing daily temperatures for each station using a dropdown list.\n\n\nShow the code\nplot_ly(data = temp_mean_wide, \n        x = ~tdate, \n        y = ~ Admiralty, \n        type = \"scatter\", \n        mode = \"lines\") |&gt; \n  layout(title = \"Temperature observed by Weather Stations\", \n       xaxis = list(title = \"Date\"), \n       yaxis = list(title = \"\", range = c(0,40)), \n      theme_ipsum_rc(plot_title_size = 13, plot_title_margin=4, subtitle_size=11, subtitle_margin=4,  \n                 axis_title_size = 8, axis_text_size=8, axis_title_face= \"bold\", plot_margin = margin(4, 4, 4, 4)),  \n       updatemenus = list(list(type = 'dropdown', \n                               xref = \"paper\", \n                               yref = \"paper\", \n                               xanchor = \"left\",\n                               x = 0.04,\n                               y = 0.95, \n                               buttons = list(\n                                 list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_wide$Admiralty)), \n                                                    list(yaxis = list(title = \"Temperature in Admiralty\", range = c(0,40)))),label = \"Admiralty\"),\n                                 list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_wide$`East Coast Parkway`)), \n                                                    list(yaxis = list(title = \"Temperature in East Coast Parkway\", range = c(0,40)))),label = \"East Coast Parkway\"), \n                                 list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_wide$`Ang Mo Kio`)), \n                                                    list(yaxis = list(title = \"Temperature in Ang Mo Kio\", range = c(0,40)))),label = \"Ang Mo Kio\"), \n                                 list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_wide$Newton)), \n                                                    list(yaxis = list(title = \"Temperature in Newton\", range = c(0,40)))),label = \"Newton\"), \n                                 list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_wide$`Tuas South`)), \n                                                    list(yaxis = list(title = \"Temperature in Tuas South\", range = c(0,40)))),label = \"Tuas South\"),\n                                  list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_wide$`Pasir Panjang`)), \n                                                    list(yaxis = list(title = \"Temperature in Pasir Panjang\", range = c(0,40)))),label = \"Pasir Panjang\"), \n                                  list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_wide$`Jurong Island`)), \n                                                    list(yaxis = list(title = \"Temperature in Jurong Island\", range = c(0,40)))),label = \"Jurong Island\"), \n                                 list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_wide$`Choa Chu Kang (South)`)), \n                                                    list(yaxis = list(title = \"Temperature in Choa Chu Kang (South)\", range = c(0,40)))),label = \"Choa Chu Kang (South)\"), \n                                 list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$Changi)), \n                                                    list(yaxis = list(title = \"Temperature in Changi\", range = c(0,40)))),label = \"Changi\"),\n                                  list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_wide$`Tai Seng`)), \n                                                    list(yaxis = list(title = \"Temperature in Tai Seng\", range = c(0,40)))),label = \"Tai Seng\"),\n                                  list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_wide$`Jurong (West)`)), \n                                                    list(yaxis = list(title = \"Temperature in Jurong West\", range = c(0,40)))),label = \"Jurong West\"), \n                                   list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$Clementi)), \n                                                    list(yaxis = list(title = \"Temperature  in Clementi\", range = c(0,40)))),label = \"Clementi\"), \n                                   list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Sentosa Island`)), \n                                                    list(yaxis = list(title = \"Temperature  in Sentosa\", range = c(0,40)))),label = \"Sentosa\"), \n                                 list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Macritchie Reservoir`)), \n                                                    list(yaxis = list(title = \"Temperature  at Macritchie Reservoir\", range = c(0,40)))),label = \"Macritchie Reservoir\"), \n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Lower Peirce Reservoir`)), \n                                                    list(yaxis = list(title = \"Temperature  at Lower Peirce Reservoir\", range = c(0,40)))),label = \"Lower Peirce Reservoir\"),\n                                 list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Lim Chu Kang`)), \n                                                    list(yaxis = list(title = \"Temperature at Lim Chu Kang\", range = c(0,40)))),label = \"Lim Chu Kang\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Marine Parade`)), \n                                                    list(yaxis = list(title = \"Temperature at Marine Parade\", range = c(0,40)))),label = \"Marine Parade\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Choa Chu Kang (Central)`)), \n                                                    list(yaxis = list(title = \"Temperature at Choa Chu Kang (Central)\", range = c(0,40)))),label = \"Choa Chu Kang (Central)\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Choa Chu Kang (Central)`)), \n                                                    list(yaxis = list(title = \"Temperature at Choa Chu Kang (Central)\", range = c(0,40)))),label = \"Choa Chu Kang (Central)\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Nicoll Highway`)), \n                                                    list(yaxis = list(title = \"Temperature at Nicoll Highway\", range = c(0,40)))),label = \"Nicoll Highway\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Botanic Garden`)), \n                                                    list(yaxis = list(title = \"Temperature at Botanic Garden\", range = c(0,40)))),label = \"Botanic Garden\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$Whampoa)), \n                                                    list(yaxis = list(title = \"Temperature at Whampoa\", range = c(0,40)))),label = \"Whampoa\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Jurong Pier`)), \n                                                    list(yaxis = list(title = \"Temperature at Jurong Pier\", range = c(0,40)))),label = \"Jurong Pier\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Ulu Pandan`)), \n                                                    list(yaxis = list(title = \"Temperature at Ulu Pandan\", range = c(0,40)))),label = \"Ulu Pandan\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$Mandai)), \n                                                    list(yaxis = list(title = \"Temperature at Mandai\", range = c(0,40)))),label = \"Mandai\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Bukit Panjang`)), \n                                                    list(yaxis = list(title = \"Temperature at Bukit Panjang\", range = c(0,40)))),label = \"Bukit Panjang\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Kranji Reservoir`)), \n                                                    list(yaxis = list(title = \"Temperature at Kranji Reservoir\", range = c(0,40)))),label = \"Kranji Reservoir\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Upper Peirce Reservoir`)), \n                                                    list(yaxis = list(title = \"Temperature at Upper Peirce Reservoir\", range = c(0,40)))),label = \"Upper Peirce Reservoir\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Kent Ridge`)), \n                                                    list(yaxis = list(title = \"Temperature at Kent Ridge\", range = c(0,40)))),label = \"Kent Ridge\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$Queenstown)), \n                                                    list(yaxis = list(title = \"Temperature at Queenstown\", range = c(0,40)))),label = \"Queenstown\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Tanjong Katong`)), \n                                                    list(yaxis = list(title = \"Temperature at Tanjong Katong\", range = c(0,40)))),label = \"Tanjong Katong\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Somerset (Road)`)), \n                                                    list(yaxis = list(title = \"Temperature at Somerset (Road)\", range = c(0,40)))),label = \"Somerset (Road)\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Punggol`)), \n                                                    list(yaxis = list(title = \"Temperature at Punggol\", range = c(0,40)))),label = \"Punggol\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Simei`)), \n                                                    list(yaxis = list(title = \"Temperature at Simei\", range = c(0,40)))),label = \"Simei\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Toa Payoh`)), \n                                                    list(yaxis = list(title = \"Temperature at Toa Payoh\", range = c(0,40)))),label = \"Toa Payoh\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Tuas`)), \n                                                    list(yaxis = list(title = \"Temperature at Tuas\", range = c(0,40)))),label = \"Tuas\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Bukit Timah`)), \n                                                    list(yaxis = list(title = \"Temperature at Bukit Timah\", range = c(0,40)))),label = \"Bukit Timah\"),\n                                list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_wide$`Pasir Ris (Central)`)), \n                                                    list(yaxis = list(title = \"Temperature at Pasir Ris (Central)\", range = c(0,40)))),label = \"Pasir Ris (Central)\")\n                               ))))  \n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nIt seems like there are some weather stations with no temperature data at all. We should remove them from the temp dataframe.\nThere are some weather stations (e.g. Admiralty) have temperature data only from a certain year onwards (e.g. 2009), and some stations (e.g. Changi) have temperature data as early as 1980s.\nFor those weather stations with temperature data, they also have missing values over a given time period. So we will need to decide how to handle these missing values in subsequent sections.\n\n\n\nLet us identify the amount of missing values for each weather station using the following code chunk.\n\n\nShow the code\nmissing_values &lt;- temp_mean_wide %&gt;%\n  gather(key = \"key\", value = \"val\") %&gt;%\n  mutate(isna = is.na(val)) %&gt;%\n  group_by(key) %&gt;%\n  mutate(total = n()) %&gt;%\n  group_by(key, total, isna) %&gt;%\n  summarise(num.isna = n()) %&gt;%\n  mutate(pct = num.isna / total * 100)\n\nlevels &lt;-\n    (missing_values  %&gt;% filter(isna == T) %&gt;% arrange(desc(pct)))$key\n\npercentage_plot &lt;- missing_values %&gt;%\n      ggplot() +\n        geom_bar(aes(x = reorder(key, desc(pct)), \n                     y = pct, fill=isna), \n                 stat = 'identity', alpha=0.8) +\n      scale_x_discrete(limits = levels) +\n      scale_fill_manual(name = \"\", \n                        values = c('steelblue', 'tomato3'), labels = c(\"Present\", \"Missing\")) +\n      coord_flip() +\n      labs(title = \"Percentage of missing values\", x =\n             'Variable', y = \"% of missing values\")\n\npercentage_plot\n\n\n\n\n\n\n\n\n\nThe above output is consistent with what we observed when exploring the data using plotly. There are numerous stations without temperature readings throughout all years and there are certain stations with temperature readings during certain time periods.\nLet us find out which stations that have no temperature readings throughout the entire time period using filter().We will filter out those weather stations that have 100% NAs.\n\n\nShow the code\nnotempdata &lt;- missing_values %&gt;%\n  filter(isna == TRUE & pct==100)\n\nnotempdata$key\n\n\n [1] \"Botanic Garden\"          \"Bukit Panjang\"          \n [3] \"Bukit Timah\"             \"Choa Chu Kang (Central)\"\n [5] \"Jurong Pier\"             \"Kent Ridge\"             \n [7] \"Kranji Reservoir\"        \"Lim Chu Kang\"           \n [9] \"Lower Peirce Reservoir\"  \"Macritchie Reservoir\"   \n[11] \"Mandai\"                  \"Marine Parade\"          \n[13] \"Nicoll Highway\"          \"Pasir Ris (Central)\"    \n[15] \"Punggol\"                 \"Queenstown\"             \n[17] \"Simei\"                   \"Somerset (Road)\"        \n[19] \"Tanjong Katong\"          \"Toa Payoh\"              \n[21] \"Tuas\"                    \"Ulu Pandan\"             \n[23] \"Upper Peirce Reservoir\"  \"Whampoa\"                \n\n\nFrom the above output, we know that these 24 weather stations have no temperature readings. We will put them into a list and create an operator to exclude them from the temp data using filter().\n\n\nShow the code\nstationstoremove &lt;- c(\"Botanic Garden\",\"Bukit Panjang\",\"Bukit Timah\",\"Choa Chu Kang (Central)\",\"Jurong Pier\",\"Kent Ridge\", \"Kranji Reservoir\", \"Lim Chu Kang\", \"Lower Peirce Reservoir\", \"Macritchie Reservoir\",\"Mandai\", \"Marine Parade\",\"Nicoll Highway\", \"Pasir Ris (Central)\", \"Punggol\", \"Queenstown\",\"Simei\", \"Somerset (Road)\",\"Tanjong Katong\", \"Toa Payoh\", \"Tuas\", \"Ulu Pandan\", \"Upper Peirce Reservoir\",\"Whampoa\")\n\n#create a operator to exclude things \n'%!in%' &lt;- function(x,y)!('%in%'(x,y))\n\n#excluded stations that have no temp data at all \ntemp_clean &lt;- temp %&gt;%\n  filter(station %!in% stationstoremove)\n\nglimpse(temp_clean)\n\n\nRows: 120,139\nColumns: 5\n$ station             &lt;chr&gt; \"Admiralty\", \"Admiralty\", \"Admiralty\", \"Admiralty\"…\n$ tdate               &lt;date&gt; 2009-01-01, 2009-01-02, 2009-01-03, 2009-01-04, 2…\n$ mean_temperature    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ maximum_temperature &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ minimum_temperature &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n\n\nShow the code\n#write it into RDS for future usage, esp when building \nwrite_rds(temp_clean, \"data/temp_clean.rds\")\n\n\n\nunique(temp_clean$station)\n\n [1] \"Admiralty\"             \"East Coast Parkway\"    \"Ang Mo Kio\"           \n [4] \"Newton\"                \"Tuas South\"            \"Pasir Panjang\"        \n [7] \"Jurong Island\"         \"Choa Chu Kang (South)\" \"Changi\"               \n[10] \"Tai Seng\"              \"Jurong (West)\"         \"Clementi\"             \n[13] \"Sentosa Island\"       \n\n\nWe will then pivot the temp_clean dataframe wider and plot the daily temperature for the remaining weather stations using plotly.\n\n\nShow the code\ntemp_mean_widec &lt;- temp_clean %&gt;%\n  select(tdate, station, mean_temperature) %&gt;%\n  pivot_wider(names_from = station, values_from = mean_temperature)\n\nglimpse(temp_mean_widec)\n\n\nRows: 16,071\nColumns: 14\n$ tdate                   &lt;date&gt; 2009-01-01, 2009-01-02, 2009-01-03, 2009-01-0…\n$ Admiralty               &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ `East Coast Parkway`    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ `Ang Mo Kio`            &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ Newton                  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ `Tuas South`            &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ `Pasir Panjang`         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ `Jurong Island`         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ `Choa Chu Kang (South)` &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ Changi                  &lt;dbl&gt; 26.6, 26.4, 26.5, 26.3, 27.0, 27.4, 27.1, 27.0…\n$ `Tai Seng`              &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ `Jurong (West)`         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ Clementi                &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ `Sentosa Island`        &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n\n\n\n\nShow the code\nplot_ly(data = temp_mean_widec, \n        x = ~tdate, \n        y = ~ Admiralty, \n        type = \"scatter\", \n        mode = \"lines+markers\") |&gt; \n  layout(title = \"Temperature observed by Weather Station\", \n       xaxis = list(title = \"Date\"), \n       yaxis = list(title = \"\", range = c(0,40)), \n      theme_ipsum_rc(plot_title_size = 13, plot_title_margin=4, subtitle_size=11, subtitle_margin=4,  \n                 axis_title_size = 8, axis_text_size=8, axis_title_face= \"bold\", plot_margin = margin(4, 4, 4, 4)),  \n       updatemenus = list(list(type = 'dropdown', \n                               xref = \"paper\", \n                               yref = \"paper\", \n                               xanchor = \"left\",\n                               x = 0.04,\n                               y = 0.95, \n                               buttons = list(\n                                 list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_widec$Admiralty)), \n                                                    list(yaxis = list(title = \"Temperature in Admiralty\", range = c(0,40)))),label = \"Admiralty\"),\n                                 list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_widec$`East Coast Parkway`)), \n                                                    list(yaxis = list(title = \"Temperature in East Coast Parkway\", range = c(0,40)))),label = \"East Coast Parkway\"), \n                                 list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_widec$`Ang Mo Kio`)), \n                                                    list(yaxis = list(title = \"Temperature in Ang Mo Kio\", range = c(0,40)))),label = \"Ang Mo Kio\"), \n                                 list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_widec$Newton)), \n                                                    list(yaxis = list(title = \"Temperature in Newton\", range = c(0,40)))),label = \"Newton\"), \n                                 list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_widec$`Tuas South`)), \n                                                    list(yaxis = list(title = \"Temperature in Tuas South\", range = c(0,40)))),label = \"Tuas South\"),\n                                  list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_widec$`Pasir Panjang`)), \n                                                    list(yaxis = list(title = \"Temperature in Pasir Panjang\", range = c(0,40)))),label = \"Pasir Panjang\"), \n                                  list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_widec$`Jurong Island`)), \n                                                    list(yaxis = list(title = \"Temperature in Jurong Island\", range = c(0,40)))),label = \"Jurong Island\"), \n                                 list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_widec$`Choa Chu Kang (South)`)), \n                                                    list(yaxis = list(title = \"Temperature in Choa Chu Kang\", range = c(0,40)))),label = \"Choa Chu Kang\"), \n                                 list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_widec$Changi)), \n                                                    list(yaxis = list(title = \"Temperature in Changi\", range = c(0,40)))),label = \"Changi\"),\n                                  list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_widec$`Tai Seng`)), \n                                                    list(yaxis = list(title = \"Temperature in Tai Seng\", range = c(0,40)))),label = \"Tai Seng\"),\n                                  list(method = \"update\",\n                                      args = list(list(y = list(temp_mean_widec$`Jurong (West)`)), \n                                                    list(yaxis = list(title = \"Temperature in Jurong West\", range = c(0,40)))),label = \"Jurong West\"), \n                                   list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_widec$Clementi)), \n                                                    list(yaxis = list(title = \"Temperature  in Clementi\", range = c(0,40)))),label = \"Clementi\"), \n                                   list(method = \"update\", \n                                        args = list(list(y = list(temp_mean_widec$`Sentosa Island`)), \n                                                    list(yaxis = list(title = \"Temperature  in Sentosa\", range = c(0,40)))),label = \"Sentosa\")\n                                   \n                               ))))  \n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nFrom the above interactive chart, we note that some stations have a longer time period with temperature readings (e.g. Changi). Almost all stations have some missing time gaps/ values in the data, hence we will need to do imputation for this missing values to ensure better accuracy of our temperature forecasting.\nAlso, we noted that daily temperature readings that range more than 20 years is too frequent for time series forecasting. Hence, we will aggregate the daily temperature readings to monthly temperature readings by calculating the mean in subsequent section.",
    "crumbs": [
      "Take-home Exercises",
      "Take-home Exercise 4: Prototyping Time Series Module for Shiny Application"
    ]
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#parameters-and-outputs-to-expose-on-shiny-application",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#parameters-and-outputs-to-expose-on-shiny-application",
    "title": "Take-home Exercise 4: Prototyping Time Series Module for Visual Analytics Shiny Application",
    "section": "5.1 Parameters and outputs to expose on Shiny Application",
    "text": "5.1 Parameters and outputs to expose on Shiny Application"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#how-to-expose-these-parameters-and-output",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#how-to-expose-these-parameters-and-output",
    "title": "Take-home Exercise 4: Prototyping Time Series Module for Visual Analytics Shiny Application",
    "section": "5.3 How to expose these parameters and output",
    "text": "5.3 How to expose these parameters and output\n\n\n\n\n\n\n\nParameter / Output\nHow to expose it\n\n\n\n\nType of data to forecast\nEach data (i.e. temperature or rainfall) is in a different page. Hence, user can choose which data to forecast by clicking on the respective page on the Navbar Pages.\n\n\nWeather Station\nA dropdown list to allow user to select which weather station they want to use for forecasting\n\n\nTemperature / rainfall data of chosen weather station\nAfter selecting the type of data to forecast and the weather station, we can display an interactive plot of the past data and show if there are any missing data. This can help the user choose which imputation method to use for the next step.\n\n\nMethod of Imputing Missing Data\nA dropdown list to allow user to select which missing data imputation method they want to use. The options are:\n\nMoving average (exponential)\nMoving average (linear)\nMoving average (simple)\nKalman Smoothing (ARIMA)\nKalman Smoothing (StrucTS)\n\n\n\nData table and plot of the output for the missing data\nAfter selecting the type of missing data imputation method, we can display an interactive plot and data table of the resultant dataframe. This can help them decide which imputation method is most suitable. For example in the earlier section, we realised that Kalman Smoothing (ARIMA) was not suitable for Admiralty weather station’s monthly temperature data after plotting the chart using the resultant dataframe.\n\n\nDecomposition of Time Series Data\nTo help the users choose the forecasting model, we can also display the output of the time series data’s decomposition\n\n\nForecasting Model to use\nA dropdown list to allow user to select which forecasting method they want to use. For temperature data, the forecasting methods are:\n\nState Space Model\nHolt Winters’ Additive Seasonality Model\nHolt Winters’ Multiplicative Model\nARIMA\n\nFor rainfall data, the forecasting methods are:\n\nNaive model\nARIMA\n\n\n\nLength of forecast\nA numeric input field to allow user key in the number of months to forecast\n\n\nData table, MAPE and plot of the forecasted results\nAfter the user has chosen the forecasting method and indicated the length of forecast, we should display the data table of forecasted results, the MAPE and plot ouf the forecasted results.",
    "crumbs": [
      "Take-home Exercises",
      "Take-home Exercise 4: Prototyping Time Series Module for Shiny Application"
    ]
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#layout-on-shiny-application",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#layout-on-shiny-application",
    "title": "Take-home Exercise 4: Prototyping Time Series Module for Visual Analytics Shiny Application",
    "section": "5.4 Layout on Shiny Application",
    "text": "5.4 Layout on Shiny Application\nBased on the above considerations, we plan to have the following layout for the forecasting module in our Shiny Application. Below is an example for the forecasting of temperature data.\n\n\n\nLayout of Forecasting Module\n\n\nAnother page with similar in layout would also be created for the forecasting of rainfall data. The sidebar would allow users to make their selections and the mainpanel will display the various plots and data table using tabsets. The MAE, RMSE and MAPE results for the chosen forecasting model will also be displayed at the bottom of the page.",
    "crumbs": [
      "Take-home Exercises",
      "Take-home Exercise 4: Prototyping Time Series Module for Shiny Application"
    ]
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#investigating-missing-values-1",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#investigating-missing-values-1",
    "title": "Take-home Exercise 4: Prototyping Time Series Module for Visual Analytics Shiny Application",
    "section": "4.1 Investigating missing values",
    "text": "4.1 Investigating missing values\nFirst, let us use summary() to check for missing data.\n\n\nShow the code\nsummary(rain)\n\n\n     tdate              station          daily_rainfall_total\n Min.   :1980-01-01   Length:329156      Min.   :  0.000     \n 1st Qu.:1997-04-29   Class :character   1st Qu.:  0.000     \n Median :2011-09-18   Mode  :character   Median :  0.200     \n Mean   :2007-07-02                      Mean   :  6.822     \n 3rd Qu.:2017-11-27                      3rd Qu.:  6.500     \n Max.   :2023-12-31                      Max.   :278.600     \n NA's   :58                              NA's   :5136        \n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nThe observations ranged from 1 Jan 1980 to 31 Dec 2023. There are 58 rows with missing dates. We should drop these rows since they are unable to tell us which day the observations were made (even if they have rainfall readings).\nThere are 5,136 rows of NAs for daily rainfall.\n\n\n\nFirst, let us drop those rows where date is missing because we would not be able to definitively identify when the temperature(s) were collected (even if there were temperature readings for these rows.\n\n\nShow the code\nrain &lt;- rain %&gt;%\n  drop_na(tdate)\n\nsummary(rain)\n\n\n     tdate              station          daily_rainfall_total\n Min.   :1980-01-01   Length:329098      Min.   :  0.000     \n 1st Qu.:1997-04-29   Class :character   1st Qu.:  0.000     \n Median :2011-09-18   Mode  :character   Median :  0.200     \n Mean   :2007-07-02                      Mean   :  6.822     \n 3rd Qu.:2017-11-27                      3rd Qu.:  6.500     \n Max.   :2023-12-31                      Max.   :278.600     \n                                         NA's   :5078        \n\n\nLet us also do a check of the weather stations.\n\n\nShow the code\nunique(rain$station)\n\n\n [1] \"Macritchie Reservoir\"    \"Lower Peirce Reservoir\" \n [3] \"Admiralty\"               \"East Coast Parkway\"     \n [5] \"Ang Mo Kio\"              \"Newton\"                 \n [7] \"Lim Chu Kang\"            \"Marine Parade\"          \n [9] \"Choa Chu Kang (Central)\" \"Tuas South\"             \n[11] \"Pasir Panjang\"           \"Jurong Island\"          \n[13] \"Nicoll Highway\"          \"Botanic Garden\"         \n[15] \"Choa Chu Kang (South)\"   \"Whampoa\"                \n[17] \"Changi\"                  \"Jurong Pier\"            \n[19] \"Ulu Pandan\"              \"Mandai\"                 \n[21] \"Tai Seng\"                \"Jurong (West)\"          \n[23] \"Clementi\"                \"Sentosa Island\"         \n[25] \"Bukit Panjang\"           \"Kranji Reservoir\"       \n[27] \"Upper Peirce Reservoir\"  \"Kent Ridge\"             \n[29] \"Queenstown\"              \"Tanjong Katong\"         \n[31] \"Somerset (Road)\"         \"Punggol\"                \n[33] \"Simei\"                   \"Toa Payoh\"              \n[35] \"Tuas\"                    \"Bukit Timah\"            \n[37] \"Pasir Ris (Central)\"",
    "crumbs": [
      "Take-home Exercises",
      "Take-home Exercise 4: Prototyping Time Series Module for Shiny Application"
    ]
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#creating-time-series-object-1",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#creating-time-series-object-1",
    "title": "Take-home Exercise 4: Prototyping Time Series Module for Visual Analytics Shiny Application",
    "section": "4.3 Creating Time Series Object",
    "text": "4.3 Creating Time Series Object\nAs mentioned earlier, for us to make use of the time series forecasting packages and their functions, we would need to convert the tibble dataframe into a time series object.\nBefore we create the time series object, let us first aggregate the daily rainfall readings to monthly rainfall readings by (1) creating the year-month column for each observation using floor_date() and specifying it to derive the year and month of each observation, and (2) aggregate the temperature readings by station and year_month then use summarise() to compute the monthly rainfall reading.\n\n\nShow the code\n#create year-month col\nrain$year_month &lt;- floor_date(rain$tdate, \"month\")\nglimpse(rain)\n\n\nRows: 329,098\nColumns: 4\n$ tdate                &lt;date&gt; 1980-01-01, 1980-01-02, 1980-01-03, 1980-01-04, …\n$ station              &lt;chr&gt; \"Macritchie Reservoir\", \"Macritchie Reservoir\", \"…\n$ daily_rainfall_total &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 22.6, 49.6, 2.4, 0.0, 0.0, 0.…\n$ year_month           &lt;date&gt; 1980-01-01, 1980-01-01, 1980-01-01, 1980-01-01, …\n\n\n\n\nShow the code\nmonthly_rain &lt;- rain %&gt;%                         \n  group_by(station, year_month) %&gt;% \n  summarise(total_rf = sum(daily_rainfall_total))\n\nglimpse(monthly_rain)\n\n\nRows: 10,813\nColumns: 3\nGroups: station [37]\n$ station    &lt;chr&gt; \"Admiralty\", \"Admiralty\", \"Admiralty\", \"Admiralty\", \"Admira…\n$ year_month &lt;date&gt; 2009-01-01, 2009-02-01, 2009-03-01, 2009-04-01, 2009-05-01…\n$ total_rf   &lt;dbl&gt; NA, 148.0, NA, 148.8, 205.6, 92.0, 103.0, 90.2, 67.6, 160.0…\n\n\nWith the monthly temperature of all weather stations, let us filter out one weather station (e.g. Admiralty) to create a tibble data frame adm_rf so that we can convert it into an xts object, which is a type of time series object.\n\nadm_rf&lt;- monthly_rain %&gt;%\n  filter(station == \"Admiralty\")\n\nsummary(adm_rf)\n\n   station            year_month            total_rf    \n Length:179         Min.   :2009-01-01   Min.   : 15.8  \n Class :character   1st Qu.:2012-09-16   1st Qu.:124.4  \n Mode  :character   Median :2016-07-01   Median :189.0  \n                    Mean   :2016-06-19   Mean   :204.9  \n                    3rd Qu.:2020-03-16   3rd Qu.:270.4  \n                    Max.   :2023-12-01   Max.   :517.4  \n                                         NA's   :18     \n\n\nWe will use xts() from xts package to create a time series object. The order.by parameter uses the dates from the adm_rf dataframe. We then use the ts_regular() function to give the time series object adm_rf_xts a regular interval by adding NA values for missing dates.\nJust in case there are missing months which we did not detected, we use the na.fill() function fills in those missing dates by extending values from previous days.\n\n\nShow the code\nadm_rf_xts &lt;- xts(adm_rf[,\"total_rf\"], order.by=as.Date(adm_rf$year_month))\nadm_rf_xts&lt;- ts_regular(adm_rf_xts)\nadm_rf_xts &lt;- na.fill(adm_rf_xts, \"extend\")\n\n\nLet us plot out the monthly rainfall of Admiralty weather station using ggplotly.\n\np3 &lt;- ggplot(adm_rf_xts, aes(x = Index, y = value)) + \n  geom_line() + theme_clean() +\n  labs(title = \"Monthly Rainfall of Admiralty Weather Station\", caption = \"Data from Weather.gov.sg\") +\n  xlab(\"Month-Year\") +\n  ylab(\"Rainfall (in mm)\") +\n  theme_ipsum_rc()\n\nggplotly(p3)\n\n\n\n\n\nFrom the above output, we see that there are missing temperatures for numerous time periods. As a result, the line for the above chart is not continuous.\nLet us investigate futher using imputeTS package’s ggplot_na_distribution, which highlights the missing values in our data.\n\nggplot_na_distribution(adm_rf_xts)\n\n\n\n\n\n\n\n\n\n4.3.1 Missing Value Imputation\nThere are several ways to impute missing data in time series objects.\n\n4.3.1.1 Moving Averages\nThis na_ma()function also allows us to use linear-weighted and exponentially-weighted moving averages.\n\nadmrf_imp_movingavg &lt;- na_ma(adm_rf_xts, weighting = \"exponential\") #default is exponential. Other options are \"simple\" and \"linear\". We can allow users to choose if the option they want. \n\n#plot chart \nggplot(admrf_imp_movingavg, aes(x = Index, y = value)) + \n  geom_line()\n\n\n\n\n\n\n\n\n\n\n4.3.1.2 Kalman Smoothing\nWe can also use Kalman Smoothing on ARIMA model to impute the missing values.\n\nadmrf_imp_kalman &lt;- na_kalman(adm_rf_xts, model = \"auto.arima\")\n\n#plot chart \n\nggplot(admrf_imp_kalman, aes(x = Index, y = value)) + \n  geom_line()\n\n\n\n\n\n\n\n\nKalman Smoothing also has a “StrucTS” option. Let us try and see how it works for our monthly rainfall data.\n\nadmrf_imp_kalmans &lt;- na_kalman(adm_rf_xts, model = \"StructTS\")\n\n#plot chart \n\nggplot(admrf_imp_kalmans, aes(x = Index, y = value)) + \n  geom_line()\n\n\n\n\n\n\n\n\nFor the subsequent sections, we will use the results from moving average to build our model.",
    "crumbs": [
      "Take-home Exercises",
      "Take-home Exercise 4: Prototyping Time Series Module for Shiny Application"
    ]
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#testing-if-the-time-series-is-stationary-1",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#testing-if-the-time-series-is-stationary-1",
    "title": "Take-home Exercise 4: Prototyping Time Series Module for Visual Analytics Shiny Application",
    "section": "4.4 Testing if the time series is stationary",
    "text": "4.4 Testing if the time series is stationary\nSimilar to the temperature data, we will also test for stationarity for our rainfall data.\n\n4.4.1 Augmented Dickey-Fuller Test\n\nNull Hypothesis: Series is non-stationary, or series has a unit root.\nAlternative Hypothesis: Series is stationary, or series has no unit root.\n\n\nadf.test(admrf_imp_movingavg$value)\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  admrf_imp_movingavg$value\nDickey-Fuller = -4.5358, Lag order = 5, p-value = 0.01\nalternative hypothesis: stationary\n\n\nSince the p-value is 0.01, which is less than critical value of 0.05, we reject the null hypothesis. This means that the time series does not have a unit root, meaning it is stationary. It does not have a time-dependent structure.\n\n\n4.4.2 Kwiatkowski-Phillips-Schmidt-Shin Test\n\nNull Hypothesis: Series is trend stationary or series has no unit root.\nAlternative Hypothesis: Series is non-stationary, or series has a unit root.\n\n\nkpss.test(admrf_imp_movingavg$value)\n\n\n    KPSS Test for Level Stationarity\n\ndata:  admrf_imp_movingavg$value\nKPSS Level = 0.20185, Truncation lag parameter = 4, p-value = 0.1\n\n\nSince the p-value is 0.1, which is greater than the critical value of 0.05, we fail to reject the null hypothesis of the KPSS test.This means we can assume that the time series is trend stationary.\nBoth ADF and KPSS tests conclude that the given series is stationary. This means that we can make use of most of the time series forecasting models such as Exponential Smoothing and ARIMA.",
    "crumbs": [
      "Take-home Exercises",
      "Take-home Exercise 4: Prototyping Time Series Module for Shiny Application"
    ]
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#decomposition-of-time-series-object-1",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#decomposition-of-time-series-object-1",
    "title": "Take-home Exercise 4: Prototyping Time Series Module for Visual Analytics Shiny Application",
    "section": "4.5 Decomposition of Time Series Object",
    "text": "4.5 Decomposition of Time Series Object\nFirst, let us plot the monthly rainfall of the Admiralty weather station.\n\np4 &lt;- ggplot(admrf_imp_movingavg, aes(x = Index, y = value)) + \n  geom_line() + \n  geom_smooth(method=lm) \n\nggplotly(p4)\n\n\n\n\n\n\nFrom the above output, it seems like there were fluctuations in monthly rainfall but there was no increasing trend. We also cannot see if there are any seasonality.\n\nLet us decompose the time series object and explore further.\n\nfit &lt;- stl(ts_ts(admrf_imp_movingavg), s.window= \"periodic\")\n\n## plot out the decomposition results \nautoplot(fit)+ \n  ggtitle(\"Decomposition for Monthly Rainfall\") +\n  xlab(\"Month-Year\") + \n  theme_clean()\n\n\n\n\n\n\n\n\n\nsummary(fit)\n\n Call:\n stl(x = ts_ts(admrf_imp_movingavg), s.window = \"periodic\")\n\n Time.series components:\n    seasonal             trend            remainder         \n Min.   :-57.77066   Min.   :136.8089   Min.   :-162.14714  \n 1st Qu.:-26.90668   1st Qu.:172.5815   1st Qu.: -58.59127  \n Median : -4.90670   Median :199.8634   Median :  -0.22377  \n Mean   :  0.00000   Mean   :203.0772   Mean   :  -0.11451  \n 3rd Qu.: 24.09846   3rd Qu.:236.0493   3rd Qu.:  51.80263  \n Max.   : 77.82946   Max.   :290.0645   Max.   : 271.94494  \n IQR:\n     STL.seasonal STL.trend STL.remainder data  \n      51.01        63.47    110.39        133.60\n   %  38.2         47.5      82.6         100.0 \n\n Weights: all == 1\n\n Other components: List of 5\n $ win  : Named num [1:3] 1801 19 13\n $ deg  : Named int [1:3] 0 1 1\n $ jump : Named num [1:3] 181 2 2\n $ inner: int 2\n $ outer: int 0\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nFrom the above output, we noted that there is no clear linear trend for the monthly rainfall over the years. However, we do note that the inital rainfall in the earlier time period were around 150mm and the later time period has a rainfall level of 200mm.\nWe also observed seasonality in the monthly rainfall over the years.",
    "crumbs": [
      "Take-home Exercises",
      "Take-home Exercise 4: Prototyping Time Series Module for Shiny Application"
    ]
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#mape-11",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#mape-11",
    "title": "Take-home Exercise 4: Prototyping Time Series Module for Visual Analytics Shiny Application",
    "section": "4.7 MAPE",
    "text": "4.7 MAPE\n\nMAPE(hw_modelm$mean, validationrf)*100\n\n[1] 63.30767"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#plot-of-forecasted-results-8",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#plot-of-forecasted-results-8",
    "title": "Take-home Exercise 4: Prototyping Time Series Module for Visual Analytics Shiny Application",
    "section": "4.8 Plot of Forecasted Results",
    "text": "4.8 Plot of Forecasted Results\n\nautoplot(hw_modelm) +\n  ggtitle(\"Holt-Winters (Multiplicative Seasonality) Forecasts for Monthly Total Rainfall\") +\n  xlab(\"Month-Year\") + \n  ylab(\"Amount of Rainfall (in mm)\") + \n  theme_ipsum_rc()\n\n\n\n\n\n\n\n\n:::\n\n4.8.1 ARIMA\n\nFitting the modelGetting the Forecasted ResultsMAPEPlotting the forecasted results\n\n\n\narima_optimal &lt;- auto.arima(trainingrf)\narima_optimal\n\nSeries: trainingrf \nARIMA(0,0,2) with non-zero mean \n\nCoefficients:\n         ma1     ma2      mean\n      0.2415  0.1251  196.2097\ns.e.  0.0809  0.0825    9.6921\n\nsigma^2 = 8037:  log likelihood = -921.24\nAIC=1850.48   AICc=1850.74   BIC=1862.68\n\n\n\n\n\narima_model &lt;- forecast(arima_optimal)\ndatatable(data.frame(arima_model))\n\n\n\n\n\n\n\n\nMAPE(arima_model$mean, validationrf)*100\n\n[1] 40.18849\n\n\n\n\n\nautoplot(arima_model) +\n  ggtitle(\"ARIMA Forecasts for Monthly Mean Temperature\") +\n  xlab(\"Month-Year\") + \n  ylab(\"Temperature (degree celsius)\") + \n  theme_ipsum_rc()\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.8.2 Comparison of Model Performance"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#parameters-to-expose-on-shiny-application",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#parameters-to-expose-on-shiny-application",
    "title": "Take-home Exercise 4: Prototyping Time Series Module for Visual Analytics Shiny Application",
    "section": "5.1 Parameters to expose on Shiny Application",
    "text": "5.1 Parameters to expose on Shiny Application\n\nType of data to forecast:\n\nTemperature or Rainfall\n\nLength of forecast:\n\nE.g. next 3 months or next 4 years (i.e. 48 months)\n\nWhich weather station\n\nOur data have many data stations, so users can choose which weather station they want\n\nMethod of imputing missing data\n\nChoose between Moving Average, Kalman Smoothing on ARIMA or Kalman Smoothing on StrucTS.\n\nForecasting model to use\n\nFor temperature data, we can consider allow users to choose from Holt-Winters, State Space and ARIMA models since we got very good data.\nFor rainfall data, if we choose to allow users to do forecasting on our Shiny application, we can consider to allow users to choose from Naive or ARIMA model.",
    "crumbs": [
      "Take-home Exercises",
      "Take-home Exercise 4: Prototyping Time Series Module for Shiny Application"
    ]
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#output-to-expose-on-shiny-application",
    "href": "Take-home_Ex/Take-home_Ex04/Take-home_Ex04a.html#output-to-expose-on-shiny-application",
    "title": "Take-home Exercise 4: Prototyping Time Series Module for Visual Analytics Shiny Application",
    "section": "5.2 Output to expose on Shiny Application",
    "text": "5.2 Output to expose on Shiny Application\n\nData table and plot of the output for missing data imputation chosen\nData table and plot of the output for the forecasting model chosen\nMAPE of the forecasting model chosen",
    "crumbs": [
      "Take-home Exercises",
      "Take-home Exercise 4: Prototyping Time Series Module for Shiny Application"
    ]
  }
]